MODEL_PATH=./qwen2.5-7b-instruct-q4_k_m-00001-of-00002-001.gguf
DB_DIR=./rag_system/db
LOG_DIR=./rag_system/logs
API_KEY=broadcast-tech-rag-2025
STREAMLIT_SERVER_PORT=8501

# CPU 최적화 (Intel i7-14700K - 14코어/28스레드)
N_THREADS=20
PARALLEL_WORKERS=10

# GPU 최적화 (RTX 4060 8GB)
N_GPU_LAYERS=-1
MAIN_GPU=0
F16_KV=true
LOW_VRAM=false

# 컨텍스트 및 배치 크기 (GPU 모드 - 답변 품질 최우선)
N_CTX=32768
N_BATCH=1024
MAX_DOCUMENTS_TO_PROCESS=20
MAX_PAGES_PER_PDF=50
BATCH_SIZE=10

# 캐싱
CACHE_MAX_SIZE=1000
RESPONSE_CACHE_TTL=7200

# 성능 모드
ENVIRONMENT=production
DEBUG_MODE=false

# ============================================================================
# 검색 설정
# ============================================================================
SEARCH_TOP_K=3
SEARCH_VECTOR_WEIGHT=0.2
SEARCH_BM25_WEIGHT=0.8

# ============================================================================
# LLM 설정 (llama-cpp-python)
# ============================================================================
LLM_BACKEND=llama_cpp
LLM_MODEL_PATH=/home/wnstn4647/AI-CHAT/models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf
LLM_N_CTX=4096
LLM_N_THREADS=28
LLM_N_GPU_LAYERS=32  # GPU 레이어 수 (권장 20~40, -1=전체, 0=CPU only)
LLM_N_BATCH=512
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2048
