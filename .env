MODEL_PATH=./models/ggml-model-Q4_K_M.gguf
DB_DIR=./rag_system/db
LOG_DIR=./rag_system/logs
API_KEY=broadcast-tech-rag-2025
STREAMLIT_SERVER_PORT=8501

# ============================================================================
# Chat Format 설정
# ============================================================================
# auto: GGUF 메타데이터의 tokenizer.chat_template 자동 사용 (권장)
# 강제 지정: llama-2, chatml, qwen, zephyr 등
CHAT_FORMAT=auto

# ============================================================================
# API 공개 URL (선택적 - WSL/외부 접속 환경에서 설정)
# ============================================================================
# PUBLIC_API_BASE=http://192.168.0.10:7860
# 설정하지 않으면 FastAPI가 Request 헤더에서 자동 추출

# CPU 최적화 (Intel i7-14700K - 14코어/28스레드)
N_THREADS=20
PARALLEL_WORKERS=10

# GPU 최적화 (RTX 4060 8GB)
N_GPU_LAYERS=-1
MAIN_GPU=0
F16_KV=true
LOW_VRAM=false

# 컨텍스트 및 배치 크기 (GPU 모드 - 답변 품질 최우선)
N_CTX=32768
N_BATCH=1024
MAX_DOCUMENTS_TO_PROCESS=20
MAX_PAGES_PER_PDF=50
BATCH_SIZE=10

# 캐싱
CACHE_MAX_SIZE=1000
RESPONSE_CACHE_TTL=7200

# 성능 모드
ENVIRONMENT=production
DEBUG_MODE=false

# ============================================================================
# 검색 설정 (운영 최적화 값)
# ============================================================================
SEARCH_TOP_K=5
# CRITICAL: BM25 위주 검색 (Vector는 OCR 텍스트에서 키워드 누락 문제로 부정확)
SEARCH_VECTOR_WEIGHT=0.01  # Vector 거의 비활성화
SEARCH_BM25_WEIGHT=0.99    # BM25 위주 (파일명 키워드로 검색)
DEFAULT_THRESHOLD=0.20

# ============================================================================
# V2 Retriever Settings (신규 2-layer RAG 아키텍처) - FORCED
# ============================================================================
USE_V2_RETRIEVER=true  # true: v2 (HybridRetrieverV2), false: v1 (legacy)
INDEX_VERSION=v2       # 인덱스 버전 강제 (v1 fallback 방지)

# V2 Index Paths (하드코딩 방지, .env 값만 사용)
BM25_INDEX_PATH=./indexes_v2/bm25/bm25.pkl
VECTOR_INDEX_PATH=./indexes_v2/faiss/faiss.index
METADATA_DB_PATH=./metadata.db

# Search Parameters
SEARCH_BM25_TOP_K=20   # BM25 검색 개수 (RRF 융합 전)
SEARCH_VEC_TOP_K=20    # Vector 검색 개수 (RRF 융합 전)
SEARCH_RRF_K=60        # RRF constant (기본 60, 높을수록 precision 증가)

# Metadata Fusion (메타데이터 검색 병합)
ENABLE_METADATA_FUSION=true   # 메타데이터 검색 활성화
METADATA_FUSION_WEIGHT=0.6    # 메타데이터 검색 가중치
HYBRID_FUSION_WEIGHT=0.4      # 하이브리드 검색 가중치
FUSION_TOP_K=5                # 최종 융합 결과 개수

# ============================================================================
# 진단 모드 (Diagnostic Mode)
# ============================================================================
DIAG_RAG=true              # RAG 파이프라인 진단 활성화 (true/false)
DIAG_LOG_LEVEL=INFO        # 진단 로그 레벨 (DEBUG/INFO/WARNING)

# ============================================================================
# LLM 설정 (llama-cpp-python)
# ============================================================================
LLM_ENABLED=true  # LLM 활성화 (true=켜짐, false=꺼짐)
LLM_BACKEND=llama_cpp
LLM_MODEL_PATH=/home/wnstn4647/AI-CHAT/models/ggml-model-Q4_K_M.gguf
QWEN_MODEL_PATH=/home/wnstn4647/AI-CHAT/models/ggml-model-Q4_K_M.gguf
LLM_N_CTX=4096
LLM_N_THREADS=28
LLM_N_GPU_LAYERS=32  # GPU 레이어 수 (권장 20~40, -1=전체, 0=CPU only)
LLM_N_BATCH=512
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2048
MAX_LLM_RETRY=1  # LLM 생성 재시도 횟수 (운영: 1회)

# ========================================
# RAG 시스템 안전 모드 설정 (2025-10-25)
# ========================================
USE_MULTILEVEL_FILTER=0
USE_RERANKER=0
USE_QUERY_EXPANSION=0
USE_DOCUMENT_COMPRESSION=0
MIN_TOP_K=3
DIAG_RAG=1

# ========================================
# 체감 품질 향상 파라미터 (2025-10-25 후속 조치)
# ========================================
DEFAULT_TOP_K=10            # 기본 검색 결과 개수
SNIPPET_MINLEN=200          # snippet 최소 길이 (200자)
SEARCH_BM25_WEIGHT=0.85     # BM25 가중치 (한글 키워드 중심)
SEARCH_VECTOR_WEIGHT=0.15   # Vector 가중치 (의미 보완)
