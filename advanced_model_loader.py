#!/usr/bin/env python3
"""
ê³ ê¸‰ ëª¨ë¸ ë¡œë”
Split GGUF íŒŒì¼ ì²˜ë¦¬ ë° ìë™ í´ë°±
"""

import os
import sys
from pathlib import Path
from typing import Optional, Dict, Any
import hashlib
import json
import time

class AdvancedModelLoader:
    """ê³ ê¸‰ ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.models_dir = Path("models")
        self.cache_file = Path(".model_cache.json")
        self.load_cache()

    def load_cache(self):
        """ëª¨ë¸ ìºì‹œ ë¡œë“œ"""
        if self.cache_file.exists():
            with open(self.cache_file, 'r') as f:
                self.cache = json.load(f)
        else:
            self.cache = {}

    def save_cache(self):
        """ëª¨ë¸ ìºì‹œ ì €ì¥"""
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f, indent=2)

    def find_available_models(self) -> Dict[str, Dict]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì°¾ê¸°"""
        models = {}

        # GGUF íŒŒì¼ ì°¾ê¸°
        gguf_files = list(self.models_dir.glob("*.gguf"))

        # Split íŒŒì¼ ê·¸ë£¹í™”
        split_groups = {}
        standalone_files = []

        for file in gguf_files:
            # Split íŒŒì¼ íŒ¨í„´ í™•ì¸
            if "-00001-of-" in file.name:
                base_name = file.name.split("-00001-of-")[0]
                total_parts = int(file.name.split("-of-")[1].split(".")[0])

                # ëª¨ë“  íŒŒíŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                all_parts = []
                for i in range(1, total_parts + 1):
                    part_file = self.models_dir / f"{base_name}-{i:05d}-of-{total_parts:05d}.gguf"
                    if part_file.exists():
                        all_parts.append(part_file)

                if len(all_parts) == total_parts:
                    split_groups[base_name] = {
                        "type": "split",
                        "files": all_parts,
                        "main_file": all_parts[0],
                        "total_size": sum(f.stat().st_size for f in all_parts) / (1024**3),
                        "parts": total_parts
                    }
            elif not any(pattern in file.name for pattern in ["-00002-of-", "-00003-of-"]):
                # Standalone íŒŒì¼
                standalone_files.append({
                    "type": "standalone",
                    "file": file,
                    "size": file.stat().st_size / (1024**3)
                })

        # ê²°ê³¼ ì •ë¦¬
        for name, info in split_groups.items():
            models[name] = info

        for info in standalone_files:
            name = info["file"].stem
            models[name] = info

        return models

    def test_model_loading(self, model_path: Path, quick_test: bool = True) -> bool:
        """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
        try:
            from llama_cpp import Llama

            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì„¤ì •
            test_config = {
                "n_ctx": 512 if quick_test else 2048,
                "n_batch": 64 if quick_test else 256,
                "n_gpu_layers": 0,  # CPUë¡œ í…ŒìŠ¤íŠ¸
                "verbose": False,
                "n_threads": 2
            }

            print(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¤‘: {model_path.name}")

            # ë¡œë”© ì‹œë„
            llm = Llama(
                model_path=str(model_path),
                **test_config
            )

            # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
            if not quick_test:
                response = llm("Test", max_tokens=5)
                if response and 'choices' in response:
                    print(f"âœ… ì‘ë‹µ ìƒì„± ì„±ê³µ")
                    return True
            else:
                # ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ë§Œ í™•ì¸
                return llm is not None

        except Exception as e:
            print(f"âŒ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

        return False

    def update_config_files(self, model_path: Path):
        """ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸"""
        updates = []

        # config.yaml ì—…ë°ì´íŠ¸
        config_yaml = Path("config.yaml")
        if config_yaml.exists():
            import yaml
            with open(config_yaml, 'r') as f:
                config = yaml.safe_load(f)

            config['models']['qwen']['path'] = str(model_path)
            config['models']['qwen']['model_file'] = model_path.name

            with open(config_yaml, 'w') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

            updates.append("config.yaml")

        # config.py ì—…ë°ì´íŠ¸
        config_py = Path("config.py")
        if config_py.exists():
            content = config_py.read_text()

            # ê¸°ì¡´ ê²½ë¡œ íŒ¨í„´ ì°¾ê¸°
            import re
            pattern = r'QWEN_MODEL_PATH\s*=\s*["\'].*?["\']'
            new_line = f'QWEN_MODEL_PATH = "{model_path}"'

            content = re.sub(pattern, new_line, content)
            config_py.write_text(content)

            updates.append("config.py")

        # config_manager.py ì—…ë°ì´íŠ¸
        config_manager = Path("config_manager.py")
        if config_manager.exists():
            content = config_manager.read_text()
            if "get_default_config" in content:
                # ê¸°ë³¸ ì„¤ì • ì—…ë°ì´íŠ¸
                content = content.replace(
                    '"path": "./models/qwen2.5-7b-instruct-q5_k_m.gguf"',
                    f'"path": "{model_path}"'
                )
                config_manager.write_text(content)
                updates.append("config_manager.py")

        if updates:
            print(f"âœ… ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸: {', '.join(updates)}")

        return updates

    def auto_select_model(self) -> Optional[Path]:
        """ìë™ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ"""
        models = self.find_available_models()

        if not models:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return None

        print(f"\nğŸ“¦ ë°œê²¬ëœ ëª¨ë¸: {len(models)}ê°œ")

        # ìš°ì„ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸
        priority_models = [
            "qwen2.5-7b-instruct-q4_k_m",
            "qwen2.5-7b-instruct-q5_k_m",
            "qwen2.5-3b-instruct",
            "qwen-7b"
        ]

        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì„ íƒ
        for priority_name in priority_models:
            for model_name, info in models.items():
                if priority_name in model_name:
                    if info["type"] == "split":
                        model_path = info["main_file"]
                        print(f"âœ… Split ëª¨ë¸ ì„ íƒ: {model_name} ({info['parts']}ê°œ íŒŒíŠ¸)")
                    else:
                        model_path = info["file"]
                        print(f"âœ… ë‹¨ì¼ ëª¨ë¸ ì„ íƒ: {model_name} ({info['size']:.2f}GB)")

                    # í…ŒìŠ¤íŠ¸
                    if self.test_model_loading(model_path, quick_test=True):
                        print(f"âœ… ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ: {model_path.name}")
                        return model_path
                    else:
                        print(f"âš ï¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ë‹¤ìŒ ëª¨ë¸ ì‹œë„...")

        # ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„ íƒ
        for model_name, info in models.items():
            if info["type"] == "split":
                model_path = info["main_file"]
            else:
                model_path = info["file"]

            if self.test_model_loading(model_path, quick_test=True):
                print(f"âœ… ëŒ€ì²´ ëª¨ë¸ ì„ íƒ: {model_path.name}")
                return model_path

        print("âŒ ë¡œë“œ ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None

    def fix_all_configs(self):
        """ëª¨ë“  ì„¤ì • ìë™ ìˆ˜ì •"""
        print("="*60)
        print("ëª¨ë¸ ë¡œë”© ìë™ ìˆ˜ì • ì‹œì‘")
        print("="*60)

        # 1. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì°¾ê¸°
        model_path = self.auto_select_model()

        if not model_path:
            return False

        # 2. ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
        self.update_config_files(model_path)

        # 3. ìºì‹œ ì €ì¥
        self.cache["last_working_model"] = str(model_path)
        self.cache["last_update"] = time.time()
        self.save_cache()

        print(f"\nâœ… ëª¨ë“  ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ì„ íƒëœ ëª¨ë¸: {model_path.name}")

        return True


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    loader = AdvancedModelLoader()

    # ìë™ ìˆ˜ì •
    if loader.fix_all_configs():
        print("\nğŸ‰ ì„±ê³µ! ì´ì œ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print("\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰:")
        print("  python3 -c \"from perfect_rag import PerfectRAG; rag = PerfectRAG(); print(rag.answer('í…ŒìŠ¤íŠ¸'))\"")

        return 0
    else:
        print("\nâŒ ìë™ ìˆ˜ì • ì‹¤íŒ¨")
        print("ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:")
        print("  https://huggingface.co/Qwen/")
        return 1


if __name__ == "__main__":
    sys.exit(main())