#!/usr/bin/env python3
"""
ê³ ê¸‰ ê²€ìƒ‰ ìµœì í™” ì‹œìŠ¤í…œ
========================
FAISS ê¸°ë°˜ ì´ˆê³ ì† ë²¡í„° ê²€ìƒ‰ ë° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìµœì í™”
"""

import numpy as np
import pickle
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import faiss
import hashlib
from sentence_transformers import SentenceTransformer
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
from collections import OrderedDict
import json

class FAISSSearchOptimizer:
    """FAISS ê¸°ë°˜ ì´ˆê³ ì† ë²¡í„° ê²€ìƒ‰"""

    def __init__(self, dimension: int = 768, index_type: str = "IVF"):
        self.dimension = dimension
        self.index_type = index_type
        self.index = None
        self.document_store = {}
        self.id_to_doc = {}
        self.embedding_model = None
        self.index_path = Path(".cache/faiss_index")
        self.index_path.mkdir(parents=True, exist_ok=True)

        # GPU ì§€ì› í™•ì¸
        self.use_gpu = faiss.get_num_gpus() > 0
        if self.use_gpu:
            print("ğŸ® GPU ê°€ì† í™œì„±í™”")

        self._initialize_index()
        self._load_embedding_model()

    def _initialize_index(self):
        """FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        if self.index_type == "IVF":
            # IVF (Inverted File) ì¸ë±ìŠ¤ - ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— íš¨ìœ¨ì 
            quantizer = faiss.IndexFlatL2(self.dimension)
            self.index = faiss.IndexIVFFlat(quantizer, self.dimension, 100)
        elif self.index_type == "HNSW":
            # HNSW (Hierarchical Navigable Small World) - ë†’ì€ ì •í™•ë„
            self.index = faiss.IndexHNSWFlat(self.dimension, 32)
        else:
            # ê¸°ë³¸: Flat ì¸ë±ìŠ¤
            self.index = faiss.IndexFlatL2(self.dimension)

        # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.use_gpu:
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)

    def _load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        try:
            self.embedding_model = SentenceTransformer(
                'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
            )
            print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except:
            # í´ë°±: ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”©
            print("âš ï¸  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, í•´ì‹œ ê¸°ë°˜ í´ë°±")
            self.embedding_model = None

    def add_documents(self, documents: List[Dict[str, Any]], batch_size: int = 100):
        """ë¬¸ì„œ ì¼ê´„ ì¶”ê°€"""
        print(f"ğŸ“š {len(documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘...")

        # ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]
            embeddings = self._batch_encode(batch)

            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            if self.index_type == "IVF" and not self.index.is_trained:
                # IVF ì¸ë±ìŠ¤ëŠ” í•™ìŠµ í•„ìš”
                self.index.train(embeddings)

            start_id = len(self.id_to_doc)
            ids = np.arange(start_id, start_id + len(batch))

            self.index.add_with_ids(embeddings, ids)

            # ë¬¸ì„œ ì €ì¥
            for j, doc in enumerate(batch):
                doc_id = start_id + j
                self.id_to_doc[doc_id] = doc
                self.document_store[doc.get('id', str(doc_id))] = doc

        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°")
        self._save_index()

    def _batch_encode(self, documents: List[Dict]) -> np.ndarray:
        """ë°°ì¹˜ ì¸ì½”ë”©"""
        texts = [doc.get('content', '') for doc in documents]

        if self.embedding_model:
            embeddings = self.embedding_model.encode(texts, batch_size=32, show_progress_bar=False)
        else:
            # í´ë°±: í•´ì‹œ ê¸°ë°˜ ì„ë² ë”©
            embeddings = []
            for text in texts:
                hash_obj = hashlib.sha256(text.encode()).digest()
                embedding = np.frombuffer(hash_obj * 24, dtype=np.float32)[:self.dimension]
                embeddings.append(embedding)
            embeddings = np.array(embeddings)

        return embeddings.astype('float32')

    def search(self, query: str, k: int = 10, threshold: float = 0.7) -> List[Dict]:
        """ì´ˆê³ ì† ë²¡í„° ê²€ìƒ‰"""
        if self.index.ntotal == 0:
            return []

        # ì¿¼ë¦¬ ì„ë² ë”©
        if self.embedding_model:
            query_embedding = self.embedding_model.encode([query])
        else:
            hash_obj = hashlib.sha256(query.encode()).digest()
            query_embedding = np.frombuffer(hash_obj * 24, dtype=np.float32)[:self.dimension].reshape(1, -1)

        query_embedding = query_embedding.astype('float32')

        # FAISS ê²€ìƒ‰
        distances, indices = self.index.search(query_embedding, k)

        results = []
        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
            if idx >= 0 and idx in self.id_to_doc:
                doc = self.id_to_doc[idx]
                score = 1 / (1 + dist)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜

                if score >= threshold:
                    results.append({
                        **doc,
                        'score': float(score),
                        'rank': i + 1
                    })

        return results

    def _save_index(self):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.index, str(self.index_path / "faiss.index"))

            # ë¬¸ì„œ ë§¤í•‘ ì €ì¥
            with open(self.index_path / "doc_mapping.pkl", 'wb') as f:
                pickle.dump({
                    'id_to_doc': self.id_to_doc,
                    'document_store': self.document_store
                }, f)

            print("ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_index(self) -> bool:
        """ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            index_file = self.index_path / "faiss.index"
            mapping_file = self.index_path / "doc_mapping.pkl"

            if index_file.exists() and mapping_file.exists():
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                self.index = faiss.read_index(str(index_file))

                # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
                if self.use_gpu:
                    res = faiss.StandardGpuResources()
                    self.index = faiss.index_cpu_to_gpu(res, 0, self.index)

                # ë¬¸ì„œ ë§¤í•‘ ë¡œë“œ
                with open(mapping_file, 'rb') as f:
                    data = pickle.load(f)
                    self.id_to_doc = data['id_to_doc']
                    self.document_store = data['document_store']

                print(f"âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°")
                return True
        except Exception as e:
            print(f"âš ï¸  ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

        return False


class HybridSearchOptimizer:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìµœì í™” (BM25 + FAISS)"""

    def __init__(self):
        self.faiss_search = FAISSSearchOptimizer()
        self.bm25_cache = OrderedDict(maxsize=1000)
        self.result_cache = OrderedDict(maxsize=500)
        self.cache_lock = threading.Lock()

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.search_times = []
        self.cache_hits = 0
        self.total_searches = 0

    def index_documents(self, documents: List[Dict]):
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        start_time = time.time()

        # ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=4) as executor:
            # FAISS ì¸ë±ì‹±
            future_faiss = executor.submit(self.faiss_search.add_documents, documents)

            # BM25 ì¤€ë¹„ (ì¶”í›„ êµ¬í˜„)
            # future_bm25 = executor.submit(self._prepare_bm25, documents)

            future_faiss.result()
            # future_bm25.result()

        index_time = time.time() - start_time
        print(f"âš¡ ì¸ë±ì‹± ì™„ë£Œ: {index_time:.2f}ì´ˆ")

    def search(self, query: str, mode: str = "hybrid", k: int = 10) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        self.total_searches += 1

        # ìºì‹œ í™•ì¸
        cache_key = f"{query}:{mode}:{k}"
        with self.cache_lock:
            if cache_key in self.result_cache:
                self.cache_hits += 1
                print(f"ğŸ’¨ ìºì‹œ íˆíŠ¸! ({self.cache_hits}/{self.total_searches})")
                return self.result_cache[cache_key]

        start_time = time.time()

        if mode == "vector":
            results = self.faiss_search.search(query, k)
        elif mode == "keyword":
            results = self._bm25_search(query, k)
        else:  # hybrid
            # ë²¡í„° ê²€ìƒ‰
            vector_results = self.faiss_search.search(query, k * 2)

            # í‚¤ì›Œë“œ ê²€ìƒ‰
            keyword_results = self._bm25_search(query, k * 2)

            # ê²°ê³¼ ë³‘í•©
            results = self._merge_results(vector_results, keyword_results, k)

        search_time = time.time() - start_time
        self.search_times.append(search_time)

        # ìºì‹œ ì €ì¥
        with self.cache_lock:
            self.result_cache[cache_key] = results
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.result_cache) > 500:
                self.result_cache.popitem(last=False)

        print(f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ: {search_time:.3f}ì´ˆ")
        return results

    def _bm25_search(self, query: str, k: int) -> List[Dict]:
        """BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ (í”Œë ˆì´ìŠ¤í™€ë”)"""
        # ì‹¤ì œ BM25 êµ¬í˜„ì€ ê¸°ì¡´ ì‹œìŠ¤í…œ í™œìš©
        return []

    def _merge_results(self, vector_results: List[Dict], keyword_results: List[Dict], k: int) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ ë³‘í•©"""
        # ì ìˆ˜ ê¸°ë°˜ ë³‘í•©
        all_results = {}

        # ë²¡í„° ê²°ê³¼ ì¶”ê°€ (ê°€ì¤‘ì¹˜ 0.6)
        for result in vector_results:
            doc_id = result.get('id', str(result))
            if doc_id not in all_results:
                all_results[doc_id] = result
                all_results[doc_id]['final_score'] = result.get('score', 0) * 0.6
            else:
                all_results[doc_id]['final_score'] += result.get('score', 0) * 0.6

        # í‚¤ì›Œë“œ ê²°ê³¼ ì¶”ê°€ (ê°€ì¤‘ì¹˜ 0.4)
        for result in keyword_results:
            doc_id = result.get('id', str(result))
            if doc_id not in all_results:
                all_results[doc_id] = result
                all_results[doc_id]['final_score'] = result.get('score', 0) * 0.4
            else:
                all_results[doc_id]['final_score'] += result.get('score', 0) * 0.4

        # ì •ë ¬ ë° ìƒìœ„ kê°œ ë°˜í™˜
        sorted_results = sorted(
            all_results.values(),
            key=lambda x: x['final_score'],
            reverse=True
        )

        return sorted_results[:k]

    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„"""
        if not self.search_times:
            return {}

        return {
            'avg_search_time': np.mean(self.search_times),
            'min_search_time': np.min(self.search_times),
            'max_search_time': np.max(self.search_times),
            'cache_hit_rate': (self.cache_hits / self.total_searches * 100) if self.total_searches > 0 else 0,
            'total_searches': self.total_searches,
            'index_size': self.faiss_search.index.ntotal if self.faiss_search.index else 0
        }


class SearchAccelerator:
    """ê²€ìƒ‰ ê°€ì†ê¸° - í”„ë¦¬í˜ì¹­ ë° ì˜ˆì¸¡"""

    def __init__(self, optimizer: HybridSearchOptimizer):
        self.optimizer = optimizer
        self.query_history = []
        self.prefetch_cache = {}
        self.prediction_model = None

    def predictive_search(self, partial_query: str) -> List[str]:
        """ì˜ˆì¸¡ ê²€ìƒ‰ - íƒ€ì´í•‘ ì¤‘ ìë™ì™„ì„±"""
        predictions = []

        # ì¿¼ë¦¬ íˆìŠ¤í† ë¦¬ì—ì„œ ìœ ì‚¬í•œ ì¿¼ë¦¬ ì°¾ê¸°
        for hist_query in self.query_history:
            if partial_query.lower() in hist_query.lower():
                predictions.append(hist_query)

        # ìì£¼ ì‚¬ìš©ë˜ëŠ” íŒ¨í„´
        common_patterns = [
            f"{partial_query} ë¬¸ì„œ",
            f"{partial_query} êµ¬ë§¤",
            f"{partial_query} ë…„ë„",
            f"{partial_query} ì¥ë¹„",
            f"2024ë…„ {partial_query}",
            f"{partial_query} í˜„í™©"
        ]

        predictions.extend(common_patterns)
        return predictions[:5]  # ìƒìœ„ 5ê°œ

    def prefetch_results(self, query: str):
        """ê²°ê³¼ í”„ë¦¬í˜ì¹­ - ë°±ê·¸ë¼ìš´ë“œ ë¡œë”©"""
        # ì˜ˆìƒ ì¿¼ë¦¬ ìƒì„±
        related_queries = [
            query,
            f"{query} ìƒì„¸",
            f"{query} ìš”ì•½",
            f"{query} ìµœê·¼"
        ]

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í”„ë¦¬í˜ì¹˜
        def _prefetch():
            for q in related_queries:
                if q not in self.prefetch_cache:
                    results = self.optimizer.search(q, k=5)
                    self.prefetch_cache[q] = results

        thread = threading.Thread(target=_prefetch)
        thread.daemon = True
        thread.start()

    def get_instant_results(self, query: str) -> Optional[List[Dict]]:
        """ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜ (í”„ë¦¬í˜ì¹˜ëœ ê²½ìš°)"""
        return self.prefetch_cache.get(query)


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
search_optimizer = HybridSearchOptimizer()
search_accelerator = SearchAccelerator(search_optimizer)


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    print("ğŸš€ ê³ ê¸‰ ê²€ìƒ‰ ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")

    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒì„±
    test_docs = [
        {'id': f'doc_{i}', 'content': f'í…ŒìŠ¤íŠ¸ ë¬¸ì„œ {i}ë²ˆì…ë‹ˆë‹¤. ë‚´ìš©: {i*100}'}
        for i in range(100)
    ]

    # ì¸ë±ì‹±
    search_optimizer.index_documents(test_docs)

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    results = search_optimizer.search("í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 50", k=5)
    print(f"\nê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    for result in results[:3]:
        print(f"  - {result.get('id')}: {result.get('final_score', result.get('score', 0)):.3f}")

    # ì„±ëŠ¥ í†µê³„
    stats = search_optimizer.get_performance_stats()
    print(f"\nğŸ“Š ì„±ëŠ¥ í†µê³„:")
    print(json.dumps(stats, indent=2, ensure_ascii=False))

    # ì˜ˆì¸¡ ê²€ìƒ‰
    predictions = search_accelerator.predictive_search("í…ŒìŠ¤íŠ¸")
    print(f"\nğŸ”® ì˜ˆì¸¡ ê²€ìƒ‰:")
    for pred in predictions:
        print(f"  - {pred}")