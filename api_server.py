#!/usr/bin/env python3
"""
FastAPI ê¸°ë°˜ ê³ ì„±ëŠ¥ API ì„œë²„
==============================

ì„¸ê³„ ìµœê³  ê°œë°œìê°€ ì„¤ê³„í•œ ì™„ë²½í•œ REST API
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import asyncio
import uvicorn
import time
import json
from datetime import datetime
from functools import lru_cache
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="AI-CHAT RAG API",
    description="ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ RAG ì‹œìŠ¤í…œ API",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== ë°ì´í„° ëª¨ë¸ ====================

class SearchRequest(BaseModel):
    """ê²€ìƒ‰ ìš”ì²­ ëª¨ë¸"""
    query: str = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬")
    mode: str = Field("document", description="ê²€ìƒ‰ ëª¨ë“œ (document/asset)")
    top_k: int = Field(5, ge=1, le=20, description="ë°˜í™˜í•  ê²°ê³¼ ìˆ˜")
    use_cache: bool = Field(True, description="ìºì‹œ ì‚¬ìš© ì—¬ë¶€")
    stream: bool = Field(False, description="ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—¬ë¶€")


class SearchResponse(BaseModel):
    """ê²€ìƒ‰ ì‘ë‹µ ëª¨ë¸"""
    query: str
    response: str
    sources: List[Dict[str, Any]]
    search_time: float
    cached: bool
    timestamp: datetime


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ì²´í¬ ì‘ë‹µ"""
    status: str
    version: str
    uptime: float
    memory_usage_gb: float
    gpu_available: bool


class MetricsResponse(BaseModel):
    """ë©”íŠ¸ë¦­ ì‘ë‹µ"""
    total_requests: int
    cache_hits: int
    cache_hit_rate: float
    avg_response_time: float
    active_connections: int


# ==================== ì „ì—­ ìƒíƒœ ====================

class APIState:
    """API ì„œë²„ ìƒíƒœ ê´€ë¦¬"""

    def __init__(self):
        self.rag_instance = None
        self.start_time = time.time()
        self.total_requests = 0
        self.cache_hits = 0
        self.response_times = []
        self.active_connections = 0
        self.initialized = False

    async def initialize(self):
        """RAG ì‹œìŠ¤í…œ ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        if not self.initialized:
            logger.info("Initializing RAG system...")
            # Lazy import
            from perfect_rag import PerfectRAG
            self.rag_instance = PerfectRAG()
            self.initialized = True
            logger.info("RAG system initialized successfully")

    def get_metrics(self) -> MetricsResponse:
        """ë©”íŠ¸ë¦­ ë°˜í™˜"""
        hit_rate = (self.cache_hits / max(1, self.total_requests)) * 100
        avg_time = sum(self.response_times) / max(1, len(self.response_times))

        return MetricsResponse(
            total_requests=self.total_requests,
            cache_hits=self.cache_hits,
            cache_hit_rate=hit_rate,
            avg_response_time=avg_time,
            active_connections=self.active_connections
        )


# ì „ì—­ ìƒíƒœ ì¸ìŠ¤í„´ìŠ¤
state = APIState()


# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸"""
    logger.info("ğŸš€ API Server starting...")
    await state.initialize()
    logger.info("âœ… API Server ready!")


@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì´ë²¤íŠ¸"""
    logger.info("Shutting down API Server...")


@app.get("/", response_model=dict)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "AI-CHAT RAG API",
        "version": "2.0.0",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    import psutil
    import torch

    memory = psutil.virtual_memory()
    uptime = time.time() - state.start_time

    return HealthResponse(
        status="healthy" if state.initialized else "initializing",
        version="2.0.0",
        uptime=uptime,
        memory_usage_gb=memory.used / (1024**3),
        gpu_available=torch.cuda.is_available()
    )


@app.post("/search", response_model=SearchResponse)
async def search(request: SearchRequest):
    """ê²€ìƒ‰ API"""

    # ì´ˆê¸°í™” í™•ì¸
    if not state.initialized:
        await state.initialize()

    state.total_requests += 1
    state.active_connections += 1

    try:
        start_time = time.time()

        # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            state.rag_instance.search_and_generate,
            request.query,
            request.mode,
            request.top_k,
            request.use_cache
        )

        search_time = time.time() - start_time
        state.response_times.append(search_time)

        # ìºì‹œ íˆíŠ¸ í™•ì¸
        cached = search_time < 0.5

        if cached:
            state.cache_hits += 1

        # ì‘ë‹µ ìƒì„±
        result = SearchResponse(
            query=request.query,
            response=response,
            sources=[],  # TODO: ì‹¤ì œ ì†ŒìŠ¤ ì¶”ê°€
            search_time=search_time,
            cached=cached,
            timestamp=datetime.now()
        )

        return result

    except Exception as e:
        logger.error(f"Search error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

    finally:
        state.active_connections -= 1


@app.post("/search/stream")
async def search_stream(request: SearchRequest):
    """ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ API"""

    async def generate():
        """ìŠ¤íŠ¸ë¦¬ë° ì œë„ˆë ˆì´í„°"""
        # ì´ˆê¸° ì‘ë‹µ
        yield json.dumps({"status": "searching"}) + "\n"

        # ê²€ìƒ‰ ìˆ˜í–‰
        response = await search(request)

        # ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
        chunks = response.response.split(". ")
        for i, chunk in enumerate(chunks):
            data = {
                "chunk": chunk + ".",
                "progress": (i + 1) / len(chunks),
                "done": i == len(chunks) - 1
            }
            yield json.dumps(data) + "\n"
            await asyncio.sleep(0.1)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼

    return StreamingResponse(
        generate(),
        media_type="application/x-ndjson"
    )


@app.get("/metrics", response_model=MetricsResponse)
async def get_metrics():
    """ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    return state.get_metrics()


@app.post("/cache/clear")
async def clear_cache():
    """ìºì‹œ ì´ˆê¸°í™”"""
    if state.rag_instance:
        state.rag_instance.clear_cache()
        state.cache_hits = 0
        return {"message": "Cache cleared successfully"}
    return {"message": "System not initialized"}


@app.get("/documents")
async def list_documents(limit: int = 10, offset: int = 0):
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    if not state.rag_instance:
        return {"documents": [], "total": 0}

    # ë©”íƒ€ë°ì´í„° ìºì‹œì—ì„œ ë¬¸ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    docs = list(state.rag_instance.metadata_cache.items())
    total = len(docs)

    # í˜ì´ì§•
    docs = docs[offset:offset + limit]

    return {
        "documents": [
            {
                "id": doc_id,
                "filename": meta.get('filename'),
                "year": meta.get('year'),
                "category": meta.get('category'),
                "size": meta.get('file_size')
            }
            for doc_id, meta in docs
        ],
        "total": total,
        "limit": limit,
        "offset": offset
    }


@app.post("/index/rebuild")
async def rebuild_index(background_tasks: BackgroundTasks):
    """ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (ë°±ê·¸ë¼ìš´ë“œ)"""

    def rebuild():
        logger.info("Starting index rebuild...")
        if state.rag_instance:
            state.rag_instance._build_initial_index()
        logger.info("Index rebuild completed")

    background_tasks.add_task(rebuild)
    return {"message": "Index rebuild started in background"}


# ==================== WebSocket ì§€ì› (ì„ íƒì‚¬í•­) ====================

from fastapi import WebSocket, WebSocketDisconnect


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket ì—”ë“œí¬ì¸íŠ¸ (ì‹¤ì‹œê°„ ê²€ìƒ‰)"""
    await websocket.accept()
    logger.info("WebSocket connection established")

    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹ 
            data = await websocket.receive_text()
            request = json.loads(data)

            # ê²€ìƒ‰ ìˆ˜í–‰
            search_req = SearchRequest(**request)
            response = await search(search_req)

            # ì‘ë‹µ ì „ì†¡
            await websocket.send_json(response.dict())

    except WebSocketDisconnect:
        logger.info("WebSocket connection closed")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        await websocket.close()


# ==================== ì‹¤í–‰ ====================

def run_server(host="0.0.0.0", port=8000, reload=False):
    """API ì„œë²„ ì‹¤í–‰"""
    logger.info(f"Starting API server on {host}:{port}")

    uvicorn.run(
        "api_server:app",
        host=host,
        port=port,
        reload=reload,
        log_level="info",
        access_log=True
    )


if __name__ == "__main__":
    print("="*60)
    print("ğŸš€ AI-CHAT RAG API Server")
    print("ì„¸ê³„ ìµœê³  ê°œë°œìê°€ ë§Œë“  ê³ ì„±ëŠ¥ API")
    print("="*60)
    print()
    print("ğŸ“Œ Endpoints:")
    print("   â€¢ Docs: http://localhost:8000/docs")
    print("   â€¢ Health: http://localhost:8000/health")
    print("   â€¢ Search: POST http://localhost:8000/search")
    print("   â€¢ Metrics: http://localhost:8000/metrics")
    print()
    print("="*60)

    run_server()