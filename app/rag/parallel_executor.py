#!/usr/bin/env python3
"""Parallel Execution Utilities v2.0 for RAG System

2025-11-11 v2.0 ê°œì„ ì‚¬í•­:
- ì •ë°€ íƒ€ì´ë°: perf_counter_ns() + ì˜ˆì™¸ ì•ˆì „
- íƒ€ì„ì•„ì›ƒ/ì·¨ì†Œ: ì „ì²´+ê°œë³„ íƒ€ì„ì•„ì›ƒ, ì·¨ì†Œ ì‹œê·¸ë„ ì „íŒŒ
- ì—ëŸ¬ ê´€ì¸¡ì„±: _errors ë§µ ë°˜í™˜
- í‚¤ ê¸°ë°˜ êµì§‘í•©: ê°’ ë¹„êµë¡œ ì •í™•ë„ í–¥ìƒ
- ê²½ìŸ ì‹¤í–‰: í”„ë¼ì´ë¨¸ë¦¬/í´ë°± ë™ì‹œ ì‹¤í–‰ í›„ ìŠ¹ì ì±„íƒ
- ì‹±ê¸€í„´ ê´€ë¦¬: ì¬êµ¬ì„± API + atexit ë“±ë¡
"""
import atexit
import logging
import threading
import time
from concurrent.futures import (
    FIRST_COMPLETED,
    ThreadPoolExecutor,
    as_completed,
    wait,
)
from concurrent.futures import (
    TimeoutError as FuturesTimeout,
)
from functools import wraps
from typing import Any, Callable, Dict, Hashable, List, Optional

logger = logging.getLogger(__name__)


def timed_execution(func_name: str, level: int = logging.DEBUG):
    """Decorator to log execution time with exception safety

    Args:
        func_name: í•¨ìˆ˜ ì´ë¦„ (ë¡œê·¸ìš©)
        level: ë¡œê·¸ ë ˆë²¨ (ê¸°ë³¸ DEBUG)

    Returns:
        Decorated function
    """

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            t0 = time.perf_counter_ns()
            try:
                return func(*args, **kwargs)
            finally:
                dt_ms = (time.perf_counter_ns() - t0) / 1_000_000
                logger.log(level, f"â± {func_name} took {dt_ms:.3f} ms")

        return wrapper

    return decorator


class ParallelSearchExecutor:
    """Execute multiple search operations in parallel v2.0"""

    def __init__(self, max_workers: int = 6):
        """Initialize parallel executor

        Args:
            max_workers: Maximum number of parallel threads (default 6)
        """
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

    def execute_searches(
        self,
        search_tasks: List[Dict[str, Any]],
        per_task_timeout: Optional[float] = None,
        total_timeout: Optional[float] = None,
    ) -> Dict[str, Any]:
        """Execute multiple search tasks in parallel with timeout support

        Args:
            search_tasks: List of search task dictionaries:
                [
                    {
                        "name": "bm25_search",
                        "func": callable,
                        "args": tuple,
                        "kwargs": dict
                    },
                    ...
                ]
            per_task_timeout: ê°œë³„ íƒœìŠ¤í¬ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            total_timeout: ì „ì²´ ë°°ì¹˜ íƒ€ì„ì•„ì›ƒ (ì´ˆ)

        Returns:
            Dict mapping task names to results:
            {
                "bm25_search": [...],
                "vector_search": [...],
                "_errors": {"task_name": "error_msg", ...}
            }
        """
        if not search_tasks:
            return {}

        results: Dict[str, Any] = {}
        errors: Dict[str, str] = {}
        futures = {}
        t0 = time.perf_counter_ns()

        # Submit all tasks
        for task in search_tasks:
            name = task["name"]
            func = task["func"]
            args = task.get("args", ())
            kwargs = task.get("kwargs", {})

            future = self.executor.submit(func, *args, **kwargs)
            futures[future] = name
            logger.debug(f"ğŸš€ Submitted: {name}")

        done = set()
        try:
            while len(done) < len(futures):
                # ì „ì²´ íƒ€ì„ë°•ìŠ¤ ì²´í¬
                if total_timeout is not None:
                    elapsed_sec = (time.perf_counter_ns() - t0) / 1_000_000_000
                    remaining = max(0.0, total_timeout - elapsed_sec)
                    if remaining == 0.0:
                        raise FuturesTimeout("total timeout exceeded")
                else:
                    remaining = None

                # ì™„ë£Œ ëŒ€ê¸°
                pending_futures = [f for f in futures if f not in done]
                for fut in as_completed(pending_futures, timeout=remaining):
                    name = futures[fut]
                    try:
                        res = fut.result(timeout=per_task_timeout)
                        results[name] = res
                        logger.debug(f"âœ… Done: {name}")
                    except Exception as e:
                        errors[name] = repr(e)
                        logger.error(f"âŒ Failed: {name} - {e}")
                        results[name] = []
                    finally:
                        done.add(fut)

        except FuturesTimeout:
            # ë‚¨ì€ ì‘ì—… ì·¨ì†Œ ì‹œë„
            for fut in futures:
                if fut not in done:
                    fut.cancel()
                    name = futures[fut]
                    errors[name] = "Timeout"
                    results[name] = []
            logger.warning("â³ Parallel batch timed out")

        total_ms = (time.perf_counter_ns() - t0) / 1_000_000
        logger.info(f"ğŸ All tasks finished in {total_ms:.3f} ms")

        # ì—ëŸ¬ ë§µ ì¶”ê°€
        results["_errors"] = errors
        return results

    def execute_filters(
        self,
        items: List[Any],
        filter_funcs: List[Callable[[Any], bool]],
        key: Optional[Callable[[Any], Hashable]] = None,
    ) -> List[Any]:
        """Apply multiple filter functions in parallel with key-based intersection

        Args:
            items: List of items to filter
            filter_funcs: List of filter functions (each returns bool)
            key: Key function for intersection (default: identity)

        Returns:
            List of items that passed ALL filters
        """
        if not filter_funcs or not items:
            return items

        # í‚¤ í•¨ìˆ˜ ê¸°ë³¸ê°’: identity
        key_fn = key or (lambda x: x)

        # Execute filters in parallel
        futures = {}
        for i, filter_func in enumerate(filter_funcs):

            def apply_filter(func, data, keyfn):
                return [keyfn(it) for it in data if func(it)]

            future = self.executor.submit(apply_filter, filter_func, items, key_fn)
            futures[future] = f"filter_{i}"

        # Collect filtered results
        result_sets: List[set] = []
        for future in as_completed(futures):
            try:
                result = future.result()
                result_sets.append(set(result))
            except Exception as e:
                name = futures[future]
                logger.error(f"âŒ Filter failed: {name} - {e}")

        # Intersection of all filter results
        if not result_sets:
            return items

        intersection = set.intersection(*result_sets)
        return [it for it in items if key_fn(it) in intersection]

    def shutdown(self):
        """Shutdown the thread pool executor"""
        self.executor.shutdown(wait=True)
        logger.info("ğŸ”„ Parallel executor shutdown")


# Global executor instance with lock
_global_executor: Optional[ParallelSearchExecutor] = None
_global_executor_lock = threading.Lock()


def get_parallel_executor(max_workers: int = 6) -> ParallelSearchExecutor:
    """Get or create global parallel executor instance (thread-safe singleton)

    Args:
        max_workers: Maximum number of parallel threads (default 6)

    Returns:
        ParallelSearchExecutor instance
    """
    global _global_executor
    with _global_executor_lock:
        if _global_executor is None:
            _global_executor = ParallelSearchExecutor(max_workers=max_workers)
            atexit.register(lambda: _global_executor.shutdown())
            logger.info(
                f"âœ… Global parallel executor initialized (max_workers={max_workers})"
            )
    return _global_executor


def reconfigure_parallel_executor(max_workers: int) -> None:
    """Reconfigure global executor with new max_workers

    Args:
        max_workers: New maximum number of parallel threads
    """
    global _global_executor
    with _global_executor_lock:
        if _global_executor is not None:
            _global_executor.shutdown()
        _global_executor = ParallelSearchExecutor(max_workers=max_workers)
        atexit.register(lambda: _global_executor.shutdown())
        logger.info(f"ğŸ”§ Reconfigured executor (max_workers={max_workers})")


def parallel_search_with_fallback(
    primary_search: Callable,
    fallback_search: Optional[Callable] = None,
    timeout: float = 5.0,
) -> Any:
    """Execute search with race execution: primary + fallback simultaneously

    í”„ë¼ì´ë¨¸ë¦¬ì™€ í´ë°±ì„ ë™ì‹œ ì‹¤í–‰í•˜ê³ , ë¨¼ì € ì™„ë£Œëœ ê²°ê³¼ë¥¼ ì±„íƒ.
    ìŠ¹ì ì™¸ ë‚˜ë¨¸ì§€ëŠ” ì·¨ì†Œ ì‹œë„.

    Args:
        primary_search: Primary search function
        fallback_search: Fallback search function (optional)
        timeout: Timeout in seconds

    Returns:
        Search results from first-completed (primary or fallback)
    """
    executor = get_parallel_executor()
    futures = [executor.executor.submit(primary_search)]
    labels = ["primary"]

    if fallback_search:
        futures.append(executor.executor.submit(fallback_search))
        labels.append("fallback")

    # ê²½ìŸ ì‹¤í–‰: ë¨¼ì € ëë‚œ ê²ƒ ì±„íƒ
    done, pending = wait(futures, timeout=timeout, return_when=FIRST_COMPLETED)

    if not done:
        # íƒ€ì„ì•„ì›ƒ: ëª¨ë“  ì‘ì—… ì·¨ì†Œ
        for f in futures:
            f.cancel()
        logger.warning("â³ Both searches timed out")
        return []

    # ìŠ¹ì ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    winner = next(iter(done))
    winner_label = labels[futures.index(winner)]

    try:
        result = winner.result()
        logger.debug(f"ğŸ† Winner: {winner_label}")
    except Exception as e:
        logger.warning(f"âš ï¸ Winner ({winner_label}) failed: {e}")
        # ìŠ¹ìê°€ ì‹¤íŒ¨í–ˆìœ¼ë©´, ëŒ€ê¸° ì¤‘ì¸ ì‘ì—… ê²°ê³¼ ì‹œë„
        for p in pending:
            try:
                fallback_result = p.result(timeout=timeout)
                logger.info("ğŸ”„ Fallback succeeded after primary failure")
                return fallback_result or []
            except Exception as e2:
                logger.error(f"âŒ Fallback also failed: {e2}")
                return []
        return []

    # íŒ¨ì ì·¨ì†Œ ì‹œë„
    for p in pending:
        cancelled = p.cancel()
        if cancelled:
            logger.debug("ğŸš« Cancelled pending search")

    return result or []
