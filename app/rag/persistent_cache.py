#!/usr/bin/env python3
"""Persistent Query Cache v2.0 using SQLite

2025-11-11 v2.0 ê°œì„ ì‚¬í•­:
- WAL ëª¨ë“œ: ë©€í‹°ìŠ¤ë ˆë“œ ë™ì‹œì„± ëŒ€í­ í–¥ìƒ
- UPSERT: í†µê³„/íƒ€ì„ìŠ¤íƒ¬í”„ ë³´ì¡´ (INSERT OR REPLACE ëŒ€ì‹ )
- JSON+zlib ì§ë ¬í™”: pickle ë³´ì•ˆ ì·¨ì•½ì  ì œê±°
- ìŠ¬ë¼ì´ë”© TTL: ìì£¼ ì‚¬ìš©ë˜ëŠ” ìºì‹œ ìœ ì§€
- í¬ê¸° ê¸°ë°˜ LRU: ë””ìŠ¤í¬ í’€ë¦¼ ë°©ì§€
- í™•ë¥ ì  ì •ë¦¬: ëŒ€ëŸ‰ íŠ¸ë˜í”½ì—ì„œ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
- ì—°ê²° í—¬í¼: PRAGMA ì¼ê´„ ì ìš©
"""
import json
import logging
import os
import random
import sqlite3
import time
import zlib
from pathlib import Path
from typing import Any, Callable, Dict, Optional

from app.rag.smart_cache_key import generate_smart_cache_key

logger = logging.getLogger(__name__)


# ---- ì•ˆì „ ì§ë ¬í™” (JSON + zlib, pickle ë¯¸ì‚¬ìš©) --------------------------------


def _dumps(obj: Any, compress: bool = True) -> bytes:
    """JSON ì§ë ¬í™” + ì„ íƒì  ì••ì¶•

    Args:
        obj: ì§ë ¬í™”í•  ê°ì²´
        compress: zlib ì••ì¶• ì—¬ë¶€

    Returns:
        ì§ë ¬í™”ëœ ë°”ì´íŠ¸
    """
    data = json.dumps(
        {"v": 1, "payload": obj}, ensure_ascii=False, separators=(",", ":")
    ).encode("utf-8")
    return zlib.compress(data, level=6) if compress else data


def _loads(data: bytes, compressed: bool = True) -> Any:
    """JSON ì—­ì§ë ¬í™” + ì„ íƒì  ì••ì¶• í•´ì œ

    Args:
        data: ì§ë ¬í™”ëœ ë°”ì´íŠ¸
        compressed: zlib ì••ì¶• ì—¬ë¶€

    Returns:
        ì—­ì§ë ¬í™”ëœ ê°ì²´

    Raises:
        ValueError: ìŠ¤í‚¤ë§ˆ ë²„ì „ ë¶ˆì¼ì¹˜
    """
    raw = zlib.decompress(data) if compressed else data
    obj = json.loads(raw.decode("utf-8"))
    if obj.get("v") != 1:
        raise ValueError("Unsupported schema version")
    return obj["payload"]


# ---- ë©”ì¸ ìºì‹œ í´ë˜ìŠ¤ ---------------------------------------------------------


class PersistentCache:
    """SQLite-based persistent cache v2.0

    - WAL ì €ë„ ëª¨ë“œ
    - UPSERTë¡œ í†µê³„ ë³´ì¡´
    - ìŠ¬ë¼ì´ë”©/ì ˆëŒ€ TTL
    - í¬ê¸° ì œí•œ + LRU ì¶•ì¶œ
    """

    def __init__(
        self,
        db_path: str = "var/cache/query_cache.db",
        ttl: int = 7200,
        ttl_mode: str = "sliding",  # "absolute" | "sliding"
        max_db_mb: int = 256,  # íŒŒì¼ ìƒí•œ (MB)
        cleanup_prob: float = 0.01,  # set() í˜¸ì¶œ ì‹œ í™•ë¥ ì  ì •ë¦¬
        compress: bool = True,
        key_func: Optional[Callable[[str, Optional[str]], str]] = None,
    ):
        """Initialize persistent cache

        Args:
            db_path: SQLite DB íŒŒì¼ ê²½ë¡œ
            ttl: TTL (ì´ˆ, ê¸°ë³¸ 2ì‹œê°„)
            ttl_mode: "absolute" (ìƒì„± ê¸°ì¤€) ë˜ëŠ” "sliding" (ë§ˆì§€ë§‰ ì ‘ê·¼ ê¸°ì¤€)
            max_db_mb: DB íŒŒì¼ ìµœëŒ€ í¬ê¸° (MB)
            cleanup_prob: set() í˜¸ì¶œ ì‹œ ì •ë¦¬ í™•ë¥  (0.0~1.0)
            compress: zlib ì••ì¶• ì—¬ë¶€
            key_func: ì»¤ìŠ¤í…€ í‚¤ ìƒì„± í•¨ìˆ˜ (ê¸°ë³¸: generate_smart_cache_key)
        """
        self.db_path = db_path
        self.ttl = ttl
        self.ttl_mode = ttl_mode
        self.max_db_mb = max_db_mb
        self.cleanup_prob = cleanup_prob
        self.compress = compress
        self._generate_key = key_func or (
            lambda q, m=None: generate_smart_cache_key(q, m)
        )

        # ë””ë ‰í† ë¦¬ ìƒì„±
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)

        # ì´ˆê¸°í™”
        self._init_db()
        self._cleanup_expired()
        self._enforce_size_limit()

    # ---- SQLite í—¬í¼ ----------------------------------------------------------

    def _connect(self) -> sqlite3.Connection:
        """SQLite ì—°ê²° ìƒì„± (PRAGMA ì ìš©)

        Returns:
            ì„¤ì •ëœ SQLite ì—°ê²°
        """
        conn = sqlite3.connect(self.db_path, timeout=5.0, isolation_level=None)
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA synchronous=NORMAL;")
        conn.execute("PRAGMA temp_store=MEMORY;")
        conn.execute("PRAGMA mmap_size=268435456;")  # 256MB
        conn.execute("PRAGMA busy_timeout=5000;")
        return conn

    def _init_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""
        with self._connect() as conn:
            cur = conn.cursor()
            cur.execute("""
                CREATE TABLE IF NOT EXISTS query_cache (
                    cache_key   TEXT PRIMARY KEY,
                    query       TEXT NOT NULL,
                    result_data BLOB NOT NULL,
                    created_at  REAL NOT NULL,
                    accessed_at REAL NOT NULL,
                    access_count INTEGER NOT NULL DEFAULT 1,
                    compressed  INTEGER NOT NULL DEFAULT 1
                )
            """)
            # ì¸ë±ìŠ¤
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_accessed_at ON query_cache(accessed_at)"
            )
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_created_at ON query_cache(created_at)"
            )
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_access_count ON query_cache(access_count)"
            )
            logger.info(f"Persistent cache v2.0 initialized: {self.db_path}")

    # ---- í•µì‹¬ API --------------------------------------------------------------

    def get(self, query: str, mode: str | None = None) -> Optional[Any]:
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            mode: ê²€ìƒ‰ ëª¨ë“œ

        Returns:
            ìºì‹œëœ ê²°ê³¼ (ì—†ê±°ë‚˜ ë§Œë£Œ ì‹œ None)
        """
        key = self._generate_key(query, mode)
        now = time.time()

        with self._connect() as conn:
            cur = conn.cursor()
            cur.execute(
                """
                SELECT result_data, created_at, accessed_at, access_count, compressed
                FROM query_cache WHERE cache_key=?
            """,
                (key,),
            )
            row = cur.fetchone()

            if not row:
                logger.debug(f"Persistent Cache MISS for query: {query[:50]}...")
                return None

            result_data, created_at, accessed_at, access_count, compressed = row

            # TTL ì²´í¬
            expired_ref = (
                created_at if self.ttl_mode == "absolute" else max(created_at, accessed_at)
            )
            if now - expired_ref > self.ttl:
                cur.execute("DELETE FROM query_cache WHERE cache_key=?", (key,))
                logger.debug(f"Cache expired (deleted): {query[:50]}...")
                return None

            # ì ‘ê·¼ ì‹œê°„/ì¹´ìš´íŠ¸ ê°±ì‹ 
            cur.execute(
                """
                UPDATE query_cache
                SET accessed_at=?, access_count=access_count+1
                WHERE cache_key=?
            """,
                (now, key),
            )

        # ì—­ì§ë ¬í™”
        try:
            result = _loads(result_data, compressed=bool(compressed))
        except Exception as e:
            logger.warning(f"Deserialization failed, evicting key={key}: {e}")
            with self._connect() as conn:
                conn.execute("DELETE FROM query_cache WHERE cache_key=?", (key,))
            return None

        logger.info(
            f"âœ… Persistent Cache HIT: {query[:50]}... (accessed {access_count + 1} times)"
        )
        return result

    def set(self, query: str, result: Any, mode: str | None = None):
        """ê²°ê³¼ ìºì‹±

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            result: ìºì‹œí•  ê²°ê³¼
            mode: ê²€ìƒ‰ ëª¨ë“œ
        """
        key = self._generate_key(query, mode)
        now = time.time()
        blob = _dumps(result, compress=self.compress)

        with self._connect() as conn:
            cur = conn.cursor()
            # UPSERT: created_at ìœ ì§€, ë‚˜ë¨¸ì§€ ê°±ì‹ 
            cur.execute(
                """
                INSERT INTO query_cache (cache_key, query, result_data, created_at, accessed_at, access_count, compressed)
                VALUES (?, ?, ?, ?, ?, 1, ?)
                ON CONFLICT(cache_key) DO UPDATE SET
                    query=excluded.query,
                    result_data=excluded.result_data,
                    accessed_at=excluded.accessed_at,
                    access_count=query_cache.access_count+1,
                    compressed=excluded.compressed,
                    created_at=CASE
                        WHEN query_cache.created_at IS NULL THEN excluded.created_at
                        ELSE query_cache.created_at
                    END
            """,
                (key, query, blob, now, now, 1 if self.compress else 0),
            )

        # í™•ë¥ ì  ì •ë¦¬
        if random.random() < self.cleanup_prob:
            self._cleanup_expired()
            self._enforce_size_limit()

        logger.info(f"ğŸ’¾ Persistent cache set: {query[:50]}...")

    # ---- ìœ ì§€ë³´ìˆ˜ --------------------------------------------------------------

    def clear(self):
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        with self._connect() as conn:
            conn.execute("DELETE FROM query_cache")
        logger.info("Persistent cache cleared")

    def _cleanup_expired(self):
        """ë§Œë£Œ í•­ëª© ì œê±°"""
        now = time.time()
        ref_col = "created_at" if self.ttl_mode == "absolute" else "MAX(created_at, accessed_at)"

        with self._connect() as conn:
            cur = conn.cursor()
            cur.execute(
                f"DELETE FROM query_cache WHERE (? - {ref_col}) > ?", (now, self.ttl)
            )
            deleted = cur.rowcount or 0

        if deleted > 0:
            logger.info(f"Cleaned up {deleted} expired cache entries")

    def _db_file_size_mb(self) -> float:
        """DB íŒŒì¼ í¬ê¸° (MB)

        Returns:
            íŒŒì¼ í¬ê¸° (MB)
        """
        if not os.path.exists(self.db_path):
            return 0.0
        return os.path.getsize(self.db_path) / (1024 * 1024)

    def _enforce_size_limit(self):
        """í¬ê¸° ì œí•œ ê°•ì œ (LRU ì¶•ì¶œ)"""
        size = self._db_file_size_mb()
        if size <= self.max_db_mb:
            return

        # 10% ì—¬ìœ  í™•ë³´ë¥¼ ìœ„í•´ LRU ì¶•ì¶œ

        with self._connect() as conn:
            cur = conn.cursor()
            # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±°
            cur.execute(
                """
                SELECT cache_key FROM query_cache
                ORDER BY accessed_at ASC LIMIT 1000
            """
            )
            keys = [k for (k,) in cur.fetchall()]
            if keys:
                cur.executemany(
                    "DELETE FROM query_cache WHERE cache_key=?", [(k,) for k in keys]
                )
                logger.info(
                    f"Size limit exceeded ({size:.1f}MB). Evicted {len(keys)} entries"
                )

    # ---- í†µê³„ & ìœ í‹¸ë¦¬í‹° -------------------------------------------------------

    def invalidate(self, prefix: str):
        """í”„ë¦¬í”½ìŠ¤ë¡œ ìºì‹œ ë¬´íš¨í™”

        Args:
            prefix: ìºì‹œ í‚¤ ì ‘ë‘ì‚¬
        """
        with self._connect() as conn:
            cur = conn.cursor()
            cur.execute("DELETE FROM query_cache WHERE cache_key LIKE ?", (f"{prefix}%",))
            deleted = cur.rowcount or 0
        logger.info(f"Invalidated {deleted} keys with prefix={prefix}")

    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„

        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        with self._connect() as conn:
            cur = conn.cursor()

            # ì „ì²´ ì¹´ìš´íŠ¸/ì ‘ê·¼ ìˆ˜
            cur.execute(
                "SELECT COUNT(*), COALESCE(SUM(access_count),0) FROM query_cache"
            )
            total_count, total_accesses = cur.fetchone()

            # ìƒìœ„ ì¿¼ë¦¬
            cur.execute(
                """
                SELECT query, access_count FROM query_cache
                ORDER BY access_count DESC LIMIT 5
            """
            )
            top = [{"query": q[:80], "count": c} for q, c in cur.fetchall()]

        return {
            "total_entries": total_count or 0,
            "total_accesses": total_accesses or 0,
            "avg_accesses": (total_accesses / total_count) if total_count else 0,
            "top_queries": top,
            "db_size_mb": self._db_file_size_mb(),
            "ttl_mode": self.ttl_mode,
            "ttl_seconds": self.ttl,
        }


# ---- ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í—¬í¼ í•¨ìˆ˜ -----------------------------------------------

_persistent_cache_instance: Optional[PersistentCache] = None


def get_persistent_cache() -> PersistentCache:
    """ì „ì—­ persistent cache ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í„´)

    Returns:
        PersistentCache ì¸ìŠ¤í„´ìŠ¤
    """
    global _persistent_cache_instance
    if _persistent_cache_instance is None:
        _persistent_cache_instance = PersistentCache(
            db_path="var/cache/query_cache.db",
            ttl=7200,  # 2 hours
            ttl_mode="sliding",  # ìŠ¬ë¼ì´ë”© TTL ê¶Œì¥
            max_db_mb=256,
            cleanup_prob=0.01,
            compress=True,
        )
    return _persistent_cache_instance


def cache_query_result_persistent(query: str, result: Any, mode: str | None = None):
    """Helper function to cache a query result persistently

    Args:
        query: ê²€ìƒ‰ ì§ˆì˜
        result: ìºì‹œí•  ê²°ê³¼
        mode: ê²€ìƒ‰ ëª¨ë“œ
    """
    cache = get_persistent_cache()
    cache.set(query, result, mode)


def get_cached_result_persistent(query: str, mode: str | None = None) -> Optional[Any]:
    """Helper function to get persistently cached result

    Args:
        query: ê²€ìƒ‰ ì§ˆì˜
        mode: ê²€ìƒ‰ ëª¨ë“œ

    Returns:
        ìºì‹œëœ ê²°ê³¼ (ì—†ìœ¼ë©´ None)
    """
    cache = get_persistent_cache()
    return cache.get(query, mode)


def get_persistent_cache_stats() -> Dict[str, Any]:
    """Get persistent cache statistics

    Returns:
        í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    cache = get_persistent_cache()
    return cache.get_stats()
