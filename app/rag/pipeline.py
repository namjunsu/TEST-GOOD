"""RAG íŒŒì´í”„ë¼ì¸ (íŒŒì‚¬ë“œ íŒ¨í„´)

ë‹¨ì¼ ì§„ì…ì : RAGPipeline.query()
ë‚´ë¶€ íë¦„: ê²€ìƒ‰ â†’ ì••ì¶• â†’ LLM ìƒì„±

Example:
    >>> pipeline = RAGPipeline()
    >>> response = pipeline.query("ì§ˆë¬¸", top_k=5)
    >>> print(response.answer)
"""

import os
import time
from dataclasses import dataclass, field
from typing import Protocol, List, Optional, Dict, Any

from app.core.logging import get_logger
from app.core.errors import ModelError, SearchError, ErrorCode, ERROR_MESSAGES
from app.rag.query_router import QueryRouter, QueryMode

logger = get_logger(__name__)

# ì§„ë‹¨ ëª¨ë“œ ì„¤ì •
DIAG_RAG = os.getenv("DIAG_RAG", "false").lower() == "true"
DIAG_LOG_LEVEL = os.getenv("DIAG_LOG_LEVEL", "INFO").upper()


# ============================================================================
# Request / Response ë°ì´í„° í´ë˜ìŠ¤
# ============================================================================


@dataclass
class RAGRequest:
    """RAG ìš”ì²­ íŒŒë¼ë¯¸í„°

    Attributes:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
        compression_ratio: ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ë¹„ìœ¨ (0.0~1.0)
        use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
        temperature: LLM ìƒì„± ì˜¨ë„
    """

    query: str
    top_k: int = 5
    compression_ratio: float = 0.7
    use_hyde: bool = False
    temperature: float = 0.1


@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ê²°ê³¼

    Attributes:
        answer: ìƒì„±ëœ ë‹µë³€
        source_docs: ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í•˜ìœ„ í˜¸í™˜)
        evidence_chunks: Evidenceìš© ì •ê·œí™” ì²­í¬ (ê¶Œì¥)
        raw_results: ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ (Evidence ìµœì†Œ ë³´ì¥ìš©)
        latency: ì „ì²´ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
        success: ì„±ê³µ ì—¬ë¶€
        error: ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
        metrics: ë‚´ë¶€ ì§€í‘œ (ê²€ìƒ‰/ì••ì¶•/ìƒì„± ì‹œê°„ ë“±)
        diagnostics: ì§„ë‹¨ ì •ë³´ (DIAG_RAG=trueì¼ ë•Œë§Œ ì±„ì›Œì§)
    """

    answer: str
    source_docs: List[str] = field(default_factory=list)
    evidence_chunks: List[Dict[str, Any]] = field(default_factory=list)
    raw_results: List[Dict[str, Any]] = field(default_factory=list)
    latency: float = 0.0
    success: bool = True
    error: Optional[str] = None
    metrics: dict = field(default_factory=dict)
    diagnostics: dict = field(default_factory=dict)  # ì§„ë‹¨ ì •ë³´


# ============================================================================
# í”„ë¡œí† ì½œ ì •ì˜ (ì˜ì¡´ì„± ì—­ì „)
# ============================================================================


class Retriever(Protocol):
    """ê²€ìƒ‰ ì—”ì§„ ì¸í„°í˜ì´ìŠ¤"""

    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ìˆ˜í–‰ (ì •ê·œí™” ìŠ¤í‚¤ë§ˆ ë°˜í™˜)

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼

        Returns:
            [
                {
                    "doc_id": str,
                    "page": int,
                    "score": float,
                    "snippet": str,
                    "meta": dict
                }, ...
            ]
        """
        ...


class Compressor(Protocol):
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶•ê¸° ì¸í„°í˜ì´ìŠ¤"""

    def compress(
        self, chunks: List[Dict[str, Any]], ratio: float
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ì••ì¶•

        Args:
            chunks: ì›ë³¸ ì²­í¬ ëª©ë¡ (ì •ê·œí™”ëœ dict)
            ratio: ì••ì¶• ë¹„ìœ¨

        Returns:
            ì••ì¶•ëœ ì²­í¬ ëª©ë¡ (ë™ì¼ ìŠ¤í‚¤ë§ˆ)
        """
        ...


class Generator(Protocol):
    """LLM ìƒì„±ê¸° ì¸í„°í˜ì´ìŠ¤"""

    def generate(self, query: str, context: str, temperature: float) -> str:
        """ë‹µë³€ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì°¸ê³  ë¬¸ì„œ
            temperature: ìƒì„± ì˜¨ë„

        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        ...


# ============================================================================
# RAG íŒŒì´í”„ë¼ì¸ (íŒŒì‚¬ë“œ)
# ============================================================================


class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸ íŒŒì‚¬ë“œ

    ê²€ìƒ‰ â†’ ì••ì¶• â†’ ìƒì„±ì„ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ ì œê³µ.
    ë‚´ë¶€ êµ¬í˜„ì€ Retriever/Compressor/Generatorì— ìœ„ì„.

    Example:
        >>> pipeline = RAGPipeline()
        >>> response = pipeline.query("ì§ˆë¬¸", top_k=5)
        >>> if response.success:
        ...     print(response.answer)
        ...     print(f"ì°¸ê³ : {response.source_docs}")
    """

    def __init__(
        self,
        retriever: Optional[Retriever] = None,
        compressor: Optional[Compressor] = None,
        generator: Optional[Generator] = None,
    ):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            retriever: ê²€ìƒ‰ ì—”ì§„ (Noneì´ë©´ ê¸°ë³¸ HybridRetriever ì‚¬ìš©)
            compressor: ì••ì¶•ê¸° (Noneì´ë©´ ê¸°ë³¸ ContextCompressor ì‚¬ìš©)
            generator: LLM ìƒì„±ê¸° (Noneì´ë©´ ê¸°ë³¸ LlamaCppGenerator ì‚¬ìš©)
        """
        self.retriever = retriever or self._create_default_retriever()
        self.compressor = compressor or self._create_default_compressor()
        self.generator = generator or self._create_default_generator()
        self.query_router = QueryRouter()  # ğŸ¯ ëª¨ë“œ ë¼ìš°í„° ì´ˆê¸°í™”

        logger.info("RAG Pipeline initialized")

    def query(
        self,
        query: str,
        top_k: int = 5,
        compression_ratio: float = 0.7,
        use_hyde: bool = False,
        temperature: float = 0.1,
    ) -> RAGResponse:
        """RAG ì§ˆì˜ (ë‹¨ì¼ ì§„ì…ì )

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            compression_ratio: ì••ì¶• ë¹„ìœ¨
            use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
            temperature: LLM ìƒì„± ì˜¨ë„

        Returns:
            RAGResponse: ë‹µë³€ + ë©”íƒ€ë°ì´í„°
        """

        # ì…ë ¥ ê²€ì¦
        if not query or not query.strip():
            return RAGResponse(
                answer="",
                success=False,
                error="ë¹ˆ ì§ˆë¬¸ì…ë‹ˆë‹¤",
            )

        start_time = time.perf_counter()
        metrics = {}
        diagnostics = {}  # ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘

        try:
            # 1. ê²€ìƒ‰: ì •ê·œí™”ëœ ì²­í¬(dict) ë¦¬ìŠ¤íŠ¸ ê¸°ëŒ€
            search_start = time.perf_counter()
            results = self.retriever.search(query, top_k)
            metrics["search_time"] = time.perf_counter() - search_start

            # [DIAG] ê²€ìƒ‰ ê²°ê³¼ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["retrieved_k"] = len(results)
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(f"[DIAG] ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")

            if not results:
                logger.warning(f"No results found for query: {query[:50]}")
                if DIAG_RAG:
                    diagnostics["mode"] = "no_results"
                    diagnostics["generate_path"] = "fallback_no_context"
                return RAGResponse(
                    answer="ê´€ë ¨ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ë‹¤.",
                    success=True,
                    latency=time.perf_counter() - start_time,
                    metrics=metrics,
                    diagnostics=diagnostics,
                )

            # 2. ì••ì¶•: ì²­í¬ ë‹¨ìœ„ ìœ ì§€(í˜ì´ì§€/ìŠ¤ë‹ˆí«/ë©”íƒ€ ë³´ì¡´)
            compress_start = time.perf_counter()
            compressed = self.compressor.compress(results, compression_ratio)
            metrics["compress_time"] = time.perf_counter() - compress_start

            # [DIAG] ì••ì¶• í›„ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["after_compress_k"] = len(compressed)
                diagnostics["compression_ratio"] = compression_ratio
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(
                        f"[DIAG] ì••ì¶• ì™„ë£Œ: {len(results)} â†’ {len(compressed)}ê°œ ë¬¸ì„œ"
                    )

            # 3. ìƒì„±: ì»¨í…ìŠ¤íŠ¸ëŠ” ìŠ¤ë‹ˆí« ì§‘í•©ìœ¼ë¡œ êµ¬ì„±
            gen_start = time.perf_counter()

            # CRITICAL: Inject compressed chunks into generator for proper LLM context
            if hasattr(self.generator, "compressed_chunks"):
                self.generator.compressed_chunks = compressed
                logger.debug(
                    f"Injected {len(compressed)} compressed chunks into generator"
                )

            context = "\n\n".join([c.get("snippet", "") for c in compressed])

            # [DIAG] ìƒì„± ì „ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ·
            if DIAG_RAG and DIAG_LOG_LEVEL == "DEBUG":
                for i, c in enumerate(compressed[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ë¡œê·¸
                    logger.debug(
                        f"[DIAG] Context[{i}]: doc_id={c.get('doc_id')}, "
                        f"filename={c.get('filename', 'N/A')}, "
                        f"page={c.get('page', 0)}, "
                        f"snippet={c.get('snippet', '')[:120]}..."
                    )

            answer = self.generator.generate(query, context, temperature)
            metrics["generate_time"] = time.perf_counter() - gen_start

            # [DIAG] ìƒì„± ì™„ë£Œ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["mode"] = "normal"
                diagnostics["generate_path"] = "from_context"
                diagnostics["used_k"] = len(compressed)
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(
                        f"[DIAG] ìƒì„± ì™„ë£Œ: from_context ê²½ë¡œ, {len(compressed)}ê°œ ë¬¸ì„œ ì‚¬ìš©"
                    )

            total_latency = time.perf_counter() - start_time
            metrics["total_time"] = total_latency

            logger.info(
                f"RAG query completed in {total_latency:.2f}s "
                f"(search={metrics['search_time']:.2f}s, "
                f"compress={metrics['compress_time']:.2f}s, "
                f"generate={metrics['generate_time']:.2f}s)"
            )

            return RAGResponse(
                answer=answer,
                source_docs=[c.get("doc_id") for c in results[:3]],
                evidence_chunks=compressed,  # UIìš© ê·¼ê±°
                raw_results=results,  # Evidence ìµœì†Œ ë³´ì¥ìš©
                latency=total_latency,
                success=True,
                metrics=metrics,
                diagnostics=diagnostics,
            )

        except SearchError as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_RETRIEVE] ê²€ìƒ‰ ì‹¤íŒ¨: {e.message}",
                latency=time.perf_counter() - start_time,
            )

        except ModelError as e:
            logger.error(f"Model inference failed: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_GENERATE] ìƒì„± ì‹¤íŒ¨: {e.message}",
                latency=time.perf_counter() - start_time,
            )

        except Exception as e:
            logger.error(f"Unexpected error: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_UNKNOWN] {str(e)}",
                latency=time.perf_counter() - start_time,
            )

    def _make_response(
        self, text: str, selected: List[Dict[str, Any]], retrieved: List[Dict[str, Any]]
    ) -> dict:
        """í‘œì¤€ ì‘ë‹µ êµ¬ì¡° ìƒì„± (citations í¬í•¨)

        Args:
            text: ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
            selected: ì‹¤ì œ ì‚¬ìš©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ì••ì¶• í›„)
            retrieved: ê²€ìƒ‰ëœ ì›ë³¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            í‘œì¤€í™”ëœ ì‘ë‹µ dict (citations í•„ìˆ˜)
        """
        citations = [
            {
                "doc_id": c.get("doc_id"),
                "title": c.get("title") or c.get("filename") or c.get("doc_id"),
                "page": c.get("page", 1),
                "snippet": (
                    c.get("text") or c.get("snippet") or c.get("content") or ""
                )[:300],
                "preview_url": c.get("preview_url"),
                "download_url": c.get("download_url"),
            }
            for c in selected
        ]

        return {
            "text": text,
            "citations": citations,  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
            "evidence": citations,  # í•˜ìœ„ í˜¸í™˜ì„± (ë™ì¼ ë°ì´í„°)
            "status": {
                "retrieved_count": len(retrieved),
                "selected_count": len(selected),
                "found": len(selected) > 0,  # ğŸ”´ ìœ ì¼í•œ íŒì • ê¸°ì¤€
            },
        }

    def answer(self, query: str, top_k: Optional[int] = None) -> dict:
        """ë‹µë³€ ìƒì„± (Evidence í¬í•¨ êµ¬ì¡°í™”ëœ ì‘ë‹µ)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ (Noneì´ë©´ ê¸°ë³¸ê°’ 5)

        Returns:
            dict: {
                "text": ë‹µë³€ í…ìŠ¤íŠ¸,
                "citations": ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í‘œì¤€ í‚¤),
                "evidence": ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í•˜ìœ„ í˜¸í™˜),
                "status": {
                    "retrieved_count": int,
                    "selected_count": int,
                    "found": bool
                }
            }
        """
        # ğŸ”¥ CRITICAL: ê¸°ì•ˆì/ë‚ ì§œ ê²€ìƒ‰ì€ QuickFixRAGì— ìœ„ì„ (ì „ë¬¸ ë¡œì§ ë³´ìœ )
        if hasattr(self.generator, "rag"):
            import re
            import sqlite3

            # âœ… í™•ì¥ëœ ì¿¼ë¦¬ì—ì„œ ì‹¤ì œ ì§ˆë¬¸ ì¶”ì¶œ (chat_interface.py ëŒ€ì‘)
            actual_query = query
            if "í˜„ì¬ ì§ˆë¬¸:" in query:
                parts = query.split("í˜„ì¬ ì§ˆë¬¸:")
                if len(parts) > 1:
                    actual_query = parts[-1].strip()
                    logger.info(f"ğŸ“ í™•ì¥ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œ: '{actual_query[:50]}'")

            # ğŸ¯ ëª¨ë“œ ë¼ìš°íŒ…: Q&A ì˜ë„ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ íŒŒì¼ëª…ì´ ìˆì–´ë„ Q&A ëª¨ë“œ ìš°ì„ 
            query_mode = self.query_router.classify_mode(actual_query)
            router_reason = self.query_router.get_routing_reason(actual_query)
            logger.info(
                f"ğŸ”€ ë¼ìš°íŒ… ê²°ê³¼: mode={query_mode.value}, reason={router_reason}"
            )

            # ğŸ” ë””ë²„ê¹…: ì‹¤ì œ pattern matching ëŒ€ìƒ ë¡œê¹…
            logger.info(f"ğŸ” Pattern matching ëŒ€ìƒ ì¿¼ë¦¬: '{actual_query[:100]}'")

            # âœ… P0: íŒŒì¼ëª… ì§ì ‘ ì–¸ê¸‰ íŒ¨í„´ ê°ì§€ (PREVIEW ëª¨ë“œì¼ ë•Œë§Œ)
            if query_mode == QueryMode.PREVIEW:
                # íŒ¨í„´ 1: ìš”ì•½ ìš”ì²­ - "íŒŒì¼ëª….pdf ë‚´ìš© ìš”ì•½í•´ì¤˜" / "íŒŒì¼ëª….pdf ìš”ì•½"
                file_summary_pattern = (
                    r"(\S+\.pdf)\s*(ì´\s*)?(ë¬¸ì„œ\s*)?(ë‚´ìš©\s*)?(ìš”ì•½|ì •ë¦¬)"
                )
                summary_match = re.search(
                    file_summary_pattern, actual_query, re.IGNORECASE
                )

                if summary_match:
                    filename = summary_match.group(1).strip()
                    logger.info(f"ğŸ¯ P0: íŒŒì¼ ìš”ì•½ ìš”ì²­ ê°ì§€ - {filename}")

                    # PDF ì „ë¬¸ ë¡œë“œ + ë©”íƒ€ë°ì´í„° ì¡°íšŒ
                    try:
                        import pdfplumber
                        from pathlib import Path

                        conn = sqlite3.connect("metadata.db")
                        cursor = conn.cursor()
                        cursor.execute(
                            """
                            SELECT path, filename, drafter, date, category, text_preview
                            FROM documents
                            WHERE filename LIKE ?
                            LIMIT 1
                        """,
                            (f"%{filename}%",),
                        )

                        result = cursor.fetchone()
                        conn.close()

                        if result:
                            pdf_path, fname, drafter, date, category, preview = result

                            # PDF ì „ë¬¸ í…ìŠ¤íŠ¸ ë¡œë“œ
                            full_text = preview or ""
                            if pdf_path and Path(pdf_path).exists():
                                try:
                                    with pdfplumber.open(pdf_path) as pdf:
                                        pages_text = []
                                        for page in pdf.pages[:5]:  # ìµœëŒ€ 5í˜ì´ì§€
                                            page_text = page.extract_text() or ""
                                            pages_text.append(page_text)
                                            if len("".join(pages_text)) > 5000:
                                                break
                                        full_text = "\n\n".join(pages_text)
                                except Exception as e:
                                    logger.warning(f"PDF ì½ê¸° ì‹¤íŒ¨: {e}")

                            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (LLM ì—†ì´)
                            answer_text = f"**ğŸ“„ {fname}**\n\n"
                            answer_text += "**ğŸ“‹ ë¬¸ì„œ ì •ë³´**\n"
                            answer_text += f"- **ê¸°ì•ˆì:** {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += f"- **ë‚ ì§œ:** {date or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += (
                                f"- **ì¹´í…Œê³ ë¦¬:** {category or 'ì •ë³´ ì—†ìŒ'}\n"
                            )

                            answer_text += "\n**ğŸ“ ì£¼ìš” ë‚´ìš©**\n"
                            # ì²˜ìŒ 800ì ë¯¸ë¦¬ë³´ê¸°
                            content_preview = full_text[:800].strip()
                            if content_preview:
                                answer_text += content_preview
                                if len(full_text) > 800:
                                    answer_text += (
                                        "...\n\n*(ì „ì²´ ë¬¸ì„œëŠ” ë” ê¸´ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤)*"
                                    )
                            else:
                                answer_text += "*(ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤)*"

                            # Evidence êµ¬ì„±
                            evidence = [
                                {
                                    "doc_id": fname,
                                    "page": 1,
                                    "snippet": full_text[:500],
                                    "meta": {
                                        "filename": fname,
                                        "drafter": drafter,
                                        "date": date,
                                        "category": category,
                                    },
                                }
                            ]

                            return {
                                "text": answer_text,
                                "citations": [fname],
                                "evidence": evidence,
                                "status": {
                                    "retrieved_count": 1,
                                    "selected_count": 1,
                                    "found": True,
                                },
                            }
                        else:
                            logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {filename}")

                    except Exception as e:
                        logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
                        # ì˜¤ë¥˜ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±

                # íŒ¨í„´ 2: ê¸°ì•ˆì ì§ˆì˜ - "íŒŒì¼ëª….pdf ê¸°ì•ˆìê°€ ëˆ„êµ¬ì•¼?"
                # (ì»¬ëŸ¼ ìˆ˜ì • ì™„ë£Œ: doc_number ì œê±°)
                file_author_pattern = r"(\S+\.pdf)\s*(ê¸°ì•ˆì|ì‘ì„±ì).*(ëˆ„êµ¬|ì•Œë ¤ì¤˜)"
                file_match = re.search(file_author_pattern, actual_query, re.IGNORECASE)

                if file_match:
                    filename = file_match.group(1).strip()
                    logger.info(f"ğŸ¯ P0: íŒŒì¼ëª… ì§ì ‘ ì§ˆì˜ ê°ì§€ - {filename}")

                    # metadata.dbì—ì„œ ì§ì ‘ ì¡°íšŒ
                    try:
                        conn = sqlite3.connect("metadata.db")
                        cursor = conn.cursor()
                        cursor.execute(
                            """
                            SELECT filename, drafter, date, category
                            FROM documents
                            WHERE filename LIKE ?
                            LIMIT 1
                        """,
                            (f"%{filename}%",),
                        )

                        result = cursor.fetchone()
                        conn.close()

                        if result:
                            fname, drafter, date, category = result
                            answer_text = f"**{fname}**\n\n"
                            answer_text += f"ğŸ“Œ **ê¸°ì•ˆì:** {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += f"ğŸ“… **ë‚ ì§œ:** {date or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += (
                                f"ğŸ“ **ì¹´í…Œê³ ë¦¬:** {category or 'ì •ë³´ ì—†ìŒ'}\n"
                            )

                            # Evidence êµ¬ì„± (ì •í™•í•œ íŒŒì¼ 1ê±´)
                            evidence = [
                                {
                                    "doc_id": fname,
                                    "page": 1,
                                    "snippet": f"ê¸°ì•ˆì: {drafter}, ë‚ ì§œ: {date}, ì¹´í…Œê³ ë¦¬: {category}",
                                    "meta": {
                                        "filename": fname,
                                        "drafter": drafter,
                                        "date": date,
                                        "category": category,
                                    },
                                }
                            ]

                            return {
                                "text": answer_text,
                                "citations": [fname],
                                "evidence": evidence,
                                "status": {
                                    "retrieved_count": 1,
                                    "selected_count": 1,
                                    "found": True,
                                },
                            }
                        else:
                            logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {filename}")
                            return {
                                "text": f"âŒ '{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                                "citations": [],
                                "evidence": [],
                                "status": {
                                    "retrieved_count": 0,
                                    "selected_count": 0,
                                    "found": False,
                                },
                            }

                    except Exception as e:
                        logger.error(f"âŒ metadata ì¡°íšŒ ì‹¤íŒ¨: {e}")
                        # ì˜¤ë¥˜ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±

            # ê¸°ì•ˆì ê²€ìƒ‰ íŒ¨í„´ ê°ì§€ (ì‹¤ì œ ì§ˆë¬¸ì—ì„œë§Œ)
            author_patterns = [
                r"([ê°€-í£]{2,4})\s*(ë¬¸ì„œ|ê¸°ì•ˆì„œ|ê²€í† ì„œ)",
                r"([ê°€-í£]{2,4})ê°€?\s*(ì‘ì„±í•œ|ì‘ì„±ì•ˆ|ê¸°ì•ˆí•œ|ì“´|ë§Œë“ )",
                r"(ê¸°ì•ˆì|ì‘ì„±ì|ì œì•ˆì)[:\s]+([ê°€-í£]{2,4})",
            ]
            # ë‚ ì§œ ê²€ìƒ‰ íŒ¨í„´ ê°ì§€
            year_pattern = r"(\d{4})\s*ë…„"

            is_author_query = any(re.search(p, actual_query) for p in author_patterns)
            is_year_query = re.search(year_pattern, actual_query)

            if is_author_query or is_year_query:
                logger.info(
                    f"ğŸ¯ íŠ¹ìˆ˜ ê²€ìƒ‰ ëª¨ë“œ ê°ì§€: author={is_author_query}, year={is_year_query}"
                )
                # QuickFixRAG.answer()ë¡œ ì§ì ‘ ì²˜ë¦¬ (ì‹¤ì œ ì§ˆë¬¸ ì „ë‹¬)
                answer_text = self.generator.rag.answer(
                    actual_query, use_llm_summary=False
                )

                # í‘œì¤€ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                return {
                    "text": answer_text,
                    "citations": [],  # QuickFixRAG ì‘ë‹µì—ì„œ ì¶”ì¶œ ì–´ë ¤ì›€
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": "ê´€ë ¨ ë¬¸ì„œ" not in answer_text
                        and "ì—†ìŠµë‹ˆë‹¤" not in answer_text,
                    },
                }

        # ì¼ë°˜ ì¿¼ë¦¬ëŠ” ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
        response = self.query(query, top_k=top_k or 5)

        if response.success:
            # ê²€ìƒ‰/ì••ì¶•ì—ì„œ ë„˜ì–´ì˜¨ ì •ê·œí™” ì²­í¬ ì‚¬ìš© (ì‹¤ì œ page/snippet/meta ë…¸ì¶œ)
            evidence = [
                {
                    "doc_id": c.get("doc_id"),
                    "page": c.get("page", 1),
                    "snippet": c.get("snippet", ""),
                    "meta": c.get(
                        "meta", {"doc_id": c.get("doc_id"), "page": c.get("page", 1)}
                    ),
                }
                for c in (response.evidence_chunks or [])
            ]

            # CRITICAL: Evidence ìµœì†Œ ë³´ì¥ (sources_citedê°€ ë¹„ì–´ë„ ê²€ìƒ‰ ê²°ê³¼ëŠ” í‘œì‹œ)
            evidence_injected = False
            if not evidence and response.raw_results:
                logger.info("Evidence empty, using raw_results[:3] as fallback")
                evidence = [
                    {
                        "doc_id": r.get("doc_id") or r.get("chunk_id", "unknown"),
                        "page": 0,  # ê²€ìƒ‰ ê²°ê³¼ëŠ” í˜ì´ì§€ ì •ë³´ ì—†ìŒ
                        "snippet": r.get("snippet") or r.get("text_preview", "")[:500],
                        "meta": {
                            "doc_id": r.get("doc_id") or r.get("chunk_id", "unknown"),
                            "filename": r.get("filename", ""),
                            "page": 0,
                        },
                    }
                    for r in response.raw_results[:3]
                ]
                evidence_injected = True

            # [DIAG] Evidence ì§„ë‹¨ ì •ë³´ ì¶”ê°€
            if DIAG_RAG and response.diagnostics:
                response.diagnostics["evidence_count"] = len(evidence)
                response.diagnostics["evidence_injected"] = evidence_injected

            # ğŸ”¥ CRITICAL: status.found í”Œë˜ê·¸ - UI íŒì • ë‹¨ì¼ ì†ŒìŠ¤
            # retrieved_count: ê²€ìƒ‰ëœ ì›ë³¸ ê²°ê³¼ ìˆ˜
            # selected_count: ì‹¤ì œ ì‚¬ìš©ëœ ì¦ê±° ìˆ˜ (evidence)
            # found: ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€ (evidenceê°€ 1ê°œ ì´ìƒì´ë©´ True)
            status = {
                "retrieved_count": len(response.raw_results or []),
                "selected_count": len(evidence),
                "found": len(evidence) > 0,  # ğŸ”´ ìœ ì¼í•œ íŒì • ê¸°ì¤€
            }

            # ìš´ì˜ í‘œì¤€ 1í–‰ ìš”ì•½ ë¡œê·¸ (í•„ìˆ˜)
            import re

            author_mode = bool(re.search(r"(ì‘ì„±ì|ê¸°ì•ˆì|ì œì•ˆì)", query))
            search_ms = int(response.metrics.get("search_time", 0) * 1000)
            generate_ms = int(response.metrics.get("generate_time", 0) * 1000)
            total_ms = int(response.latency * 1000)

            logger.info(
                f'[RAG] query="{query[:50]}..." | '
                f"retrieved={status['retrieved_count']} | "
                f"selected={status['selected_count']} | "
                f"found={status['found']} | "
                f"author_mode={author_mode} | "
                f"backfill={evidence_injected} | "
                f"search_ms={search_ms} | "
                f"generate_ms={generate_ms} | "
                f"total_ms={total_ms}"
            )

            return {
                "text": response.answer,
                "citations": evidence,  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
                "evidence": evidence,  # í•˜ìœ„ í˜¸í™˜ì„± (ë™ì¼ ë°ì´í„°)
                "status": status,  # UIì—ì„œ ì´ê²ƒë§Œ í™•ì¸
                "diagnostics": response.diagnostics if DIAG_RAG else {},
            }
        else:
            # ì—ëŸ¬ ë°œìƒ ì‹œ (ì¤‘ë¦½ í†¤, ì‚¬ê³¼ í‘œí˜„ ê¸ˆì§€)
            error_msg = ERROR_MESSAGES.get(
                ErrorCode.E_GENERATE, "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆë‹¤."
            )
            if response.error:
                error_msg = f"{error_msg}\n\nìƒì„¸: {response.error}"

            # ìš´ì˜ í‘œì¤€ ë¡œê·¸ (ì—ëŸ¬ ì¼€ì´ìŠ¤)
            logger.error(
                f'[RAG] query="{query[:50]}..." | '
                f'status=ERROR | error="{response.error}"'
            )

            return {
                "text": error_msg,
                "citations": [],  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
                "evidence": [],  # í•˜ìœ„ í˜¸í™˜ì„±
                "status": {"retrieved_count": 0, "selected_count": 0, "found": False},
            }

    def answer_text(self, query: str) -> str:
        """ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            str: ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
        """
        result = self.answer(query)
        return result["text"]

    def warmup(self) -> None:
        """ì›Œë°ì—…: LLM + ì¸ë±ìŠ¤ ì‚¬ì „ ë¡œë”©

        ì²« ì¿¼ë¦¬ ì§€ì—° ì œê±°ë¥¼ ìœ„í•´ ì‹œì‘ ì‹œ í˜¸ì¶œ.
        """
        logger.info("Warming up RAG pipeline...")
        try:
            # ë”ë¯¸ ì¿¼ë¦¬ ì‹¤í–‰
            response = self.query("test warmup query", top_k=1)
            if response.success:
                logger.info(f"Warmup completed in {response.latency:.2f}s")
            else:
                logger.warning(f"Warmup failed: {response.error}")
        except Exception as e:
            logger.error(f"Warmup error: {e}", exc_info=True)

    # ========================================================================
    # ë‚´ë¶€ í—¬í¼: ê¸°ë³¸ êµ¬í˜„ ìƒì„±
    # ========================================================================

    def _create_default_retriever(self) -> Retriever:
        """ê¸°ë³¸ ê²€ìƒ‰ ì—”ì§„ ìƒì„± (v2 ë˜ëŠ” v1)

        í™˜ê²½ ë³€ìˆ˜ USE_V2_RETRIEVERë¡œ ì œì–´:
        - true: HybridRetrieverV2 ì‚¬ìš© (ì‹ ê·œ 2-layer ì•„í‚¤í…ì²˜)
        - false/ì—†ìŒ: HybridRetriever ì‚¬ìš© (ê¸°ì¡´ ë ˆê±°ì‹œ)
        """
        import os

        use_v2 = os.getenv("USE_V2_RETRIEVER", "false").lower() == "true"

        if use_v2:
            # V2 RetrieverëŠ” archiveë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤ (20251026)
            # ë ˆê±°ì‹œ ì½”ë“œë¥¼ ì œê±°í•˜ê³  v1ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤
            logger.warning(
                "âš ï¸ USE_V2_RETRIEVERëŠ” ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. v1 Retrieverë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            use_v2 = False
            # try:
            #     from app.rag.retriever_v2 import HybridRetrieverV2
            #     v2_retriever = HybridRetrieverV2()
            #     logger.info("âœ… HybridRetrieverV2 (v2 ì‹ ê·œ ì‹œìŠ¤í…œ) ìƒì„± ì™„ë£Œ")
            #
            #     # V2 adapter: fused_results â†’ list ë³€í™˜
            #     return _V2RetrieverAdapter(v2_retriever)
            # except Exception as e:
            #     logger.error(f"V2 Retriever ìƒì„± ì‹¤íŒ¨, v1ìœ¼ë¡œ í´ë°±: {e}")
            #     # í´ë°±: v1 ì‚¬ìš©
            #     use_v2 = False

        if not use_v2:
            try:
                from app.rag.retrievers.hybrid import HybridRetriever

                retriever = HybridRetriever()
                logger.info("Default HybridRetriever (v1 ë ˆê±°ì‹œ) ìƒì„± ì™„ë£Œ")
                return retriever
            except Exception as e:
                logger.error(f"HybridRetriever ìƒì„± ì‹¤íŒ¨: {e}")
                # í´ë°±: ë”ë¯¸ êµ¬í˜„
                return _DummyRetriever()

    def _create_default_compressor(self) -> Compressor:
        """ê¸°ë³¸ ì••ì¶•ê¸° ìƒì„± (í˜„ì¬ëŠ” no-op)"""
        logger.info("Default compressor ìƒì„± (no-op)")
        return _NoOpCompressor()

    def _create_default_generator(self) -> Generator:
        """ê¸°ë³¸ LLM ìƒì„±ê¸° ìƒì„± (ë ˆê±°ì‹œ ì–´ëŒ‘í„° ì‚¬ìš©)"""
        try:
            # ë ˆê±°ì‹œ êµ¬í˜„ ì–´ëŒ‘í„° ì‚¬ìš© (ì ì§„ì  ì´ê´€ ì¤€ë¹„)
            legacy_rag = self._create_legacy_adapter()
            logger.info("Default generator ìƒì„± (Legacy Adapter ë˜í•‘)")
            return _QuickFixGenerator(legacy_rag)
        except Exception as e:
            logger.error(f"Generator ìƒì„± ì‹¤íŒ¨: {e}")
            return _DummyGenerator()

    def _create_legacy_adapter(self):
        """ë ˆê±°ì‹œ êµ¬í˜„ ì–´ëŒ‘í„° ìƒì„± (ìº¡ìŠí™”)

        QuickFixRAGë¥¼ ë˜í•‘í•˜ì—¬ ê¸°ì¡´ ë ˆê±°ì‹œ ì‹œìŠ¤í…œê³¼ ì—°ê²°í•©ë‹ˆë‹¤.
        í–¥í›„ ì´ ë©”ì„œë“œë§Œ ìˆ˜ì •í•˜ì—¬ ì‹ ê·œ êµ¬í˜„ìœ¼ë¡œ ì ì§„ ì „í™˜ ê°€ëŠ¥.

        Returns:
            QuickFixRAG: ë ˆê±°ì‹œ RAG ì¸ìŠ¤í„´ìŠ¤
        """
        from quick_fix_rag import QuickFixRAG

        # ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logger.info("Loading legacy QuickFixRAG adapter...")
        rag = QuickFixRAG(use_hybrid=True)
        logger.info("Legacy adapter loaded successfully")

        return rag


# ============================================================================
# í´ë°± êµ¬í˜„ (ê¸°ë³¸ ë™ì‘ ë³´ì¥)
# ============================================================================


class _DummyRetriever:
    """ë”ë¯¸ ê²€ìƒ‰ê¸° (í´ë°±ìš©)"""

    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        logger.warning("Dummy retriever: ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return []


class _NoOpCompressor:
    """No-op ì••ì¶•ê¸° (ì••ì¶•í•˜ì§€ ì•ŠìŒ)"""

    def compress(
        self, chunks: List[Dict[str, Any]], ratio: float
    ) -> List[Dict[str, Any]]:
        logger.debug("No-op compressor: ì••ì¶• ìŠ¤í‚µ")
        return chunks


class _QuickFixGenerator:
    """QuickFixRAG ë˜í¼ (ê¸°ì¡´ êµ¬í˜„ í™œìš©)"""

    def __init__(self, rag):
        self.rag = rag
        self.compressed_chunks = None  # Store chunks for LLM

    def generate(self, query: str, context: str, temperature: float) -> str:
        # ì¬ê²€ìƒ‰ ê¸ˆì§€. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„±ìœ¼ë¡œ ìš°ì„  ì‹œë„.
        try:
            # 1) QuickFixRAGì— ì „ìš© ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if hasattr(self.rag, "generate_from_context"):
                return self.rag.generate_from_context(
                    query, context, temperature=temperature
                )

            # 2) ë‚´ë¶€ LLM ì§ì ‘ ì ‘ê·¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            # ğŸ”¥ CRITICAL: LLM lazy loading - ensure LLM is loaded before checking
            if hasattr(self.rag, "_ensure_llm_loaded"):
                self.rag._ensure_llm_loaded()

            if hasattr(self.rag, "llm") and hasattr(self.rag.llm, "generate_response"):
                # CRITICAL: generate_response expects List[Dict], not str
                # Convert context string back to chunks format
                if self.compressed_chunks:
                    # Use stored compressed chunks (preferred)
                    logger.debug(
                        f"Using {len(self.compressed_chunks)} compressed chunks for generation"
                    )
                    response = self.rag.llm.generate_response(
                        query, self.compressed_chunks, max_retries=1
                    )
                else:
                    # Fallback: convert context string to minimal chunks
                    logger.warning(
                        "No compressed_chunks available, converting context string"
                    )
                    snippets = context.split("\n\n")
                    chunks = [
                        {"snippet": s, "content": s} for s in snippets if s.strip()
                    ]
                    response = self.rag.llm.generate_response(
                        query, chunks, max_retries=1
                    )

                # Extract answer from RAGResponse object
                if hasattr(response, "answer"):
                    return response.answer
                return str(response)

            # 3) í´ë°±: ì¬ê²€ìƒ‰ì´ í¬í•¨ëœ answerëŠ” ìµœí›„ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ
            logger.warning("generate_from_context ë¯¸ì§€ì› â†’ í´ë°±(answer) ì‚¬ìš©")
            return self.rag.answer(query, use_llm_summary=True)
        except Exception as e:
            logger.error(f"Generation ì‹¤íŒ¨: {e}", exc_info=True)
            return f"[E_GENERATE] {str(e)}"


class _V2RetrieverAdapter:
    """V2 Retriever Adapter

    HybridRetrieverV2ì˜ ê²°ê³¼ í˜•ì‹ {"fused_results": [...]}ë¥¼
    v1 ì¸í„°í˜ì´ìŠ¤ í˜•ì‹ [...] ìœ¼ë¡œ ë³€í™˜.

    v2 results êµ¬ì¡°:
        {
            "fused_results": [
                {"id": "doc_4094", "score": 0.123, "filename": "...", ...},
                ...
            ]
        }

    v1 expected êµ¬ì¡°:
        [
            {"doc_id": "doc_4094", "snippet": "...", "page": 1, ...},
            ...
        ]
    """

    def __init__(self, v2_retriever):
        """
        Args:
            v2_retriever: HybridRetrieverV2 instance
        """
        self.v2_retriever = v2_retriever
        self.db = v2_retriever.db  # MetadataDB for content fetching

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Search using v2 retriever, convert to v1 format

        Args:
            query: Search query
            top_k: Number of results

        Returns:
            List of dicts in v1 format with keys:
            - doc_id: Document ID
            - snippet: Text snippet
            - page: Page number (default 1)
            - score: Relevance score
            - meta: Metadata dict
        """
        try:
            # Call v2 retriever
            v2_result = self.v2_retriever.search(query, top_k=top_k)
            fused_results = v2_result.get("fused_results", [])

            # Convert to v1 format
            v1_results = []
            for doc in fused_results:
                doc_id = doc.get("id", "unknown")

                # ğŸ”¥ CRITICAL: snippet ìš°ì„ ìˆœìœ„
                # 1) ê²€ìƒ‰ ê²°ê³¼ì— ì§ì ‘ í¬í•¨ëœ snippet/content
                # 2) DB ì¡°íšŒ (get_content)
                # 3) ì œëª©/íŒŒì¼ëª… ê¸°ë°˜ í´ë°±

                snippet = ""

                # Priority 1: fused_resultsì— ì´ë¯¸ í¬í•¨ëœ ë°ì´í„°
                if "snippet" in doc:
                    snippet = doc["snippet"]
                elif "content" in doc:
                    snippet = doc["content"][:500]

                # Priority 2: DB ì¡°íšŒ (app/rag/db.MetadataDB.get_content)
                if not snippet or len(snippet) < 50:
                    content = self.db.get_content(doc_id)
                    if content and len(content) >= 50:
                        snippet = content[:500]

                # Priority 3: ë©”íƒ€ë°ì´í„° í´ë°±
                if not snippet or len(snippet) < 50:
                    fallback_parts = []
                    if doc.get("title"):
                        fallback_parts.append(f"ì œëª©: {doc['title']}")
                    if doc.get("filename"):
                        fallback_parts.append(f"íŒŒì¼: {doc['filename']}")
                    if doc.get("date"):
                        fallback_parts.append(f"ë‚ ì§œ: {doc['date']}")

                    snippet = (
                        " | ".join(fallback_parts)
                        if fallback_parts
                        else f"ë¬¸ì„œ ID: {doc_id}"
                    )
                    logger.warning(
                        f"V2 Adapter: doc_id={doc_id} snippet ê²°ì†, ë©”íƒ€ë°ì´í„° í´ë°± ì‚¬ìš©"
                    )

                v1_results.append(
                    {
                        "doc_id": doc_id,
                        "snippet": snippet,
                        "page": 1,  # v2ì—ì„œëŠ” page ì •ë³´ ì—†ìŒ, ê¸°ë³¸ 1
                        "score": doc.get("score", 0.0),
                        "meta": {
                            "doc_id": doc_id,
                            "filename": doc.get("filename", ""),
                            "title": doc.get("title", ""),
                            "date": doc.get("date", ""),
                            "page": 1,
                        },
                    }
                )

            logger.info(f"V2 Adapter: {len(v1_results)} results converted")
            return v1_results

        except Exception as e:
            logger.error(f"V2 Adapter search failed: {e}", exc_info=True)
            return []

    def warmup(self):
        """ì›Œë°ì—… (v2ëŠ” í•„ìš” ì‹œ ìë™ ë¡œë“œ)"""
        logger.info("V2 Adapter warmup (no-op)")


class _DummyGenerator:
    """ë”ë¯¸ ìƒì„±ê¸° (í´ë°±ìš©)"""

    def generate(self, query: str, context: str, temperature: float) -> str:
        logger.warning("Dummy generator: ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
