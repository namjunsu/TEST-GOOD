"""RAG íŒŒì´í”„ë¼ì¸ (íŒŒì‚¬ë“œ íŒ¨í„´)

ë‹¨ì¼ ì§„ì…ì : RAGPipeline.query()
ë‚´ë¶€ íë¦„: ê²€ìƒ‰ â†’ ì••ì¶• â†’ LLM ìƒì„±

Example:
    >>> pipeline = RAGPipeline()
    >>> response = pipeline.query("ì§ˆë¬¸", top_k=5)
    >>> print(response.answer)
"""

import os
import time
import base64
import sqlite3
from pathlib import Path
from dataclasses import dataclass, field
from typing import Protocol, List, Optional, Dict, Any

from app.core.logging import get_logger
from app.core.errors import ModelError, SearchError, ErrorCode, ERROR_MESSAGES
from app.rag.query_router import QueryRouter, QueryMode

logger = get_logger(__name__)


# ============================================================================
# ë¼ìš°íŒ… í—¬í¼ í•¨ìˆ˜ë“¤ (ìŠ¤ëª°í† í¬/ì‚°ìˆ /ë„ë©”ì¸ í‚¤ì›Œë“œ ê°ì§€)
# ============================================================================

import re

# ìŠ¤ëª°í† í¬ íŒ¨í„´
SMALLTALK_PATTERNS = {
    'ì•ˆë…•', 'ì•ˆë…•í•˜ì„¸ìš”', 'ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ', 'hello', 'hi', 'hey',
    'ë•¡í', 'ê°ì‚¬', 'ê³ ë§ˆì›Œ', 'thanks', 'thank you',
    'ì˜ê°€', 'ì•ˆë…•íˆ', 'bye', 'goodbye',
    'ì–´ë–»ê²Œ', 'ì–´ë– ', 'ì–´ë•Œ', 'ë­í•´', 'ë¬´ì—‡',
}

# ë„ë©”ì¸ í‚¤ì›Œë“œ (ì¥ë¹„/í”„ë¡œì íŠ¸/ê¸°ìˆ  ìš©ì–´)
DOMAIN_KEYWORDS = {
    # ì¥ë¹„
    'nvr', 'sync', 'eco8000', 'lvm-180a', 'odin', 'vmix', 'faiss',
    'tri-level', 'sdi', 'lut', 'intercom', 'di box', 'dibox',
    'ë¬´ì„ ë§ˆì´í¬', 'ë§ˆì´í¬', 'ì¹´ë©”ë¼', 'ë Œì¦ˆ', 'ì‚¼ê°ëŒ€', 'ì¼€ì´ë¸”',
    'ê±´ì „ì§€', 'ë°°í„°ë¦¬', 'ì†Œëª¨í’ˆ', 'ì¥ë¹„', 'ì¤‘ê³„ì°¨',
    # í”„ë¡œì íŠ¸/í”„ë¡œê·¸ë¨
    'ëŒì§êµ¬ì‡¼', 'ë‰´ìŠ¤', 'ìŠ¤íŠœë””ì˜¤', 'ê´‘í™”ë¬¸', 'ì˜¤í”ˆìŠ¤íŠœë””ì˜¤',
    'ì¤‘ê³„', 'ë°©ì†¡', 'ì±„ë„ì—ì´',
    # ê¸°ìˆ /ë¬¸ì„œ
    'ê¸°ì•ˆì„œ', 'êµ¬ë§¤', 'ìˆ˜ë¦¬', 'êµì²´', 'ê²€í† ', 'ê¸°ìˆ ê²€í† ',
    'ì˜¤ë²„í™€', 'ë„ì…', 'ë…¸í›„í™”', 'ë‹¨ì¢…',
    'ì‘ì„±', 'ì‘ì„±ëœ', 'ë¬¸ì„œ', 'ë¦¬ìŠ¤íŠ¸', 'ëª©ë¡',
    # ì‘ì„±ì (ì‹¤ì œ ê¸°ì•ˆì ì´ë¦„)
    'ìµœìƒˆë¦„', 'ìœ ì¸í˜', 'ë‚¨ì¤€ìˆ˜', 'ë°•ì¤€ì„œ', 'ì´ì›êµ¬',
    'ìµœì •ì€', 'í•œê±´í¬', 'ê¹€ê²½í˜„', 'ê¹€ìˆ˜ì—°', 'ê¹€ì°½ìˆ˜', 'ì†¡ê²½ì›',
}


def is_smalltalk(query: str) -> bool:
    """ìŠ¤ëª°í† í¬/ì¸ì‚¬/ê°íƒ„ì‚¬ ê°ì§€"""
    q_lower = query.lower().strip()
    # ê¸¸ì´ ì²´í¬
    if len(q_lower) <= 3:
        return True
    # íŒ¨í„´ ë§¤ì¹­
    for pattern in SMALLTALK_PATTERNS:
        if pattern in q_lower:
            return True
    return False


def is_simple_math(query: str) -> bool:
    """ë‹¨ìˆœ ì‚°ìˆ  ì§ˆì˜ ê°ì§€ (ì˜ˆ: 1+1ì€?, 2*3=?)"""
    q_stripped = query.strip()
    # ì •ê·œì‹: ìˆ«ì ì—°ì‚°ì ìˆ«ì (ì˜µì…˜: = ê²°ê³¼)
    math_pattern = r'^\s*\d+\s*[\+\-\*/]\s*\d+\s*(=\s*\d+)?\s*[ì€?]*\s*$'
    return bool(re.match(math_pattern, q_stripped))


def has_domain_keyword(query: str) -> bool:
    """ë„ë©”ì¸ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    q_lower = query.lower()
    for keyword in DOMAIN_KEYWORDS:
        if keyword in q_lower:
            return True
    return False


def get_query_token_count(query: str) -> int:
    """ê°„ì´ í† í° ì¹´ìš´íŠ¸ (ê³µë°±/í•œê¸€ ê¸°ì¤€)"""
    # í•œê¸€: ìŒì ˆ ë‹¨ìœ„, ì˜ë¬¸: ë‹¨ì–´ ë‹¨ìœ„
    korean_chars = len([c for c in query if '\uac00' <= c <= '\ud7a3'])
    english_words = len(query.split())
    return max(korean_chars, english_words)


def get_keyword_coverage(query: str, results: list) -> int:
    """ì¿¼ë¦¬ì™€ ê²€ìƒ‰ ê²°ê³¼ ê°„ ë„ë©”ì¸ í‚¤ì›Œë“œ êµì§‘í•© ê°œìˆ˜ ê³„ì‚°

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

    Returns:
        êµì§‘í•©ëœ í‚¤ì›Œë“œ ê°œìˆ˜
    """
    q_lower = query.lower()
    # ì¿¼ë¦¬ì—ì„œ ë§¤ì¹­ëœ í‚¤ì›Œë“œ
    query_keywords = {kw for kw in DOMAIN_KEYWORDS if kw in q_lower}

    if not query_keywords:
        return 0

    # ê²€ìƒ‰ ê²°ê³¼ ì²­í¬ì—ì„œ ë°œê²¬ëœ í‚¤ì›Œë“œ
    found_keywords = set()
    for result in results[:5]:  # ìƒìœ„ 5ê°œë§Œ ì²´í¬
        chunk_text = result.get('snippet', '') + ' ' + result.get('content', '')
        chunk_lower = chunk_text.lower()
        for kw in query_keywords:
            if kw in chunk_lower:
                found_keywords.add(kw)

    return len(found_keywords)


def force_chat_mode(query: str) -> tuple[bool, str]:
    """ê°•ì œ CHAT ëª¨ë“œ ì ìš© ì—¬ë¶€ íŒë‹¨

    Returns:
        (should_force, reason)
    """
    # 1. ìŠ¤ëª°í† í¬
    if is_smalltalk(query):
        return True, "smalltalk"

    # 2. ì§§ì€ ì§ˆì˜ (í† í° <4)
    if get_query_token_count(query) < 4:
        return True, "short_query"

    # 3. ë‹¨ìˆœ ì‚°ìˆ 
    if is_simple_math(query):
        return True, "simple_math"

    return False, ""


def _encode_file_ref(filename: str) -> Optional[str]:
    """íŒŒì¼ëª…ì„ base64 refë¡œ ì¸ì½”ë”© (docs í•˜ìœ„ ê²½ë¡œ ì°¾ê¸°)

    Args:
        filename: íŒŒì¼ëª…

    Returns:
        base64 ì¸ì½”ë”©ëœ ref ë˜ëŠ” None
    """
    try:
        # 1. metadata.dbì—ì„œ ê²½ë¡œ ì°¾ê¸° ì‹œë„
        conn = sqlite3.connect("metadata.db")
        cursor = conn.cursor()
        cursor.execute(
            "SELECT path FROM documents WHERE filename = ? LIMIT 1",
            (filename,)
        )
        result = cursor.fetchone()
        conn.close()

        if result and result[0]:
            file_path = Path(result[0])
            # docs í•˜ìœ„ì¸ì§€ í™•ì¸
            if "docs" in file_path.parts and file_path.exists():
                # base64 ì¸ì½”ë”©
                ref = base64.urlsafe_b64encode(str(file_path).encode()).decode()
                return ref

        # 2. Fallback: docs í´ë”ì—ì„œ íŒŒì¼ ê²€ìƒ‰ (year í´ë” í¬í•¨)
        import re
        year_match = re.search(r'(\d{4})-', filename)
        if year_match:
            year = year_match.group(1)
            # docs/year_YYYY/ í´ë”ì—ì„œ ì°¾ê¸°
            file_path = Path(f"docs/year_{year}") / filename
            if file_path.exists():
                ref = base64.urlsafe_b64encode(str(file_path).encode()).decode()
                return ref

        # 3. Fallback2: docs í´ë” ì „ì²´ ê²€ìƒ‰
        docs_dir = Path("docs")
        if docs_dir.exists():
            for file_path in docs_dir.rglob(filename):
                if file_path.is_file():
                    ref = base64.urlsafe_b64encode(str(file_path).encode()).decode()
                    return ref

    except Exception as e:
        logger.warning(f"ref ì¸ì½”ë”© ì‹¤íŒ¨: {filename} - {e}")

    return None

# ì§„ë‹¨ ëª¨ë“œ ì„¤ì •
DIAG_RAG = os.getenv("DIAG_RAG", "false").lower() == "true"
DIAG_LOG_LEVEL = os.getenv("DIAG_LOG_LEVEL", "INFO").upper()


# ============================================================================
# Request / Response ë°ì´í„° í´ë˜ìŠ¤
# ============================================================================


@dataclass
class RAGRequest:
    """RAG ìš”ì²­ íŒŒë¼ë¯¸í„°

    Attributes:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
        compression_ratio: ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ë¹„ìœ¨ (0.0~1.0)
        use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
        temperature: LLM ìƒì„± ì˜¨ë„
    """

    query: str
    top_k: int = 5
    compression_ratio: float = 0.7
    use_hyde: bool = False
    temperature: float = 0.1


@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ê²°ê³¼

    Attributes:
        answer: ìƒì„±ëœ ë‹µë³€
        source_docs: ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í•˜ìœ„ í˜¸í™˜)
        evidence_chunks: Evidenceìš© ì •ê·œí™” ì²­í¬ (ê¶Œì¥)
        raw_results: ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ (Evidence ìµœì†Œ ë³´ì¥ìš©)
        latency: ì „ì²´ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
        success: ì„±ê³µ ì—¬ë¶€
        error: ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
        metrics: ë‚´ë¶€ ì§€í‘œ (ê²€ìƒ‰/ì••ì¶•/ìƒì„± ì‹œê°„ ë“±)
        diagnostics: ì§„ë‹¨ ì •ë³´ (DIAG_RAG=trueì¼ ë•Œë§Œ ì±„ì›Œì§)
    """

    answer: str
    source_docs: List[str] = field(default_factory=list)
    evidence_chunks: List[Dict[str, Any]] = field(default_factory=list)
    raw_results: List[Dict[str, Any]] = field(default_factory=list)
    latency: float = 0.0
    success: bool = True
    error: Optional[str] = None
    metrics: dict = field(default_factory=dict)
    diagnostics: dict = field(default_factory=dict)  # ì§„ë‹¨ ì •ë³´


# ============================================================================
# í”„ë¡œí† ì½œ ì •ì˜ (ì˜ì¡´ì„± ì—­ì „)
# ============================================================================


class Retriever(Protocol):
    """ê²€ìƒ‰ ì—”ì§„ ì¸í„°í˜ì´ìŠ¤"""

    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ìˆ˜í–‰ (ì •ê·œí™” ìŠ¤í‚¤ë§ˆ ë°˜í™˜)

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼

        Returns:
            [
                {
                    "doc_id": str,
                    "page": int,
                    "score": float,
                    "snippet": str,
                    "meta": dict
                }, ...
            ]
        """
        ...


class Compressor(Protocol):
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶•ê¸° ì¸í„°í˜ì´ìŠ¤"""

    def compress(
        self, chunks: List[Dict[str, Any]], ratio: float
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ì••ì¶•

        Args:
            chunks: ì›ë³¸ ì²­í¬ ëª©ë¡ (ì •ê·œí™”ëœ dict)
            ratio: ì••ì¶• ë¹„ìœ¨

        Returns:
            ì••ì¶•ëœ ì²­í¬ ëª©ë¡ (ë™ì¼ ìŠ¤í‚¤ë§ˆ)
        """
        ...


class Generator(Protocol):
    """LLM ìƒì„±ê¸° ì¸í„°í˜ì´ìŠ¤"""

    def generate(self, query: str, context: str, temperature: float, mode: str = "rag") -> str:
        """ë‹µë³€ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì°¸ê³  ë¬¸ì„œ
            temperature: ìƒì„± ì˜¨ë„
            mode: ìƒì„± ëª¨ë“œ ("chat", "rag", "summarize") - í† í° ì˜ˆì‚° ì œì–´

        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        ...


# ============================================================================
# RAG íŒŒì´í”„ë¼ì¸ (íŒŒì‚¬ë“œ)
# ============================================================================


class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸ íŒŒì‚¬ë“œ

    ê²€ìƒ‰ â†’ ì••ì¶• â†’ ìƒì„±ì„ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ ì œê³µ.
    ë‚´ë¶€ êµ¬í˜„ì€ Retriever/Compressor/Generatorì— ìœ„ì„.

    Example:
        >>> pipeline = RAGPipeline()
        >>> response = pipeline.query("ì§ˆë¬¸", top_k=5)
        >>> if response.success:
        ...     print(response.answer)
        ...     print(f"ì°¸ê³ : {response.source_docs}")
    """

    def __init__(
        self,
        retriever: Optional[Retriever] = None,
        compressor: Optional[Compressor] = None,
        generator: Optional[Generator] = None,
    ):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            retriever: ê²€ìƒ‰ ì—”ì§„ (Noneì´ë©´ ê¸°ë³¸ HybridRetriever ì‚¬ìš©)
            compressor: ì••ì¶•ê¸° (Noneì´ë©´ ê¸°ë³¸ ContextCompressor ì‚¬ìš©)
            generator: LLM ìƒì„±ê¸° (Noneì´ë©´ ê¸°ë³¸ LlamaCppGenerator ì‚¬ìš©)
        """
        self.retriever = retriever or self._create_default_retriever()
        self.compressor = compressor or self._create_default_compressor()
        self.generator = generator or self._create_default_generator()
        self.query_router = QueryRouter()  # ğŸ¯ ëª¨ë“œ ë¼ìš°í„° ì´ˆê¸°í™”

        # ğŸ”’ Closed-World Validation: ê³ ìœ  ê¸°ì•ˆì ìºì‹±
        self.known_drafters = self._load_known_drafters()

        logger.info(f"RAG Pipeline initialized (known_drafters: {len(self.known_drafters)}ëª…)")

    def query(
        self,
        query: str,
        top_k: int = 5,
        compression_ratio: float = 0.7,
        use_hyde: bool = False,
        temperature: float = 0.1,
    ) -> RAGResponse:
        """RAG ì§ˆì˜ (ë‹¨ì¼ ì§„ì…ì )

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            compression_ratio: ì••ì¶• ë¹„ìœ¨
            use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
            temperature: LLM ìƒì„± ì˜¨ë„

        Returns:
            RAGResponse: ë‹µë³€ + ë©”íƒ€ë°ì´í„°
        """

        # ì…ë ¥ ê²€ì¦
        if not query or not query.strip():
            return RAGResponse(
                answer="",
                success=False,
                error="ë¹ˆ ì§ˆë¬¸ì…ë‹ˆë‹¤",
            )

        start_time = time.perf_counter()
        metrics = {}
        diagnostics = {}  # ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘

        try:
            # 1. ê²€ìƒ‰: ì •ê·œí™”ëœ ì²­í¬(dict) ë¦¬ìŠ¤íŠ¸ ê¸°ëŒ€
            search_start = time.perf_counter()
            results = self.retriever.search(query, top_k)
            metrics["search_time"] = time.perf_counter() - search_start

            # [DIAG] ê²€ìƒ‰ ê²°ê³¼ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["retrieved_k"] = len(results)
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(f"[DIAG] ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")

            if not results:
                logger.warning(f"No results found for query: {query[:50]}")
                if DIAG_RAG:
                    diagnostics["mode"] = "no_results"
                    diagnostics["generate_path"] = "fallback_no_context"

                # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ CHAT ëª¨ë“œë¡œ í´ë°±
                metrics["mode"] = "chat"
                metrics["top_score"] = 0.0

                return RAGResponse(
                    answer="ê´€ë ¨ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ë‹¤.",
                    success=True,
                    latency=time.perf_counter() - start_time,
                    metrics=metrics,
                    diagnostics=diagnostics,
                )

            # 2. ì••ì¶•: ì²­í¬ ë‹¨ìœ„ ìœ ì§€(í˜ì´ì§€/ìŠ¤ë‹ˆí«/ë©”íƒ€ ë³´ì¡´)
            compress_start = time.perf_counter()
            compressed = self.compressor.compress(results, compression_ratio)
            metrics["compress_time"] = time.perf_counter() - compress_start

            # [DIAG] ì••ì¶• í›„ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["after_compress_k"] = len(compressed)
                diagnostics["compression_ratio"] = compression_ratio
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(
                        f"[DIAG] ì••ì¶• ì™„ë£Œ: {len(results)} â†’ {len(compressed)}ê°œ ë¬¸ì„œ"
                    )

            # 3. ìƒì„±: ëª¨ë“œ ê²°ì • â†’ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” â†’ ìƒì„±
            gen_start = time.perf_counter()

            # CRITICAL: Inject compressed chunks into generator for proper LLM context
            if hasattr(self.generator, "compressed_chunks"):
                self.generator.compressed_chunks = compressed
                logger.debug(
                    f"Injected {len(compressed)} compressed chunks into generator"
                )

            # [DIAG] ìƒì„± ì „ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ·
            if DIAG_RAG and DIAG_LOG_LEVEL == "DEBUG":
                for i, c in enumerate(compressed[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ë¡œê·¸
                    logger.debug(
                        f"[DIAG] Context[{i}]: doc_id={c.get('doc_id')}, "
                        f"filename={c.get('filename', 'N/A')}, "
                        f"page={c.get('page', 0)}, "
                        f"snippet={c.get('snippet', '')[:120]}..."
                    )

            # ğŸ¯ STEP 1: ëª¨ë“œ ê²°ì • (ì»¨í…ìŠ¤íŠ¸ ìµœì í™”ë³´ë‹¤ ë¨¼ì €)
            # CRITICAL: Determine mode BEFORE context hydration to apply mode-aware context limits
            mode_env = os.getenv('MODE', 'AUTO').upper()
            top_score = results[0].get('score', 0.0) if results else 0.0
            metrics["top_score"] = top_score

            if mode_env == 'AUTO':
                # â”â”â” 1. ê°•ì œ CHAT ëª¨ë“œ ì²´í¬ (ìŠ¤ëª°í† í¬/ì‚°ìˆ /ì§§ì€ ì§ˆì˜) â”â”â”
                should_force, force_reason = force_chat_mode(query)
                if should_force:
                    metrics["mode"] = "chat"
                    metrics["force_chat_reason"] = force_reason
                    logger.info(f"ğŸ¯ AUTO ëª¨ë“œ: CHAT ê°•ì œ ì ìš© (ì´ìœ : {force_reason})")
                else:
                    # â”â”â” 2. ë„ë©”ì¸ í‚¤ì›Œë“œ + ì ˆëŒ€ê°’ ì„ê³„ê°’ ê¸°ë°˜ íŒë‹¨ â”â”â”
                    has_keyword = has_domain_keyword(query)
                    token_count = get_query_token_count(query)

                    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì ˆëŒ€ê°’ ì„ê³„ê°’ ì½ê¸°
                    use_absolute = os.getenv('RAG_MIN_SCORE_POLICY', 'normalized') == 'absolute'
                    bm25_min = float(os.getenv('BM25_MIN_ABS', '5.0'))
                    vec_min = float(os.getenv('VEC_MIN_ABS', '0.25'))

                    # ì ˆëŒ€ê°’ ì •ì±… ì‚¬ìš© ì‹œ (ê¶Œì¥)
                    if use_absolute:
                        # ì‹¤ì œ BM25/ë²¡í„° ìŠ¤ì½”ì–´ë¥¼ resultsì—ì„œ ì¶”ì¶œ ì‹œë„
                        # (í˜„ì¬ëŠ” fused scoreë§Œ ìˆìœ¼ë¯€ë¡œ ê°„ì†Œí™”)
                        # ì¼ë‹¨ top_scoreë¥¼ ë²¡í„° ìŠ¤ì½”ì–´ë¡œ ê°„ì£¼
                        pass_abs_threshold = top_score >= vec_min
                        pass_domain = has_keyword
                        pass_length = token_count >= 4

                        # ğŸ”’ Coverage Gate: ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì‹¤ì œë¡œ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ëŠ”ì§€ í™•ì¸
                        keyword_coverage = get_keyword_coverage(query, results)
                        min_coverage = int(os.getenv('MIN_KEYWORD_COVERAGE', '2'))
                        pass_coverage = keyword_coverage >= min_coverage

                        should_use_rag = pass_abs_threshold and pass_domain and pass_length and pass_coverage
                        metrics["mode"] = "rag" if should_use_rag else "chat"
                        metrics["keyword_coverage"] = keyword_coverage

                        logger.info(
                            f"ğŸ¯ AUTO ëª¨ë“œ (ì ˆëŒ€ê°’): top_score={top_score:.3f}, "
                            f"has_keyword={has_keyword}, token_count={token_count}, "
                            f"coverage={keyword_coverage}/{min_coverage}, "
                            f"threshold={vec_min}, selected_mode={metrics['mode']}"
                        )
                    else:
                        # ê¸°ì¡´ ì •ê·œí™” ì •ì±… (fallback)
                        rag_min_score = float(os.getenv('RAG_MIN_SCORE', '0.35'))
                        metrics["mode"] = "rag" if top_score >= rag_min_score else "chat"
                        logger.info(
                            f"ğŸ¯ AUTO ëª¨ë“œ (ì •ê·œí™”): top_score={top_score:.3f}, "
                            f"threshold={rag_min_score}, selected_mode={metrics['mode']}"
                        )

            elif mode_env == 'CHAT':
                metrics["mode"] = "chat"
                metrics["top_score"] = 0.0
            else:  # RAG, SUMMARIZE
                metrics["mode"] = "rag"
                metrics["top_score"] = results[0].get('score', 0.0) if results else 0.0

            # ğŸ¯ STEP 2: ëª¨ë“œ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìµœì í™”
            determined_mode = metrics.get("mode", "rag")
            logger.info(f"ğŸ¯ ëª¨ë“œ={determined_mode} â†’ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì‹œì‘")

            # Context Hydrator with mode-aware optimization
            from app.rag.utils.context_hydrator import hydrate_context
            hydrate_start = time.perf_counter()
            context, hydrator_metrics = hydrate_context(compressed, max_len=10000, mode=determined_mode)
            metrics["hydrate_time"] = time.perf_counter() - hydrate_start
            # Merge hydrator metrics into main metrics
            metrics.update({f"ctx_{k}": v for k, v in hydrator_metrics.items()})

            # ğŸ¯ STEP 3: ìƒì„± (ëª¨ë“œë³„ í† í° ì˜ˆì‚° ì ìš©)
            logger.info(f"ğŸ¯ ëª¨ë“œ={determined_mode} â†’ ìƒì„± ì‹œì‘")
            llm_gen_start = time.perf_counter()
            answer = self.generator.generate(query, context, temperature, mode=determined_mode)
            metrics["generate_time"] = time.perf_counter() - llm_gen_start

            # [DIAG] ìƒì„± ì™„ë£Œ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["mode"] = "normal"
                diagnostics["generate_path"] = "from_context"
                diagnostics["used_k"] = len(compressed)
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(
                        f"[DIAG] ìƒì„± ì™„ë£Œ: from_context ê²½ë¡œ, {len(compressed)}ê°œ ë¬¸ì„œ ì‚¬ìš©"
                    )

            total_latency = time.perf_counter() - start_time
            metrics["total_time"] = total_latency

            # ğŸš¨ ì„±ëŠ¥ ê°€ë“œ: ìŠ¬ë¡œ ì¿¼ë¦¬ ì„ê³„ê°’ ì²´í¬
            if total_latency > 10.0:
                logger.warning(
                    f"âš ï¸  SLOW_QUERY (>10s): {total_latency:.2f}s | "
                    f"query='{query[:50]}...' | "
                    f"search={metrics['search_time']:.2f}s, "
                    f"hydrate={metrics.get('hydrate_time', 0):.3f}s, "
                    f"generate={metrics['generate_time']:.2f}s"
                )
            elif total_latency > 3.0:
                logger.warning(
                    f"âš ï¸  SLOW_QUERY (>3s): {total_latency:.2f}s | "
                    f"query='{query[:50]}...'"
                )

            logger.info(
                f"RAG query completed in {total_latency:.2f}s "
                f"(search={metrics['search_time']:.2f}s, "
                f"compress={metrics['compress_time']:.2f}s, "
                f"hydrate={metrics.get('hydrate_time', 0):.3f}s, "
                f"generate={metrics['generate_time']:.2f}s)"
            )

            # CHAT ëª¨ë“œì¼ ê²½ìš° ì¶œì²˜ ì œê±° (ì¼ë°˜ ëŒ€í™”ëŠ” ë¬¸ì„œ ì¸ìš© ë¶ˆí•„ìš”)
            final_source_docs = [] if determined_mode == "chat" else [c.get("doc_id") for c in results[:3]]
            final_evidence_chunks = [] if determined_mode == "chat" else compressed

            return RAGResponse(
                answer=answer,
                source_docs=final_source_docs,
                evidence_chunks=final_evidence_chunks,  # UIìš© ê·¼ê±°
                raw_results=results,  # Evidence ìµœì†Œ ë³´ì¥ìš©
                latency=total_latency,
                success=True,
                metrics=metrics,
                diagnostics=diagnostics,
            )

        except SearchError as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_RETRIEVE] ê²€ìƒ‰ ì‹¤íŒ¨: {e.message}",
                latency=time.perf_counter() - start_time,
            )

        except ModelError as e:
            logger.error(f"Model inference failed: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_GENERATE] ìƒì„± ì‹¤íŒ¨: {e.message}",
                latency=time.perf_counter() - start_time,
            )

        except Exception as e:
            logger.error(f"Unexpected error: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_UNKNOWN] {str(e)}",
                latency=time.perf_counter() - start_time,
            )

    def _make_response(
        self, text: str, selected: List[Dict[str, Any]], retrieved: List[Dict[str, Any]]
    ) -> dict:
        """í‘œì¤€ ì‘ë‹µ êµ¬ì¡° ìƒì„± (citations í¬í•¨)

        Args:
            text: ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
            selected: ì‹¤ì œ ì‚¬ìš©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ì••ì¶• í›„)
            retrieved: ê²€ìƒ‰ëœ ì›ë³¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            í‘œì¤€í™”ëœ ì‘ë‹µ dict (citations í•„ìˆ˜)
        """
        citations = []
        for c in selected:
            filename = c.get("filename") or c.get("doc_id") or c.get("title", "")
            ref = _encode_file_ref(filename) if filename else None

            citations.append({
                "doc_id": c.get("doc_id"),
                "filename": filename,
                "title": c.get("title") or filename or c.get("doc_id"),
                "page": c.get("page", 1),
                "snippet": (
                    c.get("text") or c.get("snippet") or c.get("content") or ""
                )[:400],
                "ref": ref,  # ğŸ”´ base64 ì¸ì½”ë”©ëœ íŒŒì¼ ê²½ë¡œ
                "preview_url": c.get("preview_url"),
                "download_url": c.get("download_url"),
            })

        return {
            "text": text,
            "citations": citations,  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
            "evidence": citations,  # í•˜ìœ„ í˜¸í™˜ì„± (ë™ì¼ ë°ì´í„°)
            "status": {
                "retrieved_count": len(retrieved),
                "selected_count": len(selected),
                "found": len(selected) > 0,  # ğŸ”´ ìœ ì¼í•œ íŒì • ê¸°ì¤€
            },
        }

    def answer(self, query: str, top_k: Optional[int] = None) -> dict:
        """ë‹µë³€ ìƒì„± (Evidence í¬í•¨ êµ¬ì¡°í™”ëœ ì‘ë‹µ)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ (Noneì´ë©´ ê¸°ë³¸ê°’ 5)

        Returns:
            dict: {
                "text": ë‹µë³€ í…ìŠ¤íŠ¸,
                "citations": ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í‘œì¤€ í‚¤),
                "evidence": ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í•˜ìœ„ í˜¸í™˜),
                "status": {
                    "retrieved_count": int,
                    "selected_count": int,
                    "found": bool
                }
            }
        """
        # ğŸ”¥ CRITICAL: ê¸°ì•ˆì/ë‚ ì§œ ê²€ìƒ‰ì€ QuickFixRAGì— ìœ„ì„ (ì „ë¬¸ ë¡œì§ ë³´ìœ )
        if hasattr(self.generator, "rag"):
            import re
            import sqlite3

            # âœ… í™•ì¥ëœ ì¿¼ë¦¬ì—ì„œ ì‹¤ì œ ì§ˆë¬¸ ì¶”ì¶œ (chat_interface.py ëŒ€ì‘)
            actual_query = query
            if "í˜„ì¬ ì§ˆë¬¸:" in query:
                parts = query.split("í˜„ì¬ ì§ˆë¬¸:")
                if len(parts) > 1:
                    actual_query = parts[-1].strip()
                    logger.info(f"ğŸ“ í™•ì¥ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œ: '{actual_query[:50]}'")

            # ğŸ¯ ëª¨ë“œ ë¼ìš°íŒ…: Q&A ì˜ë„ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ íŒŒì¼ëª…ì´ ìˆì–´ë„ Q&A ëª¨ë“œ ìš°ì„ 
            query_mode = self.query_router.classify_mode(actual_query)
            router_reason = self.query_router.get_routing_reason(actual_query)
            logger.info(
                f"ğŸ”€ ë¼ìš°íŒ… ê²°ê³¼: mode={query_mode.value}, reason={router_reason}"
            )

            # ğŸ’° COST_SUM ëª¨ë“œ: ë¹„ìš© í•©ê³„ ì§ì ‘ ì¡°íšŒ
            if query_mode == QueryMode.COST_SUM:
                return self._answer_cost_sum(actual_query)

            # ğŸ“‹ LIST ëª¨ë“œ: ëª©ë¡ ê²€ìƒ‰ (2ì¤„ ì¹´ë“œ í˜•ì‹)
            if query_mode == QueryMode.LIST:
                return self._answer_list(actual_query)

            # ğŸ“ SUMMARY ëª¨ë“œ: ë‚´ìš© ìš”ì•½ (5ì¤„ ì„¹ì…˜)
            if query_mode == QueryMode.SUMMARY:
                return self._answer_summary(actual_query)

            # ğŸ‘€ PREVIEW ëª¨ë“œ: ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° (ì›ë¬¸ 6-8ì¤„, ê°€ì§œ í‘œ ê¸ˆì§€)
            if query_mode == QueryMode.PREVIEW:
                return self._answer_preview(actual_query)

            # ğŸ” ë””ë²„ê¹…: ì‹¤ì œ pattern matching ëŒ€ìƒ ë¡œê¹…
            logger.info(f"ğŸ” Pattern matching ëŒ€ìƒ ì¿¼ë¦¬: '{actual_query[:100]}'")

            # âœ… P0: íŒŒì¼ëª… ì§ì ‘ ì–¸ê¸‰ íŒ¨í„´ ê°ì§€ (ë ˆê±°ì‹œ í˜¸í™˜, PREVIEW ëª¨ë“œ ì™¸)
            if False:  # ë¹„í™œì„±í™”: PREVIEW ëª¨ë“œë¡œ í†µí•©ë¨
                # íŒ¨í„´ 1: ìš”ì•½ ìš”ì²­ - "íŒŒì¼ëª….pdf ë‚´ìš© ìš”ì•½í•´ì¤˜" / "íŒŒì¼ëª….pdf ìš”ì•½"
                file_summary_pattern = (
                    r"(\S+\.pdf)\s*(ì´\s*)?(ë¬¸ì„œ\s*)?(ë‚´ìš©\s*)?(ìš”ì•½|ì •ë¦¬)"
                )
                summary_match = re.search(
                    file_summary_pattern, actual_query, re.IGNORECASE
                )

                if summary_match:
                    filename = summary_match.group(1).strip()
                    logger.info(f"ğŸ¯ P0: íŒŒì¼ ìš”ì•½ ìš”ì²­ ê°ì§€ - {filename}")

                    # PDF ì „ë¬¸ ë¡œë“œ + ë©”íƒ€ë°ì´í„° ì¡°íšŒ
                    try:
                        import pdfplumber
                        from pathlib import Path

                        conn = sqlite3.connect("metadata.db")
                        cursor = conn.cursor()
                        cursor.execute(
                            """
                            SELECT path, filename, drafter, date, category, text_preview
                            FROM documents
                            WHERE filename LIKE ?
                            LIMIT 1
                        """,
                            (f"%{filename}%",),
                        )

                        result = cursor.fetchone()
                        conn.close()

                        if result:
                            pdf_path, fname, drafter, date, category, preview = result

                            # PDF ì „ë¬¸ í…ìŠ¤íŠ¸ ë¡œë“œ
                            full_text = preview or ""
                            if pdf_path and Path(pdf_path).exists():
                                try:
                                    with pdfplumber.open(pdf_path) as pdf:
                                        pages_text = []
                                        for page in pdf.pages[:5]:  # ìµœëŒ€ 5í˜ì´ì§€
                                            page_text = page.extract_text() or ""
                                            pages_text.append(page_text)
                                            if len("".join(pages_text)) > 5000:
                                                break
                                        full_text = "\n\n".join(pages_text)
                                except Exception as e:
                                    logger.warning(f"PDF ì½ê¸° ì‹¤íŒ¨: {e}")

                            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (LLM ì—†ì´)
                            answer_text = f"**ğŸ“„ {fname}**\n\n"
                            answer_text += "**ğŸ“‹ ë¬¸ì„œ ì •ë³´**\n"
                            answer_text += f"- **ê¸°ì•ˆì:** {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += f"- **ë‚ ì§œ:** {date or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += (
                                f"- **ì¹´í…Œê³ ë¦¬:** {category or 'ì •ë³´ ì—†ìŒ'}\n"
                            )

                            answer_text += "\n**ğŸ“ ì£¼ìš” ë‚´ìš©**\n"
                            # ì²˜ìŒ 800ì ë¯¸ë¦¬ë³´ê¸°
                            content_preview = full_text[:800].strip()
                            if content_preview:
                                answer_text += content_preview
                                if len(full_text) > 800:
                                    answer_text += (
                                        "...\n\n*(ì „ì²´ ë¬¸ì„œëŠ” ë” ê¸´ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤)*"
                                    )
                            else:
                                answer_text += "*(ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤)*"

                            # Evidence êµ¬ì„±
                            evidence = [
                                {
                                    "doc_id": fname,
                                    "page": 1,
                                    "snippet": full_text[:400],  # 500 â†’ 400 (ìŠ¤ë‹ˆí« ì¼ê´€ì„±)
                                    "meta": {
                                        "filename": fname,
                                        "drafter": drafter,
                                        "date": date,
                                        "category": category,
                                    },
                                }
                            ]

                            return {
                                "text": answer_text,
                                "citations": [fname],
                                "evidence": evidence,
                                "status": {
                                    "retrieved_count": 1,
                                    "selected_count": 1,
                                    "found": True,
                                },
                            }
                        else:
                            logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {filename}")

                    except Exception as e:
                        logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
                        # ì˜¤ë¥˜ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±

                # íŒ¨í„´ 2: ê¸°ì•ˆì ì§ˆì˜ - "íŒŒì¼ëª….pdf ê¸°ì•ˆìê°€ ëˆ„êµ¬ì•¼?"
                # (ì»¬ëŸ¼ ìˆ˜ì • ì™„ë£Œ: doc_number ì œê±°)
                file_author_pattern = r"(\S+\.pdf)\s*(ê¸°ì•ˆì|ì‘ì„±ì).*(ëˆ„êµ¬|ì•Œë ¤ì¤˜)"
                file_match = re.search(file_author_pattern, actual_query, re.IGNORECASE)

                if file_match:
                    filename = file_match.group(1).strip()
                    logger.info(f"ğŸ¯ P0: íŒŒì¼ëª… ì§ì ‘ ì§ˆì˜ ê°ì§€ - {filename}")

                    # metadata.dbì—ì„œ ì§ì ‘ ì¡°íšŒ
                    try:
                        conn = sqlite3.connect("metadata.db")
                        cursor = conn.cursor()
                        cursor.execute(
                            """
                            SELECT filename, drafter, date, category
                            FROM documents
                            WHERE filename LIKE ?
                            LIMIT 1
                        """,
                            (f"%{filename}%",),
                        )

                        result = cursor.fetchone()
                        conn.close()

                        if result:
                            fname, drafter, date, category = result
                            answer_text = f"**{fname}**\n\n"
                            answer_text += f"ğŸ“Œ **ê¸°ì•ˆì:** {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += f"ğŸ“… **ë‚ ì§œ:** {date or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += (
                                f"ğŸ“ **ì¹´í…Œê³ ë¦¬:** {category or 'ì •ë³´ ì—†ìŒ'}\n"
                            )

                            # Evidence êµ¬ì„± (ì •í™•í•œ íŒŒì¼ 1ê±´)
                            evidence = [
                                {
                                    "doc_id": fname,
                                    "page": 1,
                                    "snippet": f"ê¸°ì•ˆì: {drafter}, ë‚ ì§œ: {date}, ì¹´í…Œê³ ë¦¬: {category}",
                                    "meta": {
                                        "filename": fname,
                                        "drafter": drafter,
                                        "date": date,
                                        "category": category,
                                    },
                                }
                            ]

                            return {
                                "text": answer_text,
                                "citations": [fname],
                                "evidence": evidence,
                                "status": {
                                    "retrieved_count": 1,
                                    "selected_count": 1,
                                    "found": True,
                                },
                            }
                        else:
                            logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {filename}")
                            return {
                                "text": f"âŒ '{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                                "citations": [],
                                "evidence": [],
                                "status": {
                                    "retrieved_count": 0,
                                    "selected_count": 0,
                                    "found": False,
                                },
                            }

                    except Exception as e:
                        logger.error(f"âŒ metadata ì¡°íšŒ ì‹¤íŒ¨: {e}")
                        # ì˜¤ë¥˜ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±

            # LEGACY CODE - DISABLED (êµ¬í˜„ ë¯¸ì™„ì„±, self.generator.rag ë¯¸ì¡´ì¬)
            # ê¸°ì•ˆì/ë‚ ì§œ ê²€ìƒ‰ íŒ¨í„´ì€ ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ì—¬ ì˜¤íƒ ë°œìƒ
            # ì˜ˆ: "ë¯¸ëŸ¬í´ë© ì¹´ë©”ë¼ ê²€í† ì„œ"ê°€ author íŒ¨í„´ì— ë§¤ì¹­ë˜ì–´ ì˜¤ë¥˜ ë°œìƒ
            # TODO: ì •í™•í•œ ê¸°ì•ˆì ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ metadata ê¸°ë°˜ í•„í„°ë§ í™œìš©
            pass
            # author_patterns = [
            #     r"([ê°€-í£]{2,4})\s*(ë¬¸ì„œ|ê¸°ì•ˆì„œ|ê²€í† ì„œ)",
            #     r"([ê°€-í£]{2,4})ê°€?\s*(ì‘ì„±í•œ|ì‘ì„±ì•ˆ|ê¸°ì•ˆí•œ|ì“´|ë§Œë“ )",
            #     r"(ê¸°ì•ˆì|ì‘ì„±ì|ì œì•ˆì)[:\s]+([ê°€-í£]{2,4})",
            # ]
            # year_pattern = r"(\d{4})\s*ë…„"
            # is_author_query = any(re.search(p, actual_query) for p in author_patterns)
            # is_year_query = re.search(year_pattern, actual_query)
            # if is_author_query or is_year_query:
            #     logger.info(f"ğŸ¯ íŠ¹ìˆ˜ ê²€ìƒ‰ ëª¨ë“œ ê°ì§€: author={is_author_query}, year={is_year_query}")
            #     # QuickFixRAG.answer()ë¡œ ì§ì ‘ ì²˜ë¦¬ (self.generator.rag ë¯¸ì¡´ì¬ë¡œ ì˜¤ë¥˜)
            #     answer_text = self.generator.rag.answer(actual_query, use_llm_summary=False)
            #     return {...}

        # ì¼ë°˜ ì¿¼ë¦¬ëŠ” ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
        response = self.query(query, top_k=top_k or 5)

        if response.success:
            # ê²€ìƒ‰/ì••ì¶•ì—ì„œ ë„˜ì–´ì˜¨ ì •ê·œí™” ì²­í¬ ì‚¬ìš© (ì‹¤ì œ page/snippet/meta ë…¸ì¶œ)
            evidence = [
                {
                    "doc_id": c.get("doc_id"),
                    "page": c.get("page", 1),
                    "snippet": c.get("snippet", ""),
                    "meta": c.get(
                        "meta", {"doc_id": c.get("doc_id"), "page": c.get("page", 1)}
                    ),
                }
                for c in (response.evidence_chunks or [])
            ]

            # CRITICAL: Evidence ìµœì†Œ ë³´ì¥ (sources_citedê°€ ë¹„ì–´ë„ ê²€ìƒ‰ ê²°ê³¼ëŠ” í‘œì‹œ)
            evidence_injected = False
            if not evidence and response.raw_results:
                logger.info("Evidence empty, using raw_results[:3] as fallback")
                evidence = [
                    {
                        "doc_id": r.get("doc_id") or r.get("chunk_id", "unknown"),
                        "page": 0,  # ê²€ìƒ‰ ê²°ê³¼ëŠ” í˜ì´ì§€ ì •ë³´ ì—†ìŒ
                        "snippet": r.get("snippet") or r.get("text_preview", "")[:400],  # 500 â†’ 400 (ìŠ¤ë‹ˆí« ì¼ê´€ì„±)
                        "meta": {
                            "doc_id": r.get("doc_id") or r.get("chunk_id", "unknown"),
                            "filename": r.get("filename", ""),
                            "page": 0,
                        },
                    }
                    for r in response.raw_results[:3]
                ]
                evidence_injected = True

            # [DIAG] Evidence ì§„ë‹¨ ì •ë³´ ì¶”ê°€
            if DIAG_RAG and response.diagnostics:
                response.diagnostics["evidence_count"] = len(evidence)
                response.diagnostics["evidence_injected"] = evidence_injected

            # ğŸ”¥ CRITICAL: status.found í”Œë˜ê·¸ - UI íŒì • ë‹¨ì¼ ì†ŒìŠ¤
            # retrieved_count: ê²€ìƒ‰ëœ ì›ë³¸ ê²°ê³¼ ìˆ˜
            # selected_count: ì‹¤ì œ ì‚¬ìš©ëœ ì¦ê±° ìˆ˜ (evidence)
            # found: ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€ (evidenceê°€ 1ê°œ ì´ìƒì´ë©´ True)
            status = {
                "retrieved_count": len(response.raw_results or []),
                "selected_count": len(evidence),
                "found": len(evidence) > 0,  # ğŸ”´ ìœ ì¼í•œ íŒì • ê¸°ì¤€
            }

            # ìš´ì˜ í‘œì¤€ 1í–‰ ìš”ì•½ ë¡œê·¸ (í•„ìˆ˜)
            import re

            author_mode = bool(re.search(r"(ì‘ì„±ì|ê¸°ì•ˆì|ì œì•ˆì)", query))
            search_ms = int(response.metrics.get("search_time", 0) * 1000)
            generate_ms = int(response.metrics.get("generate_time", 0) * 1000)
            total_ms = int(response.latency * 1000)

            logger.info(
                f'[RAG] query="{query[:50]}..." | '
                f"retrieved={status['retrieved_count']} | "
                f"selected={status['selected_count']} | "
                f"found={status['found']} | "
                f"author_mode={author_mode} | "
                f"backfill={evidence_injected} | "
                f"search_ms={search_ms} | "
                f"generate_ms={generate_ms} | "
                f"total_ms={total_ms}"
            )

            return {
                "text": response.answer,
                "citations": evidence,  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
                "evidence": evidence,  # í•˜ìœ„ í˜¸í™˜ì„± (ë™ì¼ ë°ì´í„°)
                "status": status,  # UIì—ì„œ ì´ê²ƒë§Œ í™•ì¸
                "diagnostics": response.diagnostics if DIAG_RAG else {},
            }
        else:
            # ì—ëŸ¬ ë°œìƒ ì‹œ (ì¤‘ë¦½ í†¤, ì‚¬ê³¼ í‘œí˜„ ê¸ˆì§€)
            error_msg = ERROR_MESSAGES.get(
                ErrorCode.E_GENERATE, "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆë‹¤."
            )
            if response.error:
                error_msg = f"{error_msg}\n\nìƒì„¸: {response.error}"

            # ìš´ì˜ í‘œì¤€ ë¡œê·¸ (ì—ëŸ¬ ì¼€ì´ìŠ¤)
            logger.error(
                f'[RAG] query="{query[:50]}..." | '
                f'status=ERROR | error="{response.error}"'
            )

            return {
                "text": error_msg,
                "citations": [],  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
                "evidence": [],  # í•˜ìœ„ í˜¸í™˜ì„±
                "status": {"retrieved_count": 0, "selected_count": 0, "found": False},
            }

    def answer_text(self, query: str) -> str:
        """ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            str: ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
        """
        result = self.answer(query)
        return result["text"]

    def _answer_list(self, query: str) -> dict:
        """ëª©ë¡ ê²€ìƒ‰ (2ì¤„ ì¹´ë“œ í˜•ì‹) - Closed-World Validation ì ìš©

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "2024ë…„ ë‚¨ì¤€ìˆ˜ ë¬¸ì„œ ì°¾ì•„ì¤˜", "year:2024 drafter:ìµœìƒˆë¦„")

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡° (2ì¤„ ì¹´ë“œ ëª©ë¡)
        """
        from modules.metadata_db import MetadataDB
        from app.rag.query_parser import QueryParser

        try:
            # ğŸ”’ Closed-World Validation: ì¿¼ë¦¬ íŒŒì‹±
            parser = QueryParser(self.known_drafters)
            filters = parser.parse_filters(query)

            year = filters['year']
            drafter = filters['drafter']
            source = filters['source']

            # 'ì „ë¶€', 'ì „ì²´' ë“± ëª…ì‹œ ì‹œ limit í™•ì¥
            if any(keyword in query for keyword in ['ì „ë¶€', 'ì „ì²´', 'ëª¨ë“ ', 'ëª¨ë‘']):
                limit = 200  # ì „ì²´ í‘œì‹œ (ìµœëŒ€ 200ê°œ)
                display_limit = 200  # Evidenceë„ 200ê°œê¹Œì§€
            else:
                limit = 20  # ê¸°ë³¸ í˜ì´ì§€ í¬ê¸°
                display_limit = 20

            logger.info(f"ğŸ“‹ ëª©ë¡ ê²€ìƒ‰: year={year}, drafter={drafter}, source={source}, limit={limit}, display={display_limit}")

            # DB ê²€ìƒ‰
            db = MetadataDB()
            docs = db.search_documents(drafter=drafter, year=year, limit=limit)

            # ì „ì²´ ì¹´ìš´íŠ¸ ì¡°íšŒ
            total_count = db.count_documents(drafter=drafter, year=year)

            if not docs:
                return {
                    "mode": "LIST",
                    "text": f"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (year={year}, drafter={drafter})",
                    "files": [],
                    "count": 0,
                    "total_count": 0,
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            # 2ì¤„ ì¹´ë“œ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ… (íŒŒì¼ëª… ê¸°ë°˜ í•µì‹¬ ìš”ì•½)
            cards = []
            for doc in docs:
                filename = doc.get("filename", "ì•Œ ìˆ˜ ì—†ìŒ")
                doctype = doc.get("doctype", "ë¬¸ì„œ")
                date = doc.get("display_date") or doc.get("date", "ë‚ ì§œ ì—†ìŒ")
                drafter_name = doc.get("drafter", "ì‘ì„±ì ë¯¸ìƒ")

                # íŒŒì¼ëª…ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ (ë‚ ì§œ ì œê±°, ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ)
                import re
                # ë‚ ì§œ íŒ¨í„´ ì œê±° (YYYY-MM-DD_)
                title = re.sub(r'^\d{4}-\d{2}-\d{2}_', '', filename)
                # .pdf í™•ì¥ì ì œê±°
                title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
                # ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
                title = title.replace('_', ' ')

                # 2ì¤„ ì¹´ë“œ: ì œëª© + ë©”íƒ€ì •ë³´
                card = f"**{title}**\nğŸ· {doctype} Â· ğŸ“… {date} Â· âœ {drafter_name}"
                cards.append(card)

            answer_text = "\n\n".join(cards[:display_limit])  # display_limit ì ìš©

            # Evidence êµ¬ì„± (íŒŒì¼ëª… ê¸°ë°˜ ìš”ì•½ + ì‹¤ì œ íŒŒì¼ ê²½ë¡œ)
            evidence = []
            for doc in docs[:display_limit]:
                filename = doc.get("filename", "")

                # íŒŒì¼ëª…ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ (ë‹µë³€ í…ìŠ¤íŠ¸ì™€ ë™ì¼í•œ ë°©ì‹)
                import re
                title = re.sub(r'^\d{4}-\d{2}-\d{2}_', '', filename)
                title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
                title = title.replace('_', ' ')

                # snippetì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš© (ê°„ê²°í•˜ê³  ì˜ë¯¸ ìˆëŠ” ì •ë³´)
                snippet = title[:160]

                # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ìƒì„± (year í´ë” ìë™ ê°ì§€)
                year_match = re.search(r'(\d{4})-', filename)
                if year_match:
                    year = year_match.group(1)
                    file_path_str = f"docs/year_{year}/{filename}"
                else:
                    file_path_str = f"docs/{filename}"

                evidence.append({
                    "doc_id": filename,
                    "filename": filename,
                    "file_path": file_path_str,  # â† ì‹¤ì œ íŒŒì¼ ê²½ë¡œ (Streamlit ë‚´ì¥ ë°©ì‹)
                    "page": 1,
                    "snippet": snippet,
                    "ref": None,  # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (FastAPI ë°©ì‹ ì œê±°)
                    "meta": {
                        "filename": filename,
                        "drafter": doc.get("drafter"),
                        "date": doc.get("display_date") or doc.get("date"),
                        "doctype": doc.get("doctype")
                    }
                })

            # íŒŒì¼ ëª©ë¡ ì¶”ì¶œ
            file_list = [doc.get("filename") for doc in docs if doc.get("filename")]

            # í’ˆì§ˆ ë°©ì–´ì„  ë¡œê·¸ (ì¬í˜„ ìš©ì´ì„±)
            logger.info({
                "mode": "LIST",
                "files": file_list[:3],
                "count": len(docs),
                "total_count": total_count,
                "llm": os.getenv("LLM_ENABLED", "false").lower() == "true"
            })

            # total_count ì •ë³´ ì¶”ê°€
            if total_count > len(docs):
                answer_text = f"ğŸ“Š **ì „ì²´ {total_count}ê±´ ì¤‘ {len(docs)}ê±´ í‘œì‹œ**\n\n" + answer_text

            return {
                "mode": "LIST",
                "text": answer_text,
                "files": file_list,
                "count": len(docs),
                "total_count": total_count,
                "citations": evidence,
                "evidence": evidence,
                "status": {
                    "retrieved_count": len(docs),
                    "selected_count": min(10, len(docs)),
                    "found": True
                }
            }

        except Exception as e:
            logger.error(f"âŒ ëª©ë¡ ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "text": f"ëª©ë¡ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def _answer_cost_sum(self, query: str) -> dict:
        """ë¹„ìš© í•©ê³„ ì§ì ‘ ì¡°íšŒ (DB claimed_total í™œìš©)

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "ì±„ë„ì—ì´ ì¤‘ê³„ì°¨ ë³´ìˆ˜ í•©ê³„ ì–¼ë§ˆì˜€ì§€?")

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡° (text, citations, evidence, status)
        """
        try:
            # 1. ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ ë¬¸ì„œ ì°¾ê¸°
            search_results = self.retriever.search(query, top_k=3)

            if not search_results:
                logger.warning(f"ë¹„ìš© ì§ˆì˜ ê²€ìƒ‰ ì‹¤íŒ¨: {query}")
                return {
                    "text": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            # 2. DBì—ì„œ claimed_total ì¡°íšŒ
            from modules.metadata_db import MetadataDB
            db = MetadataDB()

            for result in search_results:
                filename = result.get("meta", {}).get("filename") or result.get("doc_id", "")
                if not filename:
                    continue

                doc = db.get_by_filename(filename)
                if doc and doc.get("claimed_total"):
                    claimed_total = doc["claimed_total"]

                    # 3. ë‹µë³€ í¬ë§·íŒ… (VAT, ê²€ì¦ ë°°ì§€ í¬í•¨)
                    # VAT íŒë‹¨ (text_previewì—ì„œ "VAT" í‚¤ì›Œë“œ ê²€ìƒ‰)
                    text_preview = doc.get("text_preview", "")
                    vat_status = "VAT ë³„ë„" if "VAT" in text_preview or "ë¶€ê°€ì„¸" in text_preview else "VAT í¬í•¨ ì¶”ì •"

                    # sum_match ê²€ì¦ ë°°ì§€
                    sum_match = doc.get("sum_match")
                    if sum_match is None:
                        verification = "sum_match=ì—†ìŒ"
                    elif sum_match:
                        verification = "sum_match=ì¼ì¹˜ âœ…"
                    else:
                        verification = "sum_match=ë¶ˆì¼ì¹˜ âš ï¸"

                    answer_text = f"ğŸ’° í•©ê³„: **â‚©{claimed_total:,}** ({vat_status})\n"
                    answer_text += f"ì¶œì²˜: {filename} | ë‚ ì§œ: {doc.get('display_date') or doc.get('date') or 'ì •ë³´ ì—†ìŒ'} | ê¸°ì•ˆì: {doc.get('drafter') or 'ì •ë³´ ì—†ìŒ'}\n"
                    answer_text += f"ê²€ì¦: {verification}"

                    # Evidence êµ¬ì„±
                    ref = _encode_file_ref(filename)
                    evidence = [{
                        "doc_id": filename,
                        "filename": filename,
                        "page": 1,
                        "snippet": f"ë¹„ìš© í•©ê³„: â‚©{claimed_total:,}",
                        "ref": ref,  # ğŸ”´ base64 ì¸ì½”ë”©ëœ íŒŒì¼ ê²½ë¡œ
                        "meta": {
                            "filename": filename,
                            "drafter": doc.get("drafter"),
                            "date": doc.get("display_date") or doc.get("date"),
                            "claimed_total": claimed_total
                        }
                    }]

                    logger.info(f"ğŸ’° ë¹„ìš© ì§ˆì˜ ì„±ê³µ: {filename} â†’ â‚©{claimed_total:,}")

                    return {
                        "text": answer_text,
                        "citations": evidence,
                        "evidence": evidence,
                        "status": {
                            "retrieved_count": len(search_results),
                            "selected_count": 1,
                            "found": True
                        }
                    }

            # claimed_total ì—†ëŠ” ê²½ìš°
            logger.warning(f"ê²€ìƒ‰ëœ ë¬¸ì„œì— ë¹„ìš© ì •ë³´ ì—†ìŒ: {[r.get('doc_id') for r in search_results]}")
            return {
                "text": "ê²€ìƒ‰ëœ ë¬¸ì„œì— ë¹„ìš© í•©ê³„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": len(search_results),
                    "selected_count": 0,
                    "found": False
                }
            }

        except Exception as e:
            logger.error(f"âŒ ë¹„ìš© ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "text": f"ë¹„ìš© ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def _answer_preview(self, query: str) -> dict:
        """ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° (ì›ë¬¸ ì¸ìš©, ê°€ì§œ í‘œ ìƒì„± ê¸ˆì§€)

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "[íŒŒì¼ëª…].pdf ë¯¸ë¦¬ë³´ê¸°")

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡° (ì›ë¬¸ 6-8ì¤„)
        """
        import re
        import sqlite3
        from pathlib import Path

        try:
            # íŒŒì¼ëª… ì¶”ì¶œ
            filename_match = re.search(r"(\S+\.pdf)", query, re.IGNORECASE)
            if not filename_match:
                return {
                    "text": "íŒŒì¼ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            filename = filename_match.group(1)

            # DBì—ì„œ ë¬¸ì„œ ì¡°íšŒ
            conn = sqlite3.connect("metadata.db")
            cursor = conn.cursor()
            cursor.execute(
                """
                SELECT path, filename, drafter, date, display_date, text_preview
                FROM documents
                WHERE filename LIKE ?
                LIMIT 1
            """,
                (f"%{filename}%",),
            )

            result = cursor.fetchone()
            conn.close()

            if not result:
                return {
                    "text": f"'{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            pdf_path, fname, drafter, date, display_date, text_preview = result

            # ì›ë¬¸ 6-8ì¤„ ì¶”ì¶œ (cleaned_text)
            preview_text = text_preview or ""

            # ê°œí–‰ ê¸°ì¤€ 6-8ì¤„ ì¶”ì¶œ
            lines = [line.strip() for line in preview_text.split('\n') if line.strip()]
            preview_lines = lines[:8]  # ìµœëŒ€ 8ì¤„

            if len(preview_lines) == 0:
                preview_content = "(ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤)"
            else:
                preview_content = "\n".join(preview_lines)

            # ë‹µë³€ í¬ë§·íŒ… (ê°€ì§œ í‘œ ìƒì„± ì ˆëŒ€ ê¸ˆì§€)
            answer_text = f"**ğŸ“„ {fname} ë¯¸ë¦¬ë³´ê¸°**\n\n"
            answer_text += preview_content

            # Evidence êµ¬ì„±
            ref = _encode_file_ref(fname)
            evidence = [{
                "doc_id": fname,
                "filename": fname,
                "page": 1,
                "snippet": preview_content[:400],
                "ref": ref,  # ğŸ”´ base64 ì¸ì½”ë”©ëœ íŒŒì¼ ê²½ë¡œ
                "meta": {
                    "filename": fname,
                    "drafter": drafter,
                    "date": display_date or date
                }
            }]

            # í’ˆì§ˆ ë°©ì–´ì„  ë¡œê·¸
            logger.info({
                "mode": "PREVIEW",
                "files": [fname],
                "lines": len(preview_lines),
                "llm": False  # PREVIEWëŠ” LLM ì‚¬ìš© ì•ˆ í•¨
            })

            return {
                "text": answer_text,
                "citations": evidence,
                "evidence": evidence,
                "status": {
                    "retrieved_count": 1,
                    "selected_count": 1,
                    "found": True
                }
            }

        except Exception as e:
            logger.error(f"âŒ ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "text": f"ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def _safe_fname(self, meta: dict = None, doc_path: str = None) -> str:
        """íŒŒì¼ëª… ì•ˆì „ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì‹œë„)

        Args:
            meta: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            doc_path: ë¬¸ì„œ ê²½ë¡œ

        Returns:
            ì•ˆì „í•˜ê²Œ ì¶”ì¶œëœ íŒŒì¼ëª… (ê¸°ë³¸ê°’: 'ë¯¸ìƒ ë¬¸ì„œ')
        """
        import os

        meta = meta or {}

        # ë‹¤ì–‘í•œ í•„ë“œì—ì„œ íŒŒì¼ëª… ì‹œë„
        fname = (
            meta.get("fname")
            or meta.get("filename")
            or meta.get("doc_id")
            or (os.path.basename(doc_path) if doc_path else None)
            or "ë¯¸ìƒ ë¬¸ì„œ"
        )

        return fname

    def _make_chunks_for_doc(self, filename: str) -> list:
        """íŠ¹ì • ë¬¸ì„œì˜ ì²­í¬ë§Œ ë¡œë“œ (ë¬¸ì„œ ê³ ì • ëª¨ë“œìš©)

        Args:
            filename: ë¬¸ì„œ íŒŒì¼ëª…

        Returns:
            í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # HybridRetrieverë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ ê²€ìƒ‰
            # íŒŒì¼ëª…ì„ ì¿¼ë¦¬ë¡œ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰
            search_query = filename.replace('.pdf', '').replace('_', ' ')
            results = self.retriever.search(search_query, top_k=20)

            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•´ë‹¹ ë¬¸ì„œë¡œ í•„í„°ë§
            chunks = []
            for result in results:
                # doc_id ë˜ëŠ” meta.filenameì´ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ í¬í•¨
                doc_id = result.get('doc_id', '')
                meta_filename = result.get('meta', {}).get('filename', '')

                if filename in doc_id or filename in meta_filename:
                    chunks.append({
                        'doc_id': result.get('doc_id', filename),
                        'page': result.get('page', 1),
                        'text': result.get('snippet', result.get('text', '')),
                        'score': result.get('score', 0.0),
                        'filename': filename
                    })

            if not chunks:
                logger.warning(f"âš ï¸ ë¬¸ì„œ ì²­í¬ ì—†ìŒ: {filename}")

            return chunks

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def _extract_with_ocr(self, pdf_path: str, start_page: int, total_pages: int) -> str:
        """OCRì„ ì‚¬ìš©í•˜ì—¬ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (pytesseract ìš°ì„ , paddleocr í´ë°±)

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            start_page: ì‹œì‘ í˜ì´ì§€ (0-based)
            total_pages: ì „ì²´ í˜ì´ì§€ ìˆ˜

        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        try:
            from pdf2image import convert_from_path
            import pytesseract
            from PIL import Image

            # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ë 3í˜ì´ì§€ë§Œ)
            images = convert_from_path(
                pdf_path,
                first_page=start_page + 1,  # 1-based
                last_page=total_pages
            )

            text = ""
            for i, img in enumerate(images):
                try:
                    # pytesseract ì‚¬ìš©
                    page_text = pytesseract.image_to_string(img, lang='kor+eng')
                    text += page_text + "\n"
                    logger.info(f"âœ“ OCR (pytesseract) í˜ì´ì§€ {start_page + i + 1}: {len(page_text)}ì")
                except Exception as e:
                    logger.warning(f"âš ï¸ pytesseract ì‹¤íŒ¨ (í˜ì´ì§€ {start_page + i + 1}): {e}")

            if len(text.strip()) > 50:
                return text

            # pytesseract ì‹¤íŒ¨ ì‹œ paddleocr ì‹œë„
            logger.info("ğŸ”„ paddleocr í´ë°± ì‹œë„...")
            try:
                from paddleocr import PaddleOCR
                ocr = PaddleOCR(use_angle_cls=True, lang='korean')

                text = ""
                for i, img in enumerate(images):
                    # PaddleOCRëŠ” íŒŒì¼ ê²½ë¡œ ë˜ëŠ” numpy arrayë¥¼ ë°›ìŒ
                    import numpy as np
                    img_array = np.array(img)
                    result = ocr.ocr(img_array, cls=True)

                    if result and result[0]:
                        page_text = "\n".join([line[1][0] for line in result[0]])
                        text += page_text + "\n"
                        logger.info(f"âœ“ OCR (paddleocr) í˜ì´ì§€ {start_page + i + 1}: {len(page_text)}ì")

                return text

            except Exception as e:
                logger.warning(f"âš ï¸ paddleocr ì‹¤íŒ¨: {e}")
                return ""

        except Exception as e:
            logger.error(f"âŒ OCR ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""

    def _gather_summary_context(self, filename: str, pdf_path: str, doc_locked: bool = False) -> str:
        """ìš”ì•½ìš© ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì¸ë±ìŠ¤ ì²­í¬ ê¸°ë°˜, PDF tail ë¹„í™œì„±)

        Args:
            filename: íŒŒì¼ëª…
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            doc_locked: Trueë©´ í•´ë‹¹ ë¬¸ì„œ ì²­í¬ë§Œ ì‚¬ìš© (ë‹¤ë¥¸ ë¬¸ì„œ ê²€ìƒ‰ ê¸ˆì§€)

        Returns:
            ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ (ìµœëŒ€ ~3600ì, ì•½ 1.8k í† í°)
        """
        import pdfplumber
        import re
        parts = []

        # 1) PDF ë 2~3í˜ì´ì§€ ì¶”ì¶œ â†’ ë¹„í™œì„±í™” (ì¸ë±ìŠ¤ ì²­í¬ ìš°ì„  ì „ëµ)
        # ì‚¬ìœ : ê°œìš”/ë°°ê²½/ê²€í† ì‚¬ìœ /ëŒ€ì•ˆ/ê²¬ì  ë“± í•µì‹¬ ì •ë³´ê°€ ëë¶€ë¶„ì´ ì•„ë‹Œ ì¤‘ê°„ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš° ë‹¤ìˆ˜
        # ì¸ë±ìŠ¤ëœ ì²­í¬ë¡œ ì „ì²´ ë¬¸ì„œë¥¼ ì»¤ë²„í•˜ë„ë¡ ë³€ê²½
        logger.info("ğŸ“‹ ìš”ì•½ ì»¨í…ìŠ¤íŠ¸: PDF tail ì¶”ì¶œ ë¹„í™œì„± (ì¸ë±ìŠ¤ ì²­í¬ ê¸°ë°˜ ì „ëµ)")
        # try:
        #     with pdfplumber.open(pdf_path) as pdf:
        #         total_pages = len(pdf.pages)
        #         start_page = max(0, total_pages - 3)  # ë 3í˜ì´ì§€
        #         tail = ""
        #         for page in pdf.pages[start_page:]:
        #             tail += (page.extract_text() or "")
        #
        #         # OCR í´ë°± (í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì„ ê²½ìš°)
        #         if len(tail.strip()) < 50:
        #             logger.warning(f"âš ï¸ PDF í…ìŠ¤íŠ¸ ë¶€ì¡± ({len(tail)}ì), OCR ì‹œë„...")
        #             tail = self._extract_with_ocr(pdf_path, start_page, total_pages)
        #
        #         if tail.strip():
        #             parts.append("=== [ë¬¸ì„œ ê²°ë¡ /ë§ë¯¸] ===\n" + tail)
        #             logger.info(f"âœ“ PDF ë {total_pages - start_page}í˜ì´ì§€ ì¶”ì¶œ: {len(tail)}ì")
        # except Exception as e:
        #     logger.warning(f"âš ï¸ PDF ëë¶€ë¶„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # 2) ì¸ë±ìŠ¤ ì²­í¬ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì„¹ì…˜ ê°€ì¤‘ì¹˜ ì ìš©)
        # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ: ê°œìš”, ë°°ê²½, ê²€í† ì‚¬ìœ , ëŒ€ì•ˆ, ê²¬ì , ê²°ë¡ , ë¹„ìš©, ë„ì…ì‚¬ìœ 
        priority_keywords = r'(ê°œìš”|ë°°ê²½|ê²€í† ì‚¬ìœ |ê²€í† \s*ì‚¬ìœ |ëŒ€ì•ˆ|ê²¬ì |ê²°ë¡ |ë¹„ìš©|ë„ì…ì‚¬ìœ |ë„ì…\s*ì‚¬ìœ |êµ¬ë§¤ëª©ì |êµ¬ë§¤\s*ëª©ì |ì„ ì •|ê¶Œê³ |ì´ì•¡|í•©ê³„)'

        try:
            if doc_locked:
                # ë¬¸ì„œ ê³ ì • ëª¨ë“œ: í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ë§Œ ë¡œë“œ
                logger.info(f"ğŸ”’ ë¬¸ì„œ ê³ ì • ëª¨ë“œ: {filename}ì˜ ì²­í¬ë§Œ ì‚¬ìš©")
                chunks = self._make_chunks_for_doc(filename)

                # ì„¹ì…˜ ê°€ì¤‘ì¹˜ ì ìš©: ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ í¬í•¨ ì²­í¬ë¥¼ ì•ìœ¼ë¡œ
                priority_chunks = []
                normal_chunks = []
                for chunk in chunks:
                    chunk_text = chunk.get('text') or chunk.get('snippet') or chunk.get('content') or ""
                    if re.search(priority_keywords, chunk_text):
                        priority_chunks.append(chunk)
                    else:
                        normal_chunks.append(chunk)

                # ìš°ì„ ìˆœìœ„ ì²­í¬ + ì¼ë°˜ ì²­í¬ ìˆœì„œë¡œ ì¬ì¡°í•©, ìµœëŒ€ 6ê°œ
                sorted_chunks = (priority_chunks + normal_chunks)[:6]

                for i, chunk in enumerate(sorted_chunks, 1):
                    chunk_text = chunk.get('text') or chunk.get('snippet') or chunk.get('content') or ""
                    if chunk_text:
                        parts.append(f"=== [ë¬¸ì„œ ì²­í¬ {i}] ===\n" + chunk_text[:1500])

                if sorted_chunks:
                    logger.info(f"âœ“ ë¬¸ì„œ ê³ ì • ì²­í¬ {len(sorted_chunks)}ê°œ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: {len(priority_chunks)}ê°œ)")
            else:
                # ì¼ë°˜ ëª¨ë“œ: í‚¤ì›Œë“œ ê²€ìƒ‰ í›„ ê°™ì€ íŒŒì¼ í•„í„°ë§
                search_keywords = re.sub(r'^\d{4}-\d{2}-\d{2}_', '', filename)  # ë‚ ì§œ ì œê±°
                search_keywords = re.sub(r'\.pdf$', '', search_keywords, flags=re.IGNORECASE)
                search_keywords = search_keywords.replace('_', ' ')

                hits = self.retriever.search(search_keywords, top_k=10)
                same_file_hits = [h for h in hits if h.get("filename") == filename]

                # ì„¹ì…˜ ê°€ì¤‘ì¹˜ ì ìš©
                priority_hits = []
                normal_hits = []
                for h in same_file_hits:
                    chunk_text = h.get('text') or h.get('snippet') or h.get('content') or ""
                    if re.search(priority_keywords, chunk_text):
                        priority_hits.append(h)
                    else:
                        normal_hits.append(h)

                sorted_hits = (priority_hits + normal_hits)[:6]

                for i, h in enumerate(sorted_hits, 1):
                    chunk_text = h.get('text') or h.get('snippet') or h.get('content') or ""
                    if chunk_text:
                        parts.append(f"=== [ê´€ë ¨ ì²­í¬ {i}] ===\n" + chunk_text[:1500])

                if sorted_hits:
                    logger.info(f"âœ“ RAG ì²­í¬ {len(sorted_hits)}ê°œ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: {len(priority_hits)}ê°œ)")
        except Exception as e:
            logger.warning(f"âš ï¸ RAG ì²­í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # 3) OCR/ì›ë¬¸ ìŠ¤ëƒ…ìƒ· (ìˆìœ¼ë©´ - í˜„ì¬ëŠ” DB text_preview í™œìš©)
        # í–¥í›„ í™•ì¥: full_text í•„ë“œê°€ ìˆìœ¼ë©´ í™œìš©
        # if hasattr(self, 'get_fulltext'):
        #     full = self.get_fulltext(filename)
        #     if full and len(full) > 1000:
        #         parts.append("=== [ì›ë¬¸ ìŠ¤ëƒ…ìƒ·] ===\n" + full[:3000])

        # ê²°í•© ë° ê¸¸ì´ ì œí•œ (ì•½ 1.8k í† í° ~ 3600ì)
        context = "\n\n".join(parts)[:3600]
        logger.info(f"ğŸ“‹ ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)}ì (ì²­í¬ ìˆ˜: {len(parts)})")
        return context

    def _answer_summary(self, query: str) -> dict:
        """ë‚´ìš© ìš”ì•½ (ë¬¸ì„œ íƒ€ì… ìë™ ê°ì§€ + ë§ì¶¤ í”„ë¡¬í”„íŠ¸)

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "[íŒŒì¼ëª…].pdf ë‚´ìš© ìš”ì•½í•´ì¤˜" ë˜ëŠ” "ë¯¸ëŸ¬í´ë© ì¹´ë©”ë¼ ì‚¼ê°ëŒ€ ê¸°ìˆ ê²€í† ì„œ ì´ë¬¸ì„œ ë‚´ìš© ìš”ì•½í—¤ì¤˜")

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡° (JSON ê¸°ë°˜ ìš”ì•½)
        """
        import re
        import sqlite3
        import pdfplumber
        from app.rag.summary_templates import (
            detect_doc_kind,
            build_prompt,
            parse_summary_json,
            format_summary_output
        )
        from app.rag.utils.json_utils import (
            parse_summary_json_robust,
            ensure_citations,
            validate_numeric_fields
        )

        try:
            # 0. doc=<íŒŒì¼ëª…> ë˜ëŠ” [DOC]<íŒŒì¼ëª…> íŒ¨í„´ í™•ì¸ (ì •í™• ì°¸ì¡° í† í°)
            doc_ref = None
            doc_locked = False
            doc_exact_match = re.search(r"(?:doc=|DOC])\s*([^\s]+\.pdf)", query, re.IGNORECASE)
            if not doc_exact_match:
                doc_exact_match = re.search(r"\[DOC\]\s*([^\s]+\.pdf)", query, re.IGNORECASE)

            if doc_exact_match:
                doc_ref = doc_exact_match.group(1)
                doc_locked = True
                logger.info(f"ğŸ”’ ì •í™• ì°¸ì¡° ëª¨ë“œ: doc={doc_ref}")

            # 1. .pdf í™•ì¥ì í¬í•¨ íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
            filename_match = re.search(r"(\S+\.pdf)", query, re.IGNORECASE) if not doc_ref else None

            # doc_refê°€ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš©
            if doc_ref:
                conn = sqlite3.connect("metadata.db")
                cursor = conn.cursor()
                cursor.execute(
                    """
                    SELECT filename, drafter, date, display_date, category,
                           text_preview, claimed_total, doctype
                    FROM documents
                    WHERE filename = ?
                    LIMIT 1
                """,
                    (doc_ref,),
                )
            # 2. í™•ì¥ì ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
            elif not filename_match:
                # ë¶ˆìš©ì–´ ì œê±° (ìš”ì•½, ì´ë¬¸ì„œ, ë‚´ìš© ë“±)
                stopwords = ["ìš”ì•½", "ìš”ì•½í•´", "ìš”ì•½í—¤ì¤˜", "ì •ë¦¬", "ì •ë¦¬í•´", "ì´ë¬¸ì„œ", "ì´ ë¬¸ì„œ", "í•´ë‹¹ ë¬¸ì„œ",
                             "ë‚´ìš©", "í•´ì¤˜", "í—¤ì¤˜", "ì•Œë ¤ì¤˜", "ì•Œë ¤", "ë³´ì—¬ì¤˜", "ë³´ì—¬"]
                keywords = query
                for word in stopwords:
                    keywords = keywords.replace(word, " ")
                keywords = " ".join(keywords.split())  # ê³µë°± ì •ë¦¬

                if not keywords or len(keywords) < 3:
                    return {
                        "text": "ë¬¸ì„œëª…ì´ë‚˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ ë‹¤ì‹œ ì§ˆì˜í•´ì£¼ì„¸ìš”.",
                        "citations": [],
                        "evidence": [],
                        "status": {
                            "retrieved_count": 0,
                            "selected_count": 0,
                            "found": False
                        }
                    }

                # í‚¤ì›Œë“œë¡œ ë¬¸ì„œ ê²€ìƒ‰ (íŒŒì¼ëª…ì—ì„œ ê²€ìƒ‰)
                # ê³µë°±ì„ % ì™€ì¼ë“œì¹´ë“œë¡œ ë³€ê²½ (íŒŒì¼ëª…ì€ ì–¸ë”ìŠ¤ì½”ì–´ ì‚¬ìš©)
                keywords_wildcard = keywords.replace(' ', '%')
                conn = sqlite3.connect("metadata.db")
                cursor = conn.cursor()
                cursor.execute(
                    """
                    SELECT filename, drafter, date, display_date, category,
                           text_preview, claimed_total, doctype
                    FROM documents
                    WHERE filename LIKE ?
                    ORDER BY date DESC
                    LIMIT 1
                """,
                    (f"%{keywords_wildcard}%",),
                )
            else:
                # íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰
                filename = filename_match.group(1)
                conn = sqlite3.connect("metadata.db")
                cursor = conn.cursor()
                cursor.execute(
                    """
                    SELECT filename, drafter, date, display_date, category,
                           text_preview, claimed_total, doctype
                    FROM documents
                    WHERE filename LIKE ?
                    LIMIT 1
                """,
                    (f"%{filename}%",),
                )

            result = cursor.fetchone()
            conn.close()

            # ì§ì ‘ ê²€ìƒ‰ ì„±ê³µ ì‹œ doc_locked ì„¤ì • (í‚¤ì›Œë“œ/íŒŒì¼ëª… ê²€ìƒ‰ì´ ì •í™• ë§¤ì¹­)
            if result and not doc_locked:
                doc_locked = True
                logger.info(f"ğŸ”’ ì§ì ‘ ê²€ìƒ‰ ì„±ê³µ â†’ doc_locked=Trueë¡œ ì „í™˜")

            # ğŸ” í¼ì§€ ë§¤ì¹­ Fallback (ì •í™• ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ, doc_lockedì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
            if not result and not doc_locked:
                from modules.metadata_db import MetadataDB

                search_term = filename if filename_match else keywords
                logger.info(f"ğŸ” í¼ì§€ ë§¤ì¹­ ì‹œë„: {search_term}")

                # ì„ì‹œ DB ì—°ê²°ë¡œ í¼ì§€ ê²€ìƒ‰
                db = MetadataDB()
                fuzzy_doc = db.get_by_filename_fuzzy(search_term)
                db.close()

                if fuzzy_doc:
                    logger.info(f"âœ… í¼ì§€ ë§¤ì¹­ ì„±ê³µ: {fuzzy_doc.get('filename')}")
                    # result íŠœí”Œ ì¬êµ¬ì„±
                    result = (
                        fuzzy_doc.get('filename'),
                        fuzzy_doc.get('drafter'),
                        fuzzy_doc.get('date'),
                        fuzzy_doc.get('display_date'),
                        fuzzy_doc.get('category'),
                        fuzzy_doc.get('text_preview'),
                        fuzzy_doc.get('claimed_total'),
                        fuzzy_doc.get('doctype', 'proposal')
                    )
                    # í¼ì§€ ë§¤ì¹­ ì„±ê³µ ì‹œ ë¬¸ì„œ ê³ ì • ëª¨ë“œë¡œ ì „í™˜
                    doc_locked = True
                    logger.info(f"ğŸ”’ í¼ì§€ ë§¤ì¹­ ì„±ê³µ â†’ doc_locked=Trueë¡œ ì „í™˜")
                else:
                    logger.warning(f"âŒ í¼ì§€ ë§¤ì¹­ ì‹¤íŒ¨: {search_term}")
                    return {
                        "text": f"'{search_term}' ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "citations": [],
                        "evidence": [],
                        "status": {
                            "retrieved_count": 0,
                            "selected_count": 0,
                            "found": False
                        }
                    }

            # doc_locked ëª¨ë“œì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
            if doc_locked and not result:
                logger.warning(f"âŒ ì •í™• ì°¸ì¡° ë¬¸ì„œ ì—†ìŒ: {doc_ref}")
                return {
                    "text": f"'{doc_ref}' ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            fname, drafter, date, display_date, category, text_preview, claimed_total, doctype = result

            # PDF ê²½ë¡œ í™•ì¸
            year_match = re.search(r'(\d{4})-', fname)
            if year_match:
                year = year_match.group(1)
                pdf_path = f"docs/year_{year}/{fname}"
            else:
                pdf_path = f"docs/{fname}"

            # ğŸ”¥ ìƒˆ êµ¬ì¡°: ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (PDF ë + RAG + ìŠ¤ëƒ…ìƒ·)
            logger.info(f"ğŸ“‹ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘: {fname} (doc_locked={doc_locked})")
            context_text = self._gather_summary_context(fname, pdf_path, doc_locked=doc_locked)

            # Fallback: ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ text_preview ì‚¬ìš©
            if not context_text or len(context_text.strip()) < 100:
                if text_preview:
                    context_text = "=== ë¬¸ì„œ ë‚´ìš© ===\n" + text_preview
                    logger.info(f"âš ï¸ Fallback: text_preview ì‚¬ìš© ({len(text_preview)}ì)")

            # ğŸ”¥ ìƒˆ êµ¬ì¡°: ë¬¸ì„œ íƒ€ì… ìë™ ê°ì§€
            if not context_text or len(context_text.strip()) < 100:
                # ì»¨í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ ë©”íƒ€ë°ì´í„°ë§Œ í‘œì‹œ
                answer_text = f"**ğŸ“„ {fname}**\n\n"
                answer_text += "ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                answer_text += f"**ğŸ“‹ ë¬¸ì„œ ì •ë³´**\n"
                answer_text += f"- ê¸°ì•ˆì: {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                answer_text += f"- ë‚ ì§œ: {display_date or date or 'ì •ë³´ ì—†ìŒ'}\n"
                if claimed_total:
                    answer_text += f"- ê¸ˆì•¡: â‚©{claimed_total:,}\n"

            else:
                # ë¬¸ì„œ ì¢…ë¥˜ ìë™ ê°ì§€
                kind = detect_doc_kind(fname, context_text)
                logger.info(f"ğŸ¯ ë¬¸ì„œ íƒ€ì… ê°ì§€: {kind}")

                # íƒ€ì…ë³„ ë§ì¶¤ í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = build_prompt(
                    kind=kind,
                    filename=fname,
                    drafter=drafter or "ì •ë³´ ì—†ìŒ",
                    display_date=display_date or date or "ì •ë³´ ì—†ìŒ",
                    context_text=context_text,
                    claimed_total=claimed_total
                )

                logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (kind: {kind})")

                # ğŸ”¥ LLM í˜¸ì¶œ (JSON ì‘ë‹µ ìš”ì²­)
                max_retries = 2
                parsed_json = None

                for attempt in range(1, max_retries + 1):
                    try:
                        logger.info(f"ğŸ¤– LLM í˜¸ì¶œ ì‹œë„ {attempt}/{max_retries}")

                        # LLM í˜¸ì¶œ
                        llm_response = self.generator.generate(
                            query=prompt,
                            context="",  # í”„ë¡¬í”„íŠ¸ì— ì´ë¯¸ í¬í•¨ë¨
                            temperature=0.2,  # ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í–¥ìƒ
                            mode="summary"  # ìš”ì•½ ëª¨ë“œ ëª…ì‹œ
                        )

                        logger.info(f"âœ“ LLM ì‘ë‹µ ìˆ˜ì‹ : {len(llm_response)}ì")

                        # JSON íŒŒì‹± ì‹œë„ (ê°•ê±´í•œ ë²„ì „)
                        parsed_json = parse_summary_json_robust(llm_response)

                        if parsed_json:
                            # ì¸ìš© ë³´ê°• (doc_locked ëª¨ë“œì—ì„œ)
                            if doc_locked:
                                parsed_json = ensure_citations(parsed_json, doc_ref=fname)
                            # ìˆ˜ì¹˜ í•„ë“œ ê²€ì¦ (ì›ë¬¸ ëŒ€ì¡°)
                            parsed_json = validate_numeric_fields(parsed_json, context_text)
                            logger.info(f"âœ“ JSON íŒŒì‹± ì„±ê³µ (ì‹œë„ {attempt}íšŒ)")
                            break
                        else:
                            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (ì‹œë„ {attempt}íšŒ), ì¬ì‹œë„...")
                            if attempt < max_retries:
                                # ì¬ì‹œë„ ì‹œ ë¦¬ë§ˆì¸ë“œ ì¶”ê°€
                                prompt += "\n\n**ì¤‘ìš”**: ë°˜ë“œì‹œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”."

                    except Exception as e:
                        logger.error(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt}íšŒ): {e}")
                        if attempt >= max_retries:
                            break

                # ğŸ”¥ ë™ì  í¬ë§·íŒ… (ì¡´ì¬í•˜ëŠ” ì„¹ì…˜ë§Œ ë Œë”)
                if parsed_json:
                    answer_text = format_summary_output(
                        parsed_json=parsed_json,
                        kind=kind,
                        filename=fname,
                        drafter=drafter,
                        display_date=display_date or date,
                        claimed_total=claimed_total
                    )
                    logger.info("âœ“ í¬ë§·íŒ…ëœ ìš”ì•½ ìƒì„± ì™„ë£Œ")

                else:
                    # Fallback: JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ â†’ ììœ  ìš”ì•½ ìƒì„±
                    logger.warning("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ììœ  ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´...")

                    free_form_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ 3~5ë¬¸ì¥ìœ¼ë¡œ ììœ ë¡­ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
í•µì‹¬ ë‚´ìš©, ëª©ì , ê¸ˆì•¡(ìˆìœ¼ë©´), ê²°ë¡  ë“±ì„ ê°„ê²°í•˜ê²Œ í¬í•¨í•˜ì„¸ìš”.
ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

**ë¬¸ì„œëª…**: {fname}
**ê¸°ì•ˆì**: {drafter or 'ì •ë³´ ì—†ìŒ'}
**ë‚ ì§œ**: {display_date or 'ì •ë³´ ì—†ìŒ'}

[ì›ë¬¸]
{context_text[:5000]}
"""

                    try:
                        free_summary = self.generator.generate(
                            query=free_form_prompt,
                            context="",
                            temperature=0.3,
                            mode="summary"  # ìš”ì•½ ëª¨ë“œ ëª…ì‹œ
                        )

                        # ë°°ë„ˆ + ììœ  ìš”ì•½
                        answer_text = f"**ğŸ“„ {fname}**\n\n"
                        answer_text += "âš ï¸ **êµ¬ì¡°í™” ìš”ì•½ ì‹¤íŒ¨(ìŠ¤í‚¤ë§ˆ ë¯¸ì¼ì¹˜). ììœ  ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´.**\n\n"
                        answer_text += "---\n\n"
                        answer_text += free_summary.strip() + "\n\n"

                        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                        answer_text += "---\n**ğŸ“‹ ë¬¸ì„œ ì •ë³´**\n"
                        answer_text += f"- ê¸°ì•ˆì: {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                        answer_text += f"- ë‚ ì§œ: {display_date or date or 'ì •ë³´ ì—†ìŒ'}\n"
                        if claimed_total:
                            answer_text += f"- ê¸ˆì•¡: â‚©{claimed_total:,}\n"

                        logger.info("âœ“ ììœ  ìš”ì•½ ìƒì„± ì™„ë£Œ")

                    except Exception as e:
                        logger.error(f"âŒ ììœ  ìš”ì•½ ìƒì„±ë„ ì‹¤íŒ¨: {e}")
                        # ìµœì¢… í´ë°±: ì»¨í…ìŠ¤íŠ¸ ì¼ë¶€ë¼ë„ ë³´ì—¬ì£¼ê¸°
                        answer_text = f"**ğŸ“„ {fname}**\n\n"
                        answer_text += "âš ï¸ **ìš”ì•½ ìƒì„± ì‹¤íŒ¨. ë¬¸ì„œ ì¼ë¶€ ë‚´ìš©ì„ í‘œì‹œí•©ë‹ˆë‹¤.**\n\n"
                        answer_text += "---\n\n"
                        answer_text += context_text[:1000] + "...\n\n"
                        answer_text += "---\n**ğŸ“‹ ë¬¸ì„œ ì •ë³´**\n"
                        answer_text += f"- ê¸°ì•ˆì: {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                        answer_text += f"- ë‚ ì§œ: {display_date or date or 'ì •ë³´ ì—†ìŒ'}\n"
                        if claimed_total:
                            answer_text += f"- ê¸ˆì•¡: â‚©{claimed_total:,}\n"

            # Evidence êµ¬ì„± (file_path ì§ì ‘ í¬í•¨)
            # year í´ë” ìë™ ê°ì§€
            year_match = re.search(r'(\d{4})-', fname)
            if year_match:
                year = year_match.group(1)
                file_path_str = f"docs/year_{year}/{fname}"
            else:
                file_path_str = f"docs/{fname}"

            evidence = [{
                "doc_id": fname,
                "filename": fname,
                "file_path": file_path_str,  # ì§ì ‘ íŒŒì¼ ê²½ë¡œ (ref ëŒ€ì‹ )
                "page": 1,
                "snippet": text_preview[:400] if text_preview else "",
                "ref": None,  # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                "meta": {
                    "filename": fname,
                    "drafter": drafter,
                    "date": display_date or date,
                    "doctype": doctype,
                    "claimed_total": claimed_total
                }
            }]

            # í’ˆì§ˆ ë°©ì–´ì„  ë¡œê·¸ (LLM ì‚¬ìš© ì—¬ë¶€ ì •í™•íˆ í‘œì‹œ)
            used_llm = text_preview and len(text_preview.strip()) > 100
            logger.info({
                "mode": "SUMMARY",
                "files": [fname],
                "llm": used_llm,
                "text_length": len(text_preview) if text_preview else 0
            })

            return {
                "text": answer_text,
                "citations": evidence,
                "evidence": evidence,
                "status": {
                    "retrieved_count": 1,
                    "selected_count": 1,
                    "found": True
                }
            }

        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "text": f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def warmup(self) -> None:
        """ì›Œë°ì—…: LLM + ì¸ë±ìŠ¤ ì‚¬ì „ ë¡œë”©

        ì²« ì¿¼ë¦¬ ì§€ì—° ì œê±°ë¥¼ ìœ„í•´ ì‹œì‘ ì‹œ í˜¸ì¶œ.
        """
        logger.info("Warming up RAG pipeline...")
        try:
            # ë”ë¯¸ ì¿¼ë¦¬ ì‹¤í–‰
            response = self.query("test warmup query", top_k=1)
            if response.success:
                logger.info(f"Warmup completed in {response.latency:.2f}s")
            else:
                logger.warning(f"Warmup failed: {response.error}")
        except Exception as e:
            logger.error(f"Warmup error: {e}", exc_info=True)

    # ========================================================================
    # ë‚´ë¶€ í—¬í¼: ê¸°ë³¸ êµ¬í˜„ ìƒì„±
    # ========================================================================

    def _create_default_retriever(self) -> Retriever:
        """ê¸°ë³¸ ê²€ìƒ‰ ì—”ì§„ ìƒì„± (v2 ë˜ëŠ” v1)

        í™˜ê²½ ë³€ìˆ˜ USE_V2_RETRIEVERë¡œ ì œì–´:
        - true: HybridRetrieverV2 ì‚¬ìš© (ì‹ ê·œ 2-layer ì•„í‚¤í…ì²˜)
        - false/ì—†ìŒ: HybridRetriever ì‚¬ìš© (ê¸°ì¡´ ë ˆê±°ì‹œ)
        """
        import os

        use_v2 = os.getenv("USE_V2_RETRIEVER", "false").lower() == "true"

        if use_v2:
            # V2 RetrieverëŠ” archiveë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤ (20251026)
            # ë ˆê±°ì‹œ ì½”ë“œë¥¼ ì œê±°í•˜ê³  v1ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤
            logger.warning(
                "âš ï¸ USE_V2_RETRIEVERëŠ” ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. v1 Retrieverë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            use_v2 = False
            # try:
            #     from app.rag.retriever_v2 import HybridRetrieverV2
            #     v2_retriever = HybridRetrieverV2()
            #     logger.info("âœ… HybridRetrieverV2 (v2 ì‹ ê·œ ì‹œìŠ¤í…œ) ìƒì„± ì™„ë£Œ")
            #
            #     # V2 adapter: fused_results â†’ list ë³€í™˜
            #     return _V2RetrieverAdapter(v2_retriever)
            # except Exception as e:
            #     logger.error(f"V2 Retriever ìƒì„± ì‹¤íŒ¨, v1ìœ¼ë¡œ í´ë°±: {e}")
            #     # í´ë°±: v1 ì‚¬ìš©
            #     use_v2 = False

        if not use_v2:
            try:
                from app.rag.retrievers.hybrid import HybridRetriever

                retriever = HybridRetriever()
                logger.info("Default HybridRetriever (v1 ë ˆê±°ì‹œ) ìƒì„± ì™„ë£Œ")
                return retriever
            except Exception as e:
                logger.error(f"HybridRetriever ìƒì„± ì‹¤íŒ¨: {e}")
                # í´ë°±: ë”ë¯¸ êµ¬í˜„
                return _DummyRetriever()

    def _create_default_compressor(self) -> Compressor:
        """ê¸°ë³¸ ì••ì¶•ê¸° ìƒì„± (í˜„ì¬ëŠ” no-op)"""
        logger.info("Default compressor ìƒì„± (no-op)")
        return _NoOpCompressor()

    def _create_default_generator(self) -> Generator:
        """ê¸°ë³¸ LLM ìƒì„±ê¸° ìƒì„± (ë ˆê±°ì‹œ ì–´ëŒ‘í„° ì‚¬ìš©)"""
        try:
            # ë ˆê±°ì‹œ êµ¬í˜„ ì–´ëŒ‘í„° ì‚¬ìš© (ì ì§„ì  ì´ê´€ ì¤€ë¹„)
            legacy_rag = self._create_legacy_adapter()
            logger.info("Default generator ìƒì„± (Legacy Adapter ë˜í•‘)")
            return _QuickFixGenerator(legacy_rag)
        except Exception as e:
            logger.error(f"Generator ìƒì„± ì‹¤íŒ¨: {e}")
            return _DummyGenerator()

    def _create_legacy_adapter(self):
        """ë ˆê±°ì‹œ êµ¬í˜„ ì–´ëŒ‘í„° ìƒì„± (ìº¡ìŠí™”)

        QwenLLMì„ ë˜í•‘í•˜ì—¬ ê¸°ì¡´ ë ˆê±°ì‹œ ì‹œìŠ¤í…œê³¼ ì—°ê²°í•©ë‹ˆë‹¤.
        í–¥í›„ ì´ ë©”ì„œë“œë§Œ ìˆ˜ì •í•˜ì—¬ ì‹ ê·œ êµ¬í˜„ìœ¼ë¡œ ì ì§„ ì „í™˜ ê°€ëŠ¥.

        Returns:
            _LLMAdapter: LLM ì–´ëŒ‘í„° ì¸ìŠ¤í„´ìŠ¤
        """
        try:
            from rag_system.llm_singleton import LLMSingleton

            model_path = os.getenv("MODEL_PATH", "./models/ggml-model-Q4_K_M.gguf")
            logger.info(f"ğŸ” DEBUG: Attempting to load LLM with model_path={model_path}")
            logger.info(f"ğŸ” DEBUG: Model file exists: {Path(model_path).exists()}")
            llm = LLMSingleton.get_instance(model_path=model_path)
            logger.info(f"âœ… LLM adapter ìƒì„± ì™„ë£Œ (LLMSingleton ì‚¬ìš©, model={model_path})")
            return _LLMAdapter(llm)
        except Exception as e:
            logger.error(f"LLM adapter ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return None

    def _load_known_drafters(self) -> set:
        """ë©”íƒ€DBì—ì„œ ê³ ìœ  ê¸°ì•ˆì ë¡œë“œ (Closed-World Validationìš©)

        Returns:
            set: ê³ ìœ  ê¸°ì•ˆì ì´ë¦„ ì§‘í•©
        """
        try:
            from modules.metadata_db import MetadataDB

            db = MetadataDB()
            drafters = db.list_unique_drafters()
            db.close()

            logger.info(f"âœ… ê³ ìœ  ê¸°ì•ˆì {len(drafters)}ëª… ìºì‹± ì™„ë£Œ")
            return drafters
        except Exception as e:
            logger.error(f"ê¸°ì•ˆì ë¡œë“œ ì‹¤íŒ¨: {e}")
            return set()


# ============================================================================
# í´ë°± êµ¬í˜„ (ê¸°ë³¸ ë™ì‘ ë³´ì¥)
# ============================================================================


class _DummyRetriever:
    """ë”ë¯¸ ê²€ìƒ‰ê¸° (í´ë°±ìš©)"""

    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        logger.warning("Dummy retriever: ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return []


class _NoOpCompressor:
    """No-op ì••ì¶•ê¸° (ì••ì¶•í•˜ì§€ ì•ŠìŒ)"""

    def compress(
        self, chunks: List[Dict[str, Any]], ratio: float
    ) -> List[Dict[str, Any]]:
        logger.debug("No-op compressor: ì••ì¶• ìŠ¤í‚µ")
        return chunks


class _LLMAdapter:
    """QwenLLM ì–´ëŒ‘í„° (LegacyAdapter ëŒ€ì²´)

    QwenLLMì„ _QuickFixGeneratorê°€ ê¸°ëŒ€í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """

    def __init__(self, llm):
        self.llm = llm

    def generate_from_context(self, query: str, context: str, temperature: float = 0.1, mode: str = "rag") -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (í…ìŠ¤íŠ¸ í˜•ì‹)
            temperature: ìƒì„± ì˜¨ë„
            mode: ìƒì„± ëª¨ë“œ (chat/rag/summarize)

        Returns:
            str: ìƒì„±ëœ ë‹µë³€
        """
        # Contextë¥¼ ì²­í¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        chunks = [{"snippet": context, "content": context}]

        try:
            # ğŸ¯ ëª¨ë“œë³„ í† í° ì˜ˆì‚° ì ìš©
            logger.info(f"ğŸ¯ generate_from_context: mode={mode}")
            response = self.llm.generate_response(query, chunks, max_retries=1, mode=mode)

            if hasattr(response, "answer"):
                return response.answer
            return str(response)
        except Exception as e:
            logger.error(f"LLM ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return f"[E_GENERATE] {str(e)}"


class _QuickFixGenerator:
    """QuickFixRAG ë˜í¼ (ê¸°ì¡´ êµ¬í˜„ í™œìš©)"""

    def __init__(self, rag):
        self.rag = rag
        self.compressed_chunks = None  # Store chunks for LLM

    def generate(self, query: str, context: str, temperature: float, mode: str = "rag") -> str:
        # ì¬ê²€ìƒ‰ ê¸ˆì§€. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„±ìœ¼ë¡œ ìš°ì„  ì‹œë„.
        try:
            # 1) QuickFixRAGì— ì „ìš© ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if hasattr(self.rag, "generate_from_context"):
                return self.rag.generate_from_context(
                    query, context, temperature=temperature, mode=mode
                )

            # 2) ë‚´ë¶€ LLM ì§ì ‘ ì ‘ê·¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            # ğŸ”¥ CRITICAL: LLM lazy loading - ensure LLM is loaded before checking
            if hasattr(self.rag, "_ensure_llm_loaded"):
                self.rag._ensure_llm_loaded()

            if hasattr(self.rag, "llm") and hasattr(self.rag.llm, "generate_response"):
                # CRITICAL: generate_response expects List[Dict], not str
                # Convert context string back to chunks format
                if self.compressed_chunks:
                    # Use stored compressed chunks (preferred)
                    logger.debug(
                        f"Using {len(self.compressed_chunks)} compressed chunks for generation (mode={mode})"
                    )
                    response = self.rag.llm.generate_response(
                        query, self.compressed_chunks, max_retries=1, mode=mode
                    )
                else:
                    # Fallback: convert context string to minimal chunks
                    logger.warning(
                        "No compressed_chunks available, converting context string"
                    )
                    snippets = context.split("\n\n")
                    chunks = [
                        {"snippet": s, "content": s} for s in snippets if s.strip()
                    ]
                    response = self.rag.llm.generate_response(
                        query, chunks, max_retries=1, mode=mode
                    )

                # Extract answer from RAGResponse object
                if hasattr(response, "answer"):
                    return response.answer
                return str(response)

            # 3) í´ë°±: ì¬ê²€ìƒ‰ì´ í¬í•¨ëœ answerëŠ” ìµœí›„ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ
            logger.warning("generate_from_context ë¯¸ì§€ì› â†’ í´ë°±(answer) ì‚¬ìš©")
            if self.rag is None:
                logger.error("LegacyAdapter: QuickFixRAGê°€ ì—†ì–´ ë‹µë³€ ìƒì„± ë¶ˆê°€")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ ìƒì„± ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
            return self.rag.answer(query, use_llm_summary=True)
        except Exception as e:
            logger.error(f"Generation ì‹¤íŒ¨: {e}", exc_info=True)
            return f"[E_GENERATE] {str(e)}"


class _V2RetrieverAdapter:
    """V2 Retriever Adapter

    HybridRetrieverV2ì˜ ê²°ê³¼ í˜•ì‹ {"fused_results": [...]}ë¥¼
    v1 ì¸í„°í˜ì´ìŠ¤ í˜•ì‹ [...] ìœ¼ë¡œ ë³€í™˜.

    v2 results êµ¬ì¡°:
        {
            "fused_results": [
                {"id": "doc_4094", "score": 0.123, "filename": "...", ...},
                ...
            ]
        }

    v1 expected êµ¬ì¡°:
        [
            {"doc_id": "doc_4094", "snippet": "...", "page": 1, ...},
            ...
        ]
    """

    def __init__(self, v2_retriever):
        """
        Args:
            v2_retriever: HybridRetrieverV2 instance
        """
        self.v2_retriever = v2_retriever
        self.db = v2_retriever.db  # MetadataDB for content fetching

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Search using v2 retriever, convert to v1 format

        Args:
            query: Search query
            top_k: Number of results

        Returns:
            List of dicts in v1 format with keys:
            - doc_id: Document ID
            - snippet: Text snippet
            - page: Page number (default 1)
            - score: Relevance score
            - meta: Metadata dict
        """
        try:
            # Call v2 retriever
            v2_result = self.v2_retriever.search(query, top_k=top_k)
            fused_results = v2_result.get("fused_results", [])

            # Convert to v1 format
            v1_results = []
            for doc in fused_results:
                doc_id = doc.get("id", "unknown")

                # ğŸ”¥ CRITICAL: snippet ìš°ì„ ìˆœìœ„
                # 1) ê²€ìƒ‰ ê²°ê³¼ì— ì§ì ‘ í¬í•¨ëœ snippet/content
                # 2) DB ì¡°íšŒ (get_content)
                # 3) ì œëª©/íŒŒì¼ëª… ê¸°ë°˜ í´ë°±

                snippet = ""

                # Priority 1: fused_resultsì— ì´ë¯¸ í¬í•¨ëœ ë°ì´í„°
                if "snippet" in doc:
                    snippet = doc["snippet"]
                elif "content" in doc:
                    snippet = doc["content"][:500]

                # Priority 2: DB ì¡°íšŒ (app/rag/db.MetadataDB.get_content)
                if not snippet or len(snippet) < 50:
                    content = self.db.get_content(doc_id)
                    if content and len(content) >= 50:
                        snippet = content[:500]

                # Priority 3: ë©”íƒ€ë°ì´í„° í´ë°±
                if not snippet or len(snippet) < 50:
                    fallback_parts = []
                    if doc.get("title"):
                        fallback_parts.append(f"ì œëª©: {doc['title']}")
                    if doc.get("filename"):
                        fallback_parts.append(f"íŒŒì¼: {doc['filename']}")
                    if doc.get("date"):
                        fallback_parts.append(f"ë‚ ì§œ: {doc['date']}")

                    snippet = (
                        " | ".join(fallback_parts)
                        if fallback_parts
                        else f"ë¬¸ì„œ ID: {doc_id}"
                    )
                    logger.warning(
                        f"V2 Adapter: doc_id={doc_id} snippet ê²°ì†, ë©”íƒ€ë°ì´í„° í´ë°± ì‚¬ìš©"
                    )

                v1_results.append(
                    {
                        "doc_id": doc_id,
                        "snippet": snippet,
                        "page": 1,  # v2ì—ì„œëŠ” page ì •ë³´ ì—†ìŒ, ê¸°ë³¸ 1
                        "score": doc.get("score", 0.0),
                        "meta": {
                            "doc_id": doc_id,
                            "filename": doc.get("filename", ""),
                            "title": doc.get("title", ""),
                            "date": doc.get("date", ""),
                            "page": 1,
                        },
                    }
                )

            logger.info(f"V2 Adapter: {len(v1_results)} results converted")
            return v1_results

        except Exception as e:
            logger.error(f"V2 Adapter search failed: {e}", exc_info=True)
            return []

    def warmup(self):
        """ì›Œë°ì—… (v2ëŠ” í•„ìš” ì‹œ ìë™ ë¡œë“œ)"""
        logger.info("V2 Adapter warmup (no-op)")


class _DummyGenerator:
    """ë”ë¯¸ ìƒì„±ê¸° (í´ë°±ìš©)"""

    def generate(self, query: str, context: str, temperature: float, mode: str = "rag") -> str:
        logger.warning("Dummy generator: ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
