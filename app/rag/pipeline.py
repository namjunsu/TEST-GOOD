"""RAG íŒŒì´í”„ë¼ì¸ (íŒŒì‚¬ë“œ íŒ¨í„´)

ë‹¨ì¼ ì§„ì…ì : RAGPipeline.query()
ë‚´ë¶€ íë¦„: ê²€ìƒ‰ â†’ ì••ì¶• â†’ LLM ìƒì„±

Example:
    >>> pipeline = RAGPipeline()
    >>> response = pipeline.query("ì§ˆë¬¸", top_k=5)
    >>> print(response.answer)
"""

import os
import time
import base64
import sqlite3
from pathlib import Path
from dataclasses import dataclass, field
from typing import Protocol, List, Optional, Dict, Any

from app.core.logging import get_logger
from app.core.errors import ModelError, SearchError, ErrorCode, ERROR_MESSAGES
from app.rag.query_router import QueryRouter, QueryMode

logger = get_logger(__name__)


def _encode_file_ref(filename: str) -> Optional[str]:
    """íŒŒì¼ëª…ì„ base64 refë¡œ ì¸ì½”ë”© (docs í•˜ìœ„ ê²½ë¡œ ì°¾ê¸°)

    Args:
        filename: íŒŒì¼ëª…

    Returns:
        base64 ì¸ì½”ë”©ëœ ref ë˜ëŠ” None
    """
    try:
        # 1. metadata.dbì—ì„œ ê²½ë¡œ ì°¾ê¸° ì‹œë„
        conn = sqlite3.connect("metadata.db")
        cursor = conn.cursor()
        cursor.execute(
            "SELECT path FROM documents WHERE filename = ? LIMIT 1",
            (filename,)
        )
        result = cursor.fetchone()
        conn.close()

        if result and result[0]:
            file_path = Path(result[0])
            # docs í•˜ìœ„ì¸ì§€ í™•ì¸
            if "docs" in file_path.parts and file_path.exists():
                # base64 ì¸ì½”ë”©
                ref = base64.urlsafe_b64encode(str(file_path).encode()).decode()
                return ref

        # 2. Fallback: docs í´ë”ì—ì„œ íŒŒì¼ ê²€ìƒ‰ (year í´ë” í¬í•¨)
        import re
        year_match = re.search(r'(\d{4})-', filename)
        if year_match:
            year = year_match.group(1)
            # docs/year_YYYY/ í´ë”ì—ì„œ ì°¾ê¸°
            file_path = Path(f"docs/year_{year}") / filename
            if file_path.exists():
                ref = base64.urlsafe_b64encode(str(file_path).encode()).decode()
                return ref

        # 3. Fallback2: docs í´ë” ì „ì²´ ê²€ìƒ‰
        docs_dir = Path("docs")
        if docs_dir.exists():
            for file_path in docs_dir.rglob(filename):
                if file_path.is_file():
                    ref = base64.urlsafe_b64encode(str(file_path).encode()).decode()
                    return ref

    except Exception as e:
        logger.warning(f"ref ì¸ì½”ë”© ì‹¤íŒ¨: {filename} - {e}")

    return None

# ì§„ë‹¨ ëª¨ë“œ ì„¤ì •
DIAG_RAG = os.getenv("DIAG_RAG", "false").lower() == "true"
DIAG_LOG_LEVEL = os.getenv("DIAG_LOG_LEVEL", "INFO").upper()


# ============================================================================
# Request / Response ë°ì´í„° í´ë˜ìŠ¤
# ============================================================================


@dataclass
class RAGRequest:
    """RAG ìš”ì²­ íŒŒë¼ë¯¸í„°

    Attributes:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
        compression_ratio: ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ë¹„ìœ¨ (0.0~1.0)
        use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
        temperature: LLM ìƒì„± ì˜¨ë„
    """

    query: str
    top_k: int = 5
    compression_ratio: float = 0.7
    use_hyde: bool = False
    temperature: float = 0.1


@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ê²°ê³¼

    Attributes:
        answer: ìƒì„±ëœ ë‹µë³€
        source_docs: ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í•˜ìœ„ í˜¸í™˜)
        evidence_chunks: Evidenceìš© ì •ê·œí™” ì²­í¬ (ê¶Œì¥)
        raw_results: ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ (Evidence ìµœì†Œ ë³´ì¥ìš©)
        latency: ì „ì²´ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
        success: ì„±ê³µ ì—¬ë¶€
        error: ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
        metrics: ë‚´ë¶€ ì§€í‘œ (ê²€ìƒ‰/ì••ì¶•/ìƒì„± ì‹œê°„ ë“±)
        diagnostics: ì§„ë‹¨ ì •ë³´ (DIAG_RAG=trueì¼ ë•Œë§Œ ì±„ì›Œì§)
    """

    answer: str
    source_docs: List[str] = field(default_factory=list)
    evidence_chunks: List[Dict[str, Any]] = field(default_factory=list)
    raw_results: List[Dict[str, Any]] = field(default_factory=list)
    latency: float = 0.0
    success: bool = True
    error: Optional[str] = None
    metrics: dict = field(default_factory=dict)
    diagnostics: dict = field(default_factory=dict)  # ì§„ë‹¨ ì •ë³´


# ============================================================================
# í”„ë¡œí† ì½œ ì •ì˜ (ì˜ì¡´ì„± ì—­ì „)
# ============================================================================


class Retriever(Protocol):
    """ê²€ìƒ‰ ì—”ì§„ ì¸í„°í˜ì´ìŠ¤"""

    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ìˆ˜í–‰ (ì •ê·œí™” ìŠ¤í‚¤ë§ˆ ë°˜í™˜)

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼

        Returns:
            [
                {
                    "doc_id": str,
                    "page": int,
                    "score": float,
                    "snippet": str,
                    "meta": dict
                }, ...
            ]
        """
        ...


class Compressor(Protocol):
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶•ê¸° ì¸í„°í˜ì´ìŠ¤"""

    def compress(
        self, chunks: List[Dict[str, Any]], ratio: float
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ì••ì¶•

        Args:
            chunks: ì›ë³¸ ì²­í¬ ëª©ë¡ (ì •ê·œí™”ëœ dict)
            ratio: ì••ì¶• ë¹„ìœ¨

        Returns:
            ì••ì¶•ëœ ì²­í¬ ëª©ë¡ (ë™ì¼ ìŠ¤í‚¤ë§ˆ)
        """
        ...


class Generator(Protocol):
    """LLM ìƒì„±ê¸° ì¸í„°í˜ì´ìŠ¤"""

    def generate(self, query: str, context: str, temperature: float) -> str:
        """ë‹µë³€ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì°¸ê³  ë¬¸ì„œ
            temperature: ìƒì„± ì˜¨ë„

        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        ...


# ============================================================================
# RAG íŒŒì´í”„ë¼ì¸ (íŒŒì‚¬ë“œ)
# ============================================================================


class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸ íŒŒì‚¬ë“œ

    ê²€ìƒ‰ â†’ ì••ì¶• â†’ ìƒì„±ì„ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ ì œê³µ.
    ë‚´ë¶€ êµ¬í˜„ì€ Retriever/Compressor/Generatorì— ìœ„ì„.

    Example:
        >>> pipeline = RAGPipeline()
        >>> response = pipeline.query("ì§ˆë¬¸", top_k=5)
        >>> if response.success:
        ...     print(response.answer)
        ...     print(f"ì°¸ê³ : {response.source_docs}")
    """

    def __init__(
        self,
        retriever: Optional[Retriever] = None,
        compressor: Optional[Compressor] = None,
        generator: Optional[Generator] = None,
    ):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            retriever: ê²€ìƒ‰ ì—”ì§„ (Noneì´ë©´ ê¸°ë³¸ HybridRetriever ì‚¬ìš©)
            compressor: ì••ì¶•ê¸° (Noneì´ë©´ ê¸°ë³¸ ContextCompressor ì‚¬ìš©)
            generator: LLM ìƒì„±ê¸° (Noneì´ë©´ ê¸°ë³¸ LlamaCppGenerator ì‚¬ìš©)
        """
        self.retriever = retriever or self._create_default_retriever()
        self.compressor = compressor or self._create_default_compressor()
        self.generator = generator or self._create_default_generator()
        self.query_router = QueryRouter()  # ğŸ¯ ëª¨ë“œ ë¼ìš°í„° ì´ˆê¸°í™”

        logger.info("RAG Pipeline initialized")

    def query(
        self,
        query: str,
        top_k: int = 5,
        compression_ratio: float = 0.7,
        use_hyde: bool = False,
        temperature: float = 0.1,
    ) -> RAGResponse:
        """RAG ì§ˆì˜ (ë‹¨ì¼ ì§„ì…ì )

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            compression_ratio: ì••ì¶• ë¹„ìœ¨
            use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
            temperature: LLM ìƒì„± ì˜¨ë„

        Returns:
            RAGResponse: ë‹µë³€ + ë©”íƒ€ë°ì´í„°
        """

        # ì…ë ¥ ê²€ì¦
        if not query or not query.strip():
            return RAGResponse(
                answer="",
                success=False,
                error="ë¹ˆ ì§ˆë¬¸ì…ë‹ˆë‹¤",
            )

        start_time = time.perf_counter()
        metrics = {}
        diagnostics = {}  # ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘

        try:
            # 1. ê²€ìƒ‰: ì •ê·œí™”ëœ ì²­í¬(dict) ë¦¬ìŠ¤íŠ¸ ê¸°ëŒ€
            search_start = time.perf_counter()
            results = self.retriever.search(query, top_k)
            metrics["search_time"] = time.perf_counter() - search_start

            # [DIAG] ê²€ìƒ‰ ê²°ê³¼ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["retrieved_k"] = len(results)
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(f"[DIAG] ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")

            if not results:
                logger.warning(f"No results found for query: {query[:50]}")
                if DIAG_RAG:
                    diagnostics["mode"] = "no_results"
                    diagnostics["generate_path"] = "fallback_no_context"
                return RAGResponse(
                    answer="ê´€ë ¨ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ë‹¤.",
                    success=True,
                    latency=time.perf_counter() - start_time,
                    metrics=metrics,
                    diagnostics=diagnostics,
                )

            # 2. ì••ì¶•: ì²­í¬ ë‹¨ìœ„ ìœ ì§€(í˜ì´ì§€/ìŠ¤ë‹ˆí«/ë©”íƒ€ ë³´ì¡´)
            compress_start = time.perf_counter()
            compressed = self.compressor.compress(results, compression_ratio)
            metrics["compress_time"] = time.perf_counter() - compress_start

            # [DIAG] ì••ì¶• í›„ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["after_compress_k"] = len(compressed)
                diagnostics["compression_ratio"] = compression_ratio
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(
                        f"[DIAG] ì••ì¶• ì™„ë£Œ: {len(results)} â†’ {len(compressed)}ê°œ ë¬¸ì„œ"
                    )

            # 3. ìƒì„±: ì»¨í…ìŠ¤íŠ¸ëŠ” ìŠ¤ë‹ˆí« ì§‘í•©ìœ¼ë¡œ êµ¬ì„±
            gen_start = time.perf_counter()

            # CRITICAL: Inject compressed chunks into generator for proper LLM context
            if hasattr(self.generator, "compressed_chunks"):
                self.generator.compressed_chunks = compressed
                logger.debug(
                    f"Injected {len(compressed)} compressed chunks into generator"
                )

            context = "\n\n".join([c.get("snippet", "") for c in compressed])

            # [DIAG] ìƒì„± ì „ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ·
            if DIAG_RAG and DIAG_LOG_LEVEL == "DEBUG":
                for i, c in enumerate(compressed[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ë¡œê·¸
                    logger.debug(
                        f"[DIAG] Context[{i}]: doc_id={c.get('doc_id')}, "
                        f"filename={c.get('filename', 'N/A')}, "
                        f"page={c.get('page', 0)}, "
                        f"snippet={c.get('snippet', '')[:120]}..."
                    )

            answer = self.generator.generate(query, context, temperature)
            metrics["generate_time"] = time.perf_counter() - gen_start

            # [DIAG] ìƒì„± ì™„ë£Œ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["mode"] = "normal"
                diagnostics["generate_path"] = "from_context"
                diagnostics["used_k"] = len(compressed)
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(
                        f"[DIAG] ìƒì„± ì™„ë£Œ: from_context ê²½ë¡œ, {len(compressed)}ê°œ ë¬¸ì„œ ì‚¬ìš©"
                    )

            total_latency = time.perf_counter() - start_time
            metrics["total_time"] = total_latency

            logger.info(
                f"RAG query completed in {total_latency:.2f}s "
                f"(search={metrics['search_time']:.2f}s, "
                f"compress={metrics['compress_time']:.2f}s, "
                f"generate={metrics['generate_time']:.2f}s)"
            )

            return RAGResponse(
                answer=answer,
                source_docs=[c.get("doc_id") for c in results[:3]],
                evidence_chunks=compressed,  # UIìš© ê·¼ê±°
                raw_results=results,  # Evidence ìµœì†Œ ë³´ì¥ìš©
                latency=total_latency,
                success=True,
                metrics=metrics,
                diagnostics=diagnostics,
            )

        except SearchError as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_RETRIEVE] ê²€ìƒ‰ ì‹¤íŒ¨: {e.message}",
                latency=time.perf_counter() - start_time,
            )

        except ModelError as e:
            logger.error(f"Model inference failed: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_GENERATE] ìƒì„± ì‹¤íŒ¨: {e.message}",
                latency=time.perf_counter() - start_time,
            )

        except Exception as e:
            logger.error(f"Unexpected error: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_UNKNOWN] {str(e)}",
                latency=time.perf_counter() - start_time,
            )

    def _make_response(
        self, text: str, selected: List[Dict[str, Any]], retrieved: List[Dict[str, Any]]
    ) -> dict:
        """í‘œì¤€ ì‘ë‹µ êµ¬ì¡° ìƒì„± (citations í¬í•¨)

        Args:
            text: ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
            selected: ì‹¤ì œ ì‚¬ìš©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ì••ì¶• í›„)
            retrieved: ê²€ìƒ‰ëœ ì›ë³¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            í‘œì¤€í™”ëœ ì‘ë‹µ dict (citations í•„ìˆ˜)
        """
        citations = []
        for c in selected:
            filename = c.get("filename") or c.get("doc_id") or c.get("title", "")
            ref = _encode_file_ref(filename) if filename else None

            citations.append({
                "doc_id": c.get("doc_id"),
                "filename": filename,
                "title": c.get("title") or filename or c.get("doc_id"),
                "page": c.get("page", 1),
                "snippet": (
                    c.get("text") or c.get("snippet") or c.get("content") or ""
                )[:400],
                "ref": ref,  # ğŸ”´ base64 ì¸ì½”ë”©ëœ íŒŒì¼ ê²½ë¡œ
                "preview_url": c.get("preview_url"),
                "download_url": c.get("download_url"),
            })

        return {
            "text": text,
            "citations": citations,  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
            "evidence": citations,  # í•˜ìœ„ í˜¸í™˜ì„± (ë™ì¼ ë°ì´í„°)
            "status": {
                "retrieved_count": len(retrieved),
                "selected_count": len(selected),
                "found": len(selected) > 0,  # ğŸ”´ ìœ ì¼í•œ íŒì • ê¸°ì¤€
            },
        }

    def answer(self, query: str, top_k: Optional[int] = None) -> dict:
        """ë‹µë³€ ìƒì„± (Evidence í¬í•¨ êµ¬ì¡°í™”ëœ ì‘ë‹µ)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ (Noneì´ë©´ ê¸°ë³¸ê°’ 5)

        Returns:
            dict: {
                "text": ë‹µë³€ í…ìŠ¤íŠ¸,
                "citations": ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í‘œì¤€ í‚¤),
                "evidence": ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í•˜ìœ„ í˜¸í™˜),
                "status": {
                    "retrieved_count": int,
                    "selected_count": int,
                    "found": bool
                }
            }
        """
        # ğŸ”¥ CRITICAL: ê¸°ì•ˆì/ë‚ ì§œ ê²€ìƒ‰ì€ QuickFixRAGì— ìœ„ì„ (ì „ë¬¸ ë¡œì§ ë³´ìœ )
        if hasattr(self.generator, "rag"):
            import re
            import sqlite3

            # âœ… í™•ì¥ëœ ì¿¼ë¦¬ì—ì„œ ì‹¤ì œ ì§ˆë¬¸ ì¶”ì¶œ (chat_interface.py ëŒ€ì‘)
            actual_query = query
            if "í˜„ì¬ ì§ˆë¬¸:" in query:
                parts = query.split("í˜„ì¬ ì§ˆë¬¸:")
                if len(parts) > 1:
                    actual_query = parts[-1].strip()
                    logger.info(f"ğŸ“ í™•ì¥ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œ: '{actual_query[:50]}'")

            # ğŸ¯ ëª¨ë“œ ë¼ìš°íŒ…: Q&A ì˜ë„ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ íŒŒì¼ëª…ì´ ìˆì–´ë„ Q&A ëª¨ë“œ ìš°ì„ 
            query_mode = self.query_router.classify_mode(actual_query)
            router_reason = self.query_router.get_routing_reason(actual_query)
            logger.info(
                f"ğŸ”€ ë¼ìš°íŒ… ê²°ê³¼: mode={query_mode.value}, reason={router_reason}"
            )

            # ğŸ’° COST_SUM ëª¨ë“œ: ë¹„ìš© í•©ê³„ ì§ì ‘ ì¡°íšŒ
            if query_mode == QueryMode.COST_SUM:
                return self._answer_cost_sum(actual_query)

            # ğŸ“‹ LIST ëª¨ë“œ: ëª©ë¡ ê²€ìƒ‰ (2ì¤„ ì¹´ë“œ í˜•ì‹)
            if query_mode == QueryMode.LIST:
                return self._answer_list(actual_query)

            # ğŸ“ SUMMARY ëª¨ë“œ: ë‚´ìš© ìš”ì•½ (5ì¤„ ì„¹ì…˜)
            if query_mode == QueryMode.SUMMARY:
                return self._answer_summary(actual_query)

            # ğŸ‘€ PREVIEW ëª¨ë“œ: ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° (ì›ë¬¸ 6-8ì¤„, ê°€ì§œ í‘œ ê¸ˆì§€)
            if query_mode == QueryMode.PREVIEW:
                return self._answer_preview(actual_query)

            # ğŸ” ë””ë²„ê¹…: ì‹¤ì œ pattern matching ëŒ€ìƒ ë¡œê¹…
            logger.info(f"ğŸ” Pattern matching ëŒ€ìƒ ì¿¼ë¦¬: '{actual_query[:100]}'")

            # âœ… P0: íŒŒì¼ëª… ì§ì ‘ ì–¸ê¸‰ íŒ¨í„´ ê°ì§€ (ë ˆê±°ì‹œ í˜¸í™˜, PREVIEW ëª¨ë“œ ì™¸)
            if False:  # ë¹„í™œì„±í™”: PREVIEW ëª¨ë“œë¡œ í†µí•©ë¨
                # íŒ¨í„´ 1: ìš”ì•½ ìš”ì²­ - "íŒŒì¼ëª….pdf ë‚´ìš© ìš”ì•½í•´ì¤˜" / "íŒŒì¼ëª….pdf ìš”ì•½"
                file_summary_pattern = (
                    r"(\S+\.pdf)\s*(ì´\s*)?(ë¬¸ì„œ\s*)?(ë‚´ìš©\s*)?(ìš”ì•½|ì •ë¦¬)"
                )
                summary_match = re.search(
                    file_summary_pattern, actual_query, re.IGNORECASE
                )

                if summary_match:
                    filename = summary_match.group(1).strip()
                    logger.info(f"ğŸ¯ P0: íŒŒì¼ ìš”ì•½ ìš”ì²­ ê°ì§€ - {filename}")

                    # PDF ì „ë¬¸ ë¡œë“œ + ë©”íƒ€ë°ì´í„° ì¡°íšŒ
                    try:
                        import pdfplumber
                        from pathlib import Path

                        conn = sqlite3.connect("metadata.db")
                        cursor = conn.cursor()
                        cursor.execute(
                            """
                            SELECT path, filename, drafter, date, category, text_preview
                            FROM documents
                            WHERE filename LIKE ?
                            LIMIT 1
                        """,
                            (f"%{filename}%",),
                        )

                        result = cursor.fetchone()
                        conn.close()

                        if result:
                            pdf_path, fname, drafter, date, category, preview = result

                            # PDF ì „ë¬¸ í…ìŠ¤íŠ¸ ë¡œë“œ
                            full_text = preview or ""
                            if pdf_path and Path(pdf_path).exists():
                                try:
                                    with pdfplumber.open(pdf_path) as pdf:
                                        pages_text = []
                                        for page in pdf.pages[:5]:  # ìµœëŒ€ 5í˜ì´ì§€
                                            page_text = page.extract_text() or ""
                                            pages_text.append(page_text)
                                            if len("".join(pages_text)) > 5000:
                                                break
                                        full_text = "\n\n".join(pages_text)
                                except Exception as e:
                                    logger.warning(f"PDF ì½ê¸° ì‹¤íŒ¨: {e}")

                            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (LLM ì—†ì´)
                            answer_text = f"**ğŸ“„ {fname}**\n\n"
                            answer_text += "**ğŸ“‹ ë¬¸ì„œ ì •ë³´**\n"
                            answer_text += f"- **ê¸°ì•ˆì:** {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += f"- **ë‚ ì§œ:** {date or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += (
                                f"- **ì¹´í…Œê³ ë¦¬:** {category or 'ì •ë³´ ì—†ìŒ'}\n"
                            )

                            answer_text += "\n**ğŸ“ ì£¼ìš” ë‚´ìš©**\n"
                            # ì²˜ìŒ 800ì ë¯¸ë¦¬ë³´ê¸°
                            content_preview = full_text[:800].strip()
                            if content_preview:
                                answer_text += content_preview
                                if len(full_text) > 800:
                                    answer_text += (
                                        "...\n\n*(ì „ì²´ ë¬¸ì„œëŠ” ë” ê¸´ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤)*"
                                    )
                            else:
                                answer_text += "*(ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤)*"

                            # Evidence êµ¬ì„±
                            evidence = [
                                {
                                    "doc_id": fname,
                                    "page": 1,
                                    "snippet": full_text[:400],  # 500 â†’ 400 (ìŠ¤ë‹ˆí« ì¼ê´€ì„±)
                                    "meta": {
                                        "filename": fname,
                                        "drafter": drafter,
                                        "date": date,
                                        "category": category,
                                    },
                                }
                            ]

                            return {
                                "text": answer_text,
                                "citations": [fname],
                                "evidence": evidence,
                                "status": {
                                    "retrieved_count": 1,
                                    "selected_count": 1,
                                    "found": True,
                                },
                            }
                        else:
                            logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {filename}")

                    except Exception as e:
                        logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
                        # ì˜¤ë¥˜ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±

                # íŒ¨í„´ 2: ê¸°ì•ˆì ì§ˆì˜ - "íŒŒì¼ëª….pdf ê¸°ì•ˆìê°€ ëˆ„êµ¬ì•¼?"
                # (ì»¬ëŸ¼ ìˆ˜ì • ì™„ë£Œ: doc_number ì œê±°)
                file_author_pattern = r"(\S+\.pdf)\s*(ê¸°ì•ˆì|ì‘ì„±ì).*(ëˆ„êµ¬|ì•Œë ¤ì¤˜)"
                file_match = re.search(file_author_pattern, actual_query, re.IGNORECASE)

                if file_match:
                    filename = file_match.group(1).strip()
                    logger.info(f"ğŸ¯ P0: íŒŒì¼ëª… ì§ì ‘ ì§ˆì˜ ê°ì§€ - {filename}")

                    # metadata.dbì—ì„œ ì§ì ‘ ì¡°íšŒ
                    try:
                        conn = sqlite3.connect("metadata.db")
                        cursor = conn.cursor()
                        cursor.execute(
                            """
                            SELECT filename, drafter, date, category
                            FROM documents
                            WHERE filename LIKE ?
                            LIMIT 1
                        """,
                            (f"%{filename}%",),
                        )

                        result = cursor.fetchone()
                        conn.close()

                        if result:
                            fname, drafter, date, category = result
                            answer_text = f"**{fname}**\n\n"
                            answer_text += f"ğŸ“Œ **ê¸°ì•ˆì:** {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += f"ğŸ“… **ë‚ ì§œ:** {date or 'ì •ë³´ ì—†ìŒ'}\n"
                            answer_text += (
                                f"ğŸ“ **ì¹´í…Œê³ ë¦¬:** {category or 'ì •ë³´ ì—†ìŒ'}\n"
                            )

                            # Evidence êµ¬ì„± (ì •í™•í•œ íŒŒì¼ 1ê±´)
                            evidence = [
                                {
                                    "doc_id": fname,
                                    "page": 1,
                                    "snippet": f"ê¸°ì•ˆì: {drafter}, ë‚ ì§œ: {date}, ì¹´í…Œê³ ë¦¬: {category}",
                                    "meta": {
                                        "filename": fname,
                                        "drafter": drafter,
                                        "date": date,
                                        "category": category,
                                    },
                                }
                            ]

                            return {
                                "text": answer_text,
                                "citations": [fname],
                                "evidence": evidence,
                                "status": {
                                    "retrieved_count": 1,
                                    "selected_count": 1,
                                    "found": True,
                                },
                            }
                        else:
                            logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {filename}")
                            return {
                                "text": f"âŒ '{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                                "citations": [],
                                "evidence": [],
                                "status": {
                                    "retrieved_count": 0,
                                    "selected_count": 0,
                                    "found": False,
                                },
                            }

                    except Exception as e:
                        logger.error(f"âŒ metadata ì¡°íšŒ ì‹¤íŒ¨: {e}")
                        # ì˜¤ë¥˜ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±

            # ê¸°ì•ˆì ê²€ìƒ‰ íŒ¨í„´ ê°ì§€ (ì‹¤ì œ ì§ˆë¬¸ì—ì„œë§Œ)
            author_patterns = [
                r"([ê°€-í£]{2,4})\s*(ë¬¸ì„œ|ê¸°ì•ˆì„œ|ê²€í† ì„œ)",
                r"([ê°€-í£]{2,4})ê°€?\s*(ì‘ì„±í•œ|ì‘ì„±ì•ˆ|ê¸°ì•ˆí•œ|ì“´|ë§Œë“ )",
                r"(ê¸°ì•ˆì|ì‘ì„±ì|ì œì•ˆì)[:\s]+([ê°€-í£]{2,4})",
            ]
            # ë‚ ì§œ ê²€ìƒ‰ íŒ¨í„´ ê°ì§€
            year_pattern = r"(\d{4})\s*ë…„"

            is_author_query = any(re.search(p, actual_query) for p in author_patterns)
            is_year_query = re.search(year_pattern, actual_query)

            if is_author_query or is_year_query:
                logger.info(
                    f"ğŸ¯ íŠ¹ìˆ˜ ê²€ìƒ‰ ëª¨ë“œ ê°ì§€: author={is_author_query}, year={is_year_query}"
                )
                # QuickFixRAG.answer()ë¡œ ì§ì ‘ ì²˜ë¦¬ (ì‹¤ì œ ì§ˆë¬¸ ì „ë‹¬)
                answer_text = self.generator.rag.answer(
                    actual_query, use_llm_summary=False
                )

                # í‘œì¤€ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                return {
                    "text": answer_text,
                    "citations": [],  # QuickFixRAG ì‘ë‹µì—ì„œ ì¶”ì¶œ ì–´ë ¤ì›€
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": "ê´€ë ¨ ë¬¸ì„œ" not in answer_text
                        and "ì—†ìŠµë‹ˆë‹¤" not in answer_text,
                    },
                }

        # ì¼ë°˜ ì¿¼ë¦¬ëŠ” ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
        response = self.query(query, top_k=top_k or 5)

        if response.success:
            # ê²€ìƒ‰/ì••ì¶•ì—ì„œ ë„˜ì–´ì˜¨ ì •ê·œí™” ì²­í¬ ì‚¬ìš© (ì‹¤ì œ page/snippet/meta ë…¸ì¶œ)
            evidence = [
                {
                    "doc_id": c.get("doc_id"),
                    "page": c.get("page", 1),
                    "snippet": c.get("snippet", ""),
                    "meta": c.get(
                        "meta", {"doc_id": c.get("doc_id"), "page": c.get("page", 1)}
                    ),
                }
                for c in (response.evidence_chunks or [])
            ]

            # CRITICAL: Evidence ìµœì†Œ ë³´ì¥ (sources_citedê°€ ë¹„ì–´ë„ ê²€ìƒ‰ ê²°ê³¼ëŠ” í‘œì‹œ)
            evidence_injected = False
            if not evidence and response.raw_results:
                logger.info("Evidence empty, using raw_results[:3] as fallback")
                evidence = [
                    {
                        "doc_id": r.get("doc_id") or r.get("chunk_id", "unknown"),
                        "page": 0,  # ê²€ìƒ‰ ê²°ê³¼ëŠ” í˜ì´ì§€ ì •ë³´ ì—†ìŒ
                        "snippet": r.get("snippet") or r.get("text_preview", "")[:400],  # 500 â†’ 400 (ìŠ¤ë‹ˆí« ì¼ê´€ì„±)
                        "meta": {
                            "doc_id": r.get("doc_id") or r.get("chunk_id", "unknown"),
                            "filename": r.get("filename", ""),
                            "page": 0,
                        },
                    }
                    for r in response.raw_results[:3]
                ]
                evidence_injected = True

            # [DIAG] Evidence ì§„ë‹¨ ì •ë³´ ì¶”ê°€
            if DIAG_RAG and response.diagnostics:
                response.diagnostics["evidence_count"] = len(evidence)
                response.diagnostics["evidence_injected"] = evidence_injected

            # ğŸ”¥ CRITICAL: status.found í”Œë˜ê·¸ - UI íŒì • ë‹¨ì¼ ì†ŒìŠ¤
            # retrieved_count: ê²€ìƒ‰ëœ ì›ë³¸ ê²°ê³¼ ìˆ˜
            # selected_count: ì‹¤ì œ ì‚¬ìš©ëœ ì¦ê±° ìˆ˜ (evidence)
            # found: ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€ (evidenceê°€ 1ê°œ ì´ìƒì´ë©´ True)
            status = {
                "retrieved_count": len(response.raw_results or []),
                "selected_count": len(evidence),
                "found": len(evidence) > 0,  # ğŸ”´ ìœ ì¼í•œ íŒì • ê¸°ì¤€
            }

            # ìš´ì˜ í‘œì¤€ 1í–‰ ìš”ì•½ ë¡œê·¸ (í•„ìˆ˜)
            import re

            author_mode = bool(re.search(r"(ì‘ì„±ì|ê¸°ì•ˆì|ì œì•ˆì)", query))
            search_ms = int(response.metrics.get("search_time", 0) * 1000)
            generate_ms = int(response.metrics.get("generate_time", 0) * 1000)
            total_ms = int(response.latency * 1000)

            logger.info(
                f'[RAG] query="{query[:50]}..." | '
                f"retrieved={status['retrieved_count']} | "
                f"selected={status['selected_count']} | "
                f"found={status['found']} | "
                f"author_mode={author_mode} | "
                f"backfill={evidence_injected} | "
                f"search_ms={search_ms} | "
                f"generate_ms={generate_ms} | "
                f"total_ms={total_ms}"
            )

            return {
                "text": response.answer,
                "citations": evidence,  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
                "evidence": evidence,  # í•˜ìœ„ í˜¸í™˜ì„± (ë™ì¼ ë°ì´í„°)
                "status": status,  # UIì—ì„œ ì´ê²ƒë§Œ í™•ì¸
                "diagnostics": response.diagnostics if DIAG_RAG else {},
            }
        else:
            # ì—ëŸ¬ ë°œìƒ ì‹œ (ì¤‘ë¦½ í†¤, ì‚¬ê³¼ í‘œí˜„ ê¸ˆì§€)
            error_msg = ERROR_MESSAGES.get(
                ErrorCode.E_GENERATE, "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆë‹¤."
            )
            if response.error:
                error_msg = f"{error_msg}\n\nìƒì„¸: {response.error}"

            # ìš´ì˜ í‘œì¤€ ë¡œê·¸ (ì—ëŸ¬ ì¼€ì´ìŠ¤)
            logger.error(
                f'[RAG] query="{query[:50]}..." | '
                f'status=ERROR | error="{response.error}"'
            )

            return {
                "text": error_msg,
                "citations": [],  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
                "evidence": [],  # í•˜ìœ„ í˜¸í™˜ì„±
                "status": {"retrieved_count": 0, "selected_count": 0, "found": False},
            }

    def answer_text(self, query: str) -> str:
        """ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            str: ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
        """
        result = self.answer(query)
        return result["text"]

    def _answer_list(self, query: str) -> dict:
        """ëª©ë¡ ê²€ìƒ‰ (2ì¤„ ì¹´ë“œ í˜•ì‹)

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "2024ë…„ ë‚¨ì¤€ìˆ˜ ë¬¸ì„œ ì°¾ì•„ì¤˜")

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡° (2ì¤„ ì¹´ë“œ ëª©ë¡)
        """
        import re
        from modules.metadata_db import MetadataDB

        try:
            # ì—°ë„ ì¶”ì¶œ (ì˜ˆ: "2024ë…„" â†’ "2024")
            year_match = re.search(r"(\d{4})ë…„?", query)
            year = year_match.group(1) if year_match else None

            # ê¸°ì•ˆì ì¶”ì¶œ (ì˜ˆ: "ë‚¨ì¤€ìˆ˜")
            drafter_match = re.search(r"([ê°€-í£]{2,4})(ê°€|ì´)?", query)
            drafter = drafter_match.group(1) if drafter_match else None

            logger.info(f"ğŸ“‹ ëª©ë¡ ê²€ìƒ‰: year={year}, drafter={drafter}")

            # DB ê²€ìƒ‰
            db = MetadataDB()
            docs = db.search_documents(drafter=drafter, year=year, limit=20)

            if not docs:
                return {
                    "text": f"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (year={year}, drafter={drafter})",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            # 2ì¤„ ì¹´ë“œ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ… (íŒŒì¼ëª… ê¸°ë°˜ í•µì‹¬ ìš”ì•½)
            cards = []
            for doc in docs:
                filename = doc.get("filename", "ì•Œ ìˆ˜ ì—†ìŒ")
                doctype = doc.get("doctype", "ë¬¸ì„œ")
                date = doc.get("display_date") or doc.get("date", "ë‚ ì§œ ì—†ìŒ")
                drafter_name = doc.get("drafter", "ì‘ì„±ì ë¯¸ìƒ")

                # íŒŒì¼ëª…ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ (ë‚ ì§œ ì œê±°, ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ)
                import re
                # ë‚ ì§œ íŒ¨í„´ ì œê±° (YYYY-MM-DD_)
                title = re.sub(r'^\d{4}-\d{2}-\d{2}_', '', filename)
                # .pdf í™•ì¥ì ì œê±°
                title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
                # ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
                title = title.replace('_', ' ')

                # 2ì¤„ ì¹´ë“œ: ì œëª© + ë©”íƒ€ì •ë³´
                card = f"**{title}**\nğŸ· {doctype} Â· ğŸ“… {date} Â· âœ {drafter_name}"
                cards.append(card)

            answer_text = "\n\n".join(cards[:10])  # ìµœëŒ€ 10ê°œ

            # Evidence êµ¬ì„± (íŒŒì¼ëª… ê¸°ë°˜ ìš”ì•½ + ì‹¤ì œ íŒŒì¼ ê²½ë¡œ)
            evidence = []
            for doc in docs[:10]:
                filename = doc.get("filename", "")

                # íŒŒì¼ëª…ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ (ë‹µë³€ í…ìŠ¤íŠ¸ì™€ ë™ì¼í•œ ë°©ì‹)
                import re
                title = re.sub(r'^\d{4}-\d{2}-\d{2}_', '', filename)
                title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
                title = title.replace('_', ' ')

                # snippetì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš© (ê°„ê²°í•˜ê³  ì˜ë¯¸ ìˆëŠ” ì •ë³´)
                snippet = title[:160]

                # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ìƒì„± (year í´ë” ìë™ ê°ì§€)
                year_match = re.search(r'(\d{4})-', filename)
                if year_match:
                    year = year_match.group(1)
                    file_path_str = f"docs/year_{year}/{filename}"
                else:
                    file_path_str = f"docs/{filename}"

                evidence.append({
                    "doc_id": filename,
                    "filename": filename,
                    "file_path": file_path_str,  # â† ì‹¤ì œ íŒŒì¼ ê²½ë¡œ (Streamlit ë‚´ì¥ ë°©ì‹)
                    "page": 1,
                    "snippet": snippet,
                    "ref": None,  # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (FastAPI ë°©ì‹ ì œê±°)
                    "meta": {
                        "filename": filename,
                        "drafter": doc.get("drafter"),
                        "date": doc.get("display_date") or doc.get("date"),
                        "doctype": doc.get("doctype")
                    }
                })

            # í’ˆì§ˆ ë°©ì–´ì„  ë¡œê·¸ (ì¬í˜„ ìš©ì´ì„±)
            logger.info({
                "mode": "LIST",
                "files": [doc.get("filename") for doc in docs[:3]],
                "count": len(docs),
                "llm": os.getenv("LLM_ENABLED", "false").lower() == "true"
            })

            return {
                "text": answer_text,
                "citations": evidence,
                "evidence": evidence,
                "status": {
                    "retrieved_count": len(docs),
                    "selected_count": min(10, len(docs)),
                    "found": True
                }
            }

        except Exception as e:
            logger.error(f"âŒ ëª©ë¡ ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "text": f"ëª©ë¡ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def _answer_cost_sum(self, query: str) -> dict:
        """ë¹„ìš© í•©ê³„ ì§ì ‘ ì¡°íšŒ (DB claimed_total í™œìš©)

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "ì±„ë„ì—ì´ ì¤‘ê³„ì°¨ ë³´ìˆ˜ í•©ê³„ ì–¼ë§ˆì˜€ì§€?")

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡° (text, citations, evidence, status)
        """
        try:
            # 1. ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ ë¬¸ì„œ ì°¾ê¸°
            search_results = self.retriever.search(query, top_k=3)

            if not search_results:
                logger.warning(f"ë¹„ìš© ì§ˆì˜ ê²€ìƒ‰ ì‹¤íŒ¨: {query}")
                return {
                    "text": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            # 2. DBì—ì„œ claimed_total ì¡°íšŒ
            from modules.metadata_db import MetadataDB
            db = MetadataDB()

            for result in search_results:
                filename = result.get("meta", {}).get("filename") or result.get("doc_id", "")
                if not filename:
                    continue

                doc = db.get_by_filename(filename)
                if doc and doc.get("claimed_total"):
                    claimed_total = doc["claimed_total"]

                    # 3. ë‹µë³€ í¬ë§·íŒ… (VAT, ê²€ì¦ ë°°ì§€ í¬í•¨)
                    # VAT íŒë‹¨ (text_previewì—ì„œ "VAT" í‚¤ì›Œë“œ ê²€ìƒ‰)
                    text_preview = doc.get("text_preview", "")
                    vat_status = "VAT ë³„ë„" if "VAT" in text_preview or "ë¶€ê°€ì„¸" in text_preview else "VAT í¬í•¨ ì¶”ì •"

                    # sum_match ê²€ì¦ ë°°ì§€
                    sum_match = doc.get("sum_match")
                    if sum_match is None:
                        verification = "sum_match=ì—†ìŒ"
                    elif sum_match:
                        verification = "sum_match=ì¼ì¹˜ âœ…"
                    else:
                        verification = "sum_match=ë¶ˆì¼ì¹˜ âš ï¸"

                    answer_text = f"ğŸ’° í•©ê³„: **â‚©{claimed_total:,}** ({vat_status})\n"
                    answer_text += f"ì¶œì²˜: {filename} | ë‚ ì§œ: {doc.get('display_date') or doc.get('date') or 'ì •ë³´ ì—†ìŒ'} | ê¸°ì•ˆì: {doc.get('drafter') or 'ì •ë³´ ì—†ìŒ'}\n"
                    answer_text += f"ê²€ì¦: {verification}"

                    # Evidence êµ¬ì„±
                    ref = _encode_file_ref(filename)
                    evidence = [{
                        "doc_id": filename,
                        "filename": filename,
                        "page": 1,
                        "snippet": f"ë¹„ìš© í•©ê³„: â‚©{claimed_total:,}",
                        "ref": ref,  # ğŸ”´ base64 ì¸ì½”ë”©ëœ íŒŒì¼ ê²½ë¡œ
                        "meta": {
                            "filename": filename,
                            "drafter": doc.get("drafter"),
                            "date": doc.get("display_date") or doc.get("date"),
                            "claimed_total": claimed_total
                        }
                    }]

                    logger.info(f"ğŸ’° ë¹„ìš© ì§ˆì˜ ì„±ê³µ: {filename} â†’ â‚©{claimed_total:,}")

                    return {
                        "text": answer_text,
                        "citations": evidence,
                        "evidence": evidence,
                        "status": {
                            "retrieved_count": len(search_results),
                            "selected_count": 1,
                            "found": True
                        }
                    }

            # claimed_total ì—†ëŠ” ê²½ìš°
            logger.warning(f"ê²€ìƒ‰ëœ ë¬¸ì„œì— ë¹„ìš© ì •ë³´ ì—†ìŒ: {[r.get('doc_id') for r in search_results]}")
            return {
                "text": "ê²€ìƒ‰ëœ ë¬¸ì„œì— ë¹„ìš© í•©ê³„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": len(search_results),
                    "selected_count": 0,
                    "found": False
                }
            }

        except Exception as e:
            logger.error(f"âŒ ë¹„ìš© ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "text": f"ë¹„ìš© ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def _answer_preview(self, query: str) -> dict:
        """ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° (ì›ë¬¸ ì¸ìš©, ê°€ì§œ í‘œ ìƒì„± ê¸ˆì§€)

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "[íŒŒì¼ëª…].pdf ë¯¸ë¦¬ë³´ê¸°")

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡° (ì›ë¬¸ 6-8ì¤„)
        """
        import re
        import sqlite3
        from pathlib import Path

        try:
            # íŒŒì¼ëª… ì¶”ì¶œ
            filename_match = re.search(r"(\S+\.pdf)", query, re.IGNORECASE)
            if not filename_match:
                return {
                    "text": "íŒŒì¼ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            filename = filename_match.group(1)

            # DBì—ì„œ ë¬¸ì„œ ì¡°íšŒ
            conn = sqlite3.connect("metadata.db")
            cursor = conn.cursor()
            cursor.execute(
                """
                SELECT path, filename, drafter, date, display_date, text_preview
                FROM documents
                WHERE filename LIKE ?
                LIMIT 1
            """,
                (f"%{filename}%",),
            )

            result = cursor.fetchone()
            conn.close()

            if not result:
                return {
                    "text": f"'{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            pdf_path, fname, drafter, date, display_date, text_preview = result

            # ì›ë¬¸ 6-8ì¤„ ì¶”ì¶œ (cleaned_text)
            preview_text = text_preview or ""

            # ê°œí–‰ ê¸°ì¤€ 6-8ì¤„ ì¶”ì¶œ
            lines = [line.strip() for line in preview_text.split('\n') if line.strip()]
            preview_lines = lines[:8]  # ìµœëŒ€ 8ì¤„

            if len(preview_lines) == 0:
                preview_content = "(ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤)"
            else:
                preview_content = "\n".join(preview_lines)

            # ë‹µë³€ í¬ë§·íŒ… (ê°€ì§œ í‘œ ìƒì„± ì ˆëŒ€ ê¸ˆì§€)
            answer_text = f"**ğŸ“„ {fname} ë¯¸ë¦¬ë³´ê¸°**\n\n"
            answer_text += preview_content

            # Evidence êµ¬ì„±
            ref = _encode_file_ref(fname)
            evidence = [{
                "doc_id": fname,
                "filename": fname,
                "page": 1,
                "snippet": preview_content[:400],
                "ref": ref,  # ğŸ”´ base64 ì¸ì½”ë”©ëœ íŒŒì¼ ê²½ë¡œ
                "meta": {
                    "filename": fname,
                    "drafter": drafter,
                    "date": display_date or date
                }
            }]

            # í’ˆì§ˆ ë°©ì–´ì„  ë¡œê·¸
            logger.info({
                "mode": "PREVIEW",
                "files": [fname],
                "lines": len(preview_lines),
                "llm": False  # PREVIEWëŠ” LLM ì‚¬ìš© ì•ˆ í•¨
            })

            return {
                "text": answer_text,
                "citations": evidence,
                "evidence": evidence,
                "status": {
                    "retrieved_count": 1,
                    "selected_count": 1,
                    "found": True
                }
            }

        except Exception as e:
            logger.error(f"âŒ ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "text": f"ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def _answer_summary(self, query: str) -> dict:
        """ë‚´ìš© ìš”ì•½ (JSON êµ¬ì¡°í™” + doctypeë³„ ë§ì¶¤ í”„ë¡¬í”„íŠ¸)

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "[íŒŒì¼ëª…].pdf ë‚´ìš© ìš”ì•½í•´ì¤˜" ë˜ëŠ” "ë¯¸ëŸ¬í´ë© ì¹´ë©”ë¼ ì‚¼ê°ëŒ€ ê¸°ìˆ ê²€í† ì„œ ì´ë¬¸ì„œ ë‚´ìš© ìš”ì•½í—¤ì¤˜")

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡° (JSON ê¸°ë°˜ ìš”ì•½)
        """
        import re
        import sqlite3
        import pdfplumber
        from app.rag.summary_templates import (
            get_summary_prompt,
            parse_summary_json,
            format_summary_output
        )

        try:
            # 1. .pdf í™•ì¥ì í¬í•¨ íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
            filename_match = re.search(r"(\S+\.pdf)", query, re.IGNORECASE)

            # 2. í™•ì¥ì ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
            if not filename_match:
                # ë¶ˆìš©ì–´ ì œê±° (ìš”ì•½, ì´ë¬¸ì„œ, ë‚´ìš© ë“±)
                stopwords = ["ìš”ì•½", "ìš”ì•½í•´", "ìš”ì•½í—¤ì¤˜", "ì •ë¦¬", "ì •ë¦¬í•´", "ì´ë¬¸ì„œ", "ì´ ë¬¸ì„œ", "í•´ë‹¹ ë¬¸ì„œ",
                             "ë‚´ìš©", "í•´ì¤˜", "í—¤ì¤˜", "ì•Œë ¤ì¤˜", "ì•Œë ¤", "ë³´ì—¬ì¤˜", "ë³´ì—¬"]
                keywords = query
                for word in stopwords:
                    keywords = keywords.replace(word, " ")
                keywords = " ".join(keywords.split())  # ê³µë°± ì •ë¦¬

                if not keywords or len(keywords) < 3:
                    return {
                        "text": "ë¬¸ì„œëª…ì´ë‚˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ ë‹¤ì‹œ ì§ˆì˜í•´ì£¼ì„¸ìš”.",
                        "citations": [],
                        "evidence": [],
                        "status": {
                            "retrieved_count": 0,
                            "selected_count": 0,
                            "found": False
                        }
                    }

                # í‚¤ì›Œë“œë¡œ ë¬¸ì„œ ê²€ìƒ‰ (íŒŒì¼ëª…ì—ì„œ ê²€ìƒ‰)
                # ê³µë°±ì„ % ì™€ì¼ë“œì¹´ë“œë¡œ ë³€ê²½ (íŒŒì¼ëª…ì€ ì–¸ë”ìŠ¤ì½”ì–´ ì‚¬ìš©)
                keywords_wildcard = keywords.replace(' ', '%')
                conn = sqlite3.connect("metadata.db")
                cursor = conn.cursor()
                cursor.execute(
                    """
                    SELECT filename, drafter, date, display_date, category,
                           text_preview, claimed_total, doctype
                    FROM documents
                    WHERE filename LIKE ?
                    ORDER BY date DESC
                    LIMIT 1
                """,
                    (f"%{keywords_wildcard}%",),
                )
            else:
                # íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰
                filename = filename_match.group(1)
                conn = sqlite3.connect("metadata.db")
                cursor = conn.cursor()
                cursor.execute(
                    """
                    SELECT filename, drafter, date, display_date, category,
                           text_preview, claimed_total, doctype
                    FROM documents
                    WHERE filename LIKE ?
                    LIMIT 1
                """,
                    (f"%{filename}%",),
                )

            result = cursor.fetchone()
            conn.close()

            if not result:
                if filename_match:
                    search_term = filename
                else:
                    search_term = keywords
                return {
                    "text": f"'{search_term}' ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            fname, drafter, date, display_date, category, text_preview, claimed_total, doctype = result

            # PDF ê²½ë¡œ í™•ì¸
            year_match = re.search(r'(\d{4})-', fname)
            if year_match:
                year = year_match.group(1)
                pdf_path = f"docs/year_{year}/{fname}"
            else:
                pdf_path = f"docs/{fname}"

            # ğŸ”¥ ë‹¨ê³„ 1: RAG ê²€ìƒ‰ìœ¼ë¡œ Top-5 ì²­í¬ ê°€ì ¸ì˜¤ê¸°
            logger.info(f"ğŸ” RAG ê²€ìƒ‰ ì‹œì‘: {fname}")
            rag_chunks_text = ""
            try:
                # íŒŒì¼ëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                search_keywords = re.sub(r'^\d{4}-\d{2}-\d{2}_', '', fname)  # ë‚ ì§œ ì œê±°
                search_keywords = re.sub(r'\.pdf$', '', search_keywords, flags=re.IGNORECASE)  # í™•ì¥ì ì œê±°
                search_keywords = search_keywords.replace('_', ' ')  # ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ

                # RAG ê²€ìƒ‰ ì‹¤í–‰
                search_results = self.retriever.search(search_keywords, top_k=5)

                # ì²­í¬ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                for i, chunk in enumerate(search_results[:5], 1):
                    chunk_text = chunk.get('snippet') or chunk.get('text') or chunk.get('content') or ""
                    if chunk_text:
                        rag_chunks_text += f"[ì²­í¬ {i}]\n{chunk_text}\n\n"

                logger.info(f"âœ“ RAG ì²­í¬ ìˆ˜ì§‘: {len(search_results)}ê°œ, {len(rag_chunks_text)}ì")
            except Exception as e:
                logger.warning(f"âš ï¸ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                rag_chunks_text = ""

            # ğŸ”¥ ë‹¨ê³„ 2: PDF ë§ˆì§€ë§‰ 2ìª½ ì½ê¸° (ê²°ë¡  ë¶€ë¶„)
            logger.info(f"ğŸ“„ PDF ë§ˆì§€ë§‰ í˜ì´ì§€ ì½ê¸° ì‹œì‘: {pdf_path}")
            pdf_ending_text = ""
            try:
                with pdfplumber.open(pdf_path) as pdf:
                    total_pages = len(pdf.pages)
                    # ë§ˆì§€ë§‰ 2ìª½ ì½ê¸°
                    start_page = max(0, total_pages - 2)
                    for page_num in range(start_page, total_pages):
                        page_text = pdf.pages[page_num].extract_text() or ""
                        pdf_ending_text += f"[í˜ì´ì§€ {page_num+1}]\n{page_text}\n\n"

                    logger.info(f"âœ“ PDF ë§ˆì§€ë§‰ í˜ì´ì§€ ì¶”ì¶œ ì„±ê³µ: {len(pdf_ending_text)}ì")
            except Exception as e:
                logger.warning(f"âš ï¸ PDF ë§ˆì§€ë§‰ í˜ì´ì§€ ì½ê¸° ì‹¤íŒ¨: {e}")
                pdf_ending_text = ""

            # ğŸ”¥ ë‹¨ê³„ 3: ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ìš°ì„ ìˆœìœ„: PDF ëë¶€ë¶„ > RAG ì²­í¬)
            # ìµœëŒ€ 10,000ì (LLM ë¶€ë‹´ ê³ ë ¤)
            context_text = ""

            # 1) PDF ë§ˆì§€ë§‰ ë¶€ë¶„ (ê²°ë¡ ì´ ë³´í†µ ì—¬ê¸° ìˆìŒ)
            if pdf_ending_text:
                context_text += "=== ë¬¸ì„œ ê²°ë¡  ë¶€ë¶„ ===\n" + pdf_ending_text

            # 2) RAG ê²€ìƒ‰ ì²­í¬
            if rag_chunks_text:
                context_text += "\n=== ë¬¸ì„œ ì£¼ìš” ë‚´ìš© ===\n" + rag_chunks_text

            # 3) text_preview fallback (DBì— ì €ì¥ëœ ë¯¸ë¦¬ë³´ê¸°)
            if not context_text and text_preview:
                context_text = "=== ë¬¸ì„œ ë‚´ìš© ===\n" + text_preview

            # 4) ì •ë§ ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ PDF ì „ì²´ ì‹œë„
            if not context_text:
                logger.warning(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ, PDF ì „ì²´ ì¶”ì¶œ ì‹œë„")
                try:
                    with pdfplumber.open(pdf_path) as pdf:
                        full_text = ""
                        for page in pdf.pages[:5]:  # ìµœëŒ€ 5í˜ì´ì§€
                            full_text += (page.extract_text() or "")
                        context_text = full_text
                except Exception as e:
                    logger.error(f"âŒ PDF ì „ì²´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    context_text = ""

            # ê¸¸ì´ ì œí•œ (ìµœëŒ€ 10,000ì)
            if len(context_text) > 10000:
                context_text = context_text[:10000]
                logger.info(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (10,000ìë¡œ ì¶•ì†Œ)")

            logger.info(f"ğŸ“‹ ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context_text)}ì")

            # ğŸ”¥ ë‹¨ê³„ 4: doctypeë³„ ë§ì¶¤ í”„ë¡¬í”„íŠ¸ ìƒì„±
            if not context_text or len(context_text.strip()) < 100:
                # ì»¨í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ ë©”íƒ€ë°ì´í„°ë§Œ í‘œì‹œ
                answer_text = f"**ğŸ“„ {fname}**\n\n"
                answer_text += "ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                answer_text += f"**ğŸ“‹ ë¬¸ì„œ ì •ë³´**\n"
                answer_text += f"- ê¸°ì•ˆì: {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                answer_text += f"- ë‚ ì§œ: {display_date or date or 'ì •ë³´ ì—†ìŒ'}\n"
                if claimed_total:
                    answer_text += f"- ê¸ˆì•¡: â‚©{claimed_total:,}\n"

            else:
                # doctypeë³„ ë§ì¶¤ í”„ë¡¬í”„íŠ¸ ìƒì„±
                system_prompt, user_prompt = get_summary_prompt(
                    doctype=doctype or "ê¸°ë³¸",
                    filename=fname,
                    display_date=display_date or date or "ì •ë³´ ì—†ìŒ",
                    claimed_total=claimed_total,
                    context_text=context_text
                )

                logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (doctype: {doctype})")

                # ğŸ”¥ ë‹¨ê³„ 5: LLM í˜¸ì¶œ (JSON ì‘ë‹µ ìš”ì²­)
                max_retries = 2
                parsed_json = None

                for attempt in range(1, max_retries + 1):
                    try:
                        logger.info(f"ğŸ¤– LLM í˜¸ì¶œ ì‹œë„ {attempt}/{max_retries}")

                        # LLM í˜¸ì¶œ
                        llm_response = self.generator.generate(
                            query=user_prompt,
                            context="",  # í”„ë¡¬í”„íŠ¸ì— ì´ë¯¸ í¬í•¨ë¨
                            temperature=0.2  # ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í–¥ìƒ
                        )

                        logger.info(f"âœ“ LLM ì‘ë‹µ ìˆ˜ì‹ : {len(llm_response)}ì")

                        # JSON íŒŒì‹± ì‹œë„
                        parsed_json = parse_summary_json(llm_response)

                        if parsed_json:
                            logger.info(f"âœ“ JSON íŒŒì‹± ì„±ê³µ (ì‹œë„ {attempt}íšŒ)")
                            break
                        else:
                            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (ì‹œë„ {attempt}íšŒ), ì¬ì‹œë„...")
                            if attempt < max_retries:
                                # ì¬ì‹œë„ ì‹œ ë¦¬ë§ˆì¸ë“œ ì¶”ê°€
                                user_prompt += "\n\n**ì¤‘ìš”**: ë°˜ë“œì‹œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”."

                    except Exception as e:
                        logger.error(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt}íšŒ): {e}")
                        if attempt >= max_retries:
                            break

                # ğŸ”¥ ë‹¨ê³„ 6: ì¶œë ¥ í¬ë§·íŒ…
                if parsed_json:
                    # JSON ê¸°ë°˜ í¬ë§·íŒ…
                    answer_text = format_summary_output(
                        parsed_json=parsed_json,
                        doctype=doctype or "ê¸°ë³¸",
                        filename=fname,
                        drafter=drafter,
                        display_date=display_date or date,
                        claimed_total=claimed_total
                    )
                    logger.info("âœ“ í¬ë§·íŒ…ëœ ìš”ì•½ ìƒì„± ì™„ë£Œ")

                else:
                    # Fallback: JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ (ì›ë³¸ JSON ìˆ¨ê¹€)
                    logger.error("âŒ JSON íŒŒì‹± ì™„ì „ ì‹¤íŒ¨ (2íšŒ ì¬ì‹œë„ í›„)")
                    answer_text = f"**ğŸ“„ {fname}**\n\n"
                    answer_text += "âš ï¸ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n"
                    answer_text += "ë¬¸ì„œë¥¼ ì§ì ‘ í™•ì¸í•˜ì‹œë ¤ë©´ ì•„ë˜ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ì´ìš©í•´ì£¼ì„¸ìš”.\n\n"

                    # ë©”íƒ€ë°ì´í„°ë§Œ í‘œì‹œ
                    answer_text += "---\n**ğŸ“‹ ë¬¸ì„œ ì •ë³´**\n"
                    answer_text += f"- ê¸°ì•ˆì: {drafter or 'ì •ë³´ ì—†ìŒ'}\n"
                    answer_text += f"- ë‚ ì§œ: {display_date or date or 'ì •ë³´ ì—†ìŒ'}\n"
                    if claimed_total:
                        answer_text += f"- ê¸ˆì•¡: â‚©{claimed_total:,}\n"

            # Evidence êµ¬ì„± (file_path ì§ì ‘ í¬í•¨)
            # year í´ë” ìë™ ê°ì§€
            year_match = re.search(r'(\d{4})-', fname)
            if year_match:
                year = year_match.group(1)
                file_path_str = f"docs/year_{year}/{fname}"
            else:
                file_path_str = f"docs/{fname}"

            evidence = [{
                "doc_id": fname,
                "filename": fname,
                "file_path": file_path_str,  # ì§ì ‘ íŒŒì¼ ê²½ë¡œ (ref ëŒ€ì‹ )
                "page": 1,
                "snippet": text_preview[:400] if text_preview else "",
                "ref": None,  # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                "meta": {
                    "filename": fname,
                    "drafter": drafter,
                    "date": display_date or date,
                    "doctype": doctype,
                    "claimed_total": claimed_total
                }
            }]

            # í’ˆì§ˆ ë°©ì–´ì„  ë¡œê·¸ (LLM ì‚¬ìš© ì—¬ë¶€ ì •í™•íˆ í‘œì‹œ)
            used_llm = text_preview and len(text_preview.strip()) > 100
            logger.info({
                "mode": "SUMMARY",
                "files": [fname],
                "llm": used_llm,
                "text_length": len(text_preview) if text_preview else 0
            })

            return {
                "text": answer_text,
                "citations": evidence,
                "evidence": evidence,
                "status": {
                    "retrieved_count": 1,
                    "selected_count": 1,
                    "found": True
                }
            }

        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "text": f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def warmup(self) -> None:
        """ì›Œë°ì—…: LLM + ì¸ë±ìŠ¤ ì‚¬ì „ ë¡œë”©

        ì²« ì¿¼ë¦¬ ì§€ì—° ì œê±°ë¥¼ ìœ„í•´ ì‹œì‘ ì‹œ í˜¸ì¶œ.
        """
        logger.info("Warming up RAG pipeline...")
        try:
            # ë”ë¯¸ ì¿¼ë¦¬ ì‹¤í–‰
            response = self.query("test warmup query", top_k=1)
            if response.success:
                logger.info(f"Warmup completed in {response.latency:.2f}s")
            else:
                logger.warning(f"Warmup failed: {response.error}")
        except Exception as e:
            logger.error(f"Warmup error: {e}", exc_info=True)

    # ========================================================================
    # ë‚´ë¶€ í—¬í¼: ê¸°ë³¸ êµ¬í˜„ ìƒì„±
    # ========================================================================

    def _create_default_retriever(self) -> Retriever:
        """ê¸°ë³¸ ê²€ìƒ‰ ì—”ì§„ ìƒì„± (v2 ë˜ëŠ” v1)

        í™˜ê²½ ë³€ìˆ˜ USE_V2_RETRIEVERë¡œ ì œì–´:
        - true: HybridRetrieverV2 ì‚¬ìš© (ì‹ ê·œ 2-layer ì•„í‚¤í…ì²˜)
        - false/ì—†ìŒ: HybridRetriever ì‚¬ìš© (ê¸°ì¡´ ë ˆê±°ì‹œ)
        """
        import os

        use_v2 = os.getenv("USE_V2_RETRIEVER", "false").lower() == "true"

        if use_v2:
            # V2 RetrieverëŠ” archiveë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤ (20251026)
            # ë ˆê±°ì‹œ ì½”ë“œë¥¼ ì œê±°í•˜ê³  v1ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤
            logger.warning(
                "âš ï¸ USE_V2_RETRIEVERëŠ” ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. v1 Retrieverë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            use_v2 = False
            # try:
            #     from app.rag.retriever_v2 import HybridRetrieverV2
            #     v2_retriever = HybridRetrieverV2()
            #     logger.info("âœ… HybridRetrieverV2 (v2 ì‹ ê·œ ì‹œìŠ¤í…œ) ìƒì„± ì™„ë£Œ")
            #
            #     # V2 adapter: fused_results â†’ list ë³€í™˜
            #     return _V2RetrieverAdapter(v2_retriever)
            # except Exception as e:
            #     logger.error(f"V2 Retriever ìƒì„± ì‹¤íŒ¨, v1ìœ¼ë¡œ í´ë°±: {e}")
            #     # í´ë°±: v1 ì‚¬ìš©
            #     use_v2 = False

        if not use_v2:
            try:
                from app.rag.retrievers.hybrid import HybridRetriever

                retriever = HybridRetriever()
                logger.info("Default HybridRetriever (v1 ë ˆê±°ì‹œ) ìƒì„± ì™„ë£Œ")
                return retriever
            except Exception as e:
                logger.error(f"HybridRetriever ìƒì„± ì‹¤íŒ¨: {e}")
                # í´ë°±: ë”ë¯¸ êµ¬í˜„
                return _DummyRetriever()

    def _create_default_compressor(self) -> Compressor:
        """ê¸°ë³¸ ì••ì¶•ê¸° ìƒì„± (í˜„ì¬ëŠ” no-op)"""
        logger.info("Default compressor ìƒì„± (no-op)")
        return _NoOpCompressor()

    def _create_default_generator(self) -> Generator:
        """ê¸°ë³¸ LLM ìƒì„±ê¸° ìƒì„± (ë ˆê±°ì‹œ ì–´ëŒ‘í„° ì‚¬ìš©)"""
        try:
            # ë ˆê±°ì‹œ êµ¬í˜„ ì–´ëŒ‘í„° ì‚¬ìš© (ì ì§„ì  ì´ê´€ ì¤€ë¹„)
            legacy_rag = self._create_legacy_adapter()
            logger.info("Default generator ìƒì„± (Legacy Adapter ë˜í•‘)")
            return _QuickFixGenerator(legacy_rag)
        except Exception as e:
            logger.error(f"Generator ìƒì„± ì‹¤íŒ¨: {e}")
            return _DummyGenerator()

    def _create_legacy_adapter(self):
        """ë ˆê±°ì‹œ êµ¬í˜„ ì–´ëŒ‘í„° ìƒì„± (ìº¡ìŠí™”)

        QuickFixRAGë¥¼ ë˜í•‘í•˜ì—¬ ê¸°ì¡´ ë ˆê±°ì‹œ ì‹œìŠ¤í…œê³¼ ì—°ê²°í•©ë‹ˆë‹¤.
        í–¥í›„ ì´ ë©”ì„œë“œë§Œ ìˆ˜ì •í•˜ì—¬ ì‹ ê·œ êµ¬í˜„ìœ¼ë¡œ ì ì§„ ì „í™˜ ê°€ëŠ¥.

        Returns:
            QuickFixRAG: ë ˆê±°ì‹œ RAG ì¸ìŠ¤í„´ìŠ¤
        """
        from quick_fix_rag import QuickFixRAG

        # ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logger.info("Loading legacy QuickFixRAG adapter...")
        rag = QuickFixRAG(use_hybrid=True)
        logger.info("Legacy adapter loaded successfully")

        return rag


# ============================================================================
# í´ë°± êµ¬í˜„ (ê¸°ë³¸ ë™ì‘ ë³´ì¥)
# ============================================================================


class _DummyRetriever:
    """ë”ë¯¸ ê²€ìƒ‰ê¸° (í´ë°±ìš©)"""

    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        logger.warning("Dummy retriever: ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return []


class _NoOpCompressor:
    """No-op ì••ì¶•ê¸° (ì••ì¶•í•˜ì§€ ì•ŠìŒ)"""

    def compress(
        self, chunks: List[Dict[str, Any]], ratio: float
    ) -> List[Dict[str, Any]]:
        logger.debug("No-op compressor: ì••ì¶• ìŠ¤í‚µ")
        return chunks


class _QuickFixGenerator:
    """QuickFixRAG ë˜í¼ (ê¸°ì¡´ êµ¬í˜„ í™œìš©)"""

    def __init__(self, rag):
        self.rag = rag
        self.compressed_chunks = None  # Store chunks for LLM

    def generate(self, query: str, context: str, temperature: float) -> str:
        # ì¬ê²€ìƒ‰ ê¸ˆì§€. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„±ìœ¼ë¡œ ìš°ì„  ì‹œë„.
        try:
            # 1) QuickFixRAGì— ì „ìš© ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if hasattr(self.rag, "generate_from_context"):
                return self.rag.generate_from_context(
                    query, context, temperature=temperature
                )

            # 2) ë‚´ë¶€ LLM ì§ì ‘ ì ‘ê·¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            # ğŸ”¥ CRITICAL: LLM lazy loading - ensure LLM is loaded before checking
            if hasattr(self.rag, "_ensure_llm_loaded"):
                self.rag._ensure_llm_loaded()

            if hasattr(self.rag, "llm") and hasattr(self.rag.llm, "generate_response"):
                # CRITICAL: generate_response expects List[Dict], not str
                # Convert context string back to chunks format
                if self.compressed_chunks:
                    # Use stored compressed chunks (preferred)
                    logger.debug(
                        f"Using {len(self.compressed_chunks)} compressed chunks for generation"
                    )
                    response = self.rag.llm.generate_response(
                        query, self.compressed_chunks, max_retries=1
                    )
                else:
                    # Fallback: convert context string to minimal chunks
                    logger.warning(
                        "No compressed_chunks available, converting context string"
                    )
                    snippets = context.split("\n\n")
                    chunks = [
                        {"snippet": s, "content": s} for s in snippets if s.strip()
                    ]
                    response = self.rag.llm.generate_response(
                        query, chunks, max_retries=1
                    )

                # Extract answer from RAGResponse object
                if hasattr(response, "answer"):
                    return response.answer
                return str(response)

            # 3) í´ë°±: ì¬ê²€ìƒ‰ì´ í¬í•¨ëœ answerëŠ” ìµœí›„ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ
            logger.warning("generate_from_context ë¯¸ì§€ì› â†’ í´ë°±(answer) ì‚¬ìš©")
            return self.rag.answer(query, use_llm_summary=True)
        except Exception as e:
            logger.error(f"Generation ì‹¤íŒ¨: {e}", exc_info=True)
            return f"[E_GENERATE] {str(e)}"


class _V2RetrieverAdapter:
    """V2 Retriever Adapter

    HybridRetrieverV2ì˜ ê²°ê³¼ í˜•ì‹ {"fused_results": [...]}ë¥¼
    v1 ì¸í„°í˜ì´ìŠ¤ í˜•ì‹ [...] ìœ¼ë¡œ ë³€í™˜.

    v2 results êµ¬ì¡°:
        {
            "fused_results": [
                {"id": "doc_4094", "score": 0.123, "filename": "...", ...},
                ...
            ]
        }

    v1 expected êµ¬ì¡°:
        [
            {"doc_id": "doc_4094", "snippet": "...", "page": 1, ...},
            ...
        ]
    """

    def __init__(self, v2_retriever):
        """
        Args:
            v2_retriever: HybridRetrieverV2 instance
        """
        self.v2_retriever = v2_retriever
        self.db = v2_retriever.db  # MetadataDB for content fetching

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Search using v2 retriever, convert to v1 format

        Args:
            query: Search query
            top_k: Number of results

        Returns:
            List of dicts in v1 format with keys:
            - doc_id: Document ID
            - snippet: Text snippet
            - page: Page number (default 1)
            - score: Relevance score
            - meta: Metadata dict
        """
        try:
            # Call v2 retriever
            v2_result = self.v2_retriever.search(query, top_k=top_k)
            fused_results = v2_result.get("fused_results", [])

            # Convert to v1 format
            v1_results = []
            for doc in fused_results:
                doc_id = doc.get("id", "unknown")

                # ğŸ”¥ CRITICAL: snippet ìš°ì„ ìˆœìœ„
                # 1) ê²€ìƒ‰ ê²°ê³¼ì— ì§ì ‘ í¬í•¨ëœ snippet/content
                # 2) DB ì¡°íšŒ (get_content)
                # 3) ì œëª©/íŒŒì¼ëª… ê¸°ë°˜ í´ë°±

                snippet = ""

                # Priority 1: fused_resultsì— ì´ë¯¸ í¬í•¨ëœ ë°ì´í„°
                if "snippet" in doc:
                    snippet = doc["snippet"]
                elif "content" in doc:
                    snippet = doc["content"][:500]

                # Priority 2: DB ì¡°íšŒ (app/rag/db.MetadataDB.get_content)
                if not snippet or len(snippet) < 50:
                    content = self.db.get_content(doc_id)
                    if content and len(content) >= 50:
                        snippet = content[:500]

                # Priority 3: ë©”íƒ€ë°ì´í„° í´ë°±
                if not snippet or len(snippet) < 50:
                    fallback_parts = []
                    if doc.get("title"):
                        fallback_parts.append(f"ì œëª©: {doc['title']}")
                    if doc.get("filename"):
                        fallback_parts.append(f"íŒŒì¼: {doc['filename']}")
                    if doc.get("date"):
                        fallback_parts.append(f"ë‚ ì§œ: {doc['date']}")

                    snippet = (
                        " | ".join(fallback_parts)
                        if fallback_parts
                        else f"ë¬¸ì„œ ID: {doc_id}"
                    )
                    logger.warning(
                        f"V2 Adapter: doc_id={doc_id} snippet ê²°ì†, ë©”íƒ€ë°ì´í„° í´ë°± ì‚¬ìš©"
                    )

                v1_results.append(
                    {
                        "doc_id": doc_id,
                        "snippet": snippet,
                        "page": 1,  # v2ì—ì„œëŠ” page ì •ë³´ ì—†ìŒ, ê¸°ë³¸ 1
                        "score": doc.get("score", 0.0),
                        "meta": {
                            "doc_id": doc_id,
                            "filename": doc.get("filename", ""),
                            "title": doc.get("title", ""),
                            "date": doc.get("date", ""),
                            "page": 1,
                        },
                    }
                )

            logger.info(f"V2 Adapter: {len(v1_results)} results converted")
            return v1_results

        except Exception as e:
            logger.error(f"V2 Adapter search failed: {e}", exc_info=True)
            return []

    def warmup(self):
        """ì›Œë°ì—… (v2ëŠ” í•„ìš” ì‹œ ìë™ ë¡œë“œ)"""
        logger.info("V2 Adapter warmup (no-op)")


class _DummyGenerator:
    """ë”ë¯¸ ìƒì„±ê¸° (í´ë°±ìš©)"""

    def generate(self, query: str, context: str, temperature: float) -> str:
        logger.warning("Dummy generator: ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
