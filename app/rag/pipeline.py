"""RAG íŒŒì´í”„ë¼ì¸ (íŒŒì‚¬ë“œ íŒ¨í„´)

ë‹¨ì¼ ì§„ì…ì : RAGPipeline.query()
ë‚´ë¶€ íë¦„: ê²€ìƒ‰ â†’ ì••ì¶• â†’ LLM ìƒì„±

Example:
    >>> pipeline = RAGPipeline()
    >>> response = pipeline.query("ì§ˆë¬¸", top_k=5)
    >>> print(response.answer)
"""

import os
import time
import base64
import sqlite3
from pathlib import Path
from dataclasses import dataclass, field
from typing import Protocol, List, Optional, Dict, Any

from app.core.logging import get_logger
from app.core.errors import ModelError, SearchError, ErrorCode, ERROR_MESSAGES
from app.rag.query_router import QueryRouter, QueryMode
from app.rag.cache_manager import get_cached_result, cache_query_result, get_cache_stats
from app.rag.persistent_cache import get_cached_result_persistent, cache_query_result_persistent

logger = get_logger(__name__)


# ============================================================================
# ë¼ìš°íŒ… í—¬í¼ í•¨ìˆ˜ë“¤ (ìŠ¤ëª°í† í¬/ì‚°ìˆ /ë„ë©”ì¸ í‚¤ì›Œë“œ ê°ì§€)
# ============================================================================

import re

# ìŠ¤ëª°í† í¬ íŒ¨í„´
SMALLTALK_PATTERNS = {
    'ì•ˆë…•', 'ì•ˆë…•í•˜ì„¸ìš”', 'ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ', 'hello', 'hi', 'hey',
    'ë•¡í', 'ê°ì‚¬', 'ê³ ë§ˆì›Œ', 'thanks', 'thank you',
    'ì˜ê°€', 'ì•ˆë…•íˆ', 'bye', 'goodbye',
    'ì–´ë–»ê²Œ', 'ì–´ë– ', 'ì–´ë•Œ', 'ë­í•´', 'ë¬´ì—‡',
}

# ë„ë©”ì¸ í‚¤ì›Œë“œ (ì¥ë¹„/í”„ë¡œì íŠ¸/ê¸°ìˆ  ìš©ì–´)
DOMAIN_KEYWORDS = {
    # ì¥ë¹„
    'nvr', 'sync', 'eco8000', 'lvm-180a', 'odin', 'vmix', 'faiss',
    'tri-level', 'sdi', 'lut', 'intercom', 'di box', 'dibox',
    'ë¬´ì„ ë§ˆì´í¬', 'ë§ˆì´í¬', 'ì¹´ë©”ë¼', 'ë Œì¦ˆ', 'ì‚¼ê°ëŒ€', 'ì¼€ì´ë¸”',
    'ê±´ì „ì§€', 'ë°°í„°ë¦¬', 'ì†Œëª¨í’ˆ', 'ì¥ë¹„', 'ì¤‘ê³„ì°¨',
    # í”„ë¡œì íŠ¸/í”„ë¡œê·¸ë¨
    'ëŒì§êµ¬ì‡¼', 'ë‰´ìŠ¤', 'ìŠ¤íŠœë””ì˜¤', 'ê´‘í™”ë¬¸', 'ì˜¤í”ˆìŠ¤íŠœë””ì˜¤',
    'ì¤‘ê³„', 'ë°©ì†¡', 'ì±„ë„ì—ì´',
    # ê¸°ìˆ /ë¬¸ì„œ
    'ê¸°ì•ˆì„œ', 'êµ¬ë§¤', 'ìˆ˜ë¦¬', 'êµì²´', 'ê²€í† ', 'ê¸°ìˆ ê²€í† ',
    'ì˜¤ë²„í™€', 'ë„ì…', 'ë…¸í›„í™”', 'ë‹¨ì¢…',
    'ì‘ì„±', 'ì‘ì„±ëœ', 'ë¬¸ì„œ', 'ë¦¬ìŠ¤íŠ¸', 'ëª©ë¡',
    # ì‘ì„±ì (ì‹¤ì œ ê¸°ì•ˆì ì´ë¦„)
    'ìµœìƒˆë¦„', 'ìœ ì¸í˜', 'ë‚¨ì¤€ìˆ˜', 'ë°•ì¤€ì„œ', 'ì´ì›êµ¬',
    'ìµœì •ì€', 'í•œê±´í¬', 'ê¹€ê²½í˜„', 'ê¹€ìˆ˜ì—°', 'ê¹€ì°½ìˆ˜', 'ì†¡ê²½ì›',
}


def clean_ui_metadata(query: str) -> str:
    """UIì—ì„œ ë³µì‚¬í•œ ë©”íƒ€ë°ì´í„° íƒœê·¸ ì œê±° (ğŸ·, ğŸ“…, âœ ë“±)

    ì˜ˆì‹œ:
        ì…ë ¥: "2024 ì¤‘ê³„ì°¨ ğŸ· pdf Â· ğŸ“… 2024-10-24 Â· âœ ë¬¸ì„œ ë‚´ìš© ìš”ì•½í•´ ì¤˜"
        ì¶œë ¥: "2024 ì¤‘ê³„ì°¨ ë¬¸ì„œ ë‚´ìš© ìš”ì•½í•´ ì¤˜"
    """
    import re

    # ì›ë³¸ ë³´ì¡´ (ë””ë²„ê¹…ìš©)
    original = query

    # íŒ¨í„´ 1: ğŸ· [í…ìŠ¤íŠ¸] Â· í˜•íƒœ ì œê±°
    query = re.sub(r'ğŸ·[^Â·]+Â·\s*', '', query)

    # íŒ¨í„´ 2: ğŸ“… [ë‚ ì§œ] Â· í˜•íƒœ ì œê±°
    query = re.sub(r'ğŸ“…[^Â·]+Â·\s*', '', query)

    # íŒ¨í„´ 3: âœ [í…ìŠ¤íŠ¸] (ë§ˆì§€ë§‰ í•­ëª©, Â· ì—†ìŒ)
    query = re.sub(r'âœ[^Â·]+', '', query)

    # íŒ¨í„´ 4: "pdf", "í•´ ì¤˜" ê°™ì€ ë¶ˆí•„ìš”í•œ í™•ì¥ì ì–¸ê¸‰ ì œê±°
    query = re.sub(r'\s+pdf\s+', ' ', query)

    # ì—°ì† ê³µë°± ì •ë¦¬
    query = re.sub(r'\s+', ' ', query).strip()

    # ë³€ê²½ì‚¬í•­ì´ ìˆìœ¼ë©´ ë¡œê·¸ ì¶œë ¥
    if query != original:
        logger.info(f"ğŸ§¹ UI ë©”íƒ€ë°ì´í„° ì œê±°: '{original[:60]}...' â†’ '{query[:60]}...'")

    return query


def is_smalltalk(query: str) -> bool:
    """ìŠ¤ëª°í† í¬/ì¸ì‚¬/ê°íƒ„ì‚¬ ê°ì§€"""
    q_lower = query.lower().strip()
    # ê¸¸ì´ ì²´í¬
    if len(q_lower) <= 3:
        return True
    # íŒ¨í„´ ë§¤ì¹­
    for pattern in SMALLTALK_PATTERNS:
        if pattern in q_lower:
            return True
    return False


def is_simple_math(query: str) -> bool:
    """ë‹¨ìˆœ ì‚°ìˆ  ì§ˆì˜ ê°ì§€ (ì˜ˆ: 1+1ì€?, 2*3=?)"""
    q_stripped = query.strip()
    # ì •ê·œì‹: ìˆ«ì ì—°ì‚°ì ìˆ«ì (ì˜µì…˜: = ê²°ê³¼)
    math_pattern = r'^\s*\d+\s*[\+\-\*/]\s*\d+\s*(=\s*\d+)?\s*[ì€?]*\s*$'
    return bool(re.match(math_pattern, q_stripped))


def has_domain_keyword(query: str) -> bool:
    """ë„ë©”ì¸ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    q_lower = query.lower()
    for keyword in DOMAIN_KEYWORDS:
        if keyword in q_lower:
            return True
    return False


def get_query_token_count(query: str) -> int:
    """ê°„ì´ í† í° ì¹´ìš´íŠ¸ (ê³µë°±/í•œê¸€ ê¸°ì¤€)"""
    # í•œê¸€: ìŒì ˆ ë‹¨ìœ„, ì˜ë¬¸: ë‹¨ì–´ ë‹¨ìœ„
    korean_chars = len([c for c in query if '\uac00' <= c <= '\ud7a3'])
    english_words = len(query.split())
    return max(korean_chars, english_words)


def get_keyword_coverage(query: str, results: list) -> int:
    """ì¿¼ë¦¬ì™€ ê²€ìƒ‰ ê²°ê³¼ ê°„ ë„ë©”ì¸ í‚¤ì›Œë“œ êµì§‘í•© ê°œìˆ˜ ê³„ì‚°

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

    Returns:
        êµì§‘í•©ëœ í‚¤ì›Œë“œ ê°œìˆ˜
    """
    q_lower = query.lower()
    # ì¿¼ë¦¬ì—ì„œ ë§¤ì¹­ëœ í‚¤ì›Œë“œ
    query_keywords = {kw for kw in DOMAIN_KEYWORDS if kw in q_lower}

    if not query_keywords:
        return 0

    # ê²€ìƒ‰ ê²°ê³¼ ì²­í¬ì—ì„œ ë°œê²¬ëœ í‚¤ì›Œë“œ
    found_keywords = set()
    for result in results[:5]:  # ìƒìœ„ 5ê°œë§Œ ì²´í¬
        chunk_text = result.get('snippet', '') + ' ' + result.get('content', '')
        chunk_lower = chunk_text.lower()
        for kw in query_keywords:
            if kw in chunk_lower:
                found_keywords.add(kw)

    return len(found_keywords)


def force_chat_mode(query: str) -> tuple[bool, str]:
    """ê°•ì œ CHAT ëª¨ë“œ ì ìš© ì—¬ë¶€ íŒë‹¨

    Returns:
        (should_force, reason)
    """
    # 1. ìŠ¤ëª°í† í¬
    if is_smalltalk(query):
        return True, "smalltalk"

    # 2. ì§§ì€ ì§ˆì˜ (í† í° <4)
    if get_query_token_count(query) < 4:
        return True, "short_query"

    # 3. ë‹¨ìˆœ ì‚°ìˆ 
    if is_simple_math(query):
        return True, "simple_math"

    return False, ""


def _encode_file_ref(filename: str) -> Optional[str]:
    """íŒŒì¼ëª…ì„ base64 refë¡œ ì¸ì½”ë”© (docs í•˜ìœ„ ê²½ë¡œ ì°¾ê¸°)

    Args:
        filename: íŒŒì¼ëª…

    Returns:
        base64 ì¸ì½”ë”©ëœ ref ë˜ëŠ” None
    """
    try:
        # 1. metadata.dbì—ì„œ ê²½ë¡œ ì°¾ê¸° ì‹œë„
        conn = sqlite3.connect("metadata.db")
        cursor = conn.cursor()
        cursor.execute(
            "SELECT path FROM documents WHERE filename = ? LIMIT 1",
            (filename,)
        )
        result = cursor.fetchone()
        conn.close()

        if result and result[0]:
            file_path = Path(result[0])
            # docs í•˜ìœ„ì¸ì§€ í™•ì¸
            if "docs" in file_path.parts and file_path.exists():
                # base64 ì¸ì½”ë”©
                ref = base64.urlsafe_b64encode(str(file_path).encode()).decode()
                return ref

        # 2. Fallback: docs í´ë”ì—ì„œ íŒŒì¼ ê²€ìƒ‰ (year í´ë” í¬í•¨)
        import re
        year_match = re.search(r'(\d{4})-', filename)
        if year_match:
            year = year_match.group(1)
            # docs/year_YYYY/ í´ë”ì—ì„œ ì°¾ê¸°
            file_path = Path(f"docs/year_{year}") / filename
            if file_path.exists():
                ref = base64.urlsafe_b64encode(str(file_path).encode()).decode()
                return ref

        # 3. Fallback2: docs í´ë” ì „ì²´ ê²€ìƒ‰
        docs_dir = Path("docs")
        if docs_dir.exists():
            for file_path in docs_dir.rglob(filename):
                if file_path.is_file():
                    ref = base64.urlsafe_b64encode(str(file_path).encode()).decode()
                    return ref

    except Exception as e:
        logger.warning(f"ref ì¸ì½”ë”© ì‹¤íŒ¨: {filename} - {e}")

    return None

# ì§„ë‹¨ ëª¨ë“œ ì„¤ì •
DIAG_RAG = os.getenv("DIAG_RAG", "false").lower() == "true"
DIAG_LOG_LEVEL = os.getenv("DIAG_LOG_LEVEL", "INFO").upper()


# ============================================================================
# Request / Response ë°ì´í„° í´ë˜ìŠ¤
# ============================================================================


@dataclass
class RAGRequest:
    """RAG ìš”ì²­ íŒŒë¼ë¯¸í„°

    Attributes:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
        compression_ratio: ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ë¹„ìœ¨ (0.0~1.0)
        use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
        temperature: LLM ìƒì„± ì˜¨ë„
    """

    query: str
    top_k: int = 5
    compression_ratio: float = 0.7
    use_hyde: bool = False
    temperature: float = 0.1


@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ê²°ê³¼

    Attributes:
        answer: ìƒì„±ëœ ë‹µë³€
        source_docs: ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í•˜ìœ„ í˜¸í™˜)
        evidence_chunks: Evidenceìš© ì •ê·œí™” ì²­í¬ (ê¶Œì¥)
        raw_results: ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ (Evidence ìµœì†Œ ë³´ì¥ìš©)
        latency: ì „ì²´ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
        success: ì„±ê³µ ì—¬ë¶€
        error: ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
        metrics: ë‚´ë¶€ ì§€í‘œ (ê²€ìƒ‰/ì••ì¶•/ìƒì„± ì‹œê°„ ë“±)
        diagnostics: ì§„ë‹¨ ì •ë³´ (DIAG_RAG=trueì¼ ë•Œë§Œ ì±„ì›Œì§)
    """

    answer: str
    source_docs: List[str] = field(default_factory=list)
    evidence_chunks: List[Dict[str, Any]] = field(default_factory=list)
    raw_results: List[Dict[str, Any]] = field(default_factory=list)
    latency: float = 0.0
    success: bool = True
    error: Optional[str] = None
    metrics: dict = field(default_factory=dict)
    diagnostics: dict = field(default_factory=dict)  # ì§„ë‹¨ ì •ë³´


# ============================================================================
# í”„ë¡œí† ì½œ ì •ì˜ (ì˜ì¡´ì„± ì—­ì „)
# ============================================================================


class Retriever(Protocol):
    """ê²€ìƒ‰ ì—”ì§„ ì¸í„°í˜ì´ìŠ¤"""

    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ìˆ˜í–‰ (ì •ê·œí™” ìŠ¤í‚¤ë§ˆ ë°˜í™˜)

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼

        Returns:
            [
                {
                    "doc_id": str,
                    "page": int,
                    "score": float,
                    "snippet": str,
                    "meta": dict
                }, ...
            ]
        """
        ...


class Compressor(Protocol):
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶•ê¸° ì¸í„°í˜ì´ìŠ¤"""

    def compress(
        self, chunks: List[Dict[str, Any]], ratio: float
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ì••ì¶•

        Args:
            chunks: ì›ë³¸ ì²­í¬ ëª©ë¡ (ì •ê·œí™”ëœ dict)
            ratio: ì••ì¶• ë¹„ìœ¨

        Returns:
            ì••ì¶•ëœ ì²­í¬ ëª©ë¡ (ë™ì¼ ìŠ¤í‚¤ë§ˆ)
        """
        ...


class Generator(Protocol):
    """LLM ìƒì„±ê¸° ì¸í„°í˜ì´ìŠ¤"""

    def generate(self, query: str, context: str, temperature: float, mode: str = "rag") -> str:
        """ë‹µë³€ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì°¸ê³  ë¬¸ì„œ
            temperature: ìƒì„± ì˜¨ë„
            mode: ìƒì„± ëª¨ë“œ ("chat", "rag", "summarize") - í† í° ì˜ˆì‚° ì œì–´

        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        ...


# ============================================================================
# RAG íŒŒì´í”„ë¼ì¸ (íŒŒì‚¬ë“œ)
# ============================================================================


class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸ íŒŒì‚¬ë“œ

    ê²€ìƒ‰ â†’ ì••ì¶• â†’ ìƒì„±ì„ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ ì œê³µ.
    ë‚´ë¶€ êµ¬í˜„ì€ Retriever/Compressor/Generatorì— ìœ„ì„.

    Example:
        >>> pipeline = RAGPipeline()
        >>> response = pipeline.query("ì§ˆë¬¸", top_k=5)
        >>> if response.success:
        ...     print(response.answer)
        ...     print(f"ì°¸ê³ : {response.source_docs}")
    """

    def __init__(
        self,
        retriever: Optional[Retriever] = None,
        compressor: Optional[Compressor] = None,
        generator: Optional[Generator] = None,
    ):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            retriever: ê²€ìƒ‰ ì—”ì§„ (Noneì´ë©´ ê¸°ë³¸ HybridRetriever ì‚¬ìš©)
            compressor: ì••ì¶•ê¸° (Noneì´ë©´ ê¸°ë³¸ ContextCompressor ì‚¬ìš©)
            generator: LLM ìƒì„±ê¸° (Noneì´ë©´ ê¸°ë³¸ LlamaCppGenerator ì‚¬ìš©)
        """
        self.retriever = retriever or self._create_default_retriever()
        self.compressor = compressor or self._create_default_compressor()
        self.generator = generator or self._create_default_generator()
        self.query_router = QueryRouter()  # ğŸ¯ ëª¨ë“œ ë¼ìš°í„° ì´ˆê¸°í™”

        # ğŸ”’ Closed-World Validation: ê³ ìœ  ê¸°ì•ˆì ìºì‹±
        self.known_drafters = self._load_known_drafters()

        logger.info(f"RAG Pipeline initialized (known_drafters: {len(self.known_drafters)}ëª…)")

    def _load_full_text_if_short(self, filename: str, snippet: str) -> str:
        """ìŠ¤ë‹ˆí«ì´ ì§§ìœ¼ë©´ data/extractedì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë“œ"""
        EXTRACTED_DIR = Path(os.getenv("EXTRACTED_DIR", "data/extracted"))
        MIN_SNIPPET_LEN = int(os.getenv("DOC_ANCHOR_MIN_SNIPPET", "1200"))

        if len(snippet) >= MIN_SNIPPET_LEN:
            return snippet

        # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±° í›„ .txt ì°¾ê¸°
        stem = os.path.splitext(filename)[0]
        txt_path = EXTRACTED_DIR / f"{stem}.txt"

        if txt_path.exists():
            try:
                full_text = txt_path.read_text(encoding="utf-8", errors="ignore")
                logger.info(f"ğŸ“„ DOC_ANCHORED: {filename} ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë“œ ({len(full_text)}ì)")
                return full_text[:5000]  # ìµœëŒ€ 5000ì
            except Exception as e:
                logger.warning(f"âš ï¸ ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        return snippet

    def query(
        self,
        query: str,
        top_k: int = 5,
        compression_ratio: float = 0.7,
        use_hyde: bool = False,
        temperature: float = 0.1,
        selected_filename: Optional[str] = None,
    ) -> RAGResponse:
        """RAG ì§ˆì˜ (ë‹¨ì¼ ì§„ì…ì )

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            compression_ratio: ì••ì¶• ë¹„ìœ¨
            use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
            temperature: LLM ìƒì„± ì˜¨ë„
            selected_filename: ì„ íƒëœ ë¬¸ì„œ íŒŒì¼ëª… (ìš°ì„  ê²€ìƒ‰ìš©, ì„ íƒì‚¬í•­)

        Returns:
            RAGResponse: ë‹µë³€ + ë©”íƒ€ë°ì´í„°
        """

        # ì…ë ¥ ê²€ì¦
        if not query or not query.strip():
            return RAGResponse(
                answer="",
                success=False,
                error="ë¹ˆ ì§ˆë¬¸ì…ë‹ˆë‹¤",
            )

        start_time = time.perf_counter()
        metrics = {}
        diagnostics = {}  # ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘

        try:
            # 0. ê²€ìƒ‰ ì „ pre-routing: ì¥ë¹„ ì§ˆì˜ ê°ì§€ (DOC_ANCHORED í•„í„°ë§ìš©)
            # QueryRouterì˜ device term ê°ì§€ ë¡œì§ í™œìš©
            preliminary_mode = "chat"
            if hasattr(self, 'query_router') and hasattr(self.query_router, '_has_device_terms'):
                if self.query_router._has_device_terms(query):
                    preliminary_mode = "doc_anchored"
                    logger.info("ğŸ¯ ê²€ìƒ‰ ì „ DOC_ANCHORED ëª¨ë“œ ê°ì§€ (ì¥ë¹„ ìš©ì–´)")

            # 1. ê²€ìƒ‰: ì •ê·œí™”ëœ ì²­í¬(dict) ë¦¬ìŠ¤íŠ¸ ê¸°ëŒ€
            search_start = time.perf_counter()
            results = self.retriever.search(query, top_k, mode=preliminary_mode, selected_filename=selected_filename)
            metrics["search_time"] = time.perf_counter() - search_start

            # [ê²€ìƒ‰ ê²°ê³¼ Top-N ì§„ë‹¨ ë¡œê·¸]
            logger.info(f"RETRIEVE_TOPN mode={preliminary_mode}")
            for i, doc in enumerate(results[:10], 1):
                score = doc.get('score', 0.0)
                doc_id = doc.get('doc_id', 'unknown')
                snippet_preview = doc.get('snippet', '')[:60].replace('\n', ' ')
                logger.info(f"  #{i} score={score:.4f} doc={doc_id} preview={snippet_preview}...")

            # [DIAG] ê²€ìƒ‰ ê²°ê³¼ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["retrieved_k"] = len(results)
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(f"[DIAG] ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")

            if not results:
                logger.warning(f"No results found for query: {query[:50]}")
                if DIAG_RAG:
                    diagnostics["mode"] = "no_results"
                    diagnostics["generate_path"] = "fallback_no_context"

                # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ CHAT ëª¨ë“œë¡œ í´ë°±
                metrics["mode"] = "chat"
                metrics["top_score"] = 0.0

                return RAGResponse(
                    answer="ê´€ë ¨ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ë‹¤.",
                    success=True,
                    latency=time.perf_counter() - start_time,
                    metrics=metrics,
                    diagnostics=diagnostics,
                )

            # 2. ì••ì¶•: ì²­í¬ ë‹¨ìœ„ ìœ ì§€(í˜ì´ì§€/ìŠ¤ë‹ˆí«/ë©”íƒ€ ë³´ì¡´)
            compress_start = time.perf_counter()
            compressed = self.compressor.compress(results, compression_ratio)
            metrics["compress_time"] = time.perf_counter() - compress_start

            # [DIAG] ì••ì¶• í›„ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["after_compress_k"] = len(compressed)
                diagnostics["compression_ratio"] = compression_ratio
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(
                        f"[DIAG] ì••ì¶• ì™„ë£Œ: {len(results)} â†’ {len(compressed)}ê°œ ë¬¸ì„œ"
                    )

            # 3. ìƒì„±: ëª¨ë“œ ê²°ì • â†’ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” â†’ ìƒì„±
            gen_start = time.perf_counter()

            # CRITICAL: Inject compressed chunks into generator for proper LLM context
            if hasattr(self.generator, "compressed_chunks"):
                self.generator.compressed_chunks = compressed
                logger.debug(
                    f"Injected {len(compressed)} compressed chunks into generator"
                )

            # [DIAG] ìƒì„± ì „ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ·
            if DIAG_RAG and DIAG_LOG_LEVEL == "DEBUG":
                for i, c in enumerate(compressed[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ë¡œê·¸
                    logger.debug(
                        f"[DIAG] Context[{i}]: doc_id={c.get('doc_id')}, "
                        f"filename={c.get('filename', 'N/A')}, "
                        f"page={c.get('page', 0)}, "
                        f"snippet={c.get('snippet', '')[:120]}..."
                    )

            # ğŸ¯ STEP 1: QueryRouter ëª¨ë“œ ë¶„ë¥˜ (DOC_ANCHORED ìµœìš°ì„  ì²´í¬)
            # CRITICAL: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê³ ë ¤í•œ ì§€ëŠ¥í˜• ë¼ìš°íŒ…
            query_mode = self.query_router.classify_mode_with_retrieval(query, results)
            logger.info(f"ğŸ”€ QueryRouter ë¶„ë¥˜: mode={query_mode.value}")

            # ğŸ¯ STEP 2: ëª¨ë“œ ê²°ì • ë¡œì§
            # CRITICAL: Determine mode BEFORE context hydration to apply mode-aware context limits
            mode_env = os.getenv('MODE', 'AUTO').upper()
            top_score = results[0].get('score', 0.0) if results else 0.0
            metrics["top_score"] = top_score

            if mode_env == 'AUTO':
                # â”â”â” 1. ê°•ì œ CHAT ëª¨ë“œ ì²´í¬ (ìŠ¤ëª°í† í¬/ì‚°ìˆ /ì§§ì€ ì§ˆì˜) â”â”â”
                should_force, force_reason = force_chat_mode(query)
                if should_force:
                    metrics["mode"] = "chat"
                    metrics["force_chat_reason"] = force_reason
                    logger.info(f"ğŸ¯ AUTO ëª¨ë“œ: CHAT ê°•ì œ ì ìš© (ì´ìœ : {force_reason})")
                else:
                    # â”â”â” 2. ë„ë©”ì¸ í‚¤ì›Œë“œ + ì ˆëŒ€ê°’ ì„ê³„ê°’ ê¸°ë°˜ íŒë‹¨ â”â”â”
                    has_keyword = has_domain_keyword(query)
                    token_count = get_query_token_count(query)

                    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì ˆëŒ€ê°’ ì„ê³„ê°’ ì½ê¸°
                    use_absolute = os.getenv('RAG_MIN_SCORE_POLICY', 'normalized') == 'absolute'
                    bm25_min = float(os.getenv('BM25_MIN_ABS', '5.0'))
                    vec_min = float(os.getenv('VEC_MIN_ABS', '0.25'))

                    # ì ˆëŒ€ê°’ ì •ì±… ì‚¬ìš© ì‹œ (ê¶Œì¥)
                    if use_absolute:
                        # ì‹¤ì œ BM25/ë²¡í„° ìŠ¤ì½”ì–´ë¥¼ resultsì—ì„œ ì¶”ì¶œ ì‹œë„
                        # (í˜„ì¬ëŠ” fused scoreë§Œ ìˆìœ¼ë¯€ë¡œ ê°„ì†Œí™”)
                        # ì¼ë‹¨ top_scoreë¥¼ ë²¡í„° ìŠ¤ì½”ì–´ë¡œ ê°„ì£¼
                        pass_abs_threshold = top_score >= vec_min
                        pass_domain = has_keyword
                        pass_length = token_count >= 4

                        # ğŸ”’ Coverage Gate: ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì‹¤ì œë¡œ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ëŠ”ì§€ í™•ì¸
                        keyword_coverage = get_keyword_coverage(query, results)
                        min_coverage = int(os.getenv('MIN_KEYWORD_COVERAGE', '2'))
                        pass_coverage = keyword_coverage >= min_coverage

                        should_use_rag = pass_abs_threshold and pass_domain and pass_length and pass_coverage
                        metrics["mode"] = "rag" if should_use_rag else "chat"
                        metrics["keyword_coverage"] = keyword_coverage

                        logger.info(
                            f"ğŸ¯ AUTO ëª¨ë“œ (ì ˆëŒ€ê°’): top_score={top_score:.3f}, "
                            f"has_keyword={has_keyword}, token_count={token_count}, "
                            f"coverage={keyword_coverage}/{min_coverage}, "
                            f"threshold={vec_min}, selected_mode={metrics['mode']}"
                        )
                    else:
                        # ê¸°ì¡´ ì •ê·œí™” ì •ì±… (fallback)
                        rag_min_score = float(os.getenv('RAG_MIN_SCORE', '0.35'))
                        metrics["mode"] = "rag" if top_score >= rag_min_score else "chat"
                        logger.info(
                            f"ğŸ¯ AUTO ëª¨ë“œ (ì •ê·œí™”): top_score={top_score:.3f}, "
                            f"threshold={rag_min_score}, selected_mode={metrics['mode']}"
                        )

            elif mode_env == 'CHAT':
                metrics["mode"] = "chat"
                metrics["top_score"] = 0.0
            else:  # RAG, SUMMARIZE
                metrics["mode"] = "rag"
                metrics["top_score"] = results[0].get('score', 0.0) if results else 0.0

            # ğŸ¯ STEP 2: ëª¨ë“œ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìµœì í™”
            determined_mode = metrics.get("mode", "rag")
            logger.info(f"ğŸ¯ ëª¨ë“œ={determined_mode} â†’ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì‹œì‘")

            # Context Hydrator with mode-aware optimization
            from app.rag.utils.context_hydrator import hydrate_context
            hydrate_start = time.perf_counter()
            context, hydrator_metrics = hydrate_context(compressed, max_len=10000, mode=determined_mode)
            metrics["hydrate_time"] = time.perf_counter() - hydrate_start
            # Merge hydrator metrics into main metrics
            metrics.update({f"ctx_{k}": v for k, v in hydrator_metrics.items()})

            # ğŸ¯ STEP 3: ìƒì„± (ëª¨ë“œë³„ í† í° ì˜ˆì‚° ì ìš©)
            logger.info(f"ğŸ¯ ëª¨ë“œ={determined_mode} â†’ ìƒì„± ì‹œì‘")
            llm_gen_start = time.perf_counter()
            answer = self.generator.generate(query, context, temperature, mode=determined_mode)
            metrics["generate_time"] = time.perf_counter() - llm_gen_start

            # [DIAG] ìƒì„± ì™„ë£Œ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["mode"] = "normal"
                diagnostics["generate_path"] = "from_context"
                diagnostics["used_k"] = len(compressed)
                if DIAG_LOG_LEVEL in ["DEBUG", "INFO"]:
                    logger.info(
                        f"[DIAG] ìƒì„± ì™„ë£Œ: from_context ê²½ë¡œ, {len(compressed)}ê°œ ë¬¸ì„œ ì‚¬ìš©"
                    )

            total_latency = time.perf_counter() - start_time
            metrics["total_time"] = total_latency

            # ğŸš¨ ì„±ëŠ¥ ê°€ë“œ: ìŠ¬ë¡œ ì¿¼ë¦¬ ì„ê³„ê°’ ì²´í¬
            if total_latency > 10.0:
                logger.warning(
                    f"âš ï¸  SLOW_QUERY (>10s): {total_latency:.2f}s | "
                    f"query='{query[:50]}...' | "
                    f"search={metrics['search_time']:.2f}s, "
                    f"hydrate={metrics.get('hydrate_time', 0):.3f}s, "
                    f"generate={metrics['generate_time']:.2f}s"
                )
            elif total_latency > 3.0:
                logger.warning(
                    f"âš ï¸  SLOW_QUERY (>3s): {total_latency:.2f}s | "
                    f"query='{query[:50]}...'"
                )

            logger.info(
                f"RAG query completed in {total_latency:.2f}s "
                f"(search={metrics['search_time']:.2f}s, "
                f"compress={metrics['compress_time']:.2f}s, "
                f"hydrate={metrics.get('hydrate_time', 0):.3f}s, "
                f"generate={metrics['generate_time']:.2f}s)"
            )

            # CHAT ëª¨ë“œì¼ ê²½ìš° ì¶œì²˜ ì œê±° (ì¼ë°˜ ëŒ€í™”ëŠ” ë¬¸ì„œ ì¸ìš© ë¶ˆí•„ìš”)
            # "ì „ë¶€" ë˜ëŠ” "ê°œìˆ˜" ì§ˆì˜ ê°ì§€ ì‹œ ì¶œì²˜ë„ ë” ë§ì´ í‘œì‹œ
            max_sources = 200 if any(kw in query.lower() for kw in ["ì „ë¶€", "ëª¨ë‘", "ëª¨ë“ ", "ì „ì²´", "all", "ëª‡", "ê°œìˆ˜", "ì´"]) else 3
            final_source_docs = [] if determined_mode == "chat" else [c.get("doc_id") for c in results[:max_sources]]
            final_evidence_chunks = [] if determined_mode == "chat" else compressed

            return RAGResponse(
                answer=answer,
                source_docs=final_source_docs,
                evidence_chunks=final_evidence_chunks,  # UIìš© ê·¼ê±°
                raw_results=results,  # Evidence ìµœì†Œ ë³´ì¥ìš©
                latency=total_latency,
                success=True,
                metrics=metrics,
                diagnostics=diagnostics,
            )

        except SearchError as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_RETRIEVE] ê²€ìƒ‰ ì‹¤íŒ¨: {e.message}",
                latency=time.perf_counter() - start_time,
            )

        except ModelError as e:
            logger.error(f"Model inference failed: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_GENERATE] ìƒì„± ì‹¤íŒ¨: {e.message}",
                latency=time.perf_counter() - start_time,
            )

        except Exception as e:
            logger.error(f"Unexpected error: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_UNKNOWN] {str(e)}",
                latency=time.perf_counter() - start_time,
            )

    def _make_response(
        self, text: str, selected: List[Dict[str, Any]], retrieved: List[Dict[str, Any]]
    ) -> dict:
        """í‘œì¤€ ì‘ë‹µ êµ¬ì¡° ìƒì„± (citations í¬í•¨)

        Args:
            text: ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
            selected: ì‹¤ì œ ì‚¬ìš©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ì••ì¶• í›„)
            retrieved: ê²€ìƒ‰ëœ ì›ë³¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            í‘œì¤€í™”ëœ ì‘ë‹µ dict (citations í•„ìˆ˜)
        """
        citations = []
        for c in selected:
            filename = c.get("filename") or c.get("doc_id") or c.get("title", "")
            ref = _encode_file_ref(filename) if filename else None

            citations.append({
                "doc_id": c.get("doc_id"),
                "filename": filename,
                "title": c.get("title") or filename or c.get("doc_id"),
                "page": c.get("page", 1),
                "snippet": (
                    c.get("text") or c.get("snippet") or c.get("content") or ""
                )[:400],
                "ref": ref,  # ğŸ”´ base64 ì¸ì½”ë”©ëœ íŒŒì¼ ê²½ë¡œ
                "preview_url": c.get("preview_url"),
                "download_url": c.get("download_url"),
            })

        return {
            "text": text,
            "citations": citations,  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
            "evidence": citations,  # í•˜ìœ„ í˜¸í™˜ì„± (ë™ì¼ ë°ì´í„°)
            "status": {
                "retrieved_count": len(retrieved),
                "selected_count": len(selected),
                "found": len(selected) > 0,  # ğŸ”´ ìœ ì¼í•œ íŒì • ê¸°ì¤€
            },
        }

    def answer(self, query: str, top_k: Optional[int] = None, selected_filename: Optional[str] = None) -> dict:
        """ë‹µë³€ ìƒì„± (Evidence í¬í•¨ êµ¬ì¡°í™”ëœ ì‘ë‹µ)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ (Noneì´ë©´ ê¸°ë³¸ê°’ 5)
            selected_filename: ì„ íƒëœ ë¬¸ì„œ íŒŒì¼ëª… (ìš°ì„  ê²€ìƒ‰ìš©, ì„ íƒì‚¬í•­)

        Returns:
            dict: {
                "text": ë‹µë³€ í…ìŠ¤íŠ¸,
                "citations": ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í‘œì¤€ í‚¤),
                "evidence": ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í•˜ìœ„ í˜¸í™˜),
                "status": {
                    "retrieved_count": int,
                    "selected_count": int,
                    "found": bool
                }
            }
        """
        # âœ¨ 2-tier Cache check - ë©”ëª¨ë¦¬ ìºì‹œ â†’ ì˜êµ¬ ìºì‹œ
        cache_key = f"{query}:{selected_filename}" if selected_filename else query

        # Tier 1: ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸ (ê°€ì¥ ë¹ ë¦„)
        cached_result = get_cached_result(cache_key)
        if cached_result:
            logger.info(f"ğŸ¯ Memory Cache HIT! Returning cached result for query: {query[:50]}...")
            if "status" in cached_result:
                cached_result["status"]["from_cache"] = "memory"
            return cached_result

        # Tier 2: ì˜êµ¬ ìºì‹œ í™•ì¸ (ì„œë²„ ì¬ì‹œì‘ í›„ì—ë„ ìœ ì§€)
        cached_result = get_cached_result_persistent(cache_key)
        if cached_result:
            logger.info(f"ğŸ’¾ Persistent Cache HIT! Returning cached result for query: {query[:50]}...")
            # ì˜êµ¬ ìºì‹œì—ì„œ ê°€ì ¸ì˜¨ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥ (ë‹¤ìŒ ì ‘ê·¼ì„ ìœ„í•´)
            cache_query_result(cache_key, cached_result)
            if "status" in cached_result:
                cached_result["status"]["from_cache"] = "persistent"
            return cached_result

        # ğŸ”¥ CRITICAL: ê¸°ì•ˆì/ë‚ ì§œ ê²€ìƒ‰ì€ QuickFixRAGì— ìœ„ì„ (ì „ë¬¸ ë¡œì§ ë³´ìœ )
        if hasattr(self.generator, "rag"):
            import re
            import sqlite3

            # âœ… í™•ì¥ëœ ì¿¼ë¦¬ì—ì„œ ì‹¤ì œ ì§ˆë¬¸ ì¶”ì¶œ (chat_interface.py ëŒ€ì‘)
            actual_query = query
            if "í˜„ì¬ ì§ˆë¬¸:" in query:
                parts = query.split("í˜„ì¬ ì§ˆë¬¸:")
                if len(parts) > 1:
                    actual_query = parts[-1].strip()
                    logger.info(f"ğŸ“ í™•ì¥ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œ: '{actual_query[:50]}'")

            # ğŸ§¹ UI ë©”íƒ€ë°ì´í„° ì œê±° (ğŸ· pdf Â· ğŸ“… 2024-10-24 Â· âœ ë“±)
            actual_query = clean_ui_metadata(actual_query)

            # ğŸ¯ ëª¨ë“œ ë¼ìš°íŒ…: Q&A ì˜ë„ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ íŒŒì¼ëª…ì´ ìˆì–´ë„ Q&A ëª¨ë“œ ìš°ì„ 
            query_mode = self.query_router.classify_mode(actual_query)
            router_reason = self.query_router.get_routing_reason(actual_query)

            # ğŸ”§ selected_filenameì´ ìˆê³  ìš”ì•½/ë‚´ìš© ì˜ë„ê°€ ê°ì§€ë˜ë©´ DOCUMENT ëª¨ë“œë¡œ ê°•ì œ (ìš°ì„ ìˆœìœ„ ìµœìƒìœ„)
            if selected_filename and (self.query_router.SUMMARY_INTENT_PATTERN.search(actual_query) or "ë‚´ìš©" in actual_query.lower()):
                logger.info(f"ğŸ¯ ì„ íƒëœ ë¬¸ì„œ({selected_filename}) + ìš”ì•½/ë‚´ìš© ì˜ë„ ê°ì§€ â†’ DOCUMENT ëª¨ë“œë¡œ ê°•ì œ")
                query_mode = QueryMode.DOCUMENT
                router_reason = "selected_doc_content"

            # ğŸ”§ ìš”ì•½ ì˜ë„ + ì¿¼ë¦¬ì— ë‚ ì§œ/ë¬¸ì„œëª… íŒ¨í„´ì´ ìˆìœ¼ë©´ DOCUMENT ëª¨ë“œë¡œ ê°•ì œ
            import re
            has_summary_intent = self.query_router.SUMMARY_INTENT_PATTERN.search(actual_query) or "ë‚´ìš©" in actual_query.lower()
            has_date_pattern = re.search(r'\d{4}[-_]\d{2}[-_]\d{2}', actual_query)  # 2025-06-10 í˜•ì‹

            if has_summary_intent and has_date_pattern and not selected_filename:
                logger.info(f"ğŸ¯ ìš”ì•½ ì˜ë„ + ë‚ ì§œ íŒ¨í„´ ê°ì§€ â†’ DOCUMENT ëª¨ë“œë¡œ ê°•ì œ")
                query_mode = QueryMode.DOCUMENT
                router_reason = "summary_with_date_pattern"

            logger.info(
                f"ğŸ”€ ë¼ìš°íŒ… ê²°ê³¼: mode={query_mode.value}, reason={router_reason}"
            )

            # ğŸ’° COST ëª¨ë“œ: ë¹„ìš© í•©ê³„ ì§ì ‘ ì¡°íšŒ
            if query_mode == QueryMode.COST:
                return self._answer_cost_sum(actual_query)

            # ğŸ“„ DOCUMENT ëª¨ë“œ: ë¬¸ì„œ ë‚´ìš©/ìš”ì•½ (í†µí•©: PREVIEW + SUMMARY)
            if query_mode == QueryMode.DOCUMENT:
                return self._answer_document(actual_query, selected_filename=selected_filename)

            # ğŸ” SEARCH ëª¨ë“œ: ë¬¸ì„œ ê²€ìƒ‰ (í†µí•©: LIST + SEARCH + LIST_FIRST)
            if query_mode == QueryMode.SEARCH:
                return self._answer_search(actual_query)

            # ğŸ” ë””ë²„ê¹…: ì‹¤ì œ pattern matching ëŒ€ìƒ ë¡œê¹…
            logger.info(f"ğŸ” Pattern matching ëŒ€ìƒ ì¿¼ë¦¬: '{actual_query[:100]}'")

            # âœ… P0: íŒŒì¼ëª… ì§ì ‘ ì–¸ê¸‰ íŒ¨í„´ ê°ì§€ (ë ˆê±°ì‹œ í˜¸í™˜, PREVIEW ëª¨ë“œ ì™¸)
        # ì¼ë°˜ ì¿¼ë¦¬ëŠ” ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
        response = self.query(query, top_k=top_k or 5, selected_filename=selected_filename)

        if response.success:
            # ê²€ìƒ‰/ì••ì¶•ì—ì„œ ë„˜ì–´ì˜¨ ì •ê·œí™” ì²­í¬ ì‚¬ìš© (ì‹¤ì œ page/snippet/meta ë…¸ì¶œ)
            evidence = [
                {
                    "doc_id": c.get("doc_id"),
                    "page": c.get("page", 1),
                    "snippet": c.get("snippet", ""),
                    "meta": c.get(
                        "meta", {"doc_id": c.get("doc_id"), "page": c.get("page", 1)}
                    ),
                }
                for c in (response.evidence_chunks or [])
            ]

            # CRITICAL: Evidence ìµœì†Œ ë³´ì¥ (sources_citedê°€ ë¹„ì–´ë„ ê²€ìƒ‰ ê²°ê³¼ëŠ” í‘œì‹œ)
            evidence_injected = False
            if not evidence and response.raw_results:
                logger.info("Evidence empty, using raw_results[:3] as fallback")
                evidence = [
                    {
                        "doc_id": r.get("doc_id") or r.get("chunk_id", "unknown"),
                        "page": 0,  # ê²€ìƒ‰ ê²°ê³¼ëŠ” í˜ì´ì§€ ì •ë³´ ì—†ìŒ
                        "snippet": r.get("snippet") or r.get("text_preview", "")[:400],  # 500 â†’ 400 (ìŠ¤ë‹ˆí« ì¼ê´€ì„±)
                        "meta": {
                            "doc_id": r.get("doc_id") or r.get("chunk_id", "unknown"),
                            "filename": r.get("filename", ""),
                            "page": 0,
                        },
                    }
                    for r in response.raw_results[:3]
                ]
                evidence_injected = True

            # [DIAG] Evidence ì§„ë‹¨ ì •ë³´ ì¶”ê°€
            if DIAG_RAG and response.diagnostics:
                response.diagnostics["evidence_count"] = len(evidence)
                response.diagnostics["evidence_injected"] = evidence_injected

            # ğŸ”¥ CRITICAL: status.found í”Œë˜ê·¸ - UI íŒì • ë‹¨ì¼ ì†ŒìŠ¤
            # retrieved_count: ê²€ìƒ‰ëœ ì›ë³¸ ê²°ê³¼ ìˆ˜
            # selected_count: ì‹¤ì œ ì‚¬ìš©ëœ ì¦ê±° ìˆ˜ (evidence)
            # found: ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€ (evidenceê°€ 1ê°œ ì´ìƒì´ë©´ True)
            status = {
                "retrieved_count": len(response.raw_results or []),
                "selected_count": len(evidence),
                "found": len(evidence) > 0,  # ğŸ”´ ìœ ì¼í•œ íŒì • ê¸°ì¤€
            }

            # ìš´ì˜ í‘œì¤€ 1í–‰ ìš”ì•½ ë¡œê·¸ (í•„ìˆ˜)
            import re

            author_mode = bool(re.search(r"(ì‘ì„±ì|ê¸°ì•ˆì|ì œì•ˆì)", query))
            search_ms = int(response.metrics.get("search_time", 0) * 1000)
            generate_ms = int(response.metrics.get("generate_time", 0) * 1000)
            total_ms = int(response.latency * 1000)

            logger.info(
                f'[RAG] query="{query[:50]}..." | '
                f"retrieved={status['retrieved_count']} | "
                f"selected={status['selected_count']} | "
                f"found={status['found']} | "
                f"author_mode={author_mode} | "
                f"backfill={evidence_injected} | "
                f"search_ms={search_ms} | "
                f"generate_ms={generate_ms} | "
                f"total_ms={total_ms}"
            )

            result = {
                "text": response.answer,
                "citations": evidence,  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
                "evidence": evidence,  # í•˜ìœ„ í˜¸í™˜ì„± (ë™ì¼ ë°ì´í„°)
                "status": status,  # UIì—ì„œ ì´ê²ƒë§Œ í™•ì¸
                "diagnostics": response.diagnostics if DIAG_RAG else {},
            }

            # âœ¨ Cache the successful result to both tiers
            cache_key = f"{query}:{selected_filename}" if selected_filename else query
            cache_query_result(cache_key, result)  # Memory cache
            cache_query_result_persistent(cache_key, result)  # Persistent cache
            logger.info(f"ğŸ“ Cached result to memory + persistent storage for query: {query[:50]}...")

            return result
        else:
            # ì—ëŸ¬ ë°œìƒ ì‹œ (ì¤‘ë¦½ í†¤, ì‚¬ê³¼ í‘œí˜„ ê¸ˆì§€)
            error_msg = ERROR_MESSAGES.get(
                ErrorCode.E_GENERATE, "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆë‹¤."
            )
            if response.error:
                error_msg = f"{error_msg}\n\nìƒì„¸: {response.error}"

            # ìš´ì˜ í‘œì¤€ ë¡œê·¸ (ì—ëŸ¬ ì¼€ì´ìŠ¤)
            logger.error(
                f'[RAG] query="{query[:50]}..." | '
                f'status=ERROR | error="{response.error}"'
            )

            return {
                "text": error_msg,
                "citations": [],  # ğŸ”´ í‘œì¤€ í‚¤ (í•„ìˆ˜)
                "evidence": [],  # í•˜ìœ„ í˜¸í™˜ì„±
                "status": {"retrieved_count": 0, "selected_count": 0, "found": False},
            }

    def answer_text(self, query: str) -> str:
        """ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            str: ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
        """
        result = self.answer(query)
        return result["text"]

    def _answer_search(self, query: str) -> dict:
        """ë¬¸ì„œ ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜ BM25 ê²€ìƒ‰, ìƒì„¸ ì •ë³´ í¬í•¨)

        SEARCH ëª¨ë“œ í•¸ë“¤ëŸ¬ë¡œ, ì‚¬ìš©ìì˜ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ 
        ë©”íƒ€ë°ì´í„°(ê¸°ì•ˆì, ë‚ ì§œ, ë¹„ìš©)ì™€ í•¨ê»˜ ì¹´ë“œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

        ì²˜ë¦¬ íë¦„:
            1. ë¶ˆìš©ì–´ ì œê±°í•˜ì—¬ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
            2. BM25 retrieverë¡œ ìƒìœ„ 10ê°œ ë¬¸ì„œ ê²€ìƒ‰
            3. ê° ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ DBì—ì„œ ì¡°íšŒ
            4. ì¹´ë“œ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ… (ì œëª©, ê¸°ì•ˆì, ë‚ ì§œ, ë¹„ìš©, ë¯¸ë¦¬ë³´ê¸°)

        Args:
            query (str): ì‚¬ìš©ì ì§ˆì˜.
                ì˜ˆ: "ì¤‘ê³„ì°¨ ì¹´ë©”ë¼ ë Œì¦ˆê´€ë ¨ ë¬¸ì„œ ì°¾ì•„ì¤˜"
                    "ìœ ì¸í˜ ê¸°ì•ˆì„œ ë¬¸ì„œ ì°¾ì•„ì¤˜"
                    "ë Œì¦ˆ ì˜¤ë²„í™€ ë¬¸ì„œ ìˆì–´?"

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡°
                {
                    "mode": "SEARCH",
                    "text": str,  # í¬ë§·íŒ…ëœ ì¹´ë“œ ëª©ë¡
                    "files": list[str],  # íŒŒì¼ëª… ëª©ë¡
                    "count": int,  # ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜
                    "citations": list[dict],  # Evidence ì •ë³´
                    "evidence": list[dict],  # í•˜ìœ„ í˜¸í™˜ìš© (citationsì™€ ë™ì¼)
                    "status": {
                        "retrieved_count": int,
                        "selected_count": int,
                        "found": bool
                    }
                }

        Example:
            >>> pipeline._answer_search("ì¤‘ê³„ì°¨ ì¹´ë©”ë¼ ë¬¸ì„œ ì°¾ì•„ì¤˜")
            {
                "mode": "SEARCH",
                "text": "ğŸ“„ **'ì¤‘ê³„ì°¨ ì¹´ë©”ë¼' ê´€ë ¨ ë¬¸ì„œ (3ê±´)**\\n\\n1. **ì¤‘ê³„ì°¨ ì¹´ë©”ë¼ ë Œì¦ˆ ì˜¤ë²„í™€**\\n   ğŸ“‹ ê¸°ì•ˆì„œ | ğŸ“… 2024-03-15 | âœ ìœ ì¸í˜\\n   ğŸ’° 2,500,000ì›\\n   ğŸ“ Canon HJ40x10B ë Œì¦ˆ ì˜¤ë²„í™€...",
                "files": ["2024-03-15_ì¤‘ê³„ì°¨_ì¹´ë©”ë¼_ë Œì¦ˆ_ì˜¤ë²„í™€.pdf", ...],
                "count": 3,
                ...
            }

        Note:
            - ìµœëŒ€ 10ê°œ ë¬¸ì„œê¹Œì§€ ë°˜í™˜
            - ë¶ˆìš©ì–´: "ë¬¸ì„œ", "íŒŒì¼", "ê¸°ì•ˆì„œ", "ì°¾ì•„ì¤˜", "ì°¾ì•„", "ê²€ìƒ‰", "ê´€ë ¨", "ì¢€", "í•´ì¤˜"
            - ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ count=0, found=False ë°˜í™˜
        """
        from modules.metadata_db import MetadataDB
        import re

        try:
            # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶ˆìš©ì–´ ì œê±°)
            stop_words = ["ë¬¸ì„œ", "íŒŒì¼", "ê¸°ì•ˆì„œ", "ì°¾ì•„ì¤˜", "ì°¾ì•„", "ê²€ìƒ‰", "ê´€ë ¨", "ì¢€", "í•´ì¤˜"]
            keywords = query
            for word in stop_words:
                keywords = keywords.replace(word, " ")
            keywords = keywords.strip()

            # ê¸°ì•ˆìëª… ì¶”ì¶œ (ì¿¼ë¦¬ì—ì„œ í•œê¸€ ì´ë¦„ íŒ¨í„´ ê²€ìƒ‰)
            drafter_filter = None
            # DBì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ê¸°ì•ˆì ëª©ë¡ (ì¶”í›„ DB ì¡°íšŒë¡œ ê°œì„  ê°€ëŠ¥)
            common_drafters = ["ë‚¨ì¤€ìˆ˜", "ìµœìƒˆë¦„", "ìœ ì¸í˜", "ì´ì˜ì£¼", "ê°•ë³‘ê·œ", "ë°•ì—°ìˆ˜", "ì´í˜¸ì˜", "ì´ìŠ¹í—Œ"]
            for name in common_drafters:
                if name in query:
                    drafter_filter = name
                    logger.info(f"ğŸ” ê¸°ì•ˆì í•„í„° ì ìš©: {drafter_filter}")
                    break

            logger.info(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰: í‚¤ì›Œë“œ='{keywords}'{f' | ê¸°ì•ˆì={drafter_filter}' if drafter_filter else ''}")

            # "ì „ë¶€" ë˜ëŠ” "ê°œìˆ˜" ì§ˆì˜ ê°ì§€ - ê²€ìƒ‰ ê°œìˆ˜ ì¡°ì •
            # "ëª‡ê°œ", "ê°œìˆ˜" ì§ˆì˜ëŠ” ì •í™•í•œ ì¹´ìš´íŠ¸ë¥¼ ìœ„í•´ ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì•¼ í•¨
            needs_all = any(kw in query.lower() for kw in ["ì „ë¶€", "ëª¨ë‘", "ëª¨ë“ ", "ì „ì²´", "all", "ëª‡", "ê°œìˆ˜", "ì´"])
            search_top_k = 200 if needs_all else 10  # 131ê°œ ë¬¸ì„œë„ ì»¤ë²„í•˜ë„ë¡ 200ìœ¼ë¡œ ì¦ê°€
            logger.info(f"ğŸ” ê²€ìƒ‰ top_k: {search_top_k} (needs_all={needs_all})")

            # BM25 ê²€ìƒ‰ ì‹¤í–‰
            if not hasattr(self.retriever, 'search'):
                logger.error("âŒ Retrieverì— search ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                return {
                    "mode": "SEARCH",
                    "text": "ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "files": [],
                    "count": 0,
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
            search_results = self.retriever.search(keywords, top_k=search_top_k)

            # ê²°ê³¼ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
            filenames = []
            seen = set()
            for result in search_results:
                filename = result.get("filename") or result.get("doc_id")
                if filename and filename not in seen:
                    filenames.append(filename)
                    seen.add(filename)

            if not filenames:
                return {
                    "mode": "SEARCH",
                    "text": f"'{keywords}' ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                    "files": [],
                    "count": 0,
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            # ğŸ”¢ "ì´ ëª‡ê°œ" ì§ˆë¬¸ ê°ì§€ - ê°œìˆ˜ë§Œ ë‹µí•˜ê³  ë¦¬ìŠ¤íŠ¸ ìƒëµ
            # íƒ€ì´í•‘ ì˜¤ë¥˜ ëŒ€ì‘: "ëª†ê°œ" (ì˜ëª»ëœ ìëª¨ ì¡°í•©) â†’ "ëª‡ê°œ"
            count_only_query = any(kw in query.lower() for kw in ["ëª‡ê°œ", "ëª†ê°œ", "ëª‡ ê°œ", "ëª† ê°œ", "ê°œìˆ˜", "ì´", "ëª‡", "ëª†"])

            # ê° ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            db = MetadataDB()

            # ë‚ ì§œ í•„í„°ë§ (ì—°ë„ ì¶”ì¶œ)
            year_filter = None
            year_match = re.search(r'(20\d{2})ë…„?', query)
            if year_match:
                year_filter = year_match.group(1)
                logger.info(f"ğŸ“… ì—°ë„ í•„í„° ì ìš©: {year_filter}")

            # ê¸°ì•ˆì + ë‚ ì§œ í•„í„°ë¡œ ì •í™•í•œ ê°œìˆ˜ ê³„ì‚°
            if count_only_query:
                conn = db._get_conn()
                sql = "SELECT COUNT(*) as cnt FROM documents WHERE 1=1"
                params = []

                if drafter_filter:
                    sql += " AND drafter = ?"
                    params.append(drafter_filter)

                if year_filter:
                    sql += " AND (date LIKE ? OR display_date LIKE ?)"
                    params.extend([f"{year_filter}%", f"{year_filter}%"])

                cursor = conn.execute(sql, params)
                total_count = cursor.fetchone()['cnt']

                # ê°œìˆ˜ë§Œ ë‹µë³€
                drafter_text = f"{drafter_filter} " if drafter_filter else ""
                year_text = f"{year_filter}ë…„ " if year_filter else ""

                return {
                    "mode": "SEARCH",
                    "text": f"{year_text}{drafter_text}ë¬¸ì„œëŠ” ì´ **{total_count}ê°œ**ì…ë‹ˆë‹¤.",
                    "files": [],
                    "count": total_count,
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": total_count,
                        "selected_count": 0,
                        "found": total_count > 0
                    }
                }

            doc_details = []

            # "ì „ë¶€" ë˜ëŠ” "ê°œìˆ˜" ì§ˆì˜ ê°ì§€ - ìµœëŒ€ ê°œìˆ˜ ì¡°ì •
            max_docs = 200 if any(kw in query.lower() for kw in ["ì „ë¶€", "ëª¨ë‘", "ëª¨ë“ ", "ì „ì²´", "all"]) else 10

            for filename in filenames[:max_docs]:  # ìµœëŒ€ ê°œìˆ˜ê¹Œì§€
                # DBì—ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ (filename + ê¸°ì•ˆì í•„í„° + ë‚ ì§œ í•„í„°)
                conn = db._get_conn()

                # SQL ì¿¼ë¦¬ ë™ì  ìƒì„± (í•„í„° ì¡°ê±´ ì¶”ê°€)
                sql = "SELECT * FROM documents WHERE filename = ?"
                params = [filename]

                if drafter_filter:
                    sql += " AND drafter = ?"
                    params.append(drafter_filter)

                if year_filter:
                    sql += " AND (date LIKE ? OR display_date LIKE ?)"
                    params.extend([f"{year_filter}%", f"{year_filter}%"])

                sql += " LIMIT 1"
                cursor = conn.execute(sql, params)
                row = cursor.fetchone()

                if row:
                    doc = dict(row)
                    doc_details.append({
                        "filename": filename,
                        "drafter": doc.get("drafter", "ì‘ì„±ì ë¯¸ìƒ"),
                        "date": doc.get("display_date") or doc.get("date", "ë‚ ì§œ ì—†ìŒ"),
                        "doctype": doc.get("doctype", "ë¬¸ì„œ"),
                        "claimed_total": doc.get("claimed_total"),
                        "text_preview": doc.get("text_preview", "")[:100]
                    })
                else:
                    # ê¸°ì•ˆì í•„í„°ê°€ ì ìš©ëœ ê²½ìš°, ë§¤ì¹­ë˜ì§€ ì•Šì€ ë¬¸ì„œëŠ” ìŠ¤í‚µ
                    if drafter_filter:
                        logger.debug(f"ğŸ” ê¸°ì•ˆì í•„í„°ë¡œ ì œì™¸: {filename}")
                        continue
                    # ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° íŒŒì¼ëª…ë§Œ í‘œì‹œ (í•„í„° ì—†ì„ ë•Œë§Œ)
                    doc_details.append({
                        "filename": filename,
                        "drafter": "ì‘ì„±ì ë¯¸ìƒ",
                        "date": "ë‚ ì§œ ì—†ìŒ",
                        "doctype": "ë¬¸ì„œ",
                        "claimed_total": None,
                        "text_preview": ""
                    })

            # ì‘ë‹µ í…ìŠ¤íŠ¸ í¬ë§·íŒ… (ë¦¬íŒ©í† ë§ ê³„íšì„œì˜ í˜•ì‹ ì°¸ê³ )
            cards = []
            for i, doc in enumerate(doc_details, 1):
                filename = doc["filename"]

                # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ
                title = re.sub(r'^\d{4}-\d{2}-\d{2}_', '', filename)
                title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
                title = title.replace('_', ' ')

                # ì¹´ë“œ ìƒì„±
                card_lines = [f"{i}. **{title}**"]
                card_lines.append(f"   ğŸ“‹ {doc['doctype']} | ğŸ“… {doc['date']} | âœ {doc['drafter']}")

                # ë¹„ìš© ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if doc['claimed_total']:
                    card_lines.append(f"   ğŸ’° {doc['claimed_total']:,}ì›")

                # ë¯¸ë¦¬ë³´ê¸° ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if doc['text_preview']:
                    # ë§ˆì»¤ ì œê±°: [í˜ì´ì§€ X], [OCR ...], ë¶ˆí•„ìš”í•œ ê³µë°±
                    clean_text = re.sub(r'\[í˜ì´ì§€\s*\d+\]', '', doc['text_preview'])
                    clean_text = re.sub(r'\[OCR[^\]]*\]', '', clean_text)
                    clean_text = re.sub(r'\s+', ' ', clean_text).strip()

                    if clean_text:  # ì •ë¦¬ í›„ ë‚´ìš©ì´ ìˆìœ¼ë©´ í‘œì‹œ
                        preview = clean_text[:80]
                        card_lines.append(f"   ğŸ“ {preview}...")

                cards.append("\n".join(card_lines))

            # "ëª‡ê°œ", "ê°œìˆ˜" ì§ˆì˜ì¸ì§€ í™•ì¸
            is_count_query = any(kw in query.lower() for kw in ["ëª‡ê°œ", "ëª‡ ê°œ", "ê°œìˆ˜", "ì´", "count", "number"])

            if is_count_query:
                # ê°œìˆ˜ë§Œ ê°„ë‹¨íˆ ë‹µë³€
                answer_text = f"**'{keywords}' ê´€ë ¨ ë¬¸ì„œëŠ” ì´ {len(doc_details)}ê°œ**ì…ë‹ˆë‹¤.\n\n" + "\n\n".join(cards[:10])
                if len(cards) > 10:
                    answer_text += f"\n\n... ì™¸ {len(cards) - 10}ê°œ ë¬¸ì„œ (\"ì „ë¶€ ë³´ì—¬ì¤˜\"ë¥¼ ì…ë ¥í•˜ë©´ ëª¨ë“  ë¬¸ì„œë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"
            else:
                # ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼
                answer_text = f"ğŸ“„ **'{keywords}' ê´€ë ¨ ë¬¸ì„œ ({len(doc_details)}ê±´)**\n\n" + "\n\n".join(cards)

            # Evidence êµ¬ì„±
            evidence = []
            for doc in doc_details:
                filename = doc["filename"]

                # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ìƒì„±
                year_match = re.search(r'(\d{4})-', filename)
                if year_match:
                    year = year_match.group(1)
                    file_path_str = f"docs/year_{year}/{filename}"
                else:
                    file_path_str = f"docs/{filename}"

                # ì œëª© ìƒì„± (cardsì™€ ë™ì¼)
                title = re.sub(r'^\d{4}-\d{2}-\d{2}_', '', filename)
                title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
                title = title.replace('_', ' ')

                evidence.append({
                    "doc_id": filename,
                    "filename": filename,
                    "file_path": file_path_str,
                    "page": 1,
                    "snippet": title[:160],
                    "ref": None,
                    "meta": {
                        "filename": filename,
                        "drafter": doc.get("drafter"),
                        "date": doc.get("date"),
                        "doctype": doc.get("doctype")
                    }
                })

            return {
                "mode": "SEARCH",
                "text": answer_text,
                "files": filenames,
                "count": len(doc_details),
                "citations": evidence,
                "evidence": evidence,
                "status": {
                    "retrieved_count": len(doc_details),
                    "selected_count": len(doc_details),
                    "found": True
                }
            }

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "mode": "SEARCH",
                "text": f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "files": [],
                "count": 0,
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def _answer_cost_sum(self, query: str) -> dict:
        """ë¹„ìš© í•©ê³„ ì§ì ‘ ì¡°íšŒ (DB claimed_total í™œìš©)

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "ì±„ë„ì—ì´ ì¤‘ê³„ì°¨ ë³´ìˆ˜ í•©ê³„ ì–¼ë§ˆì˜€ì§€?")

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡° (text, citations, evidence, status)
        """
        try:
            # 1. ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ ë¬¸ì„œ ì°¾ê¸°
            search_results = self.retriever.search(query, top_k=3)

            if not search_results:
                logger.warning(f"ë¹„ìš© ì§ˆì˜ ê²€ìƒ‰ ì‹¤íŒ¨: {query}")
                return {
                    "text": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            # 2. DBì—ì„œ claimed_total ì¡°íšŒ
            from modules.metadata_db import MetadataDB
            db = MetadataDB()

            for result in search_results:
                filename = result.get("meta", {}).get("filename") or result.get("doc_id", "")
                if not filename:
                    continue

                doc = db.get_by_filename(filename)
                if doc and doc.get("claimed_total"):
                    claimed_total = doc["claimed_total"]

                    # 3. ë‹µë³€ í¬ë§·íŒ… (VAT, ê²€ì¦ ë°°ì§€ í¬í•¨)
                    # VAT íŒë‹¨ (text_previewì—ì„œ "VAT" í‚¤ì›Œë“œ ê²€ìƒ‰)
                    text_preview = doc.get("text_preview", "")
                    vat_status = "VAT ë³„ë„" if "VAT" in text_preview or "ë¶€ê°€ì„¸" in text_preview else "VAT í¬í•¨ ì¶”ì •"

                    # sum_match ê²€ì¦ ë°°ì§€
                    sum_match = doc.get("sum_match")
                    if sum_match is None:
                        verification = "sum_match=ì—†ìŒ"
                    elif sum_match:
                        verification = "sum_match=ì¼ì¹˜ âœ…"
                    else:
                        verification = "sum_match=ë¶ˆì¼ì¹˜ âš ï¸"

                    answer_text = f"ğŸ’° í•©ê³„: **â‚©{claimed_total:,}** ({vat_status})\n"
                    answer_text += f"ì¶œì²˜: {filename} | ë‚ ì§œ: {doc.get('display_date') or doc.get('date') or 'ì •ë³´ ì—†ìŒ'} | ê¸°ì•ˆì: {doc.get('drafter') or 'ì •ë³´ ì—†ìŒ'}\n"
                    answer_text += f"ê²€ì¦: {verification}"

                    # Evidence êµ¬ì„±
                    ref = _encode_file_ref(filename)
                    evidence = [{
                        "doc_id": filename,
                        "filename": filename,
                        "page": 1,
                        "snippet": f"ë¹„ìš© í•©ê³„: â‚©{claimed_total:,}",
                        "ref": ref,  # ğŸ”´ base64 ì¸ì½”ë”©ëœ íŒŒì¼ ê²½ë¡œ
                        "meta": {
                            "filename": filename,
                            "drafter": doc.get("drafter"),
                            "date": doc.get("display_date") or doc.get("date"),
                            "claimed_total": claimed_total
                        }
                    }]

                    logger.info(f"ğŸ’° ë¹„ìš© ì§ˆì˜ ì„±ê³µ: {filename} â†’ â‚©{claimed_total:,}")

                    return {
                        "text": answer_text,
                        "citations": evidence,
                        "evidence": evidence,
                        "status": {
                            "retrieved_count": len(search_results),
                            "selected_count": 1,
                            "found": True
                        }
                    }

            # claimed_total ì—†ëŠ” ê²½ìš°
            logger.warning(f"ê²€ìƒ‰ëœ ë¬¸ì„œì— ë¹„ìš© ì •ë³´ ì—†ìŒ: {[r.get('doc_id') for r in search_results]}")
            return {
                "text": "ê²€ìƒ‰ëœ ë¬¸ì„œì— ë¹„ìš© í•©ê³„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": len(search_results),
                    "selected_count": 0,
                    "found": False
                }
            }

        except Exception as e:
            logger.error(f"âŒ ë¹„ìš© ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "text": f"ë¹„ìš© ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def _answer_document(self, query: str, selected_filename: Optional[str] = None) -> dict:
        """ë¬¸ì„œ ë‚´ìš© ì¡°íšŒ (DOCUMENT ëª¨ë“œ: PREVIEW + SUMMARY í†µí•©)

        DOC_ANCHORED ëª¨ë“œë¥¼ ëŒ€ì²´í•˜ì—¬, ë¬¸ì„œ ì „ì²´ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        5ê°œ í•„ë“œë§Œ ì¶”ì¶œí•˜ë˜ êµ¬ì¡°ì  ì œí•œì„ ì œê±°í•˜ê³ , ì‚¬ìš©ìê°€ ìš”ì²­í•œ ë¬¸ì„œì˜
        ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜ (ì˜ˆ: "ë¯¸ëŸ¬í´ë© ì¹´ë©”ë¼ ì‚¼ê°ëŒ€ ê¸°ìˆ ê²€í† ì„œ ì´ë¬¸ì„œ ë‚´ìš© ì•Œë ¤ì¤˜")
            selected_filename: ì„ íƒëœ ë¬¸ì„œ íŒŒì¼ëª… (ìš°ì„  ê²€ìƒ‰ìš©, ì„ íƒì‚¬í•­)

        Returns:
            dict: í‘œì¤€ ì‘ë‹µ êµ¬ì¡° (ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸ í¬í•¨)

        Note:
            - ê³¼ê±° DOC_ANCHOREDì˜ 5-field ì¶”ì¶œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìƒì„±ë¨
            - ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ data/extracted/ ì—ì„œ ì§ì ‘ ë¡œë“œ
            - LLMì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        """
        import re
        import sqlite3
        from pathlib import Path

        try:
            # 1. ë¬¸ì„œ ì‹ë³„ (selected_filename ìš°ì„ , ì—†ìœ¼ë©´ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œ)
            target_filename = None

            if selected_filename:
                logger.info(f"ğŸ¯ ì„ íƒëœ ë¬¸ì„œ ìš°ì„  ì²˜ë¦¬: {selected_filename}")
                target_filename = selected_filename
            else:
                # ì¿¼ë¦¬ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
                # ì˜ˆ: "ë¯¸ëŸ¬í´ë© ì¹´ë©”ë¼ ì‚¼ê°ëŒ€ ê¸°ìˆ ê²€í† ì„œ" â†’ ê²€ìƒ‰ìœ¼ë¡œ ë¬¸ì„œ ì°¾ê¸°
                # ë¶ˆìš©ì–´ ì œê±°
                stopwords = ["ì´ë¬¸ì„œ", "ì´ ë¬¸ì„œ", "í•´ë‹¹ ë¬¸ì„œ", "ë‚´ìš©", "ì•Œë ¤ì¤˜", "ì•Œë ¤",
                             "ë³´ì—¬ì¤˜", "ë³´ì—¬", "ìì„¸í•˜ê²Œ", "ìì„¸íˆ", "ìš”ì•½", "ì •ë¦¬"]
                keywords = query
                for word in stopwords:
                    keywords = keywords.replace(word, " ")
                keywords = " ".join(keywords.split())  # ê³µë°± ì •ë¦¬

                # .pdf í™•ì¥ìê°€ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš©
                filename_match = re.search(r"(\S+\.pdf)", query, re.IGNORECASE)
                if filename_match:
                    target_filename = filename_match.group(1)
                    logger.info(f"ğŸ“„ ì¿¼ë¦¬ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ: {target_filename}")
                else:
                    # í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
                    logger.info(f"ğŸ” í‚¤ì›Œë“œë¡œ ë¬¸ì„œ ê²€ìƒ‰: {keywords}")
                    search_results = self.retriever.search(keywords, top_k=1)

                    if search_results:
                        target_filename = search_results[0].get("meta", {}).get("filename") or search_results[0].get("doc_id", "")
                        logger.info(f"âœ… ê²€ìƒ‰ìœ¼ë¡œ ë¬¸ì„œ ë°œê²¬: {target_filename}")

            if not target_filename:
                return {
                    "text": "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œëª…ì„ ëª…í™•íˆ ì…ë ¥í•´ì£¼ì„¸ìš”.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            # 2. DBì—ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            conn = sqlite3.connect("metadata.db")
            cursor = conn.cursor()
            cursor.execute(
                """
                SELECT filename, drafter, date, display_date, category, doctype
                FROM documents
                WHERE filename = ? OR filename LIKE ?
                LIMIT 1
                """,
                (target_filename, f"%{target_filename}%"),
            )
            result = cursor.fetchone()
            conn.close()

            if not result:
                return {
                    "text": f"'{target_filename}' ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 0,
                        "selected_count": 0,
                        "found": False
                    }
                }

            filename, drafter, date, display_date, category, doctype = result

            # 3. data/extracted/ ì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë“œ
            extracted_dir = Path("data/extracted")
            txt_filename = filename.replace('.pdf', '.txt')
            txt_path = extracted_dir / txt_filename

            if not txt_path.exists():
                return {
                    "text": f"'{filename}' ë¬¸ì„œì˜ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nê²½ë¡œ: {txt_path}",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 1,
                        "selected_count": 0,
                        "found": False
                    }
                }

            # ì „ì²´ í…ìŠ¤íŠ¸ ì½ê¸°
            with open(txt_path, 'r', encoding='utf-8') as f:
                full_text = f.read()

            if not full_text or len(full_text.strip()) < 10:
                return {
                    "text": f"'{filename}' ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.",
                    "citations": [],
                    "evidence": [],
                    "status": {
                        "retrieved_count": 1,
                        "selected_count": 0,
                        "found": False
                    }
                }

            # 4. ìš”ì•½ ì˜ë„ ê°ì§€
            summary_keywords = ["ìš”ì•½", "ìš”ì•½í•´", "ì •ë¦¬", "ì •ë¦¬í•´", "ë‚´ìš©", "summary"]
            needs_summary = any(kw in query.lower() for kw in summary_keywords)

            # 5. ë‹µë³€ í¬ë§·íŒ…
            answer_text = f"**ğŸ“„ {filename}**\n\n"
            answer_text += f"**ê¸°ì•ˆì**: {drafter or 'ì •ë³´ ì—†ìŒ'} | "
            answer_text += f"**ë‚ ì§œ**: {display_date or date or 'ì •ë³´ ì—†ìŒ'} | "
            answer_text += f"**ë¶„ë¥˜**: {category or 'ë¯¸ë¶„ë¥˜'}\n"
            answer_text += f"{'='*80}\n\n"

            # LLM ìš”ì•½ ë˜ëŠ” ì›ë¬¸
            if needs_summary and len(full_text) > 500:
                # LLM ìš”ì•½ ìˆ˜í–‰ (ê¸°ì¡´ RAG ì‹œìŠ¤í…œì˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
                logger.info(f"ğŸ“ ìš”ì•½ ìš”ì²­ ê°ì§€ â†’ LLM ìš”ì•½ ìˆ˜í–‰ (ì›ë¬¸ {len(full_text)}ì)")
                try:
                    # ë¬¸ì„œë¥¼ ì²­í¬ í˜•íƒœë¡œ êµ¬ì„±
                    chunks = [{
                        "text": full_text[:4000],  # ìµœëŒ€ 4000ì
                        "snippet": full_text[:4000],
                        "content": full_text[:4000],
                        "filename": filename,
                        "score": 1.0,
                        "meta": {
                            "drafter": drafter,
                            "date": display_date or date,
                            "category": category
                        }
                    }]

                    # ì§ì ‘ LLM í˜¸ì¶œ (ì¸ìš© ê²€ì¦ ìš°íšŒ)
                    # QuickFixGeneratorì˜ ë‚´ë¶€ LLM ì ‘ê·¼
                    if hasattr(self.generator, 'rag') and hasattr(self.generator.rag, 'llm'):
                        llm = self.generator.rag.llm

                        # ìœ ì—°í•œ ìš”ì•½ í”„ë¡¬í”„íŠ¸ (ë¬¸ì„œ íƒ€ì…ì— ë§ê²Œ ìë™ ì¡°ì •)
                        summary_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{full_text[:3000]}

ìš”ì•½ ê°€ì´ë“œ:
- ë¬¸ì„œë¥¼ ì½ëŠ” ì‚¬ëŒì´ ë¹ ë¥´ê²Œ í•µì‹¬ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡
- ì¤‘ìš”í•œ ì •ë³´ ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê²Œ (5-10ì¤„)
- í•„ìš”ì‹œ ë¶ˆë¦¿ í¬ì¸íŠ¸ ì‚¬ìš©
- ê¸ˆì•¡ì´ ìˆìœ¼ë©´ ëª…í™•íˆ í‘œì‹œ

ì´ì œ ìœ„ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ì„¸ìš”:"""

                        # ì§ì ‘ generate í˜¸ì¶œ (ì¸ìš© ì—†ì´)
                        from llama_cpp import Llama
                        if isinstance(llm.llm, Llama):  # QwenLLM.llm ì‚¬ìš©
                            output = llm.llm.create_chat_completion(
                                messages=[
                                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ì½ëŠ” ì‚¬ëŒì˜ ì…ì¥ì—ì„œ í•µì‹¬ë§Œ ë¹ ë¥´ê²Œ ì „ë‹¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¬¸ì„œ íƒ€ì…ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½í•˜ì„¸ìš”."},
                                    {"role": "user", "content": summary_prompt}
                                ],
                                max_tokens=500,
                                temperature=0.3
                            )
                            llm_result = output['choices'][0]['message']['content']
                        else:
                            # Fallback
                            llm_result = f"LLM íƒ€ì… ë¶ˆì¼ì¹˜: {type(llm.llm)}"
                    else:
                        llm_result = "LLM ì ‘ê·¼ ì‹¤íŒ¨"

                    # ìš”ì•½ë§Œ ì œê³µ (ì›ë¬¸ì€ evidenceì— ìˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì œê±°)
                    answer_text += f"{llm_result}"
                    use_llm = True
                except Exception as e:
                    logger.warning(f"âš ï¸ LLM ìš”ì•½ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©: {e}")
                    logger.exception(e)
                    answer_text += full_text
                    use_llm = False
            else:
                # ì „ì²´ í…ìŠ¤íŠ¸ í¬í•¨ (ê¸¸ì´ ì œí•œ ì—†ìŒ)
                answer_text += full_text
                use_llm = False

            # 5. Evidence êµ¬ì„±
            ref = _encode_file_ref(filename)
            evidence = [{
                "doc_id": filename,
                "filename": filename,
                "page": 1,
                "snippet": full_text[:1000],  # ìŠ¤ë‹ˆí«ì€ 1000ìë¡œ ì œí•œ
                "ref": ref,
                "meta": {
                    "filename": filename,
                    "drafter": drafter,
                    "date": display_date or date,
                    "category": category,
                    "doctype": doctype
                }
            }]

            logger.info({
                "mode": "DOCUMENT",
                "filename": filename,
                "text_length": len(full_text),
                "llm": use_llm,  # LLM ìš”ì•½ ì‚¬ìš© ì—¬ë¶€
                "summary_requested": needs_summary
            })

            return {
                "text": answer_text,
                "citations": evidence,
                "evidence": evidence,
                "status": {
                    "retrieved_count": 1,
                    "selected_count": 1,
                    "found": True
                }
            }

        except Exception as e:
            logger.error(f"âŒ DOCUMENT ëª¨ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "text": f"ë¬¸ì„œ ë‚´ìš© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "citations": [],
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def _safe_fname(self, meta: dict = None, doc_path: str = None) -> str:
        """íŒŒì¼ëª… ì•ˆì „ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì‹œë„)

        Args:
            meta: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            doc_path: ë¬¸ì„œ ê²½ë¡œ

        Returns:
            ì•ˆì „í•˜ê²Œ ì¶”ì¶œëœ íŒŒì¼ëª… (ê¸°ë³¸ê°’: 'ë¯¸ìƒ ë¬¸ì„œ')
        """
        import os

        meta = meta or {}

        # ë‹¤ì–‘í•œ í•„ë“œì—ì„œ íŒŒì¼ëª… ì‹œë„
        fname = (
            meta.get("fname")
            or meta.get("filename")
            or meta.get("doc_id")
            or (os.path.basename(doc_path) if doc_path else None)
            or "ë¯¸ìƒ ë¬¸ì„œ"
        )

        return fname

    def _make_chunks_for_doc(self, filename: str) -> list:
        """íŠ¹ì • ë¬¸ì„œì˜ ì²­í¬ë§Œ ë¡œë“œ (ë¬¸ì„œ ê³ ì • ëª¨ë“œìš©)

        Args:
            filename: ë¬¸ì„œ íŒŒì¼ëª…

        Returns:
            í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # BM25 ì¸ë±ìŠ¤ì—ì„œ ì§ì ‘ í•´ë‹¹ ë¬¸ì„œ ì°¾ê¸° (ê²€ìƒ‰ ëŒ€ì‹  ì§ì ‘ ì ‘ê·¼)
            if hasattr(self.retriever, 'bm25') and self.retriever.bm25:
                bm25_store = self.retriever.bm25

                # metadataì—ì„œ filenameì´ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                target_indices = []
                for i, meta in enumerate(bm25_store.metadata):
                    if meta.get('filename') == filename:
                        target_indices.append(i)
                        logger.info(f"âœ… BM25 ì¸ë±ìŠ¤ì—ì„œ ë°œê²¬: {filename} (index={i})")

                # ì°¾ì€ ë¬¸ì„œë“¤ì˜ contentë¥¼ ì²­í¬ë¡œ ë³€í™˜
                chunks = []
                for idx in target_indices:
                    content = bm25_store.documents[idx]
                    if content and len(content.strip()) > 0:
                        # ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í° ì²­í¬ë¡œ ì‚¬ìš©
                        chunks.append({
                            'doc_id': filename,
                            'page': 1,
                            'text': content,  # ì „ì²´ í…ìŠ¤íŠ¸
                            'score': 1.0,  # ì§ì ‘ ë§¤ì¹­ì´ë¯€ë¡œ ìµœê³  ìŠ¤ì½”ì–´
                            'filename': filename
                        })
                        logger.info(f"âœ“ ë¬¸ì„œ content ë¡œë“œ: {len(content)}ì")

                if chunks:
                    logger.info(f"âœ“ ë¬¸ì„œ ì²­í¬ {len(chunks)}ê°œ ë¡œë“œ ì™„ë£Œ")
                    return chunks

            # BM25 ì‚¬ìš© ë¶ˆê°€ ì‹œ í´ë°±: í‚¤ì›Œë“œ ê²€ìƒ‰
            logger.warning("âš ï¸ BM25 ì§ì ‘ ì ‘ê·¼ ë¶ˆê°€, ê²€ìƒ‰ìœ¼ë¡œ í´ë°±")
            search_query = filename.replace('.pdf', '').replace('_', ' ')
            results = self.retriever.search(search_query, top_k=20)

            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•´ë‹¹ ë¬¸ì„œë¡œ í•„í„°ë§
            chunks = []
            for result in results:
                # doc_id ë˜ëŠ” meta.filenameì´ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ í¬í•¨
                doc_id = result.get('doc_id', '')
                meta_filename = result.get('meta', {}).get('filename', '')

                if filename in doc_id or filename in meta_filename:
                    chunks.append({
                        'doc_id': result.get('doc_id', filename),
                        'page': result.get('page', 1),
                        'text': result.get('snippet', result.get('text', '')),
                        'score': result.get('score', 0.0),
                        'filename': filename
                    })

            if not chunks:
                logger.warning(f"âš ï¸ ë¬¸ì„œ ì²­í¬ ì—†ìŒ: {filename}")

            return chunks

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def _extract_with_ocr(self, pdf_path: str, start_page: int, total_pages: int) -> str:
        """OCRì„ ì‚¬ìš©í•˜ì—¬ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (pytesseract ìš°ì„ , paddleocr í´ë°±)

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            start_page: ì‹œì‘ í˜ì´ì§€ (0-based)
            total_pages: ì „ì²´ í˜ì´ì§€ ìˆ˜

        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        try:
            from pdf2image import convert_from_path
            import pytesseract
            from PIL import Image

            # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ë 3í˜ì´ì§€ë§Œ)
            images = convert_from_path(
                pdf_path,
                first_page=start_page + 1,  # 1-based
                last_page=total_pages
            )

            text = ""
            for i, img in enumerate(images):
                try:
                    # pytesseract ì‚¬ìš©
                    page_text = pytesseract.image_to_string(img, lang='kor+eng')
                    text += page_text + "\n"
                    logger.info(f"âœ“ OCR (pytesseract) í˜ì´ì§€ {start_page + i + 1}: {len(page_text)}ì")
                except Exception as e:
                    logger.warning(f"âš ï¸ pytesseract ì‹¤íŒ¨ (í˜ì´ì§€ {start_page + i + 1}): {e}")

            if len(text.strip()) > 50:
                return text

            # pytesseract ì‹¤íŒ¨ ì‹œ paddleocr ì‹œë„
            logger.info("ğŸ”„ paddleocr í´ë°± ì‹œë„...")
            try:
                from paddleocr import PaddleOCR
                ocr = PaddleOCR(use_angle_cls=True, lang='korean')

                text = ""
                for i, img in enumerate(images):
                    # PaddleOCRëŠ” íŒŒì¼ ê²½ë¡œ ë˜ëŠ” numpy arrayë¥¼ ë°›ìŒ
                    import numpy as np
                    img_array = np.array(img)
                    result = ocr.ocr(img_array, cls=True)

                    if result and result[0]:
                        page_text = "\n".join([line[1][0] for line in result[0]])
                        text += page_text + "\n"
                        logger.info(f"âœ“ OCR (paddleocr) í˜ì´ì§€ {start_page + i + 1}: {len(page_text)}ì")

                return text

            except Exception as e:
                logger.warning(f"âš ï¸ paddleocr ì‹¤íŒ¨: {e}")
                return ""

        except Exception as e:
            logger.error(f"âŒ OCR ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""

    def _gather_summary_context(self, filename: str, pdf_path: str, doc_locked: bool = False) -> str:
        """ìš”ì•½ìš© ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì¸ë±ìŠ¤ ì²­í¬ ê¸°ë°˜, PDF tail ë¹„í™œì„±)

        Args:
            filename: íŒŒì¼ëª…
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            doc_locked: Trueë©´ í•´ë‹¹ ë¬¸ì„œ ì²­í¬ë§Œ ì‚¬ìš© (ë‹¤ë¥¸ ë¬¸ì„œ ê²€ìƒ‰ ê¸ˆì§€)

        Returns:
            ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ (ìµœëŒ€ ~3600ì, ì•½ 1.8k í† í°)
        """
        import pdfplumber
        import re
        parts = []

        # 1) PDF ë 2~3í˜ì´ì§€ ì¶”ì¶œ â†’ ë¹„í™œì„±í™” (ì¸ë±ìŠ¤ ì²­í¬ ìš°ì„  ì „ëµ)
        # ì‚¬ìœ : ê°œìš”/ë°°ê²½/ê²€í† ì‚¬ìœ /ëŒ€ì•ˆ/ê²¬ì  ë“± í•µì‹¬ ì •ë³´ê°€ ëë¶€ë¶„ì´ ì•„ë‹Œ ì¤‘ê°„ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš° ë‹¤ìˆ˜
        # ì¸ë±ìŠ¤ëœ ì²­í¬ë¡œ ì „ì²´ ë¬¸ì„œë¥¼ ì»¤ë²„í•˜ë„ë¡ ë³€ê²½
        logger.info("ğŸ“‹ ìš”ì•½ ì»¨í…ìŠ¤íŠ¸: PDF tail ì¶”ì¶œ ë¹„í™œì„± (ì¸ë±ìŠ¤ ì²­í¬ ê¸°ë°˜ ì „ëµ)")
        # try:
        #     with pdfplumber.open(pdf_path) as pdf:
        #         total_pages = len(pdf.pages)
        #         start_page = max(0, total_pages - 3)  # ë 3í˜ì´ì§€
        #         tail = ""
        #         for page in pdf.pages[start_page:]:
        #             tail += (page.extract_text() or "")
        #
        #         # OCR í´ë°± (í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì„ ê²½ìš°)
        #         if len(tail.strip()) < 50:
        #             logger.warning(f"âš ï¸ PDF í…ìŠ¤íŠ¸ ë¶€ì¡± ({len(tail)}ì), OCR ì‹œë„...")
        #             tail = self._extract_with_ocr(pdf_path, start_page, total_pages)
        #
        #         if tail.strip():
        #             parts.append("=== [ë¬¸ì„œ ê²°ë¡ /ë§ë¯¸] ===\n" + tail)
        #             logger.info(f"âœ“ PDF ë {total_pages - start_page}í˜ì´ì§€ ì¶”ì¶œ: {len(tail)}ì")
        # except Exception as e:
        #     logger.warning(f"âš ï¸ PDF ëë¶€ë¶„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # 2) ì¸ë±ìŠ¤ ì²­í¬ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì„¹ì…˜ ê°€ì¤‘ì¹˜ ì ìš©)
        # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ: ê°œìš”, ë°°ê²½, ê²€í† ì‚¬ìœ , ëŒ€ì•ˆ, ê²¬ì , ê²°ë¡ , ë¹„ìš©, ë„ì…ì‚¬ìœ 
        priority_keywords = r'(ê°œìš”|ë°°ê²½|ê²€í† ì‚¬ìœ |ê²€í† \s*ì‚¬ìœ |ëŒ€ì•ˆ|ê²¬ì |ê²°ë¡ |ë¹„ìš©|ë„ì…ì‚¬ìœ |ë„ì…\s*ì‚¬ìœ |êµ¬ë§¤ëª©ì |êµ¬ë§¤\s*ëª©ì |ì„ ì •|ê¶Œê³ |ì´ì•¡|í•©ê³„)'

        try:
            if doc_locked:
                # ë¬¸ì„œ ê³ ì • ëª¨ë“œ: í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ë§Œ ë¡œë“œ
                logger.info(f"ğŸ”’ ë¬¸ì„œ ê³ ì • ëª¨ë“œ: {filename}ì˜ ì²­í¬ë§Œ ì‚¬ìš©")
                chunks = self._make_chunks_for_doc(filename)

                # ì„¹ì…˜ ê°€ì¤‘ì¹˜ ì ìš©: ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ í¬í•¨ ì²­í¬ë¥¼ ì•ìœ¼ë¡œ
                priority_chunks = []
                normal_chunks = []
                for chunk in chunks:
                    chunk_text = chunk.get('text') or chunk.get('snippet') or chunk.get('content') or ""
                    if re.search(priority_keywords, chunk_text):
                        priority_chunks.append(chunk)
                    else:
                        normal_chunks.append(chunk)

                # ìš°ì„ ìˆœìœ„ ì²­í¬ + ì¼ë°˜ ì²­í¬ ìˆœì„œë¡œ ì¬ì¡°í•©, ìµœëŒ€ 10ê°œ (ê²€í† ì„œ ìƒì„¸ ì •ë³´ í¬í•¨)
                sorted_chunks = (priority_chunks + normal_chunks)[:10]

                for i, chunk in enumerate(sorted_chunks, 1):
                    chunk_text = chunk.get('text') or chunk.get('snippet') or chunk.get('content') or ""
                    if chunk_text:
                        parts.append(f"=== [ë¬¸ì„œ ì²­í¬ {i}] ===\n" + chunk_text[:5000])

                if sorted_chunks:
                    logger.info(f"âœ“ ë¬¸ì„œ ê³ ì • ì²­í¬ {len(sorted_chunks)}ê°œ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: {len(priority_chunks)}ê°œ)")
            else:
                # ì¼ë°˜ ëª¨ë“œ: í‚¤ì›Œë“œ ê²€ìƒ‰ í›„ ê°™ì€ íŒŒì¼ í•„í„°ë§
                search_keywords = re.sub(r'^\d{4}-\d{2}-\d{2}_', '', filename)  # ë‚ ì§œ ì œê±°
                search_keywords = re.sub(r'\.pdf$', '', search_keywords, flags=re.IGNORECASE)
                search_keywords = search_keywords.replace('_', ' ')

                hits = self.retriever.search(search_keywords, top_k=10)
                same_file_hits = [h for h in hits if h.get("filename") == filename]

                # ì„¹ì…˜ ê°€ì¤‘ì¹˜ ì ìš©
                priority_hits = []
                normal_hits = []
                for h in same_file_hits:
                    chunk_text = h.get('text') or h.get('snippet') or h.get('content') or ""
                    if re.search(priority_keywords, chunk_text):
                        priority_hits.append(h)
                    else:
                        normal_hits.append(h)

                sorted_hits = (priority_hits + normal_hits)[:10]

                for i, h in enumerate(sorted_hits, 1):
                    chunk_text = h.get('text') or h.get('snippet') or h.get('content') or ""
                    if chunk_text:
                        parts.append(f"=== [ê´€ë ¨ ì²­í¬ {i}] ===\n" + chunk_text[:5000])

                if sorted_hits:
                    logger.info(f"âœ“ RAG ì²­í¬ {len(sorted_hits)}ê°œ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: {len(priority_hits)}ê°œ)")
        except Exception as e:
            logger.warning(f"âš ï¸ RAG ì²­í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # 3) OCR/ì›ë¬¸ ìŠ¤ëƒ…ìƒ· (ìˆìœ¼ë©´ - í˜„ì¬ëŠ” DB text_preview í™œìš©)
        # í–¥í›„ í™•ì¥: full_text í•„ë“œê°€ ìˆìœ¼ë©´ í™œìš©
        # if hasattr(self, 'get_fulltext'):
        #     full = self.get_fulltext(filename)
        #     if full and len(full) > 1000:
        #         parts.append("=== [ì›ë¬¸ ìŠ¤ëƒ…ìƒ·] ===\n" + full[:3000])

        # ê²°í•© ë° ê¸¸ì´ ì œí•œ (ì•½ 3k í† í° ~ 6000ì, ê²€í† ì„œ ìƒì„¸ ì •ë³´ í¬í•¨)
        context = "\n\n".join(parts)[:6000]
        logger.info(f"ğŸ“‹ ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)}ì (ì²­í¬ ìˆ˜: {len(parts)})")
        return context

    def warmup(self) -> None:
        """ì›Œë°ì—…: LLM + ì¸ë±ìŠ¤ ì‚¬ì „ ë¡œë”©

        ì²« ì¿¼ë¦¬ ì§€ì—° ì œê±°ë¥¼ ìœ„í•´ ì‹œì‘ ì‹œ í˜¸ì¶œ.
        """
        logger.info("Warming up RAG pipeline...")
        try:
            # ë”ë¯¸ ì¿¼ë¦¬ ì‹¤í–‰
            response = self.query("test warmup query", top_k=1)
            if response.success:
                logger.info(f"Warmup completed in {response.latency:.2f}s")
            else:
                logger.warning(f"Warmup failed: {response.error}")
        except Exception as e:
            logger.error(f"Warmup error: {e}", exc_info=True)

    # ========================================================================
    # ë‚´ë¶€ í—¬í¼: ê¸°ë³¸ êµ¬í˜„ ìƒì„±
    # ========================================================================

    def _create_default_retriever(self) -> Retriever:
        """ê¸°ë³¸ ê²€ìƒ‰ ì—”ì§„ ìƒì„± (v2 ë˜ëŠ” v1)

        í™˜ê²½ ë³€ìˆ˜ USE_V2_RETRIEVERë¡œ ì œì–´:
        - true: HybridRetrieverV2 ì‚¬ìš© (ì‹ ê·œ 2-layer ì•„í‚¤í…ì²˜)
        - false/ì—†ìŒ: HybridRetriever ì‚¬ìš© (ê¸°ì¡´ ë ˆê±°ì‹œ)
        """
        import os

        use_v2 = os.getenv("USE_V2_RETRIEVER", "false").lower() == "true"

        if use_v2:
            # V2 RetrieverëŠ” archiveë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤ (20251026)
            # ë ˆê±°ì‹œ ì½”ë“œë¥¼ ì œê±°í•˜ê³  v1ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤
            logger.warning(
                "âš ï¸ USE_V2_RETRIEVERëŠ” ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. v1 Retrieverë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            use_v2 = False
            # try:
            #     from app.rag.retriever_v2 import HybridRetrieverV2
            #     v2_retriever = HybridRetrieverV2()
            #     logger.info("âœ… HybridRetrieverV2 (v2 ì‹ ê·œ ì‹œìŠ¤í…œ) ìƒì„± ì™„ë£Œ")
            #
            #     # V2 adapter: fused_results â†’ list ë³€í™˜
            #     return _V2RetrieverAdapter(v2_retriever)
            # except Exception as e:
            #     logger.error(f"V2 Retriever ìƒì„± ì‹¤íŒ¨, v1ìœ¼ë¡œ í´ë°±: {e}")
            #     # í´ë°±: v1 ì‚¬ìš©
            #     use_v2 = False

        if not use_v2:
            try:
                from app.rag.retrievers.hybrid import HybridRetriever

                retriever = HybridRetriever()
                logger.info("Default HybridRetriever (v1 ë ˆê±°ì‹œ) ìƒì„± ì™„ë£Œ")
                return retriever
            except Exception as e:
                logger.error(f"HybridRetriever ìƒì„± ì‹¤íŒ¨: {e}")
                # í´ë°±: ë”ë¯¸ êµ¬í˜„
                return _DummyRetriever()

    def _create_default_compressor(self) -> Compressor:
        """ê¸°ë³¸ ì••ì¶•ê¸° ìƒì„± (í˜„ì¬ëŠ” no-op)"""
        logger.info("Default compressor ìƒì„± (no-op)")
        return _NoOpCompressor()

    def _create_default_generator(self) -> Generator:
        """ê¸°ë³¸ LLM ìƒì„±ê¸° ìƒì„± (ë ˆê±°ì‹œ ì–´ëŒ‘í„° ì‚¬ìš©)"""
        try:
            # ë ˆê±°ì‹œ êµ¬í˜„ ì–´ëŒ‘í„° ì‚¬ìš© (ì ì§„ì  ì´ê´€ ì¤€ë¹„)
            legacy_rag = self._create_legacy_adapter()
            logger.info("Default generator ìƒì„± (Legacy Adapter ë˜í•‘)")
            return _QuickFixGenerator(legacy_rag)
        except Exception as e:
            logger.error(f"Generator ìƒì„± ì‹¤íŒ¨: {e}")
            return _DummyGenerator()

    def _create_legacy_adapter(self):
        """ë ˆê±°ì‹œ êµ¬í˜„ ì–´ëŒ‘í„° ìƒì„± (ìº¡ìŠí™”)

        QwenLLMì„ ë˜í•‘í•˜ì—¬ ê¸°ì¡´ ë ˆê±°ì‹œ ì‹œìŠ¤í…œê³¼ ì—°ê²°í•©ë‹ˆë‹¤.
        í–¥í›„ ì´ ë©”ì„œë“œë§Œ ìˆ˜ì •í•˜ì—¬ ì‹ ê·œ êµ¬í˜„ìœ¼ë¡œ ì ì§„ ì „í™˜ ê°€ëŠ¥.

        Returns:
            _LLMAdapter: LLM ì–´ëŒ‘í„° ì¸ìŠ¤í„´ìŠ¤
        """
        try:
            from rag_system.llm_singleton import LLMSingleton

            model_path = os.getenv("MODEL_PATH", "./models/ggml-model-Q4_K_M.gguf")
            logger.info(f"ğŸ” DEBUG: Attempting to load LLM with model_path={model_path}")
            logger.info(f"ğŸ” DEBUG: Model file exists: {Path(model_path).exists()}")
            llm = LLMSingleton.get_instance(model_path=model_path)
            logger.info(f"âœ… LLM adapter ìƒì„± ì™„ë£Œ (LLMSingleton ì‚¬ìš©, model={model_path})")
            return _LLMAdapter(llm)
        except Exception as e:
            logger.error(f"LLM adapter ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return None

    def _load_known_drafters(self) -> set:
        """ë©”íƒ€DBì—ì„œ ê³ ìœ  ê¸°ì•ˆì ë¡œë“œ (Closed-World Validationìš©)

        Returns:
            set: ê³ ìœ  ê¸°ì•ˆì ì´ë¦„ ì§‘í•©
        """
        try:
            from modules.metadata_db import MetadataDB

            db = MetadataDB()
            drafters = db.list_unique_drafters()
            db.close()

            logger.info(f"âœ… ê³ ìœ  ê¸°ì•ˆì {len(drafters)}ëª… ìºì‹± ì™„ë£Œ")
            return drafters
        except Exception as e:
            logger.error(f"ê¸°ì•ˆì ë¡œë“œ ì‹¤íŒ¨: {e}")
            return set()


# ============================================================================
# í´ë°± êµ¬í˜„ (ê¸°ë³¸ ë™ì‘ ë³´ì¥)
# ============================================================================


class _DummyRetriever:
    """ë”ë¯¸ ê²€ìƒ‰ê¸° (í´ë°±ìš©)"""

    def search(self, query: str, top_k: int, mode: str = "chat", selected_filename: Optional[str] = None) -> List[Dict[str, Any]]:
        logger.warning("Dummy retriever: ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return []


class _NoOpCompressor:
    """No-op ì••ì¶•ê¸° (ì••ì¶•í•˜ì§€ ì•ŠìŒ)"""

    def compress(
        self, chunks: List[Dict[str, Any]], ratio: float
    ) -> List[Dict[str, Any]]:
        logger.debug("No-op compressor: ì••ì¶• ìŠ¤í‚µ")
        return chunks


class _LLMAdapter:
    """QwenLLM ì–´ëŒ‘í„° (LegacyAdapter ëŒ€ì²´)

    QwenLLMì„ _QuickFixGeneratorê°€ ê¸°ëŒ€í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """

    def __init__(self, llm):
        self.llm = llm

    def generate_from_context(self, query: str, context: str, temperature: float = 0.1, mode: str = "rag") -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (í…ìŠ¤íŠ¸ í˜•ì‹)
            temperature: ìƒì„± ì˜¨ë„
            mode: ìƒì„± ëª¨ë“œ (chat/rag/summarize)

        Returns:
            str: ìƒì„±ëœ ë‹µë³€
        """
        # Contextë¥¼ ì²­í¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        chunks = [{"snippet": context, "content": context}]

        try:
            # ğŸ¯ ëª¨ë“œë³„ í† í° ì˜ˆì‚° ì ìš©
            logger.info(f"ğŸ¯ generate_from_context: mode={mode}")
            response = self.llm.generate_response(query, chunks, max_retries=1, mode=mode)

            if hasattr(response, "answer"):
                return response.answer
            return str(response)
        except Exception as e:
            logger.error(f"LLM ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return f"[E_GENERATE] {str(e)}"


class _QuickFixGenerator:
    """QuickFixRAG ë˜í¼ (ê¸°ì¡´ êµ¬í˜„ í™œìš©)"""

    def __init__(self, rag):
        self.rag = rag
        self.compressed_chunks = None  # Store chunks for LLM

    def generate(self, query: str, context: str, temperature: float, mode: str = "rag") -> str:
        # ì¬ê²€ìƒ‰ ê¸ˆì§€. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„±ìœ¼ë¡œ ìš°ì„  ì‹œë„.
        try:
            # 1) QuickFixRAGì— ì „ìš© ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if hasattr(self.rag, "generate_from_context"):
                return self.rag.generate_from_context(
                    query, context, temperature=temperature, mode=mode
                )

            # 2) ë‚´ë¶€ LLM ì§ì ‘ ì ‘ê·¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            # ğŸ”¥ CRITICAL: LLM lazy loading - ensure LLM is loaded before checking
            if hasattr(self.rag, "_ensure_llm_loaded"):
                self.rag._ensure_llm_loaded()

            if hasattr(self.rag, "llm") and hasattr(self.rag.llm, "generate_response"):
                # CRITICAL: generate_response expects List[Dict], not str
                # Convert context string back to chunks format
                if self.compressed_chunks:
                    # Use stored compressed chunks (preferred)
                    logger.debug(
                        f"Using {len(self.compressed_chunks)} compressed chunks for generation (mode={mode})"
                    )
                    response = self.rag.llm.generate_response(
                        query, self.compressed_chunks, max_retries=1, mode=mode
                    )
                else:
                    # Fallback: convert context string to minimal chunks
                    logger.warning(
                        "No compressed_chunks available, converting context string"
                    )
                    snippets = context.split("\n\n")
                    chunks = [
                        {"snippet": s, "content": s} for s in snippets if s.strip()
                    ]
                    response = self.rag.llm.generate_response(
                        query, chunks, max_retries=1, mode=mode
                    )

                # Extract answer from RAGResponse object
                if hasattr(response, "answer"):
                    return response.answer
                return str(response)

            # 3) í´ë°±: ì¬ê²€ìƒ‰ì´ í¬í•¨ëœ answerëŠ” ìµœí›„ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ
            logger.warning("generate_from_context ë¯¸ì§€ì› â†’ í´ë°±(answer) ì‚¬ìš©")
            if self.rag is None:
                logger.error("LegacyAdapter: QuickFixRAGê°€ ì—†ì–´ ë‹µë³€ ìƒì„± ë¶ˆê°€")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ ìƒì„± ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
            return self.rag.answer(query, use_llm_summary=True)
        except Exception as e:
            logger.error(f"Generation ì‹¤íŒ¨: {e}", exc_info=True)
            return f"[E_GENERATE] {str(e)}"


class _V2RetrieverAdapter:
    """V2 Retriever Adapter

    HybridRetrieverV2ì˜ ê²°ê³¼ í˜•ì‹ {"fused_results": [...]}ë¥¼
    v1 ì¸í„°í˜ì´ìŠ¤ í˜•ì‹ [...] ìœ¼ë¡œ ë³€í™˜.

    v2 results êµ¬ì¡°:
        {
            "fused_results": [
                {"id": "doc_4094", "score": 0.123, "filename": "...", ...},
                ...
            ]
        }

    v1 expected êµ¬ì¡°:
        [
            {"doc_id": "doc_4094", "snippet": "...", "page": 1, ...},
            ...
        ]
    """

    def __init__(self, v2_retriever):
        """
        Args:
            v2_retriever: HybridRetrieverV2 instance
        """
        self.v2_retriever = v2_retriever
        self.db = v2_retriever.db  # MetadataDB for content fetching

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Search using v2 retriever, convert to v1 format

        Args:
            query: Search query
            top_k: Number of results

        Returns:
            List of dicts in v1 format with keys:
            - doc_id: Document ID
            - snippet: Text snippet
            - page: Page number (default 1)
            - score: Relevance score
            - meta: Metadata dict
        """
        try:
            # Call v2 retriever
            v2_result = self.v2_retriever.search(query, top_k=top_k)
            fused_results = v2_result.get("fused_results", [])

            # Convert to v1 format
            v1_results = []
            for doc in fused_results:
                doc_id = doc.get("id", "unknown")

                # ğŸ”¥ CRITICAL: snippet ìš°ì„ ìˆœìœ„
                # 1) ê²€ìƒ‰ ê²°ê³¼ì— ì§ì ‘ í¬í•¨ëœ snippet/content
                # 2) DB ì¡°íšŒ (get_content)
                # 3) ì œëª©/íŒŒì¼ëª… ê¸°ë°˜ í´ë°±

                snippet = ""

                # Priority 1: fused_resultsì— ì´ë¯¸ í¬í•¨ëœ ë°ì´í„°
                if "snippet" in doc:
                    snippet = doc["snippet"]
                elif "content" in doc:
                    snippet = doc["content"][:500]

                # Priority 2: DB ì¡°íšŒ (app/rag/db.MetadataDB.get_content)
                if not snippet or len(snippet) < 50:
                    content = self.db.get_content(doc_id)
                    if content and len(content) >= 50:
                        snippet = content[:500]

                # Priority 3: ë©”íƒ€ë°ì´í„° í´ë°±
                if not snippet or len(snippet) < 50:
                    fallback_parts = []
                    if doc.get("title"):
                        fallback_parts.append(f"ì œëª©: {doc['title']}")
                    if doc.get("filename"):
                        fallback_parts.append(f"íŒŒì¼: {doc['filename']}")
                    if doc.get("date"):
                        fallback_parts.append(f"ë‚ ì§œ: {doc['date']}")

                    snippet = (
                        " | ".join(fallback_parts)
                        if fallback_parts
                        else f"ë¬¸ì„œ ID: {doc_id}"
                    )
                    logger.warning(
                        f"V2 Adapter: doc_id={doc_id} snippet ê²°ì†, ë©”íƒ€ë°ì´í„° í´ë°± ì‚¬ìš©"
                    )

                v1_results.append(
                    {
                        "doc_id": doc_id,
                        "snippet": snippet,
                        "page": 1,  # v2ì—ì„œëŠ” page ì •ë³´ ì—†ìŒ, ê¸°ë³¸ 1
                        "score": doc.get("score", 0.0),
                        "meta": {
                            "doc_id": doc_id,
                            "filename": doc.get("filename", ""),
                            "title": doc.get("title", ""),
                            "date": doc.get("date", ""),
                            "page": 1,
                        },
                    }
                )

            logger.info(f"V2 Adapter: {len(v1_results)} results converted")
            return v1_results

        except Exception as e:
            logger.error(f"V2 Adapter search failed: {e}", exc_info=True)
            return []

    def warmup(self):
        """ì›Œë°ì—… (v2ëŠ” í•„ìš” ì‹œ ìë™ ë¡œë“œ)"""
        logger.info("V2 Adapter warmup (no-op)")


class _DummyGenerator:
    """ë”ë¯¸ ìƒì„±ê¸° (í´ë°±ìš©)"""

    def generate(self, query: str, context: str, temperature: float, mode: str = "rag") -> str:
        logger.warning("Dummy generator: ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
