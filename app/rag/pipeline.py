"""RAG íŒŒì´í”„ë¼ì¸ (íŒŒì‚¬ë“œ íŒ¨í„´)

ë‹¨ì¼ ì§„ì…ì : RAGPipeline.query()
ë‚´ë¶€ íë¦„: ê²€ìƒ‰ â†’ ì••ì¶• â†’ LLM ìƒì„±

Example:
    >>> pipeline = RAGPipeline()
    >>> response = pipeline.query("ì§ˆë¬¸", top_k=5)
    >>> print(response.answer)
"""

import os
import time
from dataclasses import dataclass, field
from typing import Protocol, List, Optional, Dict, Any

from app.core.logging import get_logger
from app.core.errors import ModelError, SearchError, ErrorCode, ERROR_MESSAGES

logger = get_logger(__name__)

# ì§„ë‹¨ ëª¨ë“œ ì„¤ì •
DIAG_RAG = os.getenv('DIAG_RAG', 'false').lower() == 'true'
DIAG_LOG_LEVEL = os.getenv('DIAG_LOG_LEVEL', 'INFO').upper()


# ============================================================================
# Request / Response ë°ì´í„° í´ë˜ìŠ¤
# ============================================================================

@dataclass
class RAGRequest:
    """RAG ìš”ì²­ íŒŒë¼ë¯¸í„°

    Attributes:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
        compression_ratio: ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ë¹„ìœ¨ (0.0~1.0)
        use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
        temperature: LLM ìƒì„± ì˜¨ë„
    """
    query: str
    top_k: int = 5
    compression_ratio: float = 0.7
    use_hyde: bool = False
    temperature: float = 0.1


@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ê²°ê³¼

    Attributes:
        answer: ìƒì„±ëœ ë‹µë³€
        source_docs: ì°¸ê³  ë¬¸ì„œ ëª©ë¡ (í•˜ìœ„ í˜¸í™˜)
        evidence_chunks: Evidenceìš© ì •ê·œí™” ì²­í¬ (ê¶Œì¥)
        raw_results: ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ (Evidence ìµœì†Œ ë³´ì¥ìš©)
        latency: ì „ì²´ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
        success: ì„±ê³µ ì—¬ë¶€
        error: ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
        metrics: ë‚´ë¶€ ì§€í‘œ (ê²€ìƒ‰/ì••ì¶•/ìƒì„± ì‹œê°„ ë“±)
        diagnostics: ì§„ë‹¨ ì •ë³´ (DIAG_RAG=trueì¼ ë•Œë§Œ ì±„ì›Œì§)
    """
    answer: str
    source_docs: List[str] = field(default_factory=list)
    evidence_chunks: List[Dict[str, Any]] = field(default_factory=list)
    raw_results: List[Dict[str, Any]] = field(default_factory=list)
    latency: float = 0.0
    success: bool = True
    error: Optional[str] = None
    metrics: dict = field(default_factory=dict)
    diagnostics: dict = field(default_factory=dict)  # ì§„ë‹¨ ì •ë³´


# ============================================================================
# í”„ë¡œí† ì½œ ì •ì˜ (ì˜ì¡´ì„± ì—­ì „)
# ============================================================================

class Retriever(Protocol):
    """ê²€ìƒ‰ ì—”ì§„ ì¸í„°í˜ì´ìŠ¤"""

    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ìˆ˜í–‰ (ì •ê·œí™” ìŠ¤í‚¤ë§ˆ ë°˜í™˜)

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼

        Returns:
            [
                {
                    "doc_id": str,
                    "page": int,
                    "score": float,
                    "snippet": str,
                    "meta": dict
                }, ...
            ]
        """
        ...


class Compressor(Protocol):
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶•ê¸° ì¸í„°í˜ì´ìŠ¤"""

    def compress(self, chunks: List[Dict[str, Any]], ratio: float) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ì••ì¶•

        Args:
            chunks: ì›ë³¸ ì²­í¬ ëª©ë¡ (ì •ê·œí™”ëœ dict)
            ratio: ì••ì¶• ë¹„ìœ¨

        Returns:
            ì••ì¶•ëœ ì²­í¬ ëª©ë¡ (ë™ì¼ ìŠ¤í‚¤ë§ˆ)
        """
        ...


class Generator(Protocol):
    """LLM ìƒì„±ê¸° ì¸í„°í˜ì´ìŠ¤"""

    def generate(self, query: str, context: str, temperature: float) -> str:
        """ë‹µë³€ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì°¸ê³  ë¬¸ì„œ
            temperature: ìƒì„± ì˜¨ë„

        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        ...


# ============================================================================
# RAG íŒŒì´í”„ë¼ì¸ (íŒŒì‚¬ë“œ)
# ============================================================================

class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸ íŒŒì‚¬ë“œ

    ê²€ìƒ‰ â†’ ì••ì¶• â†’ ìƒì„±ì„ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ ì œê³µ.
    ë‚´ë¶€ êµ¬í˜„ì€ Retriever/Compressor/Generatorì— ìœ„ì„.

    Example:
        >>> pipeline = RAGPipeline()
        >>> response = pipeline.query("ì§ˆë¬¸", top_k=5)
        >>> if response.success:
        ...     print(response.answer)
        ...     print(f"ì°¸ê³ : {response.source_docs}")
    """

    def __init__(
        self,
        retriever: Optional[Retriever] = None,
        compressor: Optional[Compressor] = None,
        generator: Optional[Generator] = None,
    ):
        """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            retriever: ê²€ìƒ‰ ì—”ì§„ (Noneì´ë©´ ê¸°ë³¸ HybridRetriever ì‚¬ìš©)
            compressor: ì••ì¶•ê¸° (Noneì´ë©´ ê¸°ë³¸ ContextCompressor ì‚¬ìš©)
            generator: LLM ìƒì„±ê¸° (Noneì´ë©´ ê¸°ë³¸ LlamaCppGenerator ì‚¬ìš©)
        """
        self.retriever = retriever or self._create_default_retriever()
        self.compressor = compressor or self._create_default_compressor()
        self.generator = generator or self._create_default_generator()

        logger.info("RAG Pipeline initialized")

    def query(
        self,
        query: str,
        top_k: int = 5,
        compression_ratio: float = 0.7,
        use_hyde: bool = False,
        temperature: float = 0.1,
    ) -> RAGResponse:
        """RAG ì§ˆì˜ (ë‹¨ì¼ ì§„ì…ì )

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            compression_ratio: ì••ì¶• ë¹„ìœ¨
            use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
            temperature: LLM ìƒì„± ì˜¨ë„

        Returns:
            RAGResponse: ë‹µë³€ + ë©”íƒ€ë°ì´í„°
        """
        import time

        # ì…ë ¥ ê²€ì¦
        if not query or not query.strip():
            return RAGResponse(
                answer="",
                success=False,
                error="ë¹ˆ ì§ˆë¬¸ì…ë‹ˆë‹¤",
            )

        start_time = time.perf_counter()
        metrics = {}
        diagnostics = {}  # ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘

        try:
            # 1. ê²€ìƒ‰: ì •ê·œí™”ëœ ì²­í¬(dict) ë¦¬ìŠ¤íŠ¸ ê¸°ëŒ€
            search_start = time.perf_counter()
            results = self.retriever.search(query, top_k)
            metrics["search_time"] = time.perf_counter() - search_start

            # [DIAG] ê²€ìƒ‰ ê²°ê³¼ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["retrieved_k"] = len(results)
                if DIAG_LOG_LEVEL in ['DEBUG', 'INFO']:
                    logger.info(f"[DIAG] ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")

            if not results:
                logger.warning(f"No results found for query: {query[:50]}")
                if DIAG_RAG:
                    diagnostics["mode"] = "no_results"
                    diagnostics["generate_path"] = "fallback_no_context"
                return RAGResponse(
                    answer="ê´€ë ¨ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ë‹¤.",
                    success=True,
                    latency=time.perf_counter() - start_time,
                    metrics=metrics,
                    diagnostics=diagnostics,
                )

            # 2. ì••ì¶•: ì²­í¬ ë‹¨ìœ„ ìœ ì§€(í˜ì´ì§€/ìŠ¤ë‹ˆí«/ë©”íƒ€ ë³´ì¡´)
            compress_start = time.perf_counter()
            compressed = self.compressor.compress(results, compression_ratio)
            metrics["compress_time"] = time.perf_counter() - compress_start

            # [DIAG] ì••ì¶• í›„ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["after_compress_k"] = len(compressed)
                diagnostics["compression_ratio"] = compression_ratio
                if DIAG_LOG_LEVEL in ['DEBUG', 'INFO']:
                    logger.info(f"[DIAG] ì••ì¶• ì™„ë£Œ: {len(results)} â†’ {len(compressed)}ê°œ ë¬¸ì„œ")

            # 3. ìƒì„±: ì»¨í…ìŠ¤íŠ¸ëŠ” ìŠ¤ë‹ˆí« ì§‘í•©ìœ¼ë¡œ êµ¬ì„±
            gen_start = time.perf_counter()

            # CRITICAL: Inject compressed chunks into generator for proper LLM context
            if hasattr(self.generator, "compressed_chunks"):
                self.generator.compressed_chunks = compressed
                logger.debug(f"Injected {len(compressed)} compressed chunks into generator")

            context = "\n\n".join([c.get("snippet", "") for c in compressed])

            # [DIAG] ìƒì„± ì „ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ·
            if DIAG_RAG and DIAG_LOG_LEVEL == 'DEBUG':
                for i, c in enumerate(compressed[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ë¡œê·¸
                    logger.debug(
                        f"[DIAG] Context[{i}]: doc_id={c.get('doc_id')}, "
                        f"filename={c.get('filename', 'N/A')}, "
                        f"page={c.get('page', 0)}, "
                        f"snippet={c.get('snippet', '')[:120]}..."
                    )

            answer = self.generator.generate(query, context, temperature)
            metrics["generate_time"] = time.perf_counter() - gen_start

            # [DIAG] ìƒì„± ì™„ë£Œ ì§„ë‹¨
            if DIAG_RAG:
                diagnostics["mode"] = "normal"
                diagnostics["generate_path"] = "from_context"
                diagnostics["used_k"] = len(compressed)
                if DIAG_LOG_LEVEL in ['DEBUG', 'INFO']:
                    logger.info(f"[DIAG] ìƒì„± ì™„ë£Œ: from_context ê²½ë¡œ, {len(compressed)}ê°œ ë¬¸ì„œ ì‚¬ìš©")

            total_latency = time.perf_counter() - start_time
            metrics["total_time"] = total_latency

            logger.info(
                f"RAG query completed in {total_latency:.2f}s "
                f"(search={metrics['search_time']:.2f}s, "
                f"compress={metrics['compress_time']:.2f}s, "
                f"generate={metrics['generate_time']:.2f}s)"
            )

            return RAGResponse(
                answer=answer,
                source_docs=[c.get("doc_id") for c in results[:3]],
                evidence_chunks=compressed,  # UIìš© ê·¼ê±°
                raw_results=results,  # Evidence ìµœì†Œ ë³´ì¥ìš©
                latency=total_latency,
                success=True,
                metrics=metrics,
                diagnostics=diagnostics,
            )

        except SearchError as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_RETRIEVE] ê²€ìƒ‰ ì‹¤íŒ¨: {e.message}",
                latency=time.perf_counter() - start_time,
            )

        except ModelError as e:
            logger.error(f"Model inference failed: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_GENERATE] ìƒì„± ì‹¤íŒ¨: {e.message}",
                latency=time.perf_counter() - start_time,
            )

        except Exception as e:
            logger.error(f"Unexpected error: {e}", exc_info=True)
            return RAGResponse(
                answer="",
                success=False,
                error=f"[E_UNKNOWN] {str(e)}",
                latency=time.perf_counter() - start_time,
            )

    def answer(self, query: str, top_k: Optional[int] = None) -> dict:
        """ë‹µë³€ ìƒì„± (Evidence í¬í•¨ êµ¬ì¡°í™”ëœ ì‘ë‹µ)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ (Noneì´ë©´ ê¸°ë³¸ê°’ 5)

        Returns:
            dict: {
                "text": ë‹µë³€ í…ìŠ¤íŠ¸,
                "evidence": [
                    {
                        "doc_id": ë¬¸ì„œ ID,
                        "page": í˜ì´ì§€ ë²ˆí˜¸,
                        "snippet": ë°œì·Œë¬¸,
                        "meta": {"doc_id": str, "page": int, ...}
                    }, ...
                ]
            }
        """
        response = self.query(query, top_k=top_k or 5)

        if response.success:
            # ê²€ìƒ‰/ì••ì¶•ì—ì„œ ë„˜ì–´ì˜¨ ì •ê·œí™” ì²­í¬ ì‚¬ìš© (ì‹¤ì œ page/snippet/meta ë…¸ì¶œ)
            evidence = [
                {
                    "doc_id": c.get("doc_id"),
                    "page": c.get("page", 1),
                    "snippet": c.get("snippet", ""),
                    "meta": c.get("meta", {"doc_id": c.get("doc_id"), "page": c.get("page", 1)}),
                }
                for c in (response.evidence_chunks or [])
            ]

            # CRITICAL: Evidence ìµœì†Œ ë³´ì¥ (sources_citedê°€ ë¹„ì–´ë„ ê²€ìƒ‰ ê²°ê³¼ëŠ” í‘œì‹œ)
            evidence_injected = False
            if not evidence and response.raw_results:
                logger.info(f"Evidence empty, using raw_results[:3] as fallback")
                evidence = [
                    {
                        "doc_id": r.get("doc_id") or r.get("chunk_id", "unknown"),
                        "page": 0,  # ê²€ìƒ‰ ê²°ê³¼ëŠ” í˜ì´ì§€ ì •ë³´ ì—†ìŒ
                        "snippet": r.get("snippet") or r.get("text_preview", "")[:500],
                        "meta": {
                            "doc_id": r.get("doc_id") or r.get("chunk_id", "unknown"),
                            "filename": r.get("filename", ""),
                            "page": 0,
                        },
                    }
                    for r in response.raw_results[:3]
                ]
                evidence_injected = True

            # [DIAG] Evidence ì§„ë‹¨ ì •ë³´ ì¶”ê°€
            if DIAG_RAG and response.diagnostics:
                response.diagnostics["evidence_count"] = len(evidence)
                response.diagnostics["evidence_injected"] = evidence_injected

            # ğŸ”¥ CRITICAL: status.found í”Œë˜ê·¸ - UI íŒì • ë‹¨ì¼ ì†ŒìŠ¤
            # retrieved_count: ê²€ìƒ‰ëœ ì›ë³¸ ê²°ê³¼ ìˆ˜
            # selected_count: ì‹¤ì œ ì‚¬ìš©ëœ ì¦ê±° ìˆ˜ (evidence)
            # found: ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€ (evidenceê°€ 1ê°œ ì´ìƒì´ë©´ True)
            status = {
                "retrieved_count": len(response.raw_results or []),
                "selected_count": len(evidence),
                "found": len(evidence) > 0  # ğŸ”´ ìœ ì¼í•œ íŒì • ê¸°ì¤€
            }

            return {
                "text": response.answer,
                "evidence": evidence,  # citationsì™€ ë™ì¼í•œ ë¦¬ìŠ¤íŠ¸
                "status": status,  # UIì—ì„œ ì´ê²ƒë§Œ í™•ì¸
                "diagnostics": response.diagnostics if DIAG_RAG else {}
            }
        else:
            # ì—ëŸ¬ ë°œìƒ ì‹œ (ì¤‘ë¦½ í†¤, ì‚¬ê³¼ í‘œí˜„ ê¸ˆì§€)
            error_msg = ERROR_MESSAGES.get(ErrorCode.E_GENERATE, "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆë‹¤.")
            if response.error:
                error_msg = f"{error_msg}\n\nìƒì„¸: {response.error}"

            return {
                "text": error_msg,
                "evidence": [],
                "status": {
                    "retrieved_count": 0,
                    "selected_count": 0,
                    "found": False
                }
            }

    def answer_text(self, query: str) -> str:
        """ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            str: ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
        """
        result = self.answer(query)
        return result["text"]

    def warmup(self) -> None:
        """ì›Œë°ì—…: LLM + ì¸ë±ìŠ¤ ì‚¬ì „ ë¡œë”©

        ì²« ì¿¼ë¦¬ ì§€ì—° ì œê±°ë¥¼ ìœ„í•´ ì‹œì‘ ì‹œ í˜¸ì¶œ.
        """
        logger.info("Warming up RAG pipeline...")
        try:
            # ë”ë¯¸ ì¿¼ë¦¬ ì‹¤í–‰
            response = self.query("test warmup query", top_k=1)
            if response.success:
                logger.info(f"Warmup completed in {response.latency:.2f}s")
            else:
                logger.warning(f"Warmup failed: {response.error}")
        except Exception as e:
            logger.error(f"Warmup error: {e}", exc_info=True)

    # ========================================================================
    # ë‚´ë¶€ í—¬í¼: ê¸°ë³¸ êµ¬í˜„ ìƒì„±
    # ========================================================================

    def _create_default_retriever(self) -> Retriever:
        """ê¸°ë³¸ ê²€ìƒ‰ ì—”ì§„ ìƒì„± (v2 ë˜ëŠ” v1)

        í™˜ê²½ ë³€ìˆ˜ USE_V2_RETRIEVERë¡œ ì œì–´:
        - true: HybridRetrieverV2 ì‚¬ìš© (ì‹ ê·œ 2-layer ì•„í‚¤í…ì²˜)
        - false/ì—†ìŒ: HybridRetriever ì‚¬ìš© (ê¸°ì¡´ ë ˆê±°ì‹œ)
        """
        import os
        use_v2 = os.getenv('USE_V2_RETRIEVER', 'false').lower() == 'true'

        if use_v2:
            try:
                from app.rag.retriever_v2 import HybridRetrieverV2
                v2_retriever = HybridRetrieverV2()
                logger.info("âœ… HybridRetrieverV2 (v2 ì‹ ê·œ ì‹œìŠ¤í…œ) ìƒì„± ì™„ë£Œ")

                # V2 adapter: fused_results â†’ list ë³€í™˜
                return _V2RetrieverAdapter(v2_retriever)
            except Exception as e:
                logger.error(f"V2 Retriever ìƒì„± ì‹¤íŒ¨, v1ìœ¼ë¡œ í´ë°±: {e}")
                # í´ë°±: v1 ì‚¬ìš©
                use_v2 = False

        if not use_v2:
            try:
                from app.rag.retrievers.hybrid import HybridRetriever
                retriever = HybridRetriever()
                logger.info("Default HybridRetriever (v1 ë ˆê±°ì‹œ) ìƒì„± ì™„ë£Œ")
                return retriever
            except Exception as e:
                logger.error(f"HybridRetriever ìƒì„± ì‹¤íŒ¨: {e}")
                # í´ë°±: ë”ë¯¸ êµ¬í˜„
                return _DummyRetriever()

    def _create_default_compressor(self) -> Compressor:
        """ê¸°ë³¸ ì••ì¶•ê¸° ìƒì„± (í˜„ì¬ëŠ” no-op)"""
        logger.info("Default compressor ìƒì„± (no-op)")
        return _NoOpCompressor()

    def _create_default_generator(self) -> Generator:
        """ê¸°ë³¸ LLM ìƒì„±ê¸° ìƒì„± (ë ˆê±°ì‹œ ì–´ëŒ‘í„° ì‚¬ìš©)"""
        try:
            # ë ˆê±°ì‹œ êµ¬í˜„ ì–´ëŒ‘í„° ì‚¬ìš© (ì ì§„ì  ì´ê´€ ì¤€ë¹„)
            legacy_rag = self._create_legacy_adapter()
            logger.info("Default generator ìƒì„± (Legacy Adapter ë˜í•‘)")
            return _QuickFixGenerator(legacy_rag)
        except Exception as e:
            logger.error(f"Generator ìƒì„± ì‹¤íŒ¨: {e}")
            return _DummyGenerator()

    def _create_legacy_adapter(self):
        """ë ˆê±°ì‹œ êµ¬í˜„ ì–´ëŒ‘í„° ìƒì„± (ìº¡ìŠí™”)

        QuickFixRAGë¥¼ ë˜í•‘í•˜ì—¬ ê¸°ì¡´ ë ˆê±°ì‹œ ì‹œìŠ¤í…œê³¼ ì—°ê²°í•©ë‹ˆë‹¤.
        í–¥í›„ ì´ ë©”ì„œë“œë§Œ ìˆ˜ì •í•˜ì—¬ ì‹ ê·œ êµ¬í˜„ìœ¼ë¡œ ì ì§„ ì „í™˜ ê°€ëŠ¥.

        Returns:
            QuickFixRAG: ë ˆê±°ì‹œ RAG ì¸ìŠ¤í„´ìŠ¤
        """
        from quick_fix_rag import QuickFixRAG

        # ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        logger.info("Loading legacy QuickFixRAG adapter...")
        rag = QuickFixRAG(use_hybrid=True)
        logger.info("Legacy adapter loaded successfully")

        return rag


# ============================================================================
# í´ë°± êµ¬í˜„ (ê¸°ë³¸ ë™ì‘ ë³´ì¥)
# ============================================================================

class _DummyRetriever:
    """ë”ë¯¸ ê²€ìƒ‰ê¸° (í´ë°±ìš©)"""

    def search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        logger.warning("Dummy retriever: ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return []


class _NoOpCompressor:
    """No-op ì••ì¶•ê¸° (ì••ì¶•í•˜ì§€ ì•ŠìŒ)"""

    def compress(self, chunks: List[Dict[str, Any]], ratio: float) -> List[Dict[str, Any]]:
        logger.debug("No-op compressor: ì••ì¶• ìŠ¤í‚µ")
        return chunks


class _QuickFixGenerator:
    """QuickFixRAG ë˜í¼ (ê¸°ì¡´ êµ¬í˜„ í™œìš©)"""

    def __init__(self, rag):
        self.rag = rag
        self.compressed_chunks = None  # Store chunks for LLM

    def generate(self, query: str, context: str, temperature: float) -> str:
        # ì¬ê²€ìƒ‰ ê¸ˆì§€. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„±ìœ¼ë¡œ ìš°ì„  ì‹œë„.
        try:
            # 1) QuickFixRAGì— ì „ìš© ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if hasattr(self.rag, "generate_from_context"):
                return self.rag.generate_from_context(query, context, temperature=temperature)

            # 2) ë‚´ë¶€ LLM ì§ì ‘ ì ‘ê·¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if hasattr(self.rag, "llm") and hasattr(self.rag.llm, "generate_response"):
                # CRITICAL: generate_response expects List[Dict], not str
                # Convert context string back to chunks format
                if self.compressed_chunks:
                    # Use stored compressed chunks (preferred)
                    logger.debug(f"Using {len(self.compressed_chunks)} compressed chunks for generation")
                    response = self.rag.llm.generate_response(query, self.compressed_chunks, max_retries=1)
                else:
                    # Fallback: convert context string to minimal chunks
                    logger.warning("No compressed_chunks available, converting context string")
                    snippets = context.split("\n\n")
                    chunks = [{"snippet": s, "content": s} for s in snippets if s.strip()]
                    response = self.rag.llm.generate_response(query, chunks, max_retries=1)

                # Extract answer from RAGResponse object
                if hasattr(response, "answer"):
                    return response.answer
                return str(response)

            # 3) í´ë°±: ì¬ê²€ìƒ‰ì´ í¬í•¨ëœ answerëŠ” ìµœí›„ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ
            logger.warning("generate_from_context ë¯¸ì§€ì› â†’ í´ë°±(answer) ì‚¬ìš©")
            return self.rag.answer(query, use_llm_summary=True)
        except Exception as e:
            logger.error(f"Generation ì‹¤íŒ¨: {e}", exc_info=True)
            return f"[E_GENERATE] {str(e)}"


class _V2RetrieverAdapter:
    """V2 Retriever Adapter

    HybridRetrieverV2ì˜ ê²°ê³¼ í˜•ì‹ {"fused_results": [...]}ë¥¼
    v1 ì¸í„°í˜ì´ìŠ¤ í˜•ì‹ [...] ìœ¼ë¡œ ë³€í™˜.

    v2 results êµ¬ì¡°:
        {
            "fused_results": [
                {"id": "doc_4094", "score": 0.123, "filename": "...", ...},
                ...
            ]
        }

    v1 expected êµ¬ì¡°:
        [
            {"doc_id": "doc_4094", "snippet": "...", "page": 1, ...},
            ...
        ]
    """

    def __init__(self, v2_retriever):
        """
        Args:
            v2_retriever: HybridRetrieverV2 instance
        """
        self.v2_retriever = v2_retriever
        self.db = v2_retriever.db  # MetadataDB for content fetching

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Search using v2 retriever, convert to v1 format

        Args:
            query: Search query
            top_k: Number of results

        Returns:
            List of dicts in v1 format with keys:
            - doc_id: Document ID
            - snippet: Text snippet
            - page: Page number (default 1)
            - score: Relevance score
            - meta: Metadata dict
        """
        try:
            # Call v2 retriever
            v2_result = self.v2_retriever.search(query, top_k=top_k)
            fused_results = v2_result.get("fused_results", [])

            # Convert to v1 format
            v1_results = []
            for doc in fused_results:
                doc_id = doc.get("id", "unknown")

                # ğŸ”¥ CRITICAL: snippet ìš°ì„ ìˆœìœ„
                # 1) ê²€ìƒ‰ ê²°ê³¼ì— ì§ì ‘ í¬í•¨ëœ snippet/content
                # 2) DB ì¡°íšŒ (get_content)
                # 3) ì œëª©/íŒŒì¼ëª… ê¸°ë°˜ í´ë°±

                snippet = ""

                # Priority 1: fused_resultsì— ì´ë¯¸ í¬í•¨ëœ ë°ì´í„°
                if "snippet" in doc:
                    snippet = doc["snippet"]
                elif "content" in doc:
                    snippet = doc["content"][:500]

                # Priority 2: DB ì¡°íšŒ
                if not snippet or len(snippet) < 50:
                    content = self.db.get_content(doc_id)
                    if content and len(content) >= 50:
                        snippet = content[:500]

                # Priority 3: ë©”íƒ€ë°ì´í„° í´ë°±
                if not snippet or len(snippet) < 50:
                    fallback_parts = []
                    if doc.get("title"):
                        fallback_parts.append(f"ì œëª©: {doc['title']}")
                    if doc.get("filename"):
                        fallback_parts.append(f"íŒŒì¼: {doc['filename']}")
                    if doc.get("date"):
                        fallback_parts.append(f"ë‚ ì§œ: {doc['date']}")

                    snippet = " | ".join(fallback_parts) if fallback_parts else f"ë¬¸ì„œ ID: {doc_id}"
                    logger.warning(f"V2 Adapter: doc_id={doc_id} snippet ê²°ì†, ë©”íƒ€ë°ì´í„° í´ë°± ì‚¬ìš©")

                v1_results.append({
                    "doc_id": doc_id,
                    "snippet": snippet,
                    "page": 1,  # v2ì—ì„œëŠ” page ì •ë³´ ì—†ìŒ, ê¸°ë³¸ 1
                    "score": doc.get("score", 0.0),
                    "meta": {
                        "doc_id": doc_id,
                        "filename": doc.get("filename", ""),
                        "title": doc.get("title", ""),
                        "date": doc.get("date", ""),
                        "page": 1,
                    },
                })

            logger.info(f"V2 Adapter: {len(v1_results)} results converted")
            return v1_results

        except Exception as e:
            logger.error(f"V2 Adapter search failed: {e}", exc_info=True)
            return []

    def warmup(self):
        """ì›Œë°ì—… (v2ëŠ” í•„ìš” ì‹œ ìë™ ë¡œë“œ)"""
        logger.info("V2 Adapter warmup (no-op)")


class _DummyGenerator:
    """ë”ë¯¸ ìƒì„±ê¸° (í´ë°±ìš©)"""

    def generate(self, query: str, context: str, temperature: float) -> str:
        logger.warning("Dummy generator: ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
