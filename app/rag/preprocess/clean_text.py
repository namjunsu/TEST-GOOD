"""
í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±° ëª¨ë“ˆ v2.0
2025-11-11

ë¬¸ì„œì—ì„œ í”„ë¦°íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„, URL, ë°˜ë³µ í—¤ë”/í‘¸í„° ë“±ì˜ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•©ë‹ˆë‹¤.

v2.0 ë³€ê²½ì‚¬í•­:
- ë³´í˜¸ ë¬¸ìì—´ ê°€ë“œ: URL/IP/ëª¨ë¸ëª… ë³´ì¡´
- í˜ì´ì§€ ë‹¨ìœ„ í—¤ë”/í‘¸í„° ì‹ë³„: í¼í”¼ë“œ/Page íŒ¨í„´ìœ¼ë¡œ ë¶„í• , ìƒí•˜ë‹¨ë§Œ ìŠ¤ìº”
- ë°˜ë³µ ë¼ì¸ ì •ê·œí™”Â·ì§€ë¬¸í™”: ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ íŒì •
- ì†Œê±° ê·¼ê±° ë³´ì¡´: ì œê±° ìƒ˜í”Œ ë°˜í™˜ (ê°ì‚¬ ê°€ëŠ¥ì„±)
- íŒ¨í„´ ì»´íŒŒì¼ í”Œë˜ê·¸ ì§€ì›: MULTILINE, IGNORECASE
- í•œêµ­ì–´ íŠ¹í™” íŒ¨í„´: í”„ë¦°íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„, ê´€ìš© í—¤ë”, ë°”ë‹¥ê¸€
- ë©±ë“±ì„± ë³´ì¥: clean(clean(text)) ë™ì¼ ê²°ê³¼
"""

import re
from collections import Counter
from pathlib import Path
from typing import Any, Dict, List, Tuple

import yaml

from app.core.logging import get_logger

logger = get_logger(__name__)


class TextCleaner:
    """í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±° í´ë˜ìŠ¤ v2.0"""

    def __init__(self, config_path: str = "config/document_processing.yaml"):
        """ì´ˆê¸°í™”

        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = self._load_config(config_path)

        # ë³´í˜¸ ë¬¸ìì—´ (ì˜¤íƒ ë°©ì§€: URL, IP, ëª¨ë¸ëª… ë“±)
        self.protected = tuple(
            self.config.get("text_cleaning", {}).get(
                "protected_substrings",
                ["http://", "https://", "ftp://", "10.", "192.168."],
            )
        )

        self.noise_patterns = self._compile_patterns()
        self.deduplicate_lines = self.config.get("text_cleaning", {}).get(
            "deduplicate_lines", True
        )
        self.max_duplicate_threshold = self.config.get("text_cleaning", {}).get(
            "max_duplicate_threshold", 2
        )
        self.min_repeat_for_noise = self.config.get("text_cleaning", {}).get(
            "min_repeat_for_noise", 3
        )

        # í˜ì´ì§€ í—¤ë”/í‘¸í„° ìŠ¤ìº” ë²”ìœ„ (ìƒí•˜ë‹¨ Ní–‰)
        self.page_scan_lines = self.config.get("text_cleaning", {}).get(
            "page_scan_lines", 4
        )

        logger.info(
            f"ğŸ§¹ í…ìŠ¤íŠ¸ í´ë¦¬ë„ˆ v2.0 ì´ˆê¸°í™”: {len(self.noise_patterns)}ê°œ íŒ¨í„´, "
            f"{len(self.protected)}ê°œ ë³´í˜¸ ë¬¸ìì—´"
        )

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ

        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ

        Returns:
            ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        try:
            config_file = Path(config_path)
            if not config_file.exists():
                logger.warning(f"âš ï¸ ì„¤ì • íŒŒì¼ ì—†ìŒ: {config_path}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                return {}

            with open(config_file, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f) or {}

            # í•˜ìœ„ í˜¸í™˜: v0 ìŠ¤í‚¤ë§ˆë¥¼ v1ë¡œ ì •ê·œí™”
            from app.config.compat import normalize_config
            config = normalize_config(config)

            logger.info(f"âœ“ ì„¤ì • ë¡œë“œ: {config_path} (schema v{config.get('schema_version', 0)})")
            return config

        except Exception as e:
            logger.error(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}

    def _compile_patterns(self) -> List[Tuple[re.Pattern, str]]:
        """ë…¸ì´ì¦ˆ íŒ¨í„´ ì»´íŒŒì¼ (í”Œë˜ê·¸ ì§€ì›)

        Returns:
            (ì»´íŒŒì¼ëœ íŒ¨í„´, ì„¤ëª…) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        patterns = []
        noise_config = self.config.get("text_cleaning", {}).get("noise_patterns", [])

        for pattern_config in noise_config:
            pattern_str = pattern_config.get("pattern")
            description = pattern_config.get("description", "unknown")

            try:
                # í”Œë˜ê·¸ ì£¼ì… (ì„¤ì •ì—ì„œ ì§€ì • ê°€ëŠ¥)
                flags = 0
                if pattern_config.get("multiline", False):
                    flags |= re.MULTILINE
                if pattern_config.get("ignore_case", False):
                    flags |= re.IGNORECASE

                compiled = re.compile(pattern_str, flags)
                patterns.append((compiled, description))
                logger.debug(
                    f"âœ“ íŒ¨í„´ ì»´íŒŒì¼: {description} - {pattern_str} (flags={flags})"
                )
            except re.error as e:
                logger.error(f"âŒ íŒ¨í„´ ì»´íŒŒì¼ ì‹¤íŒ¨: {pattern_str} - {e}")

        return patterns

    def _split_pages(self, text: str) -> List[List[str]]:
        """í˜ì´ì§€ ë¶„í•  (í¼í”¼ë“œ ë˜ëŠ” 'Page/ìª½' íŒ¨í„´ ê¸°ì¤€)

        Args:
            text: ì „ì²´ í…ìŠ¤íŠ¸

        Returns:
            í˜ì´ì§€ë³„ ë¼ì¸ ë¦¬ìŠ¤íŠ¸
        """
        # í¼í”¼ë“œ ë˜ëŠ” Page/ìª½ íŒíŠ¸ë¡œ ë¶„í• 
        pages = re.split(
            r"\f|(?:^|\n)page\s+\d+(?:\s+of\s+\d+)?\s*(?:\n|$)|(?:^|\n)\d+\s*ìª½\s*(?:\n|$)",
            text,
            flags=re.IGNORECASE,
        )
        return [p.split("\n") for p in pages if p.strip()]

    def _normalize_line(self, s: str) -> str:
        """ë¼ì¸ ì •ê·œí™” (ìœ ì‚¬ë„ íŒì •ìš© ì§€ë¬¸í™”)

        Args:
            s: ì›ë³¸ ë¼ì¸

        Returns:
            ì •ê·œí™”ëœ ë¼ì¸
        """
        s = s.lower()
        # ìœ ì‚¬ ë¬¸ì¥ë¶€í˜¸ í†µí•©
        s = re.sub(r"[Â·âˆ™âˆ™â€¢â€¢]+", "â€¢", s)
        # ë‹¤ì¤‘ ê³µë°±/í•˜ì´í”ˆ ì •ê·œí™”
        s = re.sub(r"[\s\-]+", " ", s).strip()
        return s

    def clean(self, text: str) -> Tuple[str, Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±° v2.0

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸

        Returns:
            (ì •ë¦¬ëœ í…ìŠ¤íŠ¸, ë…¸ì´ì¦ˆ ë©”íƒ€ ë”•ì…”ë„ˆë¦¬)
            - ë…¸ì´ì¦ˆ ë©”íƒ€ í‚¤:
              - pattern: Dict[str, int] (íŒ¨í„´ë³„ ì¹´ìš´íŠ¸)
              - repeated_headers_footers: int
              - deduplicated_lines: int
              - removed_samples: Dict[str, List[str]] (ì œê±° ìƒ˜í”Œ)
        """
        if not text:
            return "", {}

        lines = text.split("\n")
        noise_counts: Dict[str, Any] = {}
        removed_samples: Dict[str, List[str]] = {}

        # 1. íŒ¨í„´ ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±° (ë³´í˜¸ ë¬¸ìì—´ ê°€ë“œ í¬í•¨)
        lines, pattern_counts, pattern_samples = self._remove_pattern_noise(lines)
        noise_counts.update(pattern_counts)
        removed_samples["pattern"] = pattern_samples

        # 2. ë¹ˆë„ ê¸°ë°˜ ë°˜ë³µ í—¤ë”/í‘¸í„° ì œê±° (í˜ì´ì§€ ë‹¨ìœ„)
        lines, repeat_count, repeat_samples = self._remove_repeated_lines(lines)
        noise_counts["repeated_headers_footers"] = repeat_count
        removed_samples["repeated"] = repeat_samples

        # 3. ì¤‘ë³µ ë¼ì¸ ì œê±° (ì„ íƒì )
        if self.deduplicate_lines:
            lines, dedup_count = self._deduplicate_consecutive_lines(lines)
            noise_counts["deduplicated_lines"] = dedup_count

        # 4. ë¹ˆ ë¼ì¸ ì •ë¦¬ (ë©±ë“±ì„± ë³´ì¥)
        lines = self._normalize_blank_lines(lines)

        cleaned_text = "\n".join(lines)

        # ë…¸ì´ì¦ˆ ë©”íƒ€ì— ì œê±° ìƒ˜í”Œ í¬í•¨
        total = sum(
            v
            for k, v in noise_counts.items()
            if k != "removed_samples" and isinstance(v, int)
        )
        noise_counts["removed_samples"] = removed_samples

        logger.debug(f"ğŸ§¹ ì •ë¦¬ ì™„ë£Œ: {total}ê°œ ë…¸ì´ì¦ˆ ì œê±°")

        return cleaned_text, noise_counts

    def _remove_pattern_noise(
        self, lines: List[str]
    ) -> Tuple[List[str], Dict[str, int], List[str]]:
        """íŒ¨í„´ ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±° (ë³´í˜¸ ë¬¸ìì—´ ê°€ë“œ í¬í•¨)

        Args:
            lines: ì›ë³¸ ë¼ì¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            (ì •ë¦¬ëœ ë¼ì¸ ë¦¬ìŠ¤íŠ¸, íŒ¨í„´ë³„ ì¹´ìš´íŠ¸, ì œê±° ìƒ˜í”Œ)
        """
        counts = {desc: 0 for _, desc in self.noise_patterns}
        cleaned_lines = []
        samples: List[str] = []

        for line in lines:
            is_noise = False

            # íŒ¨í„´ ë§¤ì¹­ ìš°ì„  í™•ì¸
            for pattern, description in self.noise_patterns:
                if pattern.search(line):
                    counts[description] += 1
                    is_noise = True
                    # ìƒ˜í”Œ ìˆ˜ì§‘ (ìµœëŒ€ 3ê°œ)
                    if len(samples) < 3:
                        samples.append(f"{description}: {line[:50]}")
                    break

            # ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë³´í˜¸ ë¬¸ìì—´ ì²´í¬ ì ìš©
            # (íŒ¨í„´ì´ ëª…ì‹œì ìœ¼ë¡œ ì œê±°í•˜ë ¤ëŠ” ê²½ìš° ë³´í˜¸ ë¬¸ìì—´ë³´ë‹¤ ìš°ì„ )
            if not is_noise:
                cleaned_lines.append(line)

        return cleaned_lines, counts, samples

    def _remove_repeated_lines(
        self, lines: List[str]
    ) -> Tuple[List[str], int, List[str]]:
        """ë¹ˆë„ ê¸°ë°˜ ë°˜ë³µ í—¤ë”/í‘¸í„° ì œê±° (í˜ì´ì§€ ë‹¨ìœ„ ìŠ¤ìº”)

        3íšŒ ì´ìƒ ë°˜ë³µë˜ëŠ” ë¼ì¸ì€ ë…¸ì´ì¦ˆë¡œ ê°„ì£¼í•˜ê³  ì œê±°

        Args:
            lines: ì›ë³¸ ë¼ì¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            (ì •ë¦¬ëœ ë¼ì¸ ë¦¬ìŠ¤íŠ¸, ì œê±°ëœ ë¼ì¸ ìˆ˜, ì œê±° ìƒ˜í”Œ)
        """
        text = "\n".join(lines)
        pages = self._split_pages(text) or [lines]

        # í˜ì´ì§€ë³„ ìƒÂ·í•˜ Ní–‰ë§Œ í›„ë³´ë¡œ ìˆ˜ì§‘
        cand = []
        for plines in pages:
            head = [line.strip() for line in plines[: self.page_scan_lines] if len(line.strip()) > 5]
            tail = [
                line.strip() for line in plines[-self.page_scan_lines :] if len(line.strip()) > 5
            ]
            cand.extend(head + tail)

        # ì •ê·œí™” + ë¹ˆë„ ê³„ì‚°
        cnt = Counter(self._normalize_line(c) for c in cand)
        repeated = {k for k, v in cnt.items() if v >= self.min_repeat_for_noise}

        if not repeated:
            return lines, 0, []

        logger.debug(f"ğŸ” ë°˜ë³µ ë¼ì¸ {len(repeated)}ê°œ ë°œê²¬: {list(repeated)[:3]}...")

        # ë°˜ë³µ ë¼ì¸ ì œê±°
        cleaned_lines = []
        removed_count = 0
        samples: List[str] = []

        for line in lines:
            if self._normalize_line(line) in repeated:
                removed_count += 1
                if len(samples) < 3:
                    samples.append(line[:50])
            else:
                cleaned_lines.append(line)

        return cleaned_lines, removed_count, samples

    def _deduplicate_consecutive_lines(
        self, lines: List[str]
    ) -> Tuple[List[str], int]:
        """ì—°ì†ëœ ì¤‘ë³µ ë¼ì¸ ì œê±°

        ë™ì¼í•œ ë¼ì¸ì´ ì—°ì†í•´ì„œ ë‚˜íƒ€ë‚˜ë©´ max_duplicate_thresholdê°œê¹Œì§€ë§Œ ìœ ì§€

        Args:
            lines: ì›ë³¸ ë¼ì¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            (ì •ë¦¬ëœ ë¼ì¸ ë¦¬ìŠ¤íŠ¸, ì œê±°ëœ ë¼ì¸ ìˆ˜)
        """
        if not lines:
            return lines, 0

        cleaned_lines = []
        removed_count = 0
        current_line = None
        consecutive_count = 0

        for line in lines:
            if line == current_line:
                consecutive_count += 1
                if consecutive_count <= self.max_duplicate_threshold:
                    cleaned_lines.append(line)
                else:
                    removed_count += 1
            else:
                current_line = line
                consecutive_count = 1
                cleaned_lines.append(line)

        return cleaned_lines, removed_count

    def _normalize_blank_lines(self, lines: List[str]) -> List[str]:
        """ì—°ì†ëœ ë¹ˆ ë¼ì¸ì„ í•˜ë‚˜ë¡œ ì •ë¦¬ (ë©±ë“±ì„± ë³´ì¥)

        Args:
            lines: ì›ë³¸ ë¼ì¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì •ë¦¬ëœ ë¼ì¸ ë¦¬ìŠ¤íŠ¸
        """
        cleaned_lines = []
        prev_blank = False

        for line in lines:
            is_blank = not line.strip()

            if is_blank:
                if not prev_blank:
                    cleaned_lines.append("")
                prev_blank = True
            else:
                cleaned_lines.append(line)
                prev_blank = False

        # ì•ë’¤ ë¹ˆ ë¼ì¸ ì œê±°
        while cleaned_lines and not cleaned_lines[0].strip():
            cleaned_lines.pop(0)
        while cleaned_lines and not cleaned_lines[-1].strip():
            cleaned_lines.pop()

        return cleaned_lines

    def get_stats(self, noise_counts: Dict[str, Any]) -> str:
        """ë…¸ì´ì¦ˆ ì œê±° í†µê³„ ë¬¸ìì—´ ìƒì„±

        Args:
            noise_counts: ë…¸ì´ì¦ˆ ì¹´ìš´íŠ¸ ë”•ì…”ë„ˆë¦¬

        Returns:
            í†µê³„ ë¬¸ìì—´
        """
        # removed_samples ì œì™¸í•˜ê³  ìˆ«ìë§Œ í•©ì‚°
        total = sum(
            v
            for k, v in noise_counts.items()
            if k != "removed_samples" and isinstance(v, int)
        )

        if total == 0:
            return "ë…¸ì´ì¦ˆ ì—†ìŒ"

        stats = f"ì´ {total}ê°œ ë…¸ì´ì¦ˆ ì œê±°: "
        parts = [
            f"{desc}={count}"
            for desc, count in noise_counts.items()
            if isinstance(count, int) and count > 0 and desc != "removed_samples"
        ]
        stats += ", ".join(parts)

        return stats
