"""
Query Expansion using LLM
ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œì™€ ë™ì˜ì–´/ê´€ë ¨ì–´ë¥¼ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ í™•ì¥
"""

import json
import yaml
from pathlib import Path
from typing import List, Dict, Any, Set
from app.core.logging import get_logger
from rag_system.llm_singleton import LLMSingleton

logger = get_logger(__name__)

# ì„¤ì • ë¡œë“œ
CONFIG_PATH = Path(__file__).parent.parent.parent / "config" / "filters.yaml"


class QueryExpander:
    """LLM ê¸°ë°˜ ì¿¼ë¦¬ í™•ì¥"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.llm = LLMSingleton.get_instance()
        self.cache = {}  # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ
        self.search_stopwords = self._load_search_stopwords()
        logger.info(f"âœ… QueryExpander ì´ˆê¸°í™”: {len(self.search_stopwords)}ê°œ ê²€ìƒ‰ ë¶ˆìš©ì–´")

    def _load_search_stopwords(self) -> Set[str]:
        """ê²€ìƒ‰ ë¶ˆìš©ì–´ ë¡œë“œ"""
        try:
            with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            stopwords = set(config.get('search_stopwords', []))
            logger.info(f"ğŸ“‹ ê²€ìƒ‰ ë¶ˆìš©ì–´ {len(stopwords)}ê°œ ë¡œë“œë¨")
            return stopwords
        except Exception as e:
            logger.warning(f"âš ï¸ ê²€ìƒ‰ ë¶ˆìš©ì–´ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            return {'ë°', 'ê³¼', 'í•´ì¤˜', 'ì¢€', 'ë¬¸ì„œ', 'ë‚´ìš©', 'ìš”ì•½', 'ê±´'}

    def expand_query(self, query: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° í™•ì¥

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            {
                "original_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
                "expanded_keywords": ["ë™ì˜ì–´1", "ë™ì˜ì–´2", "ê´€ë ¨ì–´1"],
                "search_query": "í™•ì¥ëœ FTS ì¿¼ë¦¬"
            }
        """
        # ìºì‹œ í™•ì¸ (ë™ì¼ ì§ˆë¬¸ ë°˜ë³µ ë°©ì§€)
        if query in self.cache:
            logger.info(f"ğŸ’¾ Cache hit for query: '{query[:30]}...'")
            return self.cache[query]

        # LLM í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ì— í•„ìš”í•œ í‚¤ì›Œë“œì™€ ë™ì˜ì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´):
{{
  "keywords": ["í•µì‹¬í‚¤ì›Œë“œ1", "í•µì‹¬í‚¤ì›Œë“œ2"],
  "synonyms": {{
    "í•µì‹¬í‚¤ì›Œë“œ1": ["ë™ì˜ì–´1", "ë™ì˜ì–´2"],
    "í•µì‹¬í‚¤ì›Œë“œ2": ["ë™ì˜ì–´1", "ë™ì˜ì–´2"]
  }}
}}

ì˜ˆì‹œ:
ì§ˆë¬¸: "ì¹´ë©”ë¼ ì‚¼ê°ëŒ€ ë‹¤ë¦¬ ë¶€ë¶„ êµì²´"
{{
  "keywords": ["ì¹´ë©”ë¼", "ì‚¼ê°ëŒ€", "ë‹¤ë¦¬", "êµì²´"],
  "synonyms": {{
    "ì‚¼ê°ëŒ€": ["íŠ¸ë¼ì´í¬íŠ¸", "íŠ¸ë¼ì´í¬ë“œ", "tripod"],
    "ë‹¤ë¦¬": ["ë°œíŒ", "ì§€ì§€ëŒ€", "leg"],
    "êµì²´": ["ìˆ˜ë¦¬", "êµì²´", "ê³ ì¥", "ë³€ê²½"]
  }}
}}

ì§ˆë¬¸: "ë Œì¦ˆ ê³ ì¥ë‚¬ì–´"
{{
  "keywords": ["ë Œì¦ˆ", "ê³ ì¥"],
  "synonyms": {{
    "ê³ ì¥": ["ìˆ˜ë¦¬", "ë¶ˆëŸ‰", "íŒŒì†", "ì˜¤ë¥˜"],
    "ë Œì¦ˆ": ["lens", "ë Œì¦ˆ"]
  }}
}}

ì´ì œ ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:"""

        try:
            # LLM í˜¸ì¶œ
            # QwenLLM ì‹œê·¸ë‹ˆì²˜: generate_response(question, context_chunks, max_retries=2, enable_complex_processing=True, mode="rag")
            response = self.llm.generate_response(
                question=prompt,
                context_chunks=[],
                enable_complex_processing=False,  # ë‹¨ìˆœ í‚¤ì›Œë“œ ì¶”ì¶œì´ë¯€ë¡œ ë³µí•©ì²˜ë¦¬ ë¶ˆí•„ìš”
                mode="rag"
            )

            # JSON íŒŒì‹±
            # RAGResponse ê°ì²´ì—ì„œ answer ì¶”ì¶œ
            if hasattr(response, 'answer'):
                response_text = response.answer.strip()
            else:
                response_text = str(response).strip()

            # LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json íƒœê·¸ ì œê±°)
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()

            result = json.loads(response_text)

            keywords = result.get("keywords", [])
            synonyms_dict = result.get("synonyms", {})

            # ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘ (ì›ë³¸ + ë™ì˜ì–´)
            all_keywords = set(keywords)
            for syn_list in synonyms_dict.values():
                all_keywords.update(syn_list)

            # ë¶ˆìš©ì–´ ì œê±° (ê¸¸ì´ 1ì¸ í‚¤ì›Œë“œë„ ì œê±° - ì¡°ì‚¬/ì ‘ì†ì‚¬)
            filtered_keywords = {
                kw for kw in all_keywords
                if kw.lower() not in self.search_stopwords and len(kw) > 1
            }

            if len(filtered_keywords) < len(all_keywords):
                removed = all_keywords - filtered_keywords
                logger.info(f"ğŸ§¹ ë¶ˆìš©ì–´ ì œê±°: {removed}")

            # ìµœì†Œ 1ê°œ í‚¤ì›Œë“œëŠ” ìœ ì§€
            if not filtered_keywords:
                filtered_keywords = all_keywords
                logger.warning(f"âš ï¸ ëª¨ë“  í‚¤ì›Œë“œê°€ ë¶ˆìš©ì–´ - í•„í„°ë§ ê±´ë„ˆëœ€")

            # FTS ì¿¼ë¦¬ ìƒì„± (ORë¡œ ì—°ê²°, ê° í‚¤ì›Œë“œë¥¼ ë”°ì˜´í‘œë¡œ ê°ì‹¸ì„œ ë¦¬í„°ëŸ´ ì²˜ë¦¬)
            # ë‚ ì§œ íŒ¨í„´(2025-06-10 ë“±)ì˜ í•˜ì´í”ˆì´ SQL ì—°ì‚°ìë¡œ ì¸ì‹ë˜ì§€ ì•Šë„ë¡ ë°©ì§€
            quoted_keywords = [f'"{kw}"' for kw in filtered_keywords]
            search_query = " OR ".join(quoted_keywords)

            logger.info(f"âœ… Query expansion: {query[:30]}... â†’ {len(all_keywords)}ê°œ í‚¤ì›Œë“œ")

            result = {
                "original_keywords": keywords,
                "expanded_keywords": list(all_keywords),
                "search_query": search_query,
                "synonyms": synonyms_dict
            }

            # ìºì‹œì— ì €ì¥
            self.cache[query] = result

            return result

        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
            logger.debug(f"LLM response: {response[:500]}")
            # Fallback: ì›ë³¸ ì¿¼ë¦¬ ë‹¨ì–´ ë¶„ë¦¬ + ë¶ˆìš©ì–´ ì œê±°
            words = query.replace("?", "").replace(".", "").split()
            filtered_words = [w for w in words if w.lower() not in self.search_stopwords and len(w) > 1]
            if not filtered_words:
                filtered_words = words  # ëª¨ë‘ ë¶ˆìš©ì–´ë©´ ì›ë³¸ ìœ ì§€
            quoted_words = [f'"{w}"' for w in filtered_words]
            return {
                "original_keywords": words,
                "expanded_keywords": filtered_words,
                "search_query": " OR ".join(quoted_words),
                "synonyms": {},
                "fallback": True  # Fallback ì‚¬ìš© í‘œì‹œ
            }

        except Exception as e:
            logger.warning(f"âš ï¸ Query expansion ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
            # Fallback + ë¶ˆìš©ì–´ ì œê±°
            words = query.replace("?", "").replace(".", "").split()
            filtered_words = [w for w in words if w.lower() not in self.search_stopwords and len(w) > 1]
            if not filtered_words:
                filtered_words = words
            quoted_words = [f'"{w}"' for w in filtered_words]
            return {
                "original_keywords": words,
                "expanded_keywords": filtered_words,
                "search_query": " OR ".join(quoted_words),
                "synonyms": {},
                "fallback": True
            }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_expander = None


def get_query_expander() -> QueryExpander:
    """ì‹±ê¸€í†¤ QueryExpander ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _expander
    if _expander is None:
        _expander = QueryExpander()
    return _expander
