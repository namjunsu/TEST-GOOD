#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Query Parsing with Closed-World Validation (Hardened)
- Drafter: Closed-world exact â†’ normalized-exact â†’ fuzzy (guarded)
- Year: single/range/two-digit/relative terms with bounds
- Robust normalization (NFKC, role/honorific stripping)
- Token-pattern first, then CW pipeline with clear provenance
"""

from __future__ import annotations

import logging
import re
import unicodedata
from dataclasses import dataclass
from datetime import date
from difflib import SequenceMatcher
from pathlib import Path
from typing import Dict, Optional, Set, Tuple

import yaml

logger = logging.getLogger(__name__)

CONFIG_PATH = Path(__file__).parent.parent.parent / "config" / "filters.yaml"

# -------------------------
# ì •ê·œì‹(ì‚¬ì „ ì»´íŒŒì¼)
# -------------------------
RE_YEAR_4 = re.compile(r"(?<!\d)(19|20)\d{2}(?= *ë…„| *ë…„ë„|\b|[^0-9])")
RE_YEAR_2 = re.compile(r"(?<!\d)(\d{2})(?= *ë…„| *ë…„ë„|\b)")
RE_YEAR_RANGE = re.compile(
    r"(?<!\d)((?:19|20)\d{2}|\d{2})\s*(?:~|~?ë¶€í„°|[-â€“â€”]|~?â†’|~?to|~?>)\s*((?:19|20)\d{2}|\d{2})(?= *ë…„| *ë…„ë„|\b)?"
)
RE_RELATIVE = re.compile(r"(ì˜¬í•´|ì‘ë…„|ì¬ì‘ë…„)")
RE_KOREAN_NAME_2_4 = re.compile(r"[ê°€-í£]{2,4}")
RE_ROLE_SUFFIX = re.compile(r"(ê³¼ì¥|ì°¨ì¥|ë¶€ì¥|íŒ€ì¥|ì‹¤ì¥|êµ­ì¥|ë³¸ë¶€ì¥|ë§¤ë‹ˆì €|ëŒ€ë¦¬|ì‚¬ì›|ë‹´ë‹¹|ì„ ì„|ì±…ì„)\s*")
RE_PAREN = re.compile(r"[\(\)\[\]{}]")
RE_MULTI_SP = re.compile(r"\s+")
RE_TOKEN_DIRECTIVES = {
    "year": re.compile(r"year\s*[:=]\s*(\d{2,4})", re.IGNORECASE),
    "drafter": re.compile(r"drafter\s*[:=]\s*([A-Za-zê°€-í£ ]{2,30})", re.IGNORECASE),
}


# -------------------------
# ì„¤ì • ë¡œë“œ ìœ í‹¸
# -------------------------
def _load_yaml_config() -> dict:
    try:
        if CONFIG_PATH.exists():
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f) or {}
            return cfg
    except Exception as e:
        logger.warning(f"filters.yaml ë¡œë“œ ì‹¤íŒ¨: {e}")
    return {}


def _normalize_text(s: str) -> str:
    return unicodedata.normalize("NFKC", s or "").strip()


def _normalize_name(s: str) -> str:
    # ê´„í˜¸ ì œê±°, ê³µë°± ì •ë¦¬, ì§í•¨ ì œê±°, ì˜ë¬¸ ì†Œë¬¸ìí™”
    s = _normalize_text(RE_PAREN.sub("", s))
    s = s.replace(" ", "")
    s = RE_ROLE_SUFFIX.sub("", s)
    return s.lower()


def _jamo_approx(s: str) -> str:
    # ì´ˆê°„ë‹¨ ìëª¨ ê·¼ì‚¬: NFKD í›„ ìëª¨ë§Œ ë³´ì¡´
    decomp = unicodedata.normalize("NFKD", s)
    return "".join(ch for ch in decomp if 0x1100 <= ord(ch) <= 0x11FF)


def _sequence_ratio(a: str, b: str) -> float:
    return SequenceMatcher(None, a, b).ratio()


def _two_digit_to_four(yy: int, today: date) -> int:
    # '24 â†’ 2024 í•´ì„: í˜„ì¬ ì„¸ê¸° ê¸°ì¤€, +/- 20ë…„ ìœˆë„ìš°
    century = today.year // 100 * 100
    cand = century + yy
    if cand > today.year + 20:
        cand -= 100
    elif cand < 1900:
        cand += 100
    return cand


# -------------------------
# ë°ì´í„° êµ¬ì¡°
# -------------------------
@dataclass(frozen=True)
class ParseResult:
    year: Optional[str]
    drafter: Optional[str]
    source: Optional[str]  # 'token'|'closed_world'|'fuzzy'|None


# -------------------------
# ë³¸ì²´
# -------------------------
class QueryParser:
    """ì¿¼ë¦¬ íŒŒì„œ - Closed-World Validation + ì—°ë„ íŒŒì„œ ê³ ë„í™”"""

    def __init__(self, known_drafters: Set[str], today: Optional[date] = None):
        """
        Args:
            known_drafters: ë©”íƒ€DBì—ì„œ ë¡œë“œí•œ 'ê¸°ì•ˆì ì›í˜•' ì§‘í•© (ì˜ˆ: {'ë‚¨ì¤€ìˆ˜','ìµœìƒˆë¦„',...})
            today: ìƒëŒ€ ì—°ë„ í•´ì„ ê¸°ì¤€ì¼ (í…ŒìŠ¤íŠ¸ ìš©ì´ì„±)
        """
        self.cfg = _load_yaml_config()
        self.stopwords = self._load_stopwords()
        self.token_patterns = self._load_token_patterns()
        self.today = today or date.today()

        # Closed-World ì¸ë±ìŠ¤
        original = {d for d in (known_drafters or set()) if d and isinstance(d, str)}
        norm_pairs = {d: _normalize_name(d) for d in original}
        # ì—­ì¸ë±ìŠ¤(ì •ê·œí™”ê°’ â†’ ì›í˜• ë‹¤ì¤‘ ë§¤í•‘ ë°©ì§€: ê°€ì¥ ê¸´ ì›í˜• ìš°ì„ )
        rev: Dict[str, str] = {}
        for raw, norm in norm_pairs.items():
            keep = rev.get(norm)
            if keep is None or len(raw) > len(keep):
                rev[norm] = raw

        self._known_original: Set[str] = original
        self._known_norm_set: Set[str] = set(rev.keys())
        self._norm_to_original: Dict[str, str] = rev

        logger.info(
            f"âœ… QueryParser ì´ˆê¸°í™”: ê¸°ì•ˆì {len(self._known_original)}ëª… "
            f"(ì •ê·œí™” {len(self._known_norm_set)}), ë¶ˆìš©ì–´ {len(self.stopwords)}ê°œ"
        )

    # ---------- ì„¤ì • ----------
    def _load_stopwords(self) -> Set[str]:
        sw = set(map(_normalize_text, self.cfg.get("drafter_stopwords", []) or []))
        if not sw:
            sw = {"ë¬¸ì„œ", "ìë£Œ", "íŒŒì¼", "ë³´ê³ ì„œ", "ì „ì²´", "ëª¨ë“ "}
        return sw

    def _load_token_patterns(self) -> Dict[str, str]:
        return self.cfg.get("query_tokens", {}) or {}

    # ---------- Public API ----------
    def parse_filters(self, query: str) -> Dict[str, Optional[str]]:
        """
        Returns: {"year": str|None, "drafter": str|None, "source": str|None}
        ìš°ì„ ìˆœìœ„: í† í° ê·œì¹™ â†’ (ì—°ë„) â†’ CW/í¼ì§€ ê¸°ì•ˆì
        """
        q = _normalize_text(query)

        # 1) í† í° ê·œì¹™ ìš°ì„ 
        token_res = self._extract_from_tokens(q)
        if token_res["year"] or token_res["drafter"]:
            token_res["source"] = "token"
            logger.info(f"ğŸ¯ í† í° íŒŒì‹±: year={token_res['year']}, drafter={token_res['drafter']}")
            return token_res

        # 2) ì—°ë„
        year = self._extract_year(q)

        # 3) ê¸°ì•ˆì (CW â†’ ì •ê·œí™”-CW â†’ í¼ì§€)
        drafter, source = self._extract_drafter_closed_world(q)

        result = {"year": year, "drafter": drafter, "source": source}
        logger.info(f"ğŸ“‹ íŒŒì‹± ê²°ê³¼: year={year}, drafter={drafter}, source={source}")
        return result

    # ---------- Token rules ----------
    def _extract_from_tokens(self, q: str) -> Dict[str, Optional[str]]:
        res = {"year": None, "drafter": None}

        # year
        pat_y = self.token_patterns.get("year")
        if pat_y:
            m = re.search(pat_y, q, re.IGNORECASE)
        else:
            m = RE_TOKEN_DIRECTIVES["year"].search(q)
        if m:
            y = m.group(1)
            if len(y) == 2:
                y4 = _two_digit_to_four(int(y), self.today)
                res["year"] = str(y4)
            elif len(y) == 4 and 1900 <= int(y) <= 2100:
                res["year"] = y

        # drafter
        pat_d = self.token_patterns.get("drafter")
        if pat_d:
            m = re.search(pat_d, q, re.IGNORECASE)
        else:
            m = RE_TOKEN_DIRECTIVES["drafter"].search(q)
        if m:
            cand_raw = m.group(1).strip()
            cand_norm = _normalize_name(cand_raw)
            if cand_norm in self._known_norm_set:
                res["drafter"] = self._norm_to_original[cand_norm]
        return res

    # ---------- Year ----------
    def _extract_year(self, q: str) -> Optional[str]:
        # ìƒëŒ€ í‘œí˜„
        rel = RE_RELATIVE.search(q)
        if rel:
            term = rel.group(1)
            if term == "ì˜¬í•´":
                return str(self.today.year)
            if term == "ì‘ë…„":
                return str(self.today.year - 1)
            if term == "ì¬ì‘ë…„":
                return str(self.today.year - 2)

        # ë²”ìœ„(ìš°ì¸¡ ìš°ì„ ): "2023~25", "'24-'25", "2024-2025"
        m = RE_YEAR_RANGE.search(q)
        if m:
            a, b = m.group(1), m.group(2)
            y1 = int(a) if len(a) == 4 else _two_digit_to_four(int(a), self.today)
            y2 = int(b) if len(b) == 4 else _two_digit_to_four(int(b), self.today)
            y1, y2 = min(y1, y2), max(y1, y2)
            if 1900 <= y1 <= 2100 and 1900 <= y2 <= 2100:
                return f"{y1}-{y2}"

        # ë‹¨ì¼ 4ìë¦¬
        m = RE_YEAR_4.search(q)
        if m:
            y = int(m.group(0))
            if 1900 <= y <= 2100:
                return str(y)

        # ë‹¨ì¼ 2ìë¦¬
        m = RE_YEAR_2.search(q)
        if m:
            y4 = _two_digit_to_four(int(m.group(1)), self.today)
            if 1900 <= y4 <= 2100:
                return str(y4)

        return None

    # ---------- Drafter (CW â†’ fuzzy) ----------
    def _extract_drafter_closed_world(self, q: str) -> Tuple[Optional[str], Optional[str]]:
        # í›„ë³´ ì¶”ì¶œ: í•œê¸€ 2-4ì + ê³µë°± í¬í•¨ëœ íŒ¨í„´ë„ ì¶”ì¶œ
        candidates_ko = RE_KOREAN_NAME_2_4.findall(q)

        # ê³µë°± í¬í•¨ í•œê¸€ íŒ¨í„´ ì¶”ê°€ (ì˜ˆ: "ìµœ ìƒˆ ë¦„", "ë‚¨ ì¤€ìˆ˜")
        # íŒ¨í„´: 1-2ì ìŒì ˆì´ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ 2-4ê°œ ìˆëŠ” ê²½ìš°
        spaced_pattern = re.compile(r'[ê°€-í£]{1,2}(?: +[ê°€-í£]{1,2}){1,3}')
        candidates_spaced = spaced_pattern.findall(q)

        # ê³µë°± í¬í•¨ í›„ë³´ì—ì„œ ë¶ˆìš©ì–´ ì œê±° (ë‹¨ì–´ ë‹¨ìœ„)
        def remove_stopwords(s: str) -> str:
            words = s.split()
            filtered = [w for w in words if _normalize_text(w) not in self.stopwords]
            return " ".join(filtered) if filtered else ""

        candidates_spaced = [remove_stopwords(c) for c in candidates_spaced]
        # ë¹ˆ ë¬¸ìì—´ ì œê±° + ê³µë°± ì œê±° í›„ ê¸¸ì´ê°€ 2-4ìì¸ ê²ƒë§Œ ìœ ì§€
        candidates_spaced = [c for c in candidates_spaced if c and 2 <= len(c.replace(" ", "")) <= 4]

        # ëª¨ë“  í›„ë³´ í•©ì¹˜ê¸°
        all_candidates = candidates_ko + candidates_spaced

        # ì§í•¨ ì œê±°/ì •ê·œí™”
        cand_norm = [_normalize_name(c) for c in all_candidates if _normalize_text(c) not in self.stopwords]
        # ë¹ˆ ê°’ ì œê±° ë° ê¸¸ì´ ì²´í¬ (í•œêµ­ ì´ë¦„ì€ ë³´í†µ 2-4ì)
        cand_norm = [c for c in cand_norm if c and 2 <= len(c) <= 4]

        if not cand_norm:
            return None, None

        # 1) ì •ê·œí™”-Closed World
        for n in cand_norm:
            if n in self._known_norm_set:
                return self._norm_to_original[n], "closed_world"

        # 2) í¼ì§€ ë§¤ì¹­(ì•ˆì „ì¥ì¹˜ í¬í•¨)
        best: Tuple[Optional[str], float] = (None, 0.0)
        # í›„ë³´Â·ëŒ€ìƒ ìƒí•œ
        max_checks = 50
        checked = 0
        for n in cand_norm:
            nj = _jamo_approx(n)
            for k in self._known_norm_set:
                if checked >= max_checks:
                    break
                # ê¸¸ì´ ì°¨ ê³¼ë„ ì‹œ skip
                if abs(len(n) - len(k)) > 1:
                    checked += 1
                    continue
                kj = _jamo_approx(k)
                # ìëª¨ ê·¼ì ‘ì„± 1ì°¨ ì»·
                if _sequence_ratio(nj, kj) < 0.80:
                    checked += 1
                    continue
                # ì›ë¬¸ ê·¼ì ‘ì„±(ê°€ì¤‘)
                score = 0.5 * _sequence_ratio(n, k) + 0.5 * _sequence_ratio(nj, kj)
                if score > best[1]:
                    best = (k, score)
                checked += 1

        if best[0] and best[1] >= 0.87:
            return self._norm_to_original[best[0]], "fuzzy"

        return None, None


# ---------- í•¨ìˆ˜í˜• API ----------
def parse_filters_simple(query: str, known_drafters: Set[str]) -> Dict[str, Optional[str]]:
    parser = QueryParser(known_drafters)
    return parser.parse_filters(query)
