"""í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ (BM25 ì¸ë±ìŠ¤ ê¸°ë°˜)

BM25Storeë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰
"""

import os
import re
import yaml
from pathlib import Path
from typing import List, Dict, Any, Optional
from app.core.logging import get_logger
from modules.metadata_db import MetadataDB
from app.rag.query_parser import QueryParser
from rag_system.bm25_store import BM25Store  # ì¸ë±ì„œì™€ ë™ì¼ ëª¨ë“ˆ ì‚¬ìš©
from app.rag.parallel_executor import get_parallel_executor
from app.rag.query_expander import get_query_expander

logger = get_logger(__name__)


class HybridRetriever:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ (BM25 ì¸ë±ìŠ¤ ê¸°ë°˜)

    RAGPipelineì˜ Retriever í”„ë¡œí† ì½œì„ êµ¬í˜„í•˜ë©°,
    ë‚´ë¶€ì ìœ¼ë¡œ BM25Storeë¥¼ ì‚¬ìš©í•´ ì „ì²´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """ì´ˆê¸°í™” - BM25Store ë° MetadataDB ë¡œë“œ"""
        try:
            # ìŠ¤ë‹ˆí« ê¸¸ì´ ì„¤ì • (3600ì = ~1200 í† í°)
            self.snippet_max_length = int(os.getenv("SNIPPET_MAX_LENGTH", "3600"))

            # ê²€ìƒ‰ ë°±ì—”ë“œ ì„¤ì •
            self.use_bm25 = os.getenv("RETRIEVER_BACKEND", "bm25").lower() == "bm25"

            # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
            self.enable_parallel = os.getenv("ENABLE_PARALLEL_SEARCH", "true").lower() == "true"
            if self.enable_parallel:
                self.parallel_executor = get_parallel_executor(max_workers=3)
                logger.info("âœ… Parallel search execution enabled")

            # MetadataDB ì´ˆê¸°í™” (í•„í„°ë§ìš©)
            self.metadata_db = MetadataDB()
            self.known_drafters = self.metadata_db.list_unique_drafters()
            self.parser = QueryParser(self.known_drafters)

            # Query Expander ì´ˆê¸°í™” (Lazy - ì²« ì‚¬ìš© ì‹œ ì´ˆê¸°í™”)
            self.query_expander = None
            logger.info("âœ… Query Expander (Lazy initialization)")

            # BM25Store ì´ˆê¸°í™”
            self.bm25 = None
            if self.use_bm25:
                index_path = os.getenv("BM25_INDEX_PATH", "var/index/bm25_index.pkl")
                logger.info(f"ğŸ” DEBUG: BM25_INDEX_PATH={index_path} (exists={os.path.exists(index_path)})")
                self.bm25 = BM25Store(index_path=index_path)
                logger.info(f"âœ… HybridRetriever ì´ˆê¸°í™” ì™„ë£Œ (BM25 ë°±ì—”ë“œ, {len(self.bm25.documents)}ê°œ ë¬¸ì„œ, path={self.bm25.index_path})")
            else:
                logger.info("âœ… HybridRetriever ì´ˆê¸°í™” ì™„ë£Œ (MetadataDB í´ë°± ëª¨ë“œ)")

            # ì¸ë±ìŠ¤ íŒŒì¼ mtime ì¶”ì  (ìë™ ì¬ë¡œë“œìš©)
            self._last_index_mtime = self._get_index_mtime()

            # DOC_ANCHORED í‚¤ì›Œë“œ ë¡œë“œ (YAML ì™¸ë¶€í™”)
            self._load_router_keywords()

        except Exception as e:
            logger.error(f"âŒ HybridRetriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _load_router_keywords(self):
        """ë¼ìš°í„° í‚¤ì›Œë“œ YAML ë¡œë“œ (ìš´ì˜ ì¤‘ ìˆ˜ì • ê°€ëŠ¥)"""
        try:
            config_path = Path("config/router_keywords.yaml")
            if config_path.exists():
                config = yaml.safe_load(config_path.read_text())
                allow_patterns = config["doc_anchored"]["allow"]
                self.device_pattern = "|".join(allow_patterns)
                logger.info(f"âœ… DOC_ANCHORED í‚¤ì›Œë“œ ë¡œë“œ ì™„ë£Œ ({len(allow_patterns)}ê°œ íŒ¨í„´)")
            else:
                # í´ë°±: í•˜ë“œì½”ë”© íŒ¨í„´ ì‚¬ìš©
                self.device_pattern = (
                    r"\bHRD[-\s]?\d{3,4}\b|DVR|NVR|"
                    r"Hanwha(?:\s+(?:Techwin|Vision))?|"
                    r"ë³´ì¡´ìš©|ë…¹í™”ìš©|êµì²´|ë…¸í›„|ì¥ë¹„|ì¹´ë©”ë¼|ëª¨ë‹ˆí„°"
                )
                logger.warning("âš ï¸ router_keywords.yaml ì—†ìŒ, í´ë°± íŒ¨í„´ ì‚¬ìš©")
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ ë¡œë“œ ì‹¤íŒ¨: {e}, í´ë°± íŒ¨í„´ ì‚¬ìš©")
            self.device_pattern = (
                r"\bHRD[-\s]?\d{3,4}\b|DVR|NVR|"
                r"Hanwha(?:\s+(?:Techwin|Vision))?|"
                r"ë³´ì¡´ìš©|ë…¹í™”ìš©|êµì²´|ë…¸í›„|ì¥ë¹„|ì¹´ë©”ë¼|ëª¨ë‹ˆí„°"
            )

    def _get_index_mtime(self) -> float:
        """ì¸ë±ìŠ¤ íŒŒì¼ì˜ ìˆ˜ì • ì‹œê°„ ë°˜í™˜"""
        if not self.use_bm25:
            return 0.0
        index_path = os.getenv("BM25_INDEX_PATH", "var/index/bm25_index.pkl")
        return os.path.getmtime(index_path) if os.path.exists(index_path) else 0.0

    def _reload_if_index_rotated(self):
        """ì¸ë±ìŠ¤ íŒŒì¼ì´ ê°±ì‹ ë˜ë©´ ìë™ ë¦¬ë¡œë“œ"""
        if not self.use_bm25:
            return

        current_mtime = self._get_index_mtime()
        if current_mtime > self._last_index_mtime:
            logger.info("ğŸ”„ ì¸ë±ìŠ¤ íŒŒì¼ ê°±ì‹  ê°ì§€, ì¬ë¡œë“œ ì¤‘...")
            index_path = os.getenv("BM25_INDEX_PATH", "var/index/bm25_index.pkl")
            self.bm25 = BM25Store(index_path=index_path)
            self._last_index_mtime = current_mtime
            logger.info(f"âœ… ì¸ë±ìŠ¤ ì¬ë¡œë“œ ì™„ë£Œ ({len(self.bm25.documents)}ê°œ ë¬¸ì„œ)")

    def _find_selected_document(self, selected_filename: str) -> Optional[Dict[str, Any]]:
        """
        ì„ íƒëœ ë¬¸ì„œë¥¼ MetadataDBì—ì„œ ì§ì ‘ ê²€ìƒ‰

        Args:
            selected_filename: ê²€ìƒ‰í•  íŒŒì¼ëª…

        Returns:
            ë³€í™˜ëœ ë¬¸ì„œ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        try:
            all_docs = self.metadata_db.search_documents(limit=500)
            for doc in all_docs:
                if doc.get("filename") == selected_filename:
                    # BM25 result í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    return {
                        "doc_id": doc.get("filename", "unknown"),
                        "snippet": doc.get("text_preview") or "",  # ì „ì²´ ë¬¸ì„œ ì‚¬ìš©
                        "score": 99.9,  # ìµœìš°ì„  ìŠ¤ì½”ì–´
                        "page": 1,
                        "filename": doc.get("filename"),
                        "file_path": doc.get("path"),
                        "meta": {
                            "filename": doc.get("filename"),
                            "date": doc.get("date"),
                            "drafter": doc.get("drafter"),
                            "category": doc.get("category"),
                        }
                    }
            logger.warning(f"âš ï¸ ì„ íƒëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {selected_filename}")
            return None
        except Exception as e:
            logger.error(f"âŒ ì„ íƒ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def _search_fts(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """FTS (Full-Text Search) ê²€ìƒ‰ ìˆ˜í–‰ with Query Expansion

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼

        Returns:
            ë³€í™˜ëœ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            conn = self.metadata_db._get_conn()

            # Lazy initialization: Query Expander ì²« ì‚¬ìš© ì‹œ ì´ˆê¸°í™”
            if self.query_expander is None:
                try:
                    self.query_expander = get_query_expander()
                    logger.info("âœ… Query Expander ì´ˆê¸°í™” ì™„ë£Œ (lazy)")
                except Exception as e:
                    logger.warning(f"âš ï¸ Query Expander ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ê²€ìƒ‰ ì‚¬ìš©: {e}")
                    # Expanderë¥¼ Falseë¡œ ì„¤ì • (ë‹¤ìŒ í˜¸ì¶œ ì‹œ ì¬ì‹œë„ ë°©ì§€)
                    self.query_expander = False

            # LLM ê¸°ë°˜ Query Expansion (ì´ˆê¸°í™” ì„±ê³µí•œ ê²½ìš°ë§Œ)
            if self.query_expander and self.query_expander is not False:
                try:
                    expansion_result = self.query_expander.expand_query(query)
                    fts_query = expansion_result["search_query"]
                    logger.info(f"ğŸ” Query expansion: '{query}' â†’ '{fts_query[:100]}...'")
                    logger.info(f"ğŸ“Š í™•ì¥ëœ í‚¤ì›Œë“œ: {len(expansion_result['expanded_keywords'])}ê°œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ Query expansion ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
                    # Fallback: ê¸°ë³¸ í‚¤ì›Œë“œ ë¶„ë¦¬ (ë”°ì˜´í‘œë¡œ ê°ì‹¸ì„œ ë¦¬í„°ëŸ´ ì²˜ë¦¬)
                    fts_query = ' OR '.join(f'"{word}"' for word in query.split())
            else:
                # Fallback: ê¸°ë³¸ í‚¤ì›Œë“œ ë¶„ë¦¬ (ë”°ì˜´í‘œë¡œ ê°ì‹¸ì„œ ë¦¬í„°ëŸ´ ì²˜ë¦¬)
                fts_query = ' OR '.join(f'"{word}"' for word in query.split())

            cursor = conn.execute("""
                SELECT d.filename, d.title, d.drafter, d.date, d.category, d.text_preview
                FROM documents_fts fts
                JOIN documents d ON fts.rowid = d.rowid
                WHERE documents_fts MATCH ?
                ORDER BY rank
                LIMIT ?
            """, (fts_query, top_k * 2))  # ì—¬ìœ ìˆê²Œ ê°€ì ¸ì˜¤ê¸°

            results = []
            for row in cursor.fetchall():
                filename, title, drafter, date, category, text_preview = row

                results.append({
                    "doc_id": filename,
                    "snippet": text_preview or "",
                    "score": 1.0,  # FTSëŠ” rankë§Œ ì œê³µ, ì„ì˜ ìŠ¤ì½”ì–´
                    "page": 1,
                    "filename": filename,
                    "file_path": None,  # FTSì—ì„œëŠ” path ì—†ìŒ
                    "meta": {
                        "filename": filename,
                        "date": date,
                        "drafter": drafter,
                        "category": category,
                    }
                })

            logger.info(f"âœ… FTS ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ (ì›ë³¸='{query}', í™•ì¥={len(expansion_result['expanded_keywords'])}ê°œ)")
            return results[:top_k]

        except Exception as e:
            logger.error(f"âŒ FTS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _calculate_relevance_score(self, query: str, doc: Dict[str, Any]) -> float:
        """ì¿¼ë¦¬ì™€ ë¬¸ì„œ ê°„ relevance ìŠ¤ì½”ì–´ ê³„ì‚° (BM25 ìœ ì‚¬) + íŒŒì¼ëª… ë§¤ì¹­ ê°•í™”

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            doc: ë¬¸ì„œ ë”•ì…”ë„ˆë¦¬ (filename, text_preview í¬í•¨)

        Returns:
            0.0~10.0 ë²”ìœ„ì˜ relevance ìŠ¤ì½”ì–´ (íŒŒì¼ëª… ë§¤ì¹­ ì‹œ ìµœëŒ€ 10ì )
        """
        # ì¿¼ë¦¬ í† í°í™” (ê³µë°± + íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        query_tokens = set(re.findall(r'\w+', query.lower()))
        if not query_tokens:
            return 0.5  # í† í° ì—†ìœ¼ë©´ ì¤‘ë¦½ ìŠ¤ì½”ì–´

        # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¤€ë¹„ (snippetê³¼ metaì—ì„œ ì¶”ì¶œ)
        filename = (doc.get('filename') or '').lower()
        snippet = (doc.get('snippet') or '').lower()  # Fixed: text_preview â†’ snippet
        meta = doc.get('meta', {})
        drafter = (meta.get('drafter') or '').lower()  # Fixed: metaì—ì„œ drafter ì¶”ì¶œ
        doc_text = filename + ' ' + snippet + ' ' + drafter

        # ë§¤ì¹­ëœ í† í° ìˆ˜ ê³„ì‚°
        matched_tokens = sum(1 for token in query_tokens if token in doc_text)

        # ê¸°ë³¸ ìŠ¤ì½”ì–´: ë§¤ì¹­ë¥ 
        match_ratio = matched_tokens / len(query_tokens) if query_tokens else 0.0

        # ë³´ë„ˆìŠ¤ 1: ì™„ì „ ì¼ì¹˜í•˜ëŠ” êµ¬ë¬¸ì´ ìˆìœ¼ë©´ ê°€ì‚°ì 
        if query.lower() in doc_text:
            match_ratio = min(1.0, match_ratio + 0.3)

        # í˜ë„í‹°: ë¬¸ì„œê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì  (ì‹ ë¢°ë„ ì €í•˜)
        text_len = len(doc.get('snippet') or '')  # Fixed: text_preview â†’ snippet
        if text_len < 100:
            match_ratio *= 0.7

        # ì•ˆì „ì¥ì¹˜: match_ratioê°€ 0ì¼ ë•Œ ìµœì†Œ epsilon ê°’ ë¶€ì—¬
        if match_ratio == 0.0 and matched_tokens == 0:
            match_ratio = 1e-6  # epsilon to avoid all-zero scores

        # ğŸ¯ íŒŒì¼ëª… ë§¤ì¹­ ê°•í™” (CRITICAL: ì •í™•ë„ í–¥ìƒì˜ í•µì‹¬)
        filename_bonus = 0.0
        filename_exact_match_count = 0

        for token in query_tokens:
            # íŒŒì¼ëª…ì— ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í† í°ì´ ìˆìœ¼ë©´ í° ë³´ë„ˆìŠ¤
            if token in filename:
                filename_exact_match_count += 1
                filename_bonus += 2.0  # í† í°ë‹¹ +2ì 

        # íŒŒì¼ëª…ì—ì„œ ì—°ì†ëœ êµ¬ë¬¸ ë§¤ì¹­ ì‹œ ì¶”ê°€ ë³´ë„ˆìŠ¤
        if len(query_tokens) >= 2 and query.lower() in filename:
            filename_bonus += 3.0  # ì™„ì „ êµ¬ë¬¸ ë§¤ì¹­ +3ì 

        # ìµœì¢… ìŠ¤ì½”ì–´: ê¸°ë³¸ ìŠ¤ì½”ì–´(0~1) + íŒŒì¼ëª… ë³´ë„ˆìŠ¤(0~10)
        final_score = match_ratio + filename_bonus

        if filename_bonus > 0:
            logger.debug(f"ğŸ“Š Scoring '{filename[:50]}': match={match_ratio:.2f}, filename_bonus={filename_bonus:.1f}, final={final_score:.1f}")

        return max(0.0, min(10.0, final_score))

    def search(self, query: str, top_k: int, mode: str = "chat", selected_filename: Optional[str] = None) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼
            mode: ê²€ìƒ‰ ëª¨ë“œ ("chat", "doc_anchored" ë“±)
            selected_filename: ì„ íƒëœ ë¬¸ì„œ íŒŒì¼ëª… (ìš°ì„  ê²€ìƒ‰ìš©, ì„ íƒì‚¬í•­)

        Returns:
            ì •ê·œí™”ëœ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (score_stats ì†ì„± í¬í•¨):
            [
                {
                    "doc_id": str,
                    "page": int,
                    "score": float,
                    "snippet": str,
                    "meta": dict
                }, ...
            ]
        """
        try:
            # ì¸ë±ìŠ¤ ê°±ì‹  ì²´í¬
            self._reload_if_index_rotated()

            # FTS ìš°ì„  ê²€ìƒ‰, BM25 ë³´ì¶© ì „ëµ
            if self.use_bm25 and self.bm25:
                # DOC_ANCHORED ëª¨ë“œ: ë„‰ë„‰í•˜ê²Œ ê²€ìƒ‰ í›„ í•„í„°ë§
                search_k = 50 if mode.lower() == "doc_anchored" else top_k

                # 1. FTS ë¨¼ì € ì‹œë„ (ë” ì •í™•í•¨)
                fts_results = self._search_fts(query, top_k=search_k)

                # 2. FTS ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ FTSë§Œ ì‚¬ìš©, ë¶€ì¡±í•˜ë©´ BM25 ë³´ì¶©
                if len(fts_results) >= search_k * 0.5:  # FTS ê²°ê³¼ê°€ ëª©í‘œì˜ 50% ì´ìƒì´ë©´
                    logger.info(f"âœ… FTS ê²°ê³¼ ì¶©ë¶„ ({len(fts_results)}ê°œ), BM25 ìƒëµ")
                    bm25_results = []  # BM25 ìƒëµ
                    selected_doc = None
                else:
                    # FTS ê²°ê³¼ ë¶€ì¡±, BM25ë¡œ ë³´ì¶©
                    logger.info(f"âš ï¸ FTS ê²°ê³¼ ë¶€ì¡± ({len(fts_results)}ê°œ), BM25ë¡œ ë³´ì¶©")

                    # Parallel execution: BM25 searchì™€ selected doc lookup ë™ì‹œ ì‹¤í–‰
                    if self.enable_parallel and selected_filename:
                        # ë³‘ë ¬ ì‹¤í–‰: BM25 ê²€ìƒ‰ + ì„ íƒ ë¬¸ì„œ ê²€ìƒ‰
                        search_tasks = [
                            {
                                "name": "bm25_search",
                                "func": self.bm25.search,
                                "args": (query,),
                                "kwargs": {"top_k": search_k}
                            },
                            {
                                "name": "selected_doc_lookup",
                                "func": self._find_selected_document,
                                "args": (selected_filename,),
                                "kwargs": {}
                            }
                        ]
                        parallel_results = self.parallel_executor.execute_searches(search_tasks)
                        bm25_results = parallel_results.get("bm25_search", [])
                        selected_doc = parallel_results.get("selected_doc_lookup")
                    else:
                        # ìˆœì°¨ ì‹¤í–‰ (ê¸°ë³¸)
                        bm25_results = self.bm25.search(query, top_k=search_k)
                        selected_doc = None

                # FTS + BM25 ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                converted_results = []
                seen_filenames = set()

                # 1. FTS ê²°ê³¼ ë¨¼ì € ì¶”ê°€ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
                for result in fts_results:
                    filename = result.get("filename")
                    if filename and filename not in seen_filenames:
                        converted_results.append(result)
                        seen_filenames.add(filename)

                # 2. BM25 ê²°ê³¼ ì¶”ê°€ (FTSì— ì—†ëŠ” ê²ƒë§Œ)
                for result in bm25_results:
                    filename = result.get("filename")
                    if filename and filename not in seen_filenames:
                        converted_results.append({
                            "doc_id": result.get("filename", "unknown"),
                            "snippet": result.get("content", ""),
                            "score": result.get("score", 0.0),
                            "page": 1,
                            "filename": result.get("filename"),
                            "file_path": result.get("path"),
                            "meta": {
                                "filename": result.get("filename"),
                                "date": result.get("date"),
                                "drafter": result.get("drafter"),
                                "category": result.get("category"),
                            }
                        })
                        seen_filenames.add(filename)

                logger.info(f"ğŸ“Š ë³‘í•© ê²°ê³¼: FTS {len(fts_results)}ê°œ + BM25 {len(bm25_results)}ê°œ = ì´ {len(converted_results)}ê°œ (ì¤‘ë³µ ì œê±°)")

                # ğŸ¯ íŒŒì¼ëª… ë§¤ì¹­ ê¸°ë°˜ ì¬ìŠ¤ì½”ì–´ë§ (CRITICAL: ì •í™•ë„ í–¥ìƒ)
                if converted_results:
                    for result in converted_results:
                        # ê° ë¬¸ì„œì— ëŒ€í•´ íŒŒì¼ëª… ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚°
                        enhanced_score = self._calculate_relevance_score(query, result)
                        result["score"] = enhanced_score

                    # ìŠ¤ì½”ì–´ ê¸°ì¤€ ì¬ì •ë ¬ (ë†’ì€ ìˆœ)
                    converted_results.sort(key=lambda x: x.get("score", 0.0), reverse=True)
                    logger.info(f"ğŸ”„ íŒŒì¼ëª… ë§¤ì¹­ ì¬ìŠ¤ì½”ì–´ë§ ì™„ë£Œ (top score: {converted_results[0]['score']:.2f})")

                # DOC_ANCHORED í•„í„°ë§: ì¥ë¹„ ê´€ë ¨ í‚¤ì›Œë“œë§Œ í†µê³¼
                if mode.lower() == "doc_anchored":
                    filtered = []
                    for result in converted_results:
                        text = result.get("snippet", "") + " " + result.get("doc_id", "")
                        if re.search(self.device_pattern, text, re.IGNORECASE):
                            filtered.append(result)

                    # í•„í„° ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ìƒìœ„ N*3 ì‚¬ìš© (ë¯¸íƒ ë°©ì§€)
                    if not filtered:
                        logger.warning("âš ï¸ DOC_ANCHORED í•„í„°ë§ ê²°ê³¼ ì—†ìŒ, ì›ë³¸ ìƒìœ„ ì‚¬ìš©")
                        normalized = converted_results[:top_k * 3]
                    else:
                        logger.info(f"ğŸ¯ DOC_ANCHORED í•„í„°ë§: {len(converted_results)}ê°œ â†’ {len(filtered)}ê°œ")
                        normalized = filtered[:top_k]
                else:
                    normalized = converted_results

                # ì„ íƒëœ ë¬¸ì„œ ê°•ì œ ì¶”ê°€ (ì‚¬ìš©ì ìš”ì²­ ìš°ì„  ì²˜ë¦¬)
                if selected_filename:
                    # ë³‘ë ¬ ì‹¤í–‰ì—ì„œ ì´ë¯¸ ê°€ì ¸ì˜¨ ê²½ìš° ì‚¬ìš©, ì•„ë‹ˆë©´ ê²€ìƒ‰
                    if self.enable_parallel and 'selected_doc' in locals() and selected_doc:
                        # ì´ë¯¸ ë³‘ë ¬ë¡œ ê°€ì ¸ì˜´
                        logger.info(f"âœ… ë³‘ë ¬ ê²€ìƒ‰ìœ¼ë¡œ ì„ íƒ ë¬¸ì„œ ë°œê²¬: {selected_filename}")
                    else:
                        # 1. BM25 ê²°ê³¼ì—ì„œ ë¨¼ì € ì°¾ê¸°
                        selected_doc = None
                        for result in converted_results:
                            if result.get("filename") == selected_filename:
                                selected_doc = result.copy()
                                selected_doc["score"] = 99.9
                                break

                        # 2. BM25ì— ì—†ìœ¼ë©´ MetadataDBì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                        if not selected_doc:
                            logger.info(f"ğŸ” BM25ì— ì—†ìŒ, MetadataDBì—ì„œ ì§ì ‘ ê²€ìƒ‰: {selected_filename}")
                            selected_doc = self._find_selected_document(selected_filename)

                    # 3. ì°¾ì•˜ìœ¼ë©´ ìµœìƒìœ„ì— ê°•ì œ ì¶”ê°€
                    if selected_doc:
                        # ê¸°ì¡´ ê²°ê³¼ì—ì„œ ì œê±° (ì¤‘ë³µ ë°©ì§€)
                        normalized = [r for r in normalized if r.get("filename") != selected_filename]
                        # ìµœìƒìœ„ì— ê°•ì œ ì¶”ê°€
                        selected_doc_priority = selected_doc.copy()
                        selected_doc_priority["score"] = 99.9
                        normalized = [selected_doc_priority] + normalized[:top_k-1]
                        logger.info(f"ğŸ¯ ì„ íƒëœ ë¬¸ì„œ ìµœìƒìœ„ ê°•ì œ ì¶”ê°€: {selected_filename} (score=99.9)")
                    else:
                        logger.warning(f"âš ï¸ ì„ íƒëœ ë¬¸ì„œ '{selected_filename}'ë¥¼ ì°¾ì§€ ëª»í•¨ (BM25/MetadataDB ëª¨ë‘)")

            else:
                # Fallback: MetadataDB ê¸°ë°˜ (ë¹„ê¶Œì¥, 500ì ì œí•œ)
                logger.warning("âš ï¸ BM25 ë¹„í™œì„±í™”, MetadataDB í´ë°± ëª¨ë“œ (text_preview 500ì ì œí•œ)")
                filters = self.parser.parse_filters(query)
                year = filters.get('year')
                drafter = filters.get('drafter')

                results = self.metadata_db.search_documents(
                    year=year,
                    drafter=drafter,
                    limit=top_k * 3
                )

                normalized = []
                for doc in results:
                    snippet = doc.get('text_preview') or doc.get('content') or ""  # ì „ì²´ ë¬¸ì„œ ì‚¬ìš©
                    if not snippet:
                        snippet = f"[{doc.get('filename', 'unknown')}]"

                    relevance_score = self._calculate_relevance_score(query, doc)

                    normalized.append({
                        "doc_id": doc.get("filename", "unknown"),
                        "page": 1,
                        "score": relevance_score,
                        "snippet": snippet,
                        "meta": {
                            "filename": doc.get("filename", ""),
                            "drafter": doc.get("drafter", ""),
                            "date": doc.get("date", ""),
                            "category": doc.get("category", "pdf"),
                            "doc_id": doc.get("filename", "unknown"),
                        }
                    })

                normalized.sort(key=lambda x: x['score'], reverse=True)
                normalized = normalized[:top_k]

            # âœ¨ íŒŒì¼ëª… ë§¤ì¹­ ë³´ë„ˆìŠ¤ (ì¿¼ë¦¬ì™€ íŒŒì¼ëª…ì´ ìœ ì‚¬í•˜ë©´ ì ìˆ˜ ì¦ê°€)
            if not selected_filename:  # ì„ íƒëœ ë¬¸ì„œê°€ ì—†ì„ ë•Œë§Œ ì ìš©
                normalized = self._apply_filename_bonus(query, normalized)
                # ë³´ë„ˆìŠ¤ ì ìš© í›„ ì¬ì •ë ¬
                normalized.sort(key=lambda x: x['score'], reverse=True)

            # ìŠ¤ì½”ì–´ ë¶„í¬ í†µê³„ ê³„ì‚° (low-confidence ê°€ë“œë ˆì¼ìš©)
            scores = [r["score"] for r in normalized]
            top1 = scores[0] if len(scores) > 0 else 0.0
            top2 = scores[1] if len(scores) > 1 else 0.0
            top3 = scores[2] if len(scores) > 2 else 0.0

            score_stats = {
                "hits": len(normalized),
                "top1": top1,
                "top2": top2,
                "top3": top3,
                "delta12": max(0.0, top1 - top2),
                "delta13": max(0.0, top1 - top3)
            }

            # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— score_stats ì†ì„± ì¶”ê°€ (duck typing)
            class ResultsWithStats(list):
                def __init__(self, items, stats):
                    super().__init__(items)
                    self.score_stats = stats

            results_with_stats = ResultsWithStats(normalized, score_stats)

            backend = "BM25" if (self.use_bm25 and self.bm25) else "MetadataDB"
            logger.info(
                f"ğŸ” HybridRetriever ({backend}): {len(normalized)}ê±´ ê²€ìƒ‰ ì™„ë£Œ "
                f"(top1={top1:.2f}, delta12={score_stats['delta12']:.2f})"
            )
            return results_with_stats

        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def _apply_filename_bonus(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """íŒŒì¼ëª… ìœ ì‚¬ë„ì— ë”°ë¼ ì ìˆ˜ ë³´ë„ˆìŠ¤ ì ìš©

        ì¿¼ë¦¬ì™€ íŒŒì¼ëª…ì´ ë§¤ìš° ìœ ì‚¬í•  ë•Œ ê²€ìƒ‰ ìˆœìœ„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ë³´ì •.

        Args:
            query: ì‚¬ìš©ì ê²€ìƒ‰ ì§ˆì˜
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë³´ë„ˆìŠ¤ê°€ ì ìš©ëœ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        import re

        # ì¿¼ë¦¬ ì •ê·œí™”: ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ í† í°í™”
        query_normalized = re.sub(r'[^\w\s]', ' ', query.lower())
        query_tokens = set(query_normalized.split())

        if not query_tokens:
            return results

        for result in results:
            filename = result.get('filename', '')
            if not filename:
                continue

            # íŒŒì¼ëª… ì •ê·œí™”
            # 1. ë‚ ì§œ íŒ¨í„´ ì œê±° (YYYY-MM-DD_)
            filename_clean = re.sub(r'^\d{4}-\d{2}-\d{2}_', '', filename)
            # 2. í™•ì¥ì ì œê±°
            filename_clean = re.sub(r'\.\w+$', '', filename_clean)
            # 3. ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
            filename_clean = filename_clean.replace('_', ' ')
            # 4. íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ì†Œë¬¸ì ë³€í™˜
            filename_clean = re.sub(r'[^\w\s]', ' ', filename_clean.lower())

            # íŒŒì¼ëª… í† í°í™”
            filename_tokens = set(filename_clean.split())

            if not filename_tokens:
                continue

            # í† í° ë§¤ì¹­ë¥  ê³„ì‚° (Jaccard similarity)
            intersection = query_tokens & filename_tokens
            union = query_tokens | filename_tokens
            jaccard_score = len(intersection) / len(union) if union else 0.0

            # ì¿¼ë¦¬ í† í°ì´ íŒŒì¼ëª…ì— ëª¨ë‘ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            query_coverage = len(intersection) / len(query_tokens) if query_tokens else 0.0

            # ìµœì¢… ìœ ì‚¬ë„ ì ìˆ˜ (ë‘˜ ì¤‘ ë†’ì€ ê°’ ì‚¬ìš©)
            similarity = max(jaccard_score, query_coverage)

            # ë³´ë„ˆìŠ¤ ì ìˆ˜ ê³„ì‚° (0~30ì  ë²”ìœ„)
            # - 80% ì´ìƒ ìœ ì‚¬: +30ì 
            # - 60~80% ìœ ì‚¬: +20ì 
            # - 40~60% ìœ ì‚¬: +10ì 
            # - 40% ë¯¸ë§Œ: ë³´ë„ˆìŠ¤ ì—†ìŒ
            bonus = 0.0
            if similarity >= 0.8:
                bonus = 30.0
            elif similarity >= 0.6:
                bonus = 20.0
            elif similarity >= 0.4:
                bonus = 10.0

            # ë³´ë„ˆìŠ¤ ì ìš©
            if bonus > 0:
                original_score = result['score']
                result['score'] += bonus
                logger.info(
                    f"ğŸ“ˆ íŒŒì¼ëª… ë³´ë„ˆìŠ¤: {filename[:40]} | "
                    f"ìœ ì‚¬ë„={similarity:.2f} | "
                    f"ì ìˆ˜ {original_score:.2f} â†’ {result['score']:.2f} (+{bonus:.1f})"
                )

        return results
