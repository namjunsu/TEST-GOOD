#!/usr/bin/env python3
"""
Context Hydrator - ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° PDF ë³´ê°•
"""

import logging
import os
import re
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple

logger = logging.getLogger(__name__)


# ============================================================================
# ë¬¸ì¥ ìŠ¤ì½”ì–´ë§ íœ´ë¦¬ìŠ¤í‹± ê°€ì¤‘ì¹˜ (í™˜ê²½ë³€ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
# ============================================================================
SCORE_WEIGHTS = {
    "numeric": int(os.getenv("HYDRATOR_W_NUMERIC", "3")),        # ê¸ˆì•¡/ë‚ ì§œ/ìˆ˜ëŸ‰
    "keyword": int(os.getenv("HYDRATOR_W_KEYWORD", "1")),        # ë„ë©”ì¸ í‚¤ì›Œë“œ
    "conclusion": int(os.getenv("HYDRATOR_W_CONCLUSION", "2")),  # ê²°ë¡ /ìš”ì•½ ë§ˆì»¤
    "edge_bonus": int(os.getenv("HYDRATOR_W_EDGE", "1")),        # ì²«/ë§ˆì§€ë§‰ ë¬¸ì¥ ê°€ì¤‘
}

# ì„¸ë¯¸ë²„(v1.2.3) íŒ¨í„´ì„ ìˆ«ì ê°€ì¤‘ì—ì„œ ì œì™¸ (ê³¼ë‹¤ ë°˜ì˜ ë°©ì§€)
IGNORE_SEMVER_IN_NUMERIC = os.getenv("IGNORE_SEMVER_IN_NUMERIC", "false").lower() == "true"

# ë§ë¯¸ ê°€ì¤‘ì¹˜ (ë§ˆì§€ë§‰ ë¬¸ì¥/ë¬¸ë‹¨ ë³´ì •)
END_BONUS = int(os.getenv("END_BONUS", "1"))


def hydrate_context(chunks: List[Dict[str, Any]], max_len: int = 10000, mode: str = "rag") -> Tuple[str, Dict[str, Any]]:
    """
    ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë¶€ì¡±í•˜ë©´ PDF ë³´ê°•

    Args:
        chunks: ê²€ìƒ‰ ê²°ê³¼ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        max_len: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ì ìˆ˜)
        mode: ìƒì„± ëª¨ë“œ (chat/rag/summarize)

    Returns:
        (context_text, metrics)
    """
    start_time = time.perf_counter()

    # ğŸ¯ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì»¨í…ìŠ¤íŠ¸ í† í° ìƒí•œ ì½ê¸°
    context_max_tokens = int(os.getenv("CONTEXT_MAX_TOKENS", "1200"))

    # ğŸ¯ í† í°â†”ë¬¸ì ë³€í™˜ ê³„ìˆ˜ (ëª¨ë¸ë³„ í¸ì°¨ ê³ ë ¤)
    # ê¸°ë³¸ê°’ 0.33: 1 char â‰ˆ 0.33 token (ì¦‰, 1 token â‰ˆ 3 chars)
    tokens_per_char = float(os.getenv("TOKENS_PER_CHAR", "0.33"))
    context_max_chars = int(context_max_tokens / max(tokens_per_char, 1e-6))
    effective_max_len = min(max_len, context_max_chars)

    # ğŸ¯ RAG ìŠ¤íƒ€ì¼ ì••ì¶• ëª¨ë“œ í™•ì¸
    rag_style_compact = os.getenv("RAG_STYLE_COMPACT", "true").lower() == "true"

    metrics = {
        "chunks_received": len(chunks),
        "chunks_used": 0,
        "pdf_tail_pages": 0,
        "pdf_tail_status": "skipped",  # skipped | success | fail
        "total_length": 0,
        "fallback_chain": [],
        "context_max_tokens": context_max_tokens,
        "context_max_chars": context_max_chars,
        "extraction_time": 0.0,
        "compression_applied": False,
        "token_estimate": 0,
        "truncate_reason": "none"
    }

    parts = []

    # 1. ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í‚¤ í´ë°± ì²´ì¸)
    for chunk in chunks:
        text = _extract_text_from_chunk(chunk, metrics)
        if text:
            parts.append(text)
            metrics["chunks_used"] += 1

    # 2. ê¸¸ì´ ì²´í¬
    current_text = "\n\n".join(parts)
    current_len = len(current_text)

    if current_len < 500 and chunks:
        # PDF ë³´ê°• ì‹œë„
        pdf_text = _extract_pdf_tail(chunks[0], metrics, needed=effective_max_len - current_len)
        if pdf_text:
            parts.insert(0, pdf_text)
            current_text = "\n\n".join(parts)
            current_len = len(current_text)

    # 3. ğŸ¯ ëª¨ë“œë³„ ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì •ì±…
    if mode == "rag" and rag_style_compact and current_len > effective_max_len:
        # RAG ëª¨ë“œ: í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ì •ë³´ ë°€ë„ ê¸°ë°˜ ì••ì¶•)
        current_text = _extract_core_sentences(current_text, effective_max_len)
        current_len = len(current_text)
        metrics["compression_applied"] = True
    elif mode == "summarize":
        # Summarize ëª¨ë“œ: ì••ì¶• ë¹„ì ìš©, í•˜ë“œ ì»·ë§Œ ìˆ˜í–‰
        pass

    # 4. ìµœëŒ€ ê¸¸ì´ ì œí•œ (í•˜ë“œ ì»· - ë¬¸ë‹¨ ë‹¨ìœ„)
    if current_len > effective_max_len:
        current_text = _hard_cut_paragraphwise(current_text, effective_max_len)
        current_len = len(current_text)

    metrics["total_length"] = current_len
    metrics["token_estimate"] = int(current_len * tokens_per_char)

    # íŠ¸ë ì¼€ì´ì…˜ ì‚¬ìœ  ê²°ì •
    if metrics["compression_applied"]:
        metrics["truncate_reason"] = "compact"
    elif current_len >= effective_max_len:
        metrics["truncate_reason"] = "hardcut"
    else:
        metrics["truncate_reason"] = "none"

    metrics["extraction_time"] = time.perf_counter() - start_time

    # 5. ë¡œê¹… (ë‹¨ì¼ ë¼ì¸ ìš”ì•½ - ìš´ì˜ ëª¨ë‹ˆí„°ë§)
    parts_info = []
    if metrics["chunks_used"] > 0:
        parts_info.append(f"chunks:{metrics['chunks_used']}")
    if metrics["pdf_tail_pages"] > 0:
        parts_info.append(f"pdf_tail:{metrics['pdf_tail_pages']}")

    logger.info(
        f"CTX len={current_len}/{effective_max_len} "
        f"tokens~={metrics['token_estimate']} "
        f"src=[{','.join(parts_info)}] "
        f"truncate={metrics['truncate_reason']} "
        f"mode={mode} "
        f"coef={tokens_per_char:.2f} "
        f"time={metrics['extraction_time']:.3f}s"
    )

    if current_len == 0:
        logger.error(
            f"âŒ CONTEXT_EMPTY chunks={len(chunks)} "
            f"fallback_chain={metrics['fallback_chain']} "
            f"pdf_tail_status={metrics['pdf_tail_status']} "
            f"metrics={metrics}"
        )

    return current_text, metrics


def _hard_cut_paragraphwise(text: str, max_len: int) -> str:
    """
    ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ í•˜ë“œ ì»· (ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´)

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        max_len: ìµœëŒ€ ê¸¸ì´

    Returns:
        ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ì˜ë¦° í…ìŠ¤íŠ¸
    """
    paras = [p.strip() for p in re.split(r"\n{2,}", text) if p.strip()]
    out, total = [], 0
    for p in paras:
        if total + len(p) + 2 > max_len:
            break
        out.append(p)
        total += len(p) + 2
    return "\n\n".join(out) if out else text[:max_len]


def _extract_text_from_chunk(chunk: Dict[str, Any], metrics: Dict[str, Any]) -> str:
    """
    ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í´ë°± ì²´ì¸ + ê²½ëŸ‰ ì •ê·œí™”)

    í´ë°± ìˆœì„œ: text â†’ content â†’ page_content â†’ raw_text â†’ snippet â†’ text_preview â†’ abstract â†’ ""
    """
    # HTML/ë§ˆí¬ë‹¤ìš´ ì”ì—¬ë¬¼ íŒ¨í„´
    html_tags = re.compile(r"<[^>]+>")
    md_artifacts = re.compile(r"^\s*[#>\-*|`]{1,}", re.MULTILINE)

    # í´ë°± ì²´ì¸ (í™•ì¥)
    keys = ["text", "content", "page_content", "raw_text", "snippet", "text_preview", "abstract"]

    for key in keys:
        val = chunk.get(key)
        if not val:
            continue
        text = str(val).strip()
        if not text:
            continue

        # ê²½ëŸ‰ ì •ê·œí™”: HTML íƒœê·¸, ë§ˆí¬ë‹¤ìš´ ì•„í‹°íŒ©íŠ¸ ì œê±°
        text = html_tags.sub(" ", text)
        text = md_artifacts.sub(" ", text)
        text = re.sub(r"[ \t]+", " ", text).strip()

        if text:
            if key not in metrics["fallback_chain"]:
                metrics["fallback_chain"].append(key)
            return text

    return ""


def _is_under_docs(path: Path) -> bool:
    """
    ê²½ë¡œê°€ docs ë””ë ‰í† ë¦¬ í•˜ìœ„ì¸ì§€ ë³´ì•ˆ ê²€ì¦

    Args:
        path: ê²€ì¦í•  ê²½ë¡œ

    Returns:
        docs í•˜ìœ„ ì—¬ë¶€
    """
    try:
        base_docs = Path(os.getenv("DOCS_DIR", "docs")).resolve()
        resolved_path = path.resolve()
        return base_docs in resolved_path.parents or resolved_path == base_docs
    except Exception:
        return False


def _extract_pdf_tail(chunk: Dict[str, Any], metrics: Dict[str, Any], needed: int = 5000) -> str:
    """
    PDF ë§ˆì§€ë§‰ 2í˜ì´ì§€ ì¶”ì¶œ (ê²°ë¡ /ìš”ì•½ ê°€ëŠ¥ì„± ë†’ìŒ)

    ë³´ì•ˆ ê°•í™”:
    - ê²½ë¡œ ê²€ì¦: resolve() í›„ docs í•˜ìœ„ í™•ì¸
    - í™•ì¥ì ê²€ì¦: .pdfë§Œ í—ˆìš©
    - íŒŒì¼ í¬ê¸° ì œí•œ: 64MB ìƒí•œ

    ì‹¤íŒ¨ ëª¨ë“œ:
    - metrics["pdf_tail_status"] = "fail" ê¸°ë¡

    Args:
        chunk: ì²« ë²ˆì§¸ ì²­í¬ (íŒŒì¼ ê²½ë¡œ í¬í•¨)
        metrics: ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        needed: í•„ìš”í•œ í…ìŠ¤íŠ¸ ê¸¸ì´

    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    try:
        # íŒŒì¼ ê²½ë¡œ ì°¾ê¸° (ë‹¤ì¤‘ í‚¤ í´ë°±)
        file_path = chunk.get("file_path") or chunk.get("path") or chunk.get("filename")

        if not file_path:
            return ""

        p = Path(file_path)

        # ë³´ì•ˆ ê²€ì¦ 1: PDF í™•ì¥ì í™•ì¸
        if p.suffix.lower() != ".pdf":
            return ""

        # ë³´ì•ˆ ê²€ì¦ 2: docs ë””ë ‰í† ë¦¬ í•˜ìœ„ì¸ì§€ í™•ì¸ (ì‹¬ë³¼ë¦­ ë§í¬ ìš°íšŒ ë°©ì§€)
        if not _is_under_docs(p):
            logger.warning(f"âš ï¸ PDF outside docs root: {p}")
            metrics["pdf_tail_status"] = "fail"
            return ""

        # ë³´ì•ˆ ê²€ì¦ 3: íŒŒì¼ ì¡´ì¬ ë° í¬ê¸° ì œí•œ (64MB)
        if not p.exists():
            logger.warning(f"âš ï¸ PDF not found: {p}")
            metrics["pdf_tail_status"] = "fail"
            return ""

        if p.stat().st_size > 64 * 1024 * 1024:  # 64MB ìƒí•œ
            logger.warning(f"âš ï¸ PDF too large: {p} ({p.stat().st_size / 1024 / 1024:.1f}MB)")
            metrics["pdf_tail_status"] = "fail"
            return ""

        # PDF ì½ê¸°
        import pdfplumber

        with pdfplumber.open(p) as pdf:
            total_pages = len(pdf.pages)

            if total_pages == 0:
                metrics["pdf_tail_status"] = "fail"
                return ""

            # ë§ˆì§€ë§‰ 2í˜ì´ì§€ (ë˜ëŠ” ì „ì²´)
            start_page = max(0, total_pages - 2)

            text_parts = []
            for page_num in range(start_page, total_pages):
                page_text = pdf.pages[page_num].extract_text()
                if page_text:
                    text_parts.append(page_text)

            if text_parts:
                metrics["pdf_tail_pages"] = len(text_parts)
                metrics["pdf_tail_status"] = "success"
                combined = "\n\n".join(text_parts)

                # í•„ìš”í•œ ë§Œí¼ë§Œ ìë¥´ê¸°
                if len(combined) > needed:
                    combined = combined[:needed]

                return combined
            else:
                # í…ìŠ¤íŠ¸ ë ˆì´ì–´ ì—†ìŒ (OCR í•„ìš”)
                metrics["pdf_tail_status"] = "fail"
                return ""

    except Exception as e:
        logger.warning(f"âš ï¸ PDF extraction failed: {e}")
        metrics["pdf_tail_status"] = "fail"

    return ""


def _extract_core_sentences(text: str, max_len: int) -> str:
    """
    í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ì •ë³´ ë°€ë„ ê¸°ë°˜ í•„í„°ë§)

    ìš°ì„ ìˆœìœ„:
    1. ìˆ˜ì¹˜ ì •ë³´ (ê¸ˆì•¡, ë‚ ì§œ, ìˆ˜ëŸ‰)
    2. êµ¬ì²´ì  í’ˆëª©ëª…/ëª¨ë¸ëª…
    3. ê²°ë¡ /ìš”ì•½ ë¬¸ì¥

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        max_len: ìµœëŒ€ ê¸¸ì´

    Returns:
        ì••ì¶•ëœ í…ìŠ¤íŠ¸
    """
    # ë¬¸ì¥ ë¶„ë¦¬ (lookbehindë¡œ êµ¬ë¶„ì ë¯¸í¬í•¨)
    sent_boundary = re.compile(r"(?<=[.!?])\s+|\n{2,}")
    sentences = [s.strip() for s in sent_boundary.split(text) if len(s.strip()) > 10]

    # ìˆ˜ì¹˜/ë‹¨ìœ„ íŒ¨í„´ (ì‰¼í‘œ êµ¬ë¶„ ìˆ«ì + ë‹¨ìœ„)
    numeric = re.compile(r"\b\d{1,3}(?:,\d{3})+\b|\b\d+\b")
    semver = re.compile(r"\bv?\d+\.\d+(?:\.\d+)?\b")  # ì„¸ë¯¸ë²„ íŒ¨í„´ (v1.2.3)
    units = ("ì›", "ë…„", "ì›”", "ì¼", "ê°œ", "ëŒ€", "ë§Œ", "ì–µ", "ì‹", "í†µ", "ê±´", "ëª…", "ê¶Œ", "ëŒ€")

    # ë„ë©”ì¸ í‚¤ì›Œë“œ í™•ì¥
    keywords = ("ê¸°ì•ˆ", "êµ¬ë§¤", "ìˆ˜ë¦¬", "êµì²´", "ë Œì¦ˆ", "ë§ˆì´í¬", "ì¹´ë©”ë¼", "ì¼€ì´ë¸”",
                "í’ˆëª©", "ëª¨ë¸", "ê¸ˆì•¡", "ë‚ ì§œ", "ì‘ì„±", "ì‹ ì²­", "ë°°í„°ë¦¬", "ê±´ì „ì§€",
                "ì†Œëª¨í’ˆ", "ì¥ë¹„", "ë°©ì†¡", "ì´¬ì˜", "ì‚¬ì˜¥", "ê²€í† ")

    # ê° ë¬¸ì¥ì— ì ìˆ˜ ë¶€ì—¬ (ì¸ë±ìŠ¤ ë³´ì¡´)
    scored = []
    for idx, sent in enumerate(sentences):
        score = 0

        # 1) ìˆ˜ì¹˜ ì •ë³´ (ì‰¼í‘œ êµ¬ë¶„ ìˆ«ì + ë‹¨ìœ„)
        has_numeric = numeric.search(sent) or any(u in sent for u in units)
        if has_numeric:
            # ì„¸ë¯¸ë²„ ë¬´ì‹œ ì˜µì…˜: v1.2.3 ê°™ì€ ë²„ì „ ë²ˆí˜¸ ì œì™¸
            if IGNORE_SEMVER_IN_NUMERIC and semver.search(sent):
                pass  # ìˆ«ì ê°€ì¤‘ì¹˜ ë¯¸ì ìš©
            else:
                score += SCORE_WEIGHTS["numeric"]

        # 2) ë„ë©”ì¸ í‚¤ì›Œë“œ
        score += sum(SCORE_WEIGHTS["keyword"] for kw in keywords if kw in sent)

        # 3) ê²°ë¡ Â·ìš”ì•½ ë§ˆì»¤
        if any(m in sent for m in ("ë”°ë¼ì„œ", "ìš”ì•½", "ê²°ë¡ ", "ëª©ì ", "ì¢…í•©", "ê¶Œê³ ", "í™•ì •", "ë‚´ìš©")):
            score += SCORE_WEIGHTS["conclusion"]

        # 4) ì„œë‘/ë§ë¯¸ ê°€ì¤‘ì¹˜
        if END_BONUS and idx in (0, len(sentences) - 1):
            score += SCORE_WEIGHTS["edge_bonus"]

        scored.append((score, idx, sent))

    # ì ìˆ˜ ìˆœ ì •ë ¬ (ì ìˆ˜ ë™ì¼ ì‹œ ì›ë¬¸ ìˆœì„œ ìœ ì§€)
    scored.sort(key=lambda x: (-x[0], x[1]))

    # ìƒìœ„ ë¬¸ì¥ ì„ íƒ (ê¸¸ì´ ì œí•œ)
    selected, total = [], 0
    for _, idx, s in scored:
        if total + len(s) > max_len:
            continue
        selected.append((idx, s))
        total += len(s)
        if total >= max_len:
            break

    # ì›ë³¸ ìˆœì„œ ë³µì› (ì¸ë±ìŠ¤ ê¸°ë°˜)
    selected.sort(key=lambda x: x[0])
    ordered = [s for _, s in selected]

    return "\n\n".join(ordered)
