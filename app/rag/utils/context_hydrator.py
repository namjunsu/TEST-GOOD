#!/usr/bin/env python3
"""
Context Hydrator - ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° PDF ë³´ê°•
"""

from typing import List, Dict, Any, Tuple
from pathlib import Path
import logging
import os
import re
import time

logger = logging.getLogger(__name__)


def hydrate_context(chunks: List[Dict[str, Any]], max_len: int = 10000, mode: str = "rag") -> Tuple[str, Dict[str, Any]]:
    """
    ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë¶€ì¡±í•˜ë©´ PDF ë³´ê°•

    Args:
        chunks: ê²€ìƒ‰ ê²°ê³¼ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        max_len: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ì ìˆ˜)
        mode: ìƒì„± ëª¨ë“œ (chat/rag/summarize)

    Returns:
        (context_text, metrics)
    """
    start_time = time.perf_counter()

    # ğŸ¯ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì»¨í…ìŠ¤íŠ¸ í† í° ìƒí•œ ì½ê¸° (1 token â‰ˆ 3 chars in Korean)
    context_max_tokens = int(os.getenv("CONTEXT_MAX_TOKENS", "1200"))
    context_max_chars = context_max_tokens * 3  # 1200 tokens = 3600 chars
    effective_max_len = min(max_len, context_max_chars)

    # ğŸ¯ RAG ìŠ¤íƒ€ì¼ ì••ì¶• ëª¨ë“œ í™•ì¸
    rag_style_compact = os.getenv("RAG_STYLE_COMPACT", "true").lower() == "true"

    metrics = {
        "chunks_received": len(chunks),
        "chunks_used": 0,
        "pdf_tail_pages": 0,
        "total_length": 0,
        "fallback_chain": [],
        "context_max_tokens": context_max_tokens,
        "context_max_chars": context_max_chars,
        "extraction_time": 0.0,
        "compression_applied": False
    }

    parts = []

    # 1. ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í‚¤ í´ë°± ì²´ì¸)
    for i, chunk in enumerate(chunks):
        text = _extract_text_from_chunk(chunk, metrics)
        if text:
            parts.append(text)
            metrics["chunks_used"] += 1

    # 2. ê¸¸ì´ ì²´í¬
    current_text = "\n\n".join(parts)
    current_len = len(current_text)

    if current_len < 500 and chunks:
        # PDF ë³´ê°• ì‹œë„
        pdf_text = _extract_pdf_tail(chunks[0], metrics, needed=effective_max_len - current_len)
        if pdf_text:
            parts.insert(0, pdf_text)
            current_text = "\n\n".join(parts)
            current_len = len(current_text)

    # 3. ğŸ¯ RAG ìŠ¤íƒ€ì¼ ì••ì¶•: í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ëª¨ë“œê°€ ragì´ê³  compactê°€ í™œì„±í™”ëœ ê²½ìš°)
    if mode == "rag" and rag_style_compact and current_len > effective_max_len:
        current_text = _extract_core_sentences(current_text, effective_max_len)
        current_len = len(current_text)
        metrics["compression_applied"] = True

    # 4. ìµœëŒ€ ê¸¸ì´ ì œí•œ (í•˜ë“œ ì»·)
    if current_len > effective_max_len:
        current_text = current_text[:effective_max_len]
        current_len = effective_max_len

    metrics["total_length"] = current_len
    metrics["extraction_time"] = time.perf_counter() - start_time

    # 5. ë¡œê¹…
    parts_info = []
    if metrics["pdf_tail_pages"] > 0:
        parts_info.append(f"pdf_tail:{metrics['pdf_tail_pages']}")
    if metrics["chunks_used"] > 0:
        parts_info.append(f"chunks:{metrics['chunks_used']}")

    logger.info(
        f"LLM_CTX len={current_len}/{effective_max_len}; "
        f"parts=[{', '.join(parts_info)}]; "
        f"compact={metrics['compression_applied']}; "
        f"time={metrics['extraction_time']:.3f}s"
    )

    if current_len == 0:
        logger.warning(f"âš ï¸ Context is empty! chunks={len(chunks)}, fallback_chain={metrics['fallback_chain']}")

    return current_text, metrics


def _extract_text_from_chunk(chunk: Dict[str, Any], metrics: Dict[str, Any]) -> str:
    """
    ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í´ë°± ì²´ì¸)

    í´ë°± ìˆœì„œ: text â†’ content â†’ snippet â†’ text_preview â†’ ""
    """
    # í´ë°± ì²´ì¸
    keys = ["text", "content", "snippet", "text_preview"]

    for key in keys:
        if key in chunk and chunk[key]:
            text = str(chunk[key]).strip()
            if text:
                if key not in metrics["fallback_chain"]:
                    metrics["fallback_chain"].append(key)
                return text

    return ""


def _extract_pdf_tail(chunk: Dict[str, Any], metrics: Dict[str, Any], needed: int = 5000) -> str:
    """
    PDF ë§ˆì§€ë§‰ 2í˜ì´ì§€ ì¶”ì¶œ (ê²°ë¡ /ìš”ì•½ ê°€ëŠ¥ì„± ë†’ìŒ)

    Args:
        chunk: ì²« ë²ˆì§¸ ì²­í¬ (íŒŒì¼ ê²½ë¡œ í¬í•¨)
        metrics: ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        needed: í•„ìš”í•œ í…ìŠ¤íŠ¸ ê¸¸ì´

    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    try:
        # íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        file_path = chunk.get("file_path") or chunk.get("path") or chunk.get("filename")

        if not file_path:
            return ""

        file_path = Path(file_path)

        # ë³´ì•ˆ: docs í´ë” ì™¸ë¶€ ì°¨ë‹¨
        if "docs" not in file_path.parts:
            logger.warning(f"âš ï¸ PDF path outside docs: {file_path}")
            return ""

        if not file_path.exists():
            logger.warning(f"âš ï¸ PDF not found: {file_path}")
            return ""

        # PDF ì½ê¸°
        import pdfplumber

        with pdfplumber.open(file_path) as pdf:
            total_pages = len(pdf.pages)

            if total_pages == 0:
                return ""

            # ë§ˆì§€ë§‰ 2í˜ì´ì§€ (ë˜ëŠ” ì „ì²´)
            start_page = max(0, total_pages - 2)
            pages_to_read = min(2, total_pages)

            text_parts = []
            for page_num in range(start_page, total_pages):
                page_text = pdf.pages[page_num].extract_text()
                if page_text:
                    text_parts.append(page_text)

            if text_parts:
                metrics["pdf_tail_pages"] = len(text_parts)
                combined = "\n\n".join(text_parts)

                # í•„ìš”í•œ ë§Œí¼ë§Œ ìë¥´ê¸°
                if len(combined) > needed:
                    combined = combined[:needed]

                return combined

    except Exception as e:
        logger.warning(f"âš ï¸ PDF extraction failed: {e}")

    return ""


def _extract_core_sentences(text: str, max_len: int) -> str:
    """
    í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ì •ë³´ ë°€ë„ ê¸°ë°˜ í•„í„°ë§)

    ìš°ì„ ìˆœìœ„:
    1. ìˆ˜ì¹˜ ì •ë³´ (ê¸ˆì•¡, ë‚ ì§œ, ìˆ˜ëŸ‰)
    2. êµ¬ì²´ì  í’ˆëª©ëª…/ëª¨ë¸ëª…
    3. ê²°ë¡ /ìš”ì•½ ë¬¸ì¥

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        max_len: ìµœëŒ€ ê¸¸ì´

    Returns:
        ì••ì¶•ëœ í…ìŠ¤íŠ¸
    """
    # ë¬¸ì¥ ë¶„ë¦¬ (í•œêµ­ì–´ ì¢…ê²°ì–´ë¯¸ ê¸°ë°˜)
    sentences = re.split(r'([.!?]\s+|\n{2,})', text)
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]

    # ê° ë¬¸ì¥ì— ì ìˆ˜ ë¶€ì—¬
    scored_sentences = []
    for sent in sentences:
        score = 0

        # 1. ìˆ˜ì¹˜ ì •ë³´ (ê¸ˆì•¡, ë‚ ì§œ, ìˆ˜ëŸ‰)
        if re.search(r'\d+[,ì›ë…„ì›”ì¼ê°œëŒ€ì‹í†µë§Œì–µ]', sent):
            score += 3

        # 2. êµ¬ì²´ì  í‚¤ì›Œë“œ (í’ˆëª©, ëª¨ë¸, ê¸°ì•ˆ, êµ¬ë§¤)
        keywords = ['ê¸°ì•ˆ', 'êµ¬ë§¤', 'ìˆ˜ë¦¬', 'êµì²´', 'ë Œì¦ˆ', 'ë§ˆì´í¬', 'ì¹´ë©”ë¼', 'ì¼€ì´ë¸”',
                    'í’ˆëª©', 'ëª¨ë¸', 'ê¸ˆì•¡', 'ë‚ ì§œ', 'ì‘ì„±', 'ì‹ ì²­', 'ê±´ì „ì§€', 'ë°°í„°ë¦¬']
        for kw in keywords:
            if kw in sent:
                score += 1

        # 3. ê²°ë¡ /ìš”ì•½ ë¬¸ì¥
        if any(marker in sent for marker in ['ë”°ë¼ì„œ', 'ìš”ì•½', 'ê²°ë¡ ', 'ëª©ì ', 'ë‚´ìš©']):
            score += 2

        # 4. ì²« ë¬¸ì¥/ë§ˆì§€ë§‰ ë¬¸ì¥ ê°€ì¤‘ì¹˜
        if sent == sentences[0] or sent == sentences[-1]:
            score += 1

        scored_sentences.append((score, sent))

    # ì ìˆ˜ ìˆœ ì •ë ¬
    scored_sentences.sort(key=lambda x: x[0], reverse=True)

    # ìƒìœ„ ë¬¸ì¥ ì„ íƒ (ê¸¸ì´ ì œí•œ)
    selected = []
    current_len = 0
    for score, sent in scored_sentences:
        if current_len + len(sent) > max_len:
            break
        selected.append(sent)
        current_len += len(sent)

    # ì›ë³¸ ìˆœì„œ ë³µì› (ê°€ë…ì„± ìœ ì§€)
    selected_set = set(selected)
    ordered = [s for s in sentences if s in selected_set]

    return "\n\n".join(ordered)
