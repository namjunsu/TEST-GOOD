#!/usr/bin/env python3
"""
ìë™ ì¸ë±ì‹± ê°ì‹œ ì‹œìŠ¤í…œ
docs/ í´ë”ì— ì‹ ê·œ PDFê°€ ì¶”ê°€ë˜ë©´ ìë™ìœ¼ë¡œ ì¸ë±ì‹±

ì‚¬ìš©ë²•:
    python3 auto_index_watcher.py  # ê³„ì† ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ ê¶Œì¥)
"""

import sys
import time
import logging
from pathlib import Path
from typing import Set
from datetime import datetime
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/auto_indexer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from rag_system.bm25_store import BM25Store
from rag_system.korean_vector_store import KoreanVectorStore
import pdfplumber


class AutoIndexWatcher:
    """ìë™ ì¸ë±ì‹± ê°ì‹œì"""

    def __init__(self, docs_dir: str = "docs", check_interval: int = 60):
        """
        Args:
            docs_dir: ê°ì‹œí•  ë¬¸ì„œ ë””ë ‰í† ë¦¬
            check_interval: ì²´í¬ ê°„ê²© (ì´ˆ)
        """
        self.docs_dir = Path(docs_dir)
        self.check_interval = check_interval

        # ì´ë¯¸ ì¸ë±ì‹±ëœ íŒŒì¼ ì¶”ì 
        self.indexed_files: Set[str] = set()
        self.file_hashes = {}  # íŒŒì¼ í•´ì‹œ ì €ì¥ (ë³€ê²½ ê°ì§€ìš©)

        # RAG ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self.bm25_store = None
        self.vector_store = None

        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        Path("logs").mkdir(exist_ok=True)

    def _load_stores(self):
        """RAG ìŠ¤í† ì–´ ë¡œë“œ (ì§€ì—° ë¡œë”©)"""
        if self.bm25_store is None:
            logger.info("BM25Store ë¡œë“œ ì¤‘...")
            self.bm25_store = BM25Store(index_path="rag_system/db/bm25_index.pkl")

        if self.vector_store is None:
            logger.info("KoreanVectorStore ë¡œë“œ ì¤‘...")
            self.vector_store = KoreanVectorStore(
                index_path="rag_system/db/korean_vector_index.faiss"
            )

    def _get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            logger.warning(f"í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨ {file_path.name}: {e}")
            return ""

    def _scan_files(self) -> Set[str]:
        """í˜„ì¬ ì¡´ì¬í•˜ëŠ” ëª¨ë“  PDF íŒŒì¼ ìŠ¤ìº”"""
        pdf_files = set()
        for pdf_path in self.docs_dir.rglob("*.pdf"):
            pdf_files.add(str(pdf_path))
        return pdf_files

    def _index_new_file(self, pdf_path: Path) -> bool:
        """ì‹ ê·œ íŒŒì¼ ì¸ë±ì‹±"""
        try:
            logger.info(f"ğŸ“„ ì‹ ê·œ íŒŒì¼ ì¸ë±ì‹±: {pdf_path.name}")

            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = ""
            with pdfplumber.open(pdf_path) as pdf:
                for page in pdf.pages[:10]:  # ìµœëŒ€ 10í˜ì´ì§€
                    page_text = page.extract_text() or ""
                    text += page_text + "\n"

            if not text.strip():
                logger.warning(f"âš ï¸  í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path.name}")
                return False

            # RAG ìŠ¤í† ì–´ ë¡œë“œ
            self._load_stores()

            # ë¬¸ì„œ ID ìƒì„±
            doc_id = f"doc_{hashlib.md5(str(pdf_path).encode()).hexdigest()[:12]}"

            # BM25 ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.bm25_store.add_document(
                doc_id=doc_id,
                content=text,
                metadata={
                    'filename': pdf_path.name,
                    'path': str(pdf_path),
                    'indexed_at': datetime.now().isoformat()
                }
            )

            # Vector ì¸ë±ìŠ¤ì— ì¶”ê°€
            content_chunk = text[:5000]  # ìµœëŒ€ 5000ì
            self.vector_store.add_document(
                doc_id=doc_id,
                content=content_chunk,
                metadata={
                    'filename': pdf_path.name,
                    'path': str(pdf_path),
                    'indexed_at': datetime.now().isoformat()
                }
            )

            # ì¸ë±ìŠ¤ ì €ì¥
            self.bm25_store.save_index()
            self.vector_store.save_index()

            # ì¶”ì ì— ì¶”ê°€
            self.indexed_files.add(str(pdf_path))
            self.file_hashes[str(pdf_path)] = self._get_file_hash(pdf_path)

            logger.info(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {pdf_path.name}")
            return True

        except Exception as e:
            logger.error(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨ {pdf_path.name}: {e}")
            return False

    def _check_for_changes(self):
        """ì‹ ê·œ/ë³€ê²½ëœ íŒŒì¼ ì²´í¬ ë° ì¸ë±ì‹±"""
        current_files = self._scan_files()

        # ì‹ ê·œ íŒŒì¼ ê°ì§€
        new_files = current_files - self.indexed_files

        if new_files:
            logger.info(f"ğŸ” ì‹ ê·œ íŒŒì¼ {len(new_files)}ê°œ ë°œê²¬")

            for file_path_str in new_files:
                file_path = Path(file_path_str)
                self._index_new_file(file_path)

        # ë³€ê²½ëœ íŒŒì¼ ê°ì§€ (ì„ íƒì )
        for file_path_str in current_files & self.indexed_files:
            file_path = Path(file_path_str)
            current_hash = self._get_file_hash(file_path)

            if current_hash != self.file_hashes.get(file_path_str):
                logger.info(f"ğŸ”„ íŒŒì¼ ë³€ê²½ ê°ì§€: {file_path.name}")
                self._index_new_file(file_path)

        # ì‚­ì œëœ íŒŒì¼ ê°ì§€ (ë¡œê·¸ë§Œ)
        deleted_files = self.indexed_files - current_files
        if deleted_files:
            logger.warning(f"ğŸ—‘ï¸  ì‚­ì œëœ íŒŒì¼ {len(deleted_files)}ê°œ ê°ì§€")
            for file_path_str in deleted_files:
                logger.warning(f"   - {Path(file_path_str).name}")
                self.indexed_files.remove(file_path_str)
                self.file_hashes.pop(file_path_str, None)

    def start(self):
        """ê°ì‹œ ì‹œì‘"""
        logger.info("ğŸš€ ìë™ ì¸ë±ì‹± ê°ì‹œ ì‹œì‘")
        logger.info(f"ğŸ“‚ ê°ì‹œ ë””ë ‰í† ë¦¬: {self.docs_dir}")
        logger.info(f"â±ï¸  ì²´í¬ ê°„ê²©: {self.check_interval}ì´ˆ")
        logger.info("=" * 60)

        # ì´ˆê¸° íŒŒì¼ ëª©ë¡ ë¡œë“œ
        logger.info("ì´ˆê¸° íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        self.indexed_files = self._scan_files()
        logger.info(f"ğŸ“Š í˜„ì¬ íŒŒì¼: {len(self.indexed_files)}ê°œ")

        # íŒŒì¼ í•´ì‹œ ì´ˆê¸°í™”
        for file_path_str in self.indexed_files:
            file_path = Path(file_path_str)
            self.file_hashes[file_path_str] = self._get_file_hash(file_path)

        # ê°ì‹œ ë£¨í”„
        try:
            while True:
                self._check_for_changes()
                time.sleep(self.check_interval)

        except KeyboardInterrupt:
            logger.info("\nâ¹ï¸  ê°ì‹œ ì¤‘ì§€ë¨")
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse

    parser = argparse.ArgumentParser(description="RAG ìë™ ì¸ë±ì‹± ê°ì‹œ")
    parser.add_argument(
        "--interval",
        type=int,
        default=60,
        help="ì²´í¬ ê°„ê²© (ì´ˆ, ê¸°ë³¸: 60)"
    )
    parser.add_argument(
        "--docs-dir",
        type=str,
        default="docs",
        help="ê°ì‹œí•  ë¬¸ì„œ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: docs)"
    )

    args = parser.parse_args()

    watcher = AutoIndexWatcher(
        docs_dir=args.docs_dir,
        check_interval=args.interval
    )

    watcher.start()


if __name__ == "__main__":
    main()
