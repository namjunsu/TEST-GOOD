==================================================
🚀 RAG 시스템 성능 최적화 Phase 1
==================================================
🔧 LLM 싱글톤 최적화 중...
  ✅ 백업 생성: rag_system/llm_singleton.py.bak
  ✅ LLM 싱글톤 최적화 완료
🔧 캐시 키 생성 최적화 중...
  ✅ 캐시 키 최적화 코드 준비 완료

📝 perfect_rag.py에 추가할 캐시 최적화 코드:
----------------------------------------

# perfect_rag.py의 _get_cache_key 메서드 개선
import re

def _get_enhanced_cache_key(self, query: str, mode: str) -> str:
    """향상된 캐시 키 생성 - 유사 질문도 캐시 히트"""
    
    # 1. 쿼리 정규화
    normalized = query.strip().lower()
    
    # 2. 조사 제거 (한국어 특화)
    particles = ['은', '는', '이', '가', '을', '를', '의', '와', '과', '로', '으로', '에', '에서']
    for particle in particles:
        normalized = normalized.replace(particle + ' ', ' ')
    
    # 3. 공백 정규화
    normalized = ' '.join(normalized.split())
    
    # 4. 핵심 키워드만 추출
    keywords = []
    for word in normalized.split():
        if len(word) >= 2:  # 2글자 이상만
            keywords.append(word)
    
    # 5. 정렬하여 순서 무관하게
    keywords.sort()
    
    # 6. 해시 생성
    cache_str = f"{mode}:{'_'.join(keywords)}"
    return hashlib.md5(cache_str.encode()).hexdigest()

----------------------------------------
🔧 동적 컨텍스트 관리 추가...
  ✅ 동적 컨텍스트 관리 코드 준비 완료

📝 perfect_rag.py에 추가할 컨텍스트 관리 코드:
----------------------------------------

def _get_optimal_context_size(self, query: str, doc_count: int) -> int:
    """쿼리와 문서 수에 따른 최적 컨텍스트 크기 결정"""
    
    query_len = len(query)
    
    # 간단한 쿼리 (20자 미만)
    if query_len < 20 and doc_count <= 3:
        return 4096
    
    # 중간 복잡도 (20-50자)
    elif query_len < 50 and doc_count <= 5:
        return 8192
    
    # 복잡한 쿼리 또는 많은 문서
    else:
        return 16384

def _smart_truncate_context(self, text: str, max_tokens: int = 8000) -> str:
    """스마트 컨텍스트 절단"""
    
    # 토큰 수 추정 (한글 1.5자 = 1토큰 기준)
    estimated_tokens = len(text) / 1.5
    
    if estimated_tokens <= max_tokens:
        return text
    
    # 문장 단위로 절단
    sentences = text.split('.')
    result = []
    current_tokens = 0
    
    for sentence in sentences:
        sentence_tokens = len(sentence) / 1.5
        if current_tokens + sentence_tokens > max_tokens:
            break
        result.append(sentence)
        current_tokens += sentence_tokens
    
    return '.'.join(result) + '.'

----------------------------------------
🔧 성능 설정 파일 생성 중...
  ✅ 성능 설정 파일 생성: performance_config.yaml

==================================================
✅ Phase 1 최적화 완료 (0.0초)
==================================================

📋 적용 방법:
1. llm_singleton.py는 자동으로 교체됨
2. perfect_rag.py에 위 코드들을 추가
3. performance_config.yaml 설정 적용

⚡ 예상 성능 향상:
- LLM 로딩: 7.73초 → 0.1초 (재사용 시)
- 응답 시간: 140초 → 30-50초
- 캐시 히트율: 30% → 70%+
