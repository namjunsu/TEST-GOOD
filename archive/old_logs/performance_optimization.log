==================================================
ğŸš€ RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” Phase 1
==================================================
ğŸ”§ LLM ì‹±ê¸€í†¤ ìµœì í™” ì¤‘...
  âœ… ë°±ì—… ìƒì„±: rag_system/llm_singleton.py.bak
  âœ… LLM ì‹±ê¸€í†¤ ìµœì í™” ì™„ë£Œ
ğŸ”§ ìºì‹œ í‚¤ ìƒì„± ìµœì í™” ì¤‘...
  âœ… ìºì‹œ í‚¤ ìµœì í™” ì½”ë“œ ì¤€ë¹„ ì™„ë£Œ

ğŸ“ perfect_rag.pyì— ì¶”ê°€í•  ìºì‹œ ìµœì í™” ì½”ë“œ:
----------------------------------------

# perfect_rag.pyì˜ _get_cache_key ë©”ì„œë“œ ê°œì„ 
import re

def _get_enhanced_cache_key(self, query: str, mode: str) -> str:
    """í–¥ìƒëœ ìºì‹œ í‚¤ ìƒì„± - ìœ ì‚¬ ì§ˆë¬¸ë„ ìºì‹œ íˆíŠ¸"""
    
    # 1. ì¿¼ë¦¬ ì •ê·œí™”
    normalized = query.strip().lower()
    
    # 2. ì¡°ì‚¬ ì œê±° (í•œêµ­ì–´ íŠ¹í™”)
    particles = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ', 'ì—', 'ì—ì„œ']
    for particle in particles:
        normalized = normalized.replace(particle + ' ', ' ')
    
    # 3. ê³µë°± ì •ê·œí™”
    normalized = ' '.join(normalized.split())
    
    # 4. í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
    keywords = []
    for word in normalized.split():
        if len(word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ
            keywords.append(word)
    
    # 5. ì •ë ¬í•˜ì—¬ ìˆœì„œ ë¬´ê´€í•˜ê²Œ
    keywords.sort()
    
    # 6. í•´ì‹œ ìƒì„±
    cache_str = f"{mode}:{'_'.join(keywords)}"
    return hashlib.md5(cache_str.encode()).hexdigest()

----------------------------------------
ğŸ”§ ë™ì  ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì¶”ê°€...
  âœ… ë™ì  ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì½”ë“œ ì¤€ë¹„ ì™„ë£Œ

ğŸ“ perfect_rag.pyì— ì¶”ê°€í•  ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì½”ë“œ:
----------------------------------------

def _get_optimal_context_size(self, query: str, doc_count: int) -> int:
    """ì¿¼ë¦¬ì™€ ë¬¸ì„œ ìˆ˜ì— ë”°ë¥¸ ìµœì  ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ê²°ì •"""
    
    query_len = len(query)
    
    # ê°„ë‹¨í•œ ì¿¼ë¦¬ (20ì ë¯¸ë§Œ)
    if query_len < 20 and doc_count <= 3:
        return 4096
    
    # ì¤‘ê°„ ë³µì¡ë„ (20-50ì)
    elif query_len < 50 and doc_count <= 5:
        return 8192
    
    # ë³µì¡í•œ ì¿¼ë¦¬ ë˜ëŠ” ë§ì€ ë¬¸ì„œ
    else:
        return 16384

def _smart_truncate_context(self, text: str, max_tokens: int = 8000) -> str:
    """ìŠ¤ë§ˆíŠ¸ ì»¨í…ìŠ¤íŠ¸ ì ˆë‹¨"""
    
    # í† í° ìˆ˜ ì¶”ì • (í•œê¸€ 1.5ì = 1í† í° ê¸°ì¤€)
    estimated_tokens = len(text) / 1.5
    
    if estimated_tokens <= max_tokens:
        return text
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì ˆë‹¨
    sentences = text.split('.')
    result = []
    current_tokens = 0
    
    for sentence in sentences:
        sentence_tokens = len(sentence) / 1.5
        if current_tokens + sentence_tokens > max_tokens:
            break
        result.append(sentence)
        current_tokens += sentence_tokens
    
    return '.'.join(result) + '.'

----------------------------------------
ğŸ”§ ì„±ëŠ¥ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...
  âœ… ì„±ëŠ¥ ì„¤ì • íŒŒì¼ ìƒì„±: performance_config.yaml

==================================================
âœ… Phase 1 ìµœì í™” ì™„ë£Œ (0.0ì´ˆ)
==================================================

ğŸ“‹ ì ìš© ë°©ë²•:
1. llm_singleton.pyëŠ” ìë™ìœ¼ë¡œ êµì²´ë¨
2. perfect_rag.pyì— ìœ„ ì½”ë“œë“¤ì„ ì¶”ê°€
3. performance_config.yaml ì„¤ì • ì ìš©

âš¡ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ:
- LLM ë¡œë”©: 7.73ì´ˆ â†’ 0.1ì´ˆ (ì¬ì‚¬ìš© ì‹œ)
- ì‘ë‹µ ì‹œê°„: 140ì´ˆ â†’ 30-50ì´ˆ
- ìºì‹œ íˆíŠ¸ìœ¨: 30% â†’ 70%+
