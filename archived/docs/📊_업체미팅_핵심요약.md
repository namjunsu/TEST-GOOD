# 🎯 업체 미팅 - 개발자 핵심 요약

> **미팅 일시**: 2025년 10월 21일 (오늘)
> **내 입장**: 개발자 / 기술 검증이 목적
> **업체 제안**: H100 80GB 서버

---

## ⚡ 30초 요약

```
나: "AI 문서 검색 시스템 개발 중입니다.
    현재 16GB GPU라 7B 모델만 돌아가는데
    답변 품질이 너무 안 좋아요.

    48GB 이상 GPU로 70B 모델 돌려보고
    성능 차이 확인하고 싶습니다.

    1개월 테스트 가능할까요?"
```

---

## 🔧 현재 상황

```
시스템: WSL2 Ubuntu 22.04
GPU: RTX 4000 16GB ⚠️
AI 모델: Qwen2.5-7B (4.4GB)
문서: 812개 PDF

문제:
❌ 표/금액 추출 실패 80%
❌ 복수 문서 비교 불가 (1-2개 한계)
❌ 답변 품질 2/5점

원인: GPU VRAM 16GB → 7B 모델만 가능
```

---

## 🎯 내가 원하는 것

```
목표: 70B 모델 테스트
필요: 48GB+ VRAM

테스트하고 싶은 것:
1. Qwen2.5-70B (42GB 필요)
2. 답변 정확도 측정
3. 표/금액 추출 성능
4. 복수 문서 처리 (8-10개)

기간: 1개월
```

---

## 🚨 업체 제안 - H100 80GB

```
제안 스펙:
GPU: H100 80GB (48GB보다 훨씬 좋음!)
RAM: 512GB
CPU: 24코어 (2x SILVER 4410Y)
OS: 확인 필요 → Ubuntu 22.04 요청

비교:
              현재        필요        업체 제안
GPU VRAM:     16GB       48GB        80GB ⭐
AI 모델:      7B         70B         70B+α
성능:         1x         4x          12x
```

---

## 💬 미팅 대응 (간단 버전)

### 1️⃣ 감사 표현
```
"H100이요? 80GB면 제가 생각한 것보다 훨씬 좋은데요!
 감사합니다."
```

### 2️⃣ 핵심 확인 3개
```
Q1. OS: "Ubuntu 22.04 설치 가능한가요?"
Q2. 기간: "1개월 테스트 가능한가요?"
Q3. 설치: "원격 접속 or 직접 설치 어떻게 되나요?"
```

### 3️⃣ 내 계획
```
"1개월 동안 이렇게 테스트하려고 합니다:

Week 1: 환경 구축 + 7B 모델 마이그레이션
Week 2-3: 70B 모델 테스트
  - Qwen2.5-70B
  - Llama-3.1-70B
  - DeepSeek-R1
Week 4: 성능 측정 + 리포트

주간 테스트 리포트 공유하겠습니다."
```

---

## 🔍 H100으로 테스트할 것

```
1. 모델 비교
   ✅ Qwen2.5-70B (42GB)
   ✅ Llama-3.1-70B (42GB)
   ✅ 70B + 7B 동시 구동 (80GB 활용)

2. 성능 측정
   ✅ 추론 속도 (목표: 30초 이내)
   ✅ 정확도 (목표: 90%+)
   ✅ 표/금액 추출 (목표: 95%+)
   ✅ 문서 동시 처리 (목표: 8-10개)

3. 최적화
   ✅ 프롬프트 엔지니어링
   ✅ Chain-of-Thought 튜닝
   ✅ 컨텍스트 사이즈 조정
   ✅ 양자화 레벨 비교
```

---

## 🛠️ 기술 질문 나오면

### "RAG가 뭔가요?"
```
"검색(Retrieval) + AI 답변(Generation) 조합입니다.

 1. 질문 받음: '중계차 수리비는?'
 2. DB 검색: 관련 문서 5개 찾음
 3. AI 분석: 5개 문서 읽고 답변 생성

 일반 ChatGPT와 차이점은
 우리 내부 문서를 실시간으로 읽는다는 거죠."
```

### "왜 70B가 필요한가요?"
```
"AI 모델 크기 = 성능입니다.

 7B (70억 파라미터): 기본 이해만 가능
 70B (700억 파라미터): 복잡한 추론 가능

 실제 테스트:
 - 7B: '문서를 확인하세요' (실패)
 - 70B: 표에서 금액 추출 + 계산 (성공)

 70B는 7B보다 10배 크지만
 성능은 3-5배 좋습니다."
```

### "GPU VRAM이 왜 중요한가요?"
```
"AI 모델을 VRAM에 올려야 작동합니다.

 70B 모델 = 약 42GB 필요
 현재 16GB → 불가능
 48GB → 가능
 80GB → 여유롭게 가능

 비유하면:
 16GB = 경차 트렁크
 80GB = 트럭 적재함"
```

---

## 📊 용어 치트시트

```
GPU = 그래픽카드 = AI 계산하는 칩
VRAM = GPU 메모리 (클수록 큰 모델 가능)
7B/70B = AI 크기 (파라미터 수)
양자화 = AI 압축 (Q4 = 1/4 크기)
LLM = Large Language Model (큰 AI)
RAG = 검색 + AI 조합
토큰 = AI가 인식하는 단어 단위
컨텍스트 = AI가 한번에 읽는 분량
```

---

## 🎮 그래픽카드 종류 (쉽게!)

### 💎 NVIDIA GPU 라인업 (AI용)

```
┌─────────────────────────────────────────────────────────┐
│ 소비자용 (게이밍 + AI 취미)                               │
├─────────────────────────────────────────────────────────┤
│ RTX 4090      24GB    소비자용 최상위   약 250만원       │
│ RTX 4080      16GB    고급 게이밍       약 180만원       │
│ RTX 4070 Ti   12GB    중급 게이밍       약 120만원       │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ 워크스테이션용 (전문가용 - 우리가 원했던 것!)             │
├─────────────────────────────────────────────────────────┤
│ RTX 6000 Ada  48GB ⭐  프로용 최상위    약 600만원       │
│ RTX 5000 Ada  32GB    프로용 중급       약 400만원       │
│ RTX 4000 Ada  20GB    프로용 입문       약 200만원       │
│ RTX 4000 (현재) 16GB  구형 프로용       -                │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ 데이터센터용 (서버급 - 업체가 제안한 것!)                 │
├─────────────────────────────────────────────────────────┤
│ H100 PCIe     80GB ⭐⭐ AI 최상위      약 1,500만원      │
│ A100          80GB    구형 최상위       약 1,200만원     │
│ A6000         48GB    구형 프로용       약 500만원       │
└─────────────────────────────────────────────────────────┘
```

### 🚗 자동차로 비유하면

```
게이밍용 (RTX 4090):
🏎️ 스포츠카 - 빠르지만 짐 적음 (24GB)
   → 개인용, AI 취미

워크스테이션용 (RTX 6000 Ada):
🚚 트럭 - 적당히 빠르고 짐 많음 (48GB)
   → 회사 업무용, AI 개발
   → 우리가 원한 것!

데이터센터용 (H100):
🚛 대형 트럭 - 엄청 빠르고 짐 엄청 많음 (80GB)
   → 서버실, AI 연구소
   → 업체가 제안한 것!
```

### 📊 실전 비교표

```
                 VRAM   속도   가격        용도
──────────────────────────────────────────────────
RTX 4090        24GB   ⭐⭐⭐   250만원    게이밍
RTX 4000 (현재) 16GB   ⭐⭐    -          구형
RTX 6000 Ada    48GB   ⭐⭐⭐⭐  600만원    AI 개발 ⭐
H100            80GB   ⭐⭐⭐⭐⭐ 1,500만원  AI 연구 ⭐⭐
──────────────────────────────────────────────────

우리 입장:
현재: RTX 4000 16GB (너무 작아서 7B만 됨)
원했던 것: RTX 6000 Ada 48GB (70B 가능)
받을 것: H100 80GB (70B 여유롭게!)
```

### 💡 간단 정리

```
RTX 4000 시리즈 = 게이밍용
   RTX 4090 (24GB) - 소비자용 최고급

RTX 5000/6000 시리즈 = 워크스테이션용
   RTX 6000 Ada (48GB) - 전문가용, AI 개발 ⭐
   RTX 5000 Ada (32GB) - 중급
   RTX 4000 Ada (20GB) - 입문

H100/A100 시리즈 = 데이터센터용
   H100 (80GB) - AI 서버 최상위 ⭐⭐
   A100 (80GB) - 구형 서버용

숫자 규칙:
- 뒤에 숫자 클수록 신형 (4000 → 5000 → 6000)
- Ada = 최신 세대 (2022-2023년)
- H/A = 서버급 (엄청 비싸고 좋음)
```

### 🎯 업체한테 이렇게 말하면 됨

```
업체: "어떤 GPU 쓰시나요?"
나: "지금은 RTX 4000 16GB요. 구형이라 작아요."

업체: "H100 괜찮으세요?"
나: "완벽하죠! H100이면 데이터센터급 최상위잖아요.
    80GB면 저희 용도엔 충분해요!"

업체: "RTX 6000 Ada는 어때요?"
나: "그것도 좋죠. 48GB면 저희가 원했던 스펙이에요.
    워크스테이션용 최상위니까요."

업체: "A100이나 A6000도 있는데요?"
나: "A 시리즈는 구형이죠? H100이나 RTX 6000 Ada가
    더 최신이고 효율 좋을 것 같아요."
```

---

## 🎯 오늘 미팅 목표

```
✅ H100 1개월 테스트 확정
✅ Ubuntu 22.04 설치 확인
✅ 원격 접속 or 직접 설치 확인
✅ 설치 일정 잡기
✅ 테스트 계획 공유

말하지 않을 것:
❌ 비용 얘기
❌ 구매 얘기
❌ ROI 계산
❌ 예산 승인

→ 순수하게 "기술 검증"만 집중!
```

---

## 💬 최종 멘트

```
"정리하면:

1. H100 80GB로 1개월 테스트
2. Ubuntu 22.04 설치 필요
3. 70B 모델 성능 검증
4. 주간 리포트 공유

이렇게 진행하면 될까요?
언제 시작 가능한가요?"
```

---

## 🚫 절대 하지 말 것

```
❌ "예산이..." "비용이..." "돈이..."
❌ "구매는..." "계약은..."
❌ "ROI가..." "절감이..."
❌ "승인이..." "결재가..."

→ 난 개발자다! 기술만 본다!
```

---

## ✅ 핵심만 기억하기

```
1. 현재: 16GB, 7B, 품질 2/5점
2. 필요: 48GB+, 70B, 품질 5/5점
3. 업체: H100 80GB (감사!)
4. 목표: 1개월 기술 검증
5. OS: Ubuntu 22.04 필수

끝!
```
