#!/usr/bin/env python3
"""
ìë™ ë°±ì—… ì‹œìŠ¤í…œ
================
ë°ì´í„° ìë™ ë°±ì—… ë° ë³µêµ¬ ì‹œìŠ¤í…œ
"""

import os
import shutil
import tarfile
import gzip
import json
import hashlib
import schedule
import threading
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import sqlite3
import pickle

class AutoBackupSystem:
    """ìë™ ë°±ì—… ì‹œìŠ¤í…œ"""

    def __init__(self, backup_dir: str = "backups"):
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(parents=True, exist_ok=True)

        # ë°±ì—… ì„¤ì •
        self.config = {
            'max_backups': 10,  # ìµœëŒ€ ë°±ì—… ìˆ˜
            'compression': 'gzip',  # ì••ì¶• ë°©ì‹
            'incremental': True,  # ì¦ë¶„ ë°±ì—… ì‚¬ìš©
            'schedule': {
                'full_backup': 'daily',  # ì „ì²´ ë°±ì—… ì£¼ê¸°
                'incremental_backup': 'hourly',  # ì¦ë¶„ ë°±ì—… ì£¼ê¸°
                'cleanup': 'weekly'  # ì •ë¦¬ ì£¼ê¸°
            }
        }

        # ë°±ì—… ëŒ€ìƒ
        self.backup_targets = {
            'database': [
                '.cache/faiss_index',
                'rag_system/indexes',
                'logs/queries.db'
            ],
            'config': [
                'config.py',
                'config_optimized.py',
                '.env'
            ],
            'cache': [
                '.cache',
                '__pycache__'
            ],
            'logs': [
                'logs/*.log',
                'logs/*.json'
            ]
        }

        # ë°±ì—… íˆìŠ¤í† ë¦¬
        self.backup_history = []
        self.load_backup_history()

        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler_thread = None
        self.running = False

    def start_scheduler(self):
        """ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        if not self.running:
            self.running = True

            # ìŠ¤ì¼€ì¤„ ì„¤ì •
            schedule.every().hour.do(self.incremental_backup)
            schedule.every().day.at("03:00").do(self.full_backup)
            schedule.every().week.do(self.cleanup_old_backups)

            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤ë ˆë“œ ì‹œì‘
            self.scheduler_thread = threading.Thread(target=self._run_scheduler, daemon=True)
            self.scheduler_thread.start()

            print("â° ìë™ ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")

    def stop_scheduler(self):
        """ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        self.running = False
        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=5)
        print("â¹ï¸  ìë™ ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€")

    def _run_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
        while self.running:
            schedule.run_pending()
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬

    def full_backup(self) -> Dict[str, Any]:
        """ì „ì²´ ë°±ì—…"""
        print("ğŸ”„ ì „ì²´ ë°±ì—… ì‹œì‘...")
        start_time = time.time()

        # ë°±ì—… ID ìƒì„±
        backup_id = f"full_{datetime.now():%Y%m%d_%H%M%S}"
        backup_path = self.backup_dir / backup_id

        try:
            # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
            backup_path.mkdir(parents=True, exist_ok=True)

            # ë©”íƒ€ë°ì´í„°
            metadata = {
                'id': backup_id,
                'type': 'full',
                'timestamp': datetime.now().isoformat(),
                'targets': {}
            }

            # ê° ëŒ€ìƒ ë°±ì—…
            for target_name, paths in self.backup_targets.items():
                target_backup_path = backup_path / target_name
                target_backup_path.mkdir(exist_ok=True)

                backed_up_files = []
                for path_pattern in paths:
                    for path in Path('.').glob(path_pattern):
                        if path.exists():
                            dest = target_backup_path / path.name

                            if path.is_dir():
                                shutil.copytree(path, dest, dirs_exist_ok=True)
                            else:
                                shutil.copy2(path, dest)

                            backed_up_files.append(str(path))

                metadata['targets'][target_name] = backed_up_files

            # ì••ì¶•
            archive_path = self._compress_backup(backup_path, backup_id)

            # ê²€ì¦
            checksum = self._calculate_checksum(archive_path)

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata.update({
                'archive_path': str(archive_path),
                'size_mb': archive_path.stat().st_size / (1024**2),
                'checksum': checksum,
                'duration': time.time() - start_time
            })

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            with open(self.backup_dir / f"{backup_id}.json", 'w') as f:
                json.dump(metadata, f, indent=2)

            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.backup_history.append(metadata)
            self.save_backup_history()

            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            shutil.rmtree(backup_path)

            print(f"âœ… ì „ì²´ ë°±ì—… ì™„ë£Œ: {archive_path.name} ({metadata['size_mb']:.2f} MB)")
            return metadata

        except Exception as e:
            print(f"âŒ ë°±ì—… ì‹¤íŒ¨: {e}")
            if backup_path.exists():
                shutil.rmtree(backup_path, ignore_errors=True)
            return None

    def incremental_backup(self) -> Dict[str, Any]:
        """ì¦ë¶„ ë°±ì—… (ë³€ê²½ëœ íŒŒì¼ë§Œ)"""
        print("ğŸ”„ ì¦ë¶„ ë°±ì—… ì‹œì‘...")

        # ë§ˆì§€ë§‰ ë°±ì—… ì´í›„ ë³€ê²½ëœ íŒŒì¼ ì°¾ê¸°
        last_backup = self._get_last_backup()
        if not last_backup:
            # ì²« ë°±ì—…ì€ ì „ì²´ ë°±ì—…
            return self.full_backup()

        last_backup_time = datetime.fromisoformat(last_backup['timestamp'])
        backup_id = f"inc_{datetime.now():%Y%m%d_%H%M%S}"
        backup_path = self.backup_dir / backup_id

        try:
            backup_path.mkdir(parents=True, exist_ok=True)

            metadata = {
                'id': backup_id,
                'type': 'incremental',
                'base_backup': last_backup['id'],
                'timestamp': datetime.now().isoformat(),
                'targets': {}
            }

            # ë³€ê²½ëœ íŒŒì¼ë§Œ ë°±ì—…
            backed_up_count = 0
            for target_name, paths in self.backup_targets.items():
                target_backup_path = backup_path / target_name
                changed_files = []

                for path_pattern in paths:
                    for path in Path('.').glob(path_pattern):
                        if path.exists():
                            # ìˆ˜ì • ì‹œê°„ í™•ì¸
                            mtime = datetime.fromtimestamp(path.stat().st_mtime)
                            if mtime > last_backup_time:
                                target_backup_path.mkdir(parents=True, exist_ok=True)
                                dest = target_backup_path / path.name

                                if path.is_dir():
                                    shutil.copytree(path, dest, dirs_exist_ok=True)
                                else:
                                    shutil.copy2(path, dest)

                                changed_files.append(str(path))
                                backed_up_count += 1

                if changed_files:
                    metadata['targets'][target_name] = changed_files

            if backed_up_count == 0:
                print("â„¹ï¸  ë³€ê²½ëœ íŒŒì¼ ì—†ìŒ")
                shutil.rmtree(backup_path)
                return None

            # ì••ì¶•
            archive_path = self._compress_backup(backup_path, backup_id)

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata.update({
                'archive_path': str(archive_path),
                'size_mb': archive_path.stat().st_size / (1024**2),
                'checksum': self._calculate_checksum(archive_path),
                'files_count': backed_up_count
            })

            # ì €ì¥
            with open(self.backup_dir / f"{backup_id}.json", 'w') as f:
                json.dump(metadata, f, indent=2)

            self.backup_history.append(metadata)
            self.save_backup_history()

            # ì •ë¦¬
            shutil.rmtree(backup_path)

            print(f"âœ… ì¦ë¶„ ë°±ì—… ì™„ë£Œ: {backed_up_count}ê°œ íŒŒì¼ ({metadata['size_mb']:.2f} MB)")
            return metadata

        except Exception as e:
            print(f"âŒ ì¦ë¶„ ë°±ì—… ì‹¤íŒ¨: {e}")
            if backup_path.exists():
                shutil.rmtree(backup_path, ignore_errors=True)
            return None

    def restore_backup(self, backup_id: str, target_dir: str = ".") -> bool:
        """ë°±ì—… ë³µì›"""
        print(f"ğŸ”„ ë°±ì—… ë³µì› ì‹œì‘: {backup_id}")

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_file = self.backup_dir / f"{backup_id}.json"
        if not metadata_file.exists():
            print(f"âŒ ë°±ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {backup_id}")
            return False

        try:
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)

            archive_path = Path(metadata['archive_path'])
            if not archive_path.exists():
                print(f"âŒ ë°±ì—… ì•„ì¹´ì´ë¸Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {archive_path}")
                return False

            # ì²´í¬ì„¬ ê²€ì¦
            if self._calculate_checksum(archive_path) != metadata['checksum']:
                print("âŒ ë°±ì—… íŒŒì¼ ì†ìƒ ê°ì§€")
                return False

            # ì••ì¶• í•´ì œ
            temp_dir = self.backup_dir / f"restore_{backup_id}"
            self._decompress_backup(archive_path, temp_dir)

            # íŒŒì¼ ë³µì›
            target_path = Path(target_dir)
            for root, dirs, files in os.walk(temp_dir):
                for file in files:
                    src = Path(root) / file
                    rel_path = src.relative_to(temp_dir)
                    dest = target_path / rel_path

                    dest.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(src, dest)

            # ì •ë¦¬
            shutil.rmtree(temp_dir)

            print(f"âœ… ë°±ì—… ë³µì› ì™„ë£Œ: {backup_id}")
            return True

        except Exception as e:
            print(f"âŒ ë³µì› ì‹¤íŒ¨: {e}")
            return False

    def _compress_backup(self, source_dir: Path, backup_id: str) -> Path:
        """ë°±ì—… ì••ì¶•"""
        archive_path = self.backup_dir / f"{backup_id}.tar.gz"

        with tarfile.open(archive_path, 'w:gz') as tar:
            tar.add(source_dir, arcname=backup_id)

        return archive_path

    def _decompress_backup(self, archive_path: Path, target_dir: Path):
        """ë°±ì—… ì••ì¶• í•´ì œ"""
        target_dir.mkdir(parents=True, exist_ok=True)

        with tarfile.open(archive_path, 'r:gz') as tar:
            tar.extractall(target_dir)

    def _calculate_checksum(self, file_path: Path) -> str:
        """ì²´í¬ì„¬ ê³„ì‚°"""
        sha256 = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                sha256.update(chunk)
        return sha256.hexdigest()

    def cleanup_old_backups(self):
        """ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        print("ğŸ§¹ ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬...")

        # ë°±ì—… ëª©ë¡ ì •ë ¬ (ë‚ ì§œìˆœ)
        self.backup_history.sort(key=lambda x: x['timestamp'])

        # ì „ì²´ ë°±ì—… ìœ ì§€
        full_backups = [b for b in self.backup_history if b['type'] == 'full']
        if len(full_backups) > self.config['max_backups']:
            to_delete = full_backups[:-self.config['max_backups']]

            for backup in to_delete:
                self._delete_backup(backup)

        # 7ì¼ ì´ìƒëœ ì¦ë¶„ ë°±ì—… ì‚­ì œ
        cutoff_date = datetime.now() - timedelta(days=7)
        inc_backups = [b for b in self.backup_history if b['type'] == 'incremental']

        for backup in inc_backups:
            if datetime.fromisoformat(backup['timestamp']) < cutoff_date:
                self._delete_backup(backup)

        self.save_backup_history()
        print(f"âœ… ì •ë¦¬ ì™„ë£Œ: {len(self.backup_history)}ê°œ ë°±ì—… ìœ ì§€")

    def _delete_backup(self, backup: Dict):
        """ë°±ì—… ì‚­ì œ"""
        try:
            # ì•„ì¹´ì´ë¸Œ ì‚­ì œ
            archive_path = Path(backup['archive_path'])
            if archive_path.exists():
                archive_path.unlink()

            # ë©”íƒ€ë°ì´í„° ì‚­ì œ
            metadata_file = self.backup_dir / f"{backup['id']}.json"
            if metadata_file.exists():
                metadata_file.unlink()

            # íˆìŠ¤í† ë¦¬ì—ì„œ ì œê±°
            self.backup_history.remove(backup)

            print(f"ğŸ—‘ï¸  ë°±ì—… ì‚­ì œ: {backup['id']}")

        except Exception as e:
            print(f"âŒ ë°±ì—… ì‚­ì œ ì‹¤íŒ¨: {e}")

    def _get_last_backup(self) -> Optional[Dict]:
        """ë§ˆì§€ë§‰ ë°±ì—… ì •ë³´"""
        if not self.backup_history:
            return None
        return max(self.backup_history, key=lambda x: x['timestamp'])

    def load_backup_history(self):
        """ë°±ì—… íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        history_file = self.backup_dir / "backup_history.pkl"
        if history_file.exists():
            try:
                with open(history_file, 'rb') as f:
                    self.backup_history = pickle.load(f)
            except:
                self.backup_history = []

    def save_backup_history(self):
        """ë°±ì—… íˆìŠ¤í† ë¦¬ ì €ì¥"""
        history_file = self.backup_dir / "backup_history.pkl"
        with open(history_file, 'wb') as f:
            pickle.dump(self.backup_history, f)

    def get_backup_status(self) -> Dict:
        """ë°±ì—… ìƒíƒœ ì¡°íšŒ"""
        if not self.backup_history:
            return {
                'status': 'no_backups',
                'last_backup': None,
                'total_backups': 0,
                'total_size_mb': 0
            }

        last_backup = self._get_last_backup()
        total_size = sum(b.get('size_mb', 0) for b in self.backup_history)

        return {
            'status': 'healthy',
            'last_backup': {
                'id': last_backup['id'],
                'type': last_backup['type'],
                'timestamp': last_backup['timestamp'],
                'size_mb': last_backup.get('size_mb', 0)
            },
            'total_backups': len(self.backup_history),
            'total_size_mb': total_size,
            'full_backups': len([b for b in self.backup_history if b['type'] == 'full']),
            'incremental_backups': len([b for b in self.backup_history if b['type'] == 'incremental'])
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
backup_system = AutoBackupSystem()


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    print("ğŸ¯ ìë™ ë°±ì—… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")

    # ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    backup = AutoBackupSystem()

    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
    backup.start_scheduler()

    # ì¦‰ì‹œ ì „ì²´ ë°±ì—…
    full_backup = backup.full_backup()
    if full_backup:
        print(f"ì „ì²´ ë°±ì—… ID: {full_backup['id']}")

    # íŒŒì¼ ìˆ˜ì • ì‹œë®¬ë ˆì´ì…˜
    time.sleep(2)
    test_file = Path("test_backup.txt")
    test_file.write_text(f"í…ŒìŠ¤íŠ¸ íŒŒì¼ - {datetime.now()}")

    # ì¦ë¶„ ë°±ì—…
    inc_backup = backup.incremental_backup()
    if inc_backup:
        print(f"ì¦ë¶„ ë°±ì—… ID: {inc_backup['id']}")

    # ë°±ì—… ìƒíƒœ
    status = backup.get_backup_status()
    print(f"\nğŸ“Š ë°±ì—… ìƒíƒœ:")
    print(json.dumps(status, indent=2, ensure_ascii=False))

    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
    if test_file.exists():
        test_file.unlink()

    # ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€
    backup.stop_scheduler()