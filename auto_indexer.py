"""
ìë™ ë¬¸ì„œ ì¸ë±ì‹± ì‹œìŠ¤í…œ
ìƒˆë¡œìš´ PDF/TXT íŒŒì¼ì´ docs í´ë”ì— ì¶”ê°€ë˜ë©´ ìë™ìœ¼ë¡œ ì¸ë±ì‹±
"""

import time
import hashlib
from pathlib import Path
from datetime import datetime
import json
import threading
from typing import Dict, Set

class AutoIndexer:
    """ìë™ ì¸ë±ì‹± í´ë˜ìŠ¤"""
    
    def __init__(self, docs_dir: str = "docs", check_interval: int = 30):
        """
        Args:
            docs_dir: ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            check_interval: ì²´í¬ ê°„ê²© (ì´ˆ)
        """
        self.docs_dir = Path(docs_dir)
        self.check_interval = check_interval
        self.index_file = Path("rag_system/file_index.json")
        self.index_file.parent.mkdir(exist_ok=True)
        
        # íŒŒì¼ ì¸ë±ìŠ¤ ë¡œë“œ
        self.file_index = self._load_index()
        self.is_running = False
        self.thread = None
        
    def _load_index(self) -> Dict:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {
            'files': {},
            'last_update': None
        }
    
    def _save_index(self):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        self.file_index['last_update'] = datetime.now().isoformat()
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.file_index, f, indent=2, ensure_ascii=False)
    
    def _get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def _rename_file_with_underscore(self, file_path: Path) -> Path:
        """íŒŒì¼ëª…ì˜ ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½"""
        if ' ' in file_path.name:
            new_name = file_path.name.replace(' ', '_')
            new_path = file_path.parent / new_name

            # ì¤‘ë³µ íŒŒì¼ëª… ì²´í¬
            if new_path.exists():
                # ì¤‘ë³µë˜ë©´ ë²ˆí˜¸ ì¶”ê°€
                base_name = new_path.stem
                extension = new_path.suffix
                counter = 1
                while new_path.exists():
                    new_path = file_path.parent / f"{base_name}_{counter}{extension}"
                    counter += 1

            try:
                file_path.rename(new_path)
                print(f"ğŸ“ íŒŒì¼ëª… ë³€ê²½: {file_path.name} â†’ {new_path.name}")
                return new_path
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ëª… ë³€ê²½ ì‹¤íŒ¨: {file_path.name} - {e}")
                return file_path
        return file_path

    def check_new_files(self) -> Dict:
        """ìƒˆ íŒŒì¼ ì²´í¬"""
        new_files = []
        modified_files = []
        deleted_files = []

        # í˜„ì¬ íŒŒì¼ ëª©ë¡ (ìƒˆë¡œìš´ í´ë” êµ¬ì¡° í¬í•¨)
        current_files = {}
        search_paths = [self.docs_dir]

        # ì—°ë„ë³„ í´ë” ì¶”ê°€
        for year in range(2014, 2026):
            year_folder = self.docs_dir / f"year_{year}"
            if year_folder.exists():
                search_paths.append(year_folder)

        # ì¹´í…Œê³ ë¦¬ë³„ í´ë” ì¶”ê°€
        for folder in ['category_purchase', 'category_repair', 'category_review',
                      'category_disposal', 'category_consumables']:
            cat_folder = self.docs_dir / folder
            if cat_folder.exists():
                search_paths.append(cat_folder)

        # íŠ¹ë³„ í´ë” ì¶”ê°€
        for folder in ['recent', 'archive', 'assets']:
            special_folder = self.docs_dir / folder
            if special_folder.exists():
                search_paths.append(special_folder)

        # ëª¨ë“  ê²½ë¡œì—ì„œ íŒŒì¼ ê²€ìƒ‰
        for path in search_paths:
            for ext in ['*.pdf', '*.txt']:
                for file_path in path.glob(ext):
                    # íŒŒì¼ëª…ì— ê³µë°±ì´ ìˆìœ¼ë©´ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
                    file_path = self._rename_file_with_underscore(file_path)

                    abs_path = file_path.resolve()
                    if str(abs_path) not in current_files:
                        file_hash = self._get_file_hash(file_path)
                        current_files[str(abs_path)] = {
                            'hash': file_hash,
                            'size': file_path.stat().st_size,
                            'modified': file_path.stat().st_mtime,
                            'added': datetime.now().isoformat()
                        }
        
        # ìƒˆ íŒŒì¼ ê°ì§€
        for file_path, info in current_files.items():
            if file_path not in self.file_index['files']:
                new_files.append(file_path)
                print(f"ğŸ†• ìƒˆ íŒŒì¼ ë°œê²¬: {Path(file_path).name}")
            elif self.file_index['files'][file_path]['hash'] != info['hash']:
                modified_files.append(file_path)
                print(f"ğŸ“ íŒŒì¼ ìˆ˜ì •ë¨: {Path(file_path).name}")
        
        # ì‚­ì œëœ íŒŒì¼ ê°ì§€
        for file_path in self.file_index['files']:
            if file_path not in current_files:
                deleted_files.append(file_path)
                print(f"ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œë¨: {Path(file_path).name}")
        
        # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        if new_files or modified_files or deleted_files:
            self.file_index['files'] = current_files
            self._save_index()
            
            # ì¸ë±ì‹± íŠ¸ë¦¬ê±°
            if new_files or modified_files:
                self._trigger_indexing(new_files + modified_files)
        
        return {
            'new': new_files,
            'modified': modified_files,
            'deleted': deleted_files,
            'total': len(current_files)
        }
    
    def _trigger_indexing(self, files: list):
        """ì¸ë±ì‹± íŠ¸ë¦¬ê±°"""
        print(f"\nğŸ”„ ì¸ë±ì‹± ì‹œì‘: {len(files)}ê°œ íŒŒì¼")
        
        # ì—¬ê¸°ì— ì‹¤ì œ ì¸ë±ì‹± ë¡œì§ í˜¸ì¶œ
        # perfect_ragì˜ ì¸ë±ì‹± ë©”ì„œë“œ í˜¸ì¶œ
        try:
            # Streamlit ì„¸ì…˜ì—ì„œ ê¸°ì¡´ RAG ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            try:
                import streamlit as st
                if 'rag' in st.session_state:
                    # ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ì˜ ìºì‹œë§Œ ì—…ë°ì´íŠ¸
                    print("â™»ï¸ ê¸°ì¡´ RAG ì¸ìŠ¤í„´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸")
                    rag = st.session_state.rag
                    # íŒŒì¼ ëª©ë¡ ê°±ì‹  (ìƒˆë¡œìš´ í´ë” êµ¬ì¡° í¬í•¨)
                    rag.pdf_files = []
                    rag.txt_files = []

                    # ë£¨íŠ¸ í´ë”
                    rag.pdf_files.extend(list(rag.docs_dir.glob('*.pdf')))
                    rag.txt_files.extend(list(rag.docs_dir.glob('*.txt')))

                    # ì—°ë„ë³„ í´ë”
                    for year in range(2014, 2026):
                        year_folder = rag.docs_dir / f"year_{year}"
                        if year_folder.exists():
                            rag.pdf_files.extend(list(year_folder.glob('*.pdf')))
                            rag.txt_files.extend(list(year_folder.glob('*.txt')))

                    # ì¹´í…Œê³ ë¦¬ë³„ í´ë”
                    for folder in ['category_purchase', 'category_repair', 'category_review',
                                 'category_disposal', 'category_consumables']:
                        cat_folder = rag.docs_dir / folder
                        if cat_folder.exists():
                            rag.pdf_files.extend(list(cat_folder.glob('*.pdf')))
                            rag.txt_files.extend(list(cat_folder.glob('*.txt')))

                    # íŠ¹ë³„ í´ë”
                    for folder in ['recent', 'archive', 'assets']:
                        special_folder = rag.docs_dir / folder
                        if special_folder.exists():
                            rag.pdf_files.extend(list(special_folder.glob('*.pdf')))
                            rag.txt_files.extend(list(special_folder.glob('*.txt')))

                    # ì¤‘ë³µ ì œê±°
                    rag.pdf_files = list(set(rag.pdf_files))
                    rag.txt_files = list(set(rag.txt_files))
                    rag.all_files = rag.pdf_files + rag.txt_files

                    # ë©”íƒ€ë°ì´í„° ìºì‹œë§Œ ì¬êµ¬ì¶•
                    rag._build_metadata_cache()
                else:
                    # ì„¸ì…˜ì— ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                    from perfect_rag import PerfectRAG
                    print("ğŸ†• ìƒˆ RAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
                    rag = PerfectRAG()
                    st.session_state.rag = rag
            except ImportError:
                # Streamlit í™˜ê²½ì´ ì•„ë‹Œ ê²½ìš° (CLI ì‹¤í–‰)
                from perfect_rag import PerfectRAG
                print("ğŸ†• ìƒˆ RAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (CLI ëª¨ë“œ)")
                rag = PerfectRAG()
            
            print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ!")
            
            # í†µê³„ ì¶œë ¥
            stats = self.get_statistics()
            print(f"ğŸ“Š ì „ì²´ íŒŒì¼: PDF {stats['pdf_count']}ê°œ, TXT {stats['txt_count']}ê°œ")
            
        except Exception as e:
            print(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
    
    def get_statistics(self) -> Dict:
        """í†µê³„ ì •ë³´"""
        pdf_count = len([f for f in self.file_index['files'] if f.endswith('.pdf')])
        txt_count = len([f for f in self.file_index['files'] if f.endswith('.txt')])
        
        return {
            'total_files': len(self.file_index['files']),
            'pdf_count': pdf_count,
            'txt_count': txt_count,
            'last_update': self.file_index.get('last_update', 'Never')
        }
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_running:
            print("âš ï¸ ì´ë¯¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        self.is_running = True
        print(f"ğŸš€ ìë™ ì¸ë±ì‹± ì‹œì‘ (ì²´í¬ ê°„ê²©: {self.check_interval}ì´ˆ)")
        
        def run():
            while self.is_running:
                try:
                    result = self.check_new_files()
                    if result['new'] or result['modified']:
                        print(f"ğŸ“ ë³€ê²½ ê°ì§€: ìƒˆ íŒŒì¼ {len(result['new'])}ê°œ, ìˆ˜ì • {len(result['modified'])}ê°œ")
                except Exception as e:
                    print(f"âŒ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
                
                time.sleep(self.check_interval)
        
        self.thread = threading.Thread(target=run, daemon=True)
        self.thread.start()
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_running = False
        if self.thread:
            self.thread.join(timeout=5)
        print("â¹ï¸ ìë™ ì¸ë±ì‹± ì¤‘ì§€")
    
    def force_reindex(self):
        """ê°•ì œ ì¬ì¸ë±ì‹±"""
        print("ğŸ”„ ê°•ì œ ì¬ì¸ë±ì‹± ì‹œì‘...")
        self.file_index = {'files': {}, 'last_update': None}
        result = self.check_new_files()
        print(f"âœ… ì¬ì¸ë±ì‹± ì™„ë£Œ: {result['total']}ê°œ íŒŒì¼")
        return result


# ë…ë¦½ ì‹¤í–‰ìš©
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ìë™ ë¬¸ì„œ ì¸ë±ì‹± ì‹œìŠ¤í…œ")
    parser.add_argument('--interval', type=int, default=30, help='ì²´í¬ ê°„ê²© (ì´ˆ)')
    parser.add_argument('--force', action='store_true', help='ê°•ì œ ì¬ì¸ë±ì‹±')
    parser.add_argument('--stats', action='store_true', help='í†µê³„ ì¶œë ¥')
    
    args = parser.parse_args()
    
    indexer = AutoIndexer(check_interval=args.interval)
    
    if args.stats:
        stats = indexer.get_statistics()
        print("ğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
        print(f"  - ì „ì²´ íŒŒì¼: {stats['total_files']}ê°œ")
        print(f"  - PDF: {stats['pdf_count']}ê°œ")
        print(f"  - TXT: {stats['txt_count']}ê°œ")
        print(f"  - ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {stats['last_update']}")
    elif args.force:
        indexer.force_reindex()
    else:
        try:
            indexer.start_monitoring()
            print("ğŸ“Œ ìë™ ì¸ë±ì‹± ì‹¤í–‰ ì¤‘... (Ctrl+Cë¡œ ì¢…ë£Œ)")
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì¢…ë£Œ ì¤‘...")
            indexer.stop_monitoring()