#!/usr/bin/env python3
"""
ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë¡œê¹…í•˜ëŠ” ì‹œìŠ¤í…œ
- ì§ˆë¬¸, ë‹µë³€, ì‹œê°„, ì„±ê³µ ì—¬ë¶€ ë“± ê¸°ë¡
- ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ íŒ¨í„´ ë¶„ì„
- ì—ëŸ¬ íŒ¨í„´ ë¶„ì„
"""

import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
from collections import Counter
import re

class QueryLogger:
    """ì§ˆë¬¸/ë‹µë³€ ë¡œê¹… ë° ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self, log_dir: str = "logs/query_logs"):
        """ë¡œê±° ì´ˆê¸°í™”"""
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # ì¼ë³„ ë¡œê·¸ íŒŒì¼
        today = datetime.now().strftime("%Y%m%d")
        self.log_file = self.log_dir / f"queries_{today}.jsonl"
        
        # í†µê³„ íŒŒì¼
        self.stats_file = self.log_dir / "query_statistics.json"
        
        # ë©”ëª¨ë¦¬ ìºì‹œ (ì‹¤ì‹œê°„ ë¶„ì„ìš©)
        self.recent_queries = []
        self.load_recent_queries()
    
    def log_query(self, 
                  query: str, 
                  answer: str,
                  mode: str = 'auto',
                  success: bool = True,
                  error: Optional[str] = None,
                  processing_time: float = 0.0,
                  metadata: Optional[Dict] = None) -> None:
        """ì§ˆë¬¸ê³¼ ë‹µë³€ ë¡œê¹…"""
        
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'query': query,
            'answer': answer[:500] if answer else None,  # ë‹µë³€ì€ 500ìë§Œ ì €ì¥
            'mode': mode,
            'success': success,
            'error': error,
            'processing_time': processing_time,
            'metadata': metadata or {}
        }
        
        # íŒŒì¼ì— ì €ì¥ (JSONL í˜•ì‹)
        with open(self.log_file, 'a', encoding='utf-8') as f:
            json.dump(log_entry, f, ensure_ascii=False)
            f.write('\n')
        
        # ë©”ëª¨ë¦¬ ìºì‹œ ì—…ë°ì´íŠ¸
        self.recent_queries.append(log_entry)
        if len(self.recent_queries) > 1000:  # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
            self.recent_queries.pop(0)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.update_statistics()
    
    def load_recent_queries(self, days: int = 7) -> None:
        """ìµœê·¼ Nì¼ê°„ì˜ ì§ˆë¬¸ ë¡œë“œ"""
        self.recent_queries = []
        
        # ìµœê·¼ Nì¼ íŒŒì¼ë“¤ ì½ê¸°
        for i in range(days):
            date = datetime.now() - timedelta(days=i)
            date_str = date.strftime("%Y%m%d")
            log_file = self.log_dir / f"queries_{date_str}.jsonl"
            
            if log_file.exists():
                with open(log_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            entry = json.loads(line.strip())
                            self.recent_queries.append(entry)
                        except:
                            continue
    
    def analyze_frequent_patterns(self, top_n: int = 20) -> Dict[str, Any]:
        """ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ íŒ¨í„´ ë¶„ì„"""
        
        if not self.recent_queries:
            return {}
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = Counter()
        entities = Counter()  # ì¥ë¹„ëª…, ìœ„ì¹˜ëª… ë“±
        question_types = Counter()  # ì§ˆë¬¸ ìœ í˜•
        error_patterns = Counter()  # ì—ëŸ¬ íŒ¨í„´
        
        for entry in self.recent_queries:
            query = entry.get('query', '')
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬ ìœ„ì£¼)
            words = re.findall(r'[ê°€-í£]+|[A-Z]+[a-z]*|\d+', query)
            for word in words:
                if len(word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ
                    keywords[word] += 1
            
            # ì¥ë¹„ëª… íŒ¨í„´
            equipment_patterns = [
                r'CCU', r'HP', r'SONY', r'Sony', r'ì¹´ë©”ë¼', r'ë§ˆì´í¬',
                r'ì›Œí¬ìŠ¤í…Œì´ì…˜', r'ì‚¼ê°ëŒ€', r'ë“œë¡ ', r'ë Œì¦ˆ'
            ]
            for pattern in equipment_patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    entities[pattern] += 1
            
            # ìœ„ì¹˜ íŒ¨í„´
            location_patterns = [
                r'ê´‘í™”ë¬¸', r'ìŠ¤íŠœë””ì˜¤', r'ë¶€ì¡°ì •ì‹¤', r'í¸ì§‘ì‹¤',
                r'\d+ì¸µ', r'ëŒ€í˜•', r'ì¤‘í˜•', r'ì†Œí˜•'
            ]
            for pattern in location_patterns:
                if re.search(pattern, query):
                    entities[pattern] += 1
            
            # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
            if 'ëˆ„êµ¬' in query or 'ê¸°ì•ˆì' in query or 'ë‹´ë‹¹ì' in query:
                question_types['ë‹´ë‹¹ì/ê¸°ì•ˆì'] += 1
            elif 'ì–¼ë§ˆ' in query or 'ë¹„ìš©' in query or 'ê°€ê²©' in query:
                question_types['ê°€ê²©/ë¹„ìš©'] += 1
            elif 'ëª‡' in query or 'ê°œìˆ˜' in query or 'ìˆ˜ëŸ‰' in query:
                question_types['ìˆ˜ëŸ‰'] += 1
            elif 'ì–¸ì œ' in query or 'ë‚ ì§œ' in query:
                question_types['ë‚ ì§œ/ì‹œê¸°'] += 1
            elif 'ì–´ë””' in query or 'ìœ„ì¹˜' in query:
                question_types['ìœ„ì¹˜'] += 1
            else:
                question_types['ì¼ë°˜'] += 1
            
            # ì—ëŸ¬ íŒ¨í„´
            if not entry.get('success'):
                error = entry.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                error_patterns[error] += 1
        
        # í†µê³„ ìƒì„±
        statistics = {
            'total_queries': len(self.recent_queries),
            'success_rate': sum(1 for e in self.recent_queries if e.get('success')) / len(self.recent_queries) * 100,
            'avg_processing_time': sum(e.get('processing_time', 0) for e in self.recent_queries) / len(self.recent_queries),
            'top_keywords': keywords.most_common(top_n),
            'top_entities': entities.most_common(top_n),
            'question_types': dict(question_types),
            'error_patterns': error_patterns.most_common(10),
            'analyzed_at': datetime.now().isoformat()
        }
        
        return statistics
    
    def update_statistics(self) -> None:
        """í†µê³„ íŒŒì¼ ì—…ë°ì´íŠ¸"""
        stats = self.analyze_frequent_patterns()
        
        with open(self.stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
    
    def get_frequent_queries(self, top_n: int = 10) -> List[str]:
        """ì‹¤ì œ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ ì¶”ì¶œ"""
        
        if not self.recent_queries:
            return []
        
        # ì§ˆë¬¸ ì •ê·œí™” ë° ì¹´ìš´íŒ…
        query_counter = Counter()
        
        for entry in self.recent_queries:
            query = entry.get('query', '')
            # ê°„ë‹¨í•œ ì •ê·œí™” (ìˆ«ì, ë‚ ì§œ ë“±ì„ ì¼ë°˜í™”)
            normalized = re.sub(r'\d{4}ë…„|\d{1,2}ì›”', 'YYYYë…„', query)
            normalized = re.sub(r'\d+', 'N', normalized)
            query_counter[normalized] += 1
        
        # ìƒìœ„ Nê°œ ë°˜í™˜
        frequent = []
        for query, count in query_counter.most_common(top_n):
            # ì •ê·œí™”ëœ ê²ƒì„ ë‹¤ì‹œ ì‹¤ì œ ì˜ˆì‹œë¡œ ë³€í™˜
            for entry in self.recent_queries:
                if self._normalize_query(entry.get('query', '')) == query:
                    frequent.append({
                        'query': entry.get('query'),
                        'count': count,
                        'normalized': query
                    })
                    break
        
        return frequent
    
    def _normalize_query(self, query: str) -> str:
        """ì§ˆë¬¸ ì •ê·œí™”"""
        normalized = re.sub(r'\d{4}ë…„|\d{1,2}ì›”', 'YYYYë…„', query)
        normalized = re.sub(r'\d+', 'N', normalized)
        return normalized
    
    def get_error_logs(self, limit: int = 50) -> List[Dict]:
        """ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ"""
        errors = []
        
        for entry in reversed(self.recent_queries):
            if not entry.get('success'):
                errors.append({
                    'timestamp': entry.get('timestamp'),
                    'query': entry.get('query'),
                    'error': entry.get('error'),
                    'mode': entry.get('mode')
                })
                
                if len(errors) >= limit:
                    break
        
        return errors
    
    def generate_report(self) -> str:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        stats = self.analyze_frequent_patterns()
        
        report = []
        report.append("=" * 60)
        report.append("ğŸ“Š ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 60)
        report.append(f"\nğŸ“… ë¶„ì„ ì‹œê°„: {stats.get('analyzed_at', 'N/A')}")
        report.append(f"ğŸ“ ì´ ì§ˆë¬¸ ìˆ˜: {stats.get('total_queries', 0)}ê°œ")
        report.append(f"âœ… ì„±ê³µë¥ : {stats.get('success_rate', 0):.1f}%")
        report.append(f"â±ï¸ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats.get('avg_processing_time', 0):.2f}ì´ˆ")
        
        report.append("\n" + "=" * 60)
        report.append("ğŸ”¥ ìì£¼ ê²€ìƒ‰ë˜ëŠ” í‚¤ì›Œë“œ TOP 10")
        report.append("-" * 40)
        for keyword, count in stats.get('top_keywords', [])[:10]:
            report.append(f"  â€¢ {keyword}: {count}íšŒ")
        
        report.append("\n" + "=" * 60)
        report.append("ğŸ¢ ìì£¼ ê²€ìƒ‰ë˜ëŠ” ì¥ë¹„/ìœ„ì¹˜ TOP 10")
        report.append("-" * 40)
        for entity, count in stats.get('top_entities', [])[:10]:
            report.append(f"  â€¢ {entity}: {count}íšŒ")
        
        report.append("\n" + "=" * 60)
        report.append("â“ ì§ˆë¬¸ ìœ í˜• ë¶„í¬")
        report.append("-" * 40)
        for q_type, count in stats.get('question_types', {}).items():
            report.append(f"  â€¢ {q_type}: {count}íšŒ")
        
        if stats.get('error_patterns'):
            report.append("\n" + "=" * 60)
            report.append("âš ï¸ ì£¼ìš” ì—ëŸ¬ íŒ¨í„´")
            report.append("-" * 40)
            for error, count in stats.get('error_patterns', []):
                report.append(f"  â€¢ {error}: {count}íšŒ")
        
        return "\n".join(report)


# ì „ì—­ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
query_logger = QueryLogger()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë° ë¦¬í¬íŠ¸ ìƒì„±
    logger = QueryLogger()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€ (ê°œë°œìš©)
    test_queries = [
        "CCU ì¥ë¹„ ëª‡ ëŒ€?",
        "2024ë…„ ì¤‘ê³„ì°¨ ë³´ìˆ˜ ê¸°ì•ˆì ëˆ„êµ¬?",
        "HP ì›Œí¬ìŠ¤í…Œì´ì…˜ ì‹œë¦¬ì–¼ë²ˆí˜¸",
        "ê´‘í™”ë¬¸ 3ì¸µ ì¥ë¹„ ëª©ë¡",
        "SONY ì¹´ë©”ë¼ ëª‡ ëŒ€ ë³´ìœ ?",
        "ì‹ ìŠ¹ë§Œ ë‹´ë‹¹ ì¥ë¹„",
        "ë·°íŒŒì¸ë” ì¼€ì´ë¸” êµ¬ë§¤ ê±´",
        "ë¬´ì„  ë§ˆì´í¬ êµ¬ë§¤ ë‚´ì—­"
    ]
    
    for query in test_queries:
        logger.log_query(
            query=query,
            answer="í…ŒìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤.",
            mode='auto',
            success=True,
            processing_time=1.5
        )
    
    # ë¦¬í¬íŠ¸ ì¶œë ¥
    print(logger.generate_report())
    
    # ìì£¼ ë¬»ëŠ” ì§ˆë¬¸
    print("\n" + "=" * 60)
    print("ğŸ¯ ì‹¤ì œ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸")
    print("-" * 40)
    for item in logger.get_frequent_queries(5):
        print(f"  â€¢ {item['query']} ({item['count']}íšŒ)")