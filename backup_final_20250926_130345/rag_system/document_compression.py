"""
Document Compression (ë¬¸ì„œ ì••ì¶•) ëª¨ë“ˆ
Advanced RAG ê¸°ë²• ì¤‘ í•˜ë‚˜ë¡œ, ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ LLM ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„± í–¥ìƒ
"""

import logging
import time
import re
import hashlib
from functools import lru_cache
from typing import List, Dict, Any, Tuple
from collections import Counter, defaultdict
import numpy as np

class DocumentCompression:
    """ë¬¸ì„œ ì••ì¶• ëª¨ë“ˆ"""
    
    # ì••ì¶• ì„¤ì • ìƒìˆ˜
    DEFAULT_TARGET_LENGTH = 1000
    DEFAULT_COMPRESSION_RATIO = 0.7
    MIN_SENTENCE_LENGTH = 10
    SHORT_SENTENCE_LENGTH = 20
    LONG_SENTENCE_LENGTH = 200
    
    # ì ìˆ˜ ê°€ì¤‘ì¹˜ ìƒìˆ˜
    QUERY_MATCH_WEIGHT = 5.0
    PATTERN_MATCH_WEIGHT = 2.0
    FIRST_SENTENCE_BONUS = 1.0
    LAST_SENTENCE_BONUS = 0.5
    SHORT_SENTENCE_PENALTY = 0.5
    LONG_SENTENCE_PENALTY = 0.8
    
    # ì •ê·œì‹ íŒ¨í„´
    WORD_PATTERN = r'[ê°€-í£a-zA-Z0-9]+'
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # ì„±ëŠ¥ í†µê³„
        self.compression_count = 0
        self.total_compression_time = 0.0
        self.total_original_chars = 0
        self.total_compressed_chars = 0

        # ì¤‘ìš” í‚¤ì›Œë“œ íŒ¨í„´ (ë°©ì†¡ê¸°ìˆ  ë„ë©”ì¸)
        self.important_patterns = [
            # ê¸ˆì•¡/ìˆ˜ëŸ‰ íŒ¨í„´
            r'(\d{1,3}(?:,\d{3})*)\s*ì›',
            r'(\d+)\s*ê°œ',
            r'(\d+)\s*ëŒ€',
            r'(\d+)\s*ì‹',
            r'ì´\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
            
            # ë‚ ì§œ íŒ¨í„´
            r'20\d{2}[.-]\d{1,2}[.-]\d{1,2}',
            r'20\d{2}ë…„\s*\d{1,2}ì›”',
            
            # ì¥ë¹„ëª… íŒ¨í„´
            r'[A-Z]+-[A-Z0-9]+',  # ëª¨ë¸ëª…
            r'[A-Z]{2,}\s*\d+[A-Z]*',  # HP Z8, LG55 ë“±
            
            # ê¸°ìˆ  ìš©ì–´
            r'ì›Œí¬ìŠ¤í…Œì´ì…˜|ëª¨ë‹ˆí„°|ì¹´ë©”ë¼|ë§ˆì´í¬|ì¼€ì´ë¸”|ì‚¼ê°ëŒ€|ë“œë¡ |ì¡°ëª…',
            r'êµì²´|êµ¬ë§¤|ìˆ˜ë¦¬|ì„¤ì¹˜|ì—…ê·¸ë ˆì´ë“œ|ê°±ì‹ ',
            r'ê²€í† |ê¸°ì•ˆ|ê³„íš|ë³´ê³ |ìš”ì²­'
        ]
        
        # ë¬¸ì¥ ì¤‘ìš”ë„ ê³„ì‚°ìš© í‚¤ì›Œë“œ
        self.high_value_keywords = {
            # ê¸ˆì•¡ ê´€ë ¨ (ë†’ì€ ê°€ì¤‘ì¹˜)
            'ê¸ˆì•¡': 3.0, 'ê°€ê²©': 3.0, 'ë¹„ìš©': 3.0, 'ì´ì•¡': 3.0, 'ì˜ˆì‚°': 3.0,
            'ì›': 2.5,
            
            # ì¥ë¹„ëª… (ë†’ì€ ê°€ì¤‘ì¹˜)  
            'HP': 2.8, 'LG': 2.8, 'Sony': 2.8, 'Canon': 2.8, 'TVLogic': 2.8,
            'Z8': 2.5, 'ì›Œí¬ìŠ¤í…Œì´ì…˜': 2.5, 'ëª¨ë‹ˆí„°': 2.5, 'ì¹´ë©”ë¼': 2.5,
            
            # í–‰ë™ ê´€ë ¨
            'êµì²´': 2.0, 'êµ¬ë§¤': 2.0, 'ìˆ˜ë¦¬': 2.0, 'ì„¤ì¹˜': 2.0,
            'ê²€í† ': 1.8, 'ê¸°ì•ˆ': 1.8, 'ê³„íš': 1.8, 'ìš”ì²­': 1.8,
            
            # ì¼ë°˜ ìš©ì–´
            'ì¥ë¹„': 1.5, 'ê¸°ê¸°': 1.5, 'ë„êµ¬': 1.5, 'ì‹œì„¤': 1.5,
            'ë¬¸ì„œ': 1.2, 'ë³´ê³ ì„œ': 1.2, 'ê¸°ì•ˆì„œ': 1.2
        }
        
        # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ íŒ¨í„´
        self.noise_patterns = [
            r'php\?mode=getPrint.*',
            r'gw\.channela-mt\.com.*',
            r'\[í˜ì´ì§€\s+\d+\]',
            r'^\s*-+\s*$',
            r'^\s*=+\s*$',
            r'^\s*\d+\s*$'
        ]

        # íŒ¨í„´ ì»´íŒŒì¼
        self._compile_patterns()

    def _compile_patterns(self):
        """ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼"""
        # ì¤‘ìš” íŒ¨í„´ ì»´íŒŒì¼
        self.compiled_important = [re.compile(p, re.IGNORECASE) for p in self.important_patterns]

        # ë…¸ì´ì¦ˆ íŒ¨í„´ ì»´íŒŒì¼
        self.compiled_noise = [re.compile(p, re.IGNORECASE | re.MULTILINE) for p in self.noise_patterns]

        # ë‹¨ì–´ ì¶”ì¶œ íŒ¨í„´ ì»´íŒŒì¼
        self.compiled_word_pattern = re.compile(self.WORD_PATTERN)

        # ë¬¸ì¥ ë¶„í•  íŒ¨í„´ ì»´íŒŒì¼
        self.sentence_split_patterns = [
            re.compile(r'[.!?ã€‚]+\s+'),
            re.compile(r'ë‹¤\.\s+'),
            re.compile(r'ìŠµë‹ˆë‹¤\.\s+'),
            re.compile(r'ì…ë‹ˆë‹¤\.\s+'),
            re.compile(r'ìš”\.\s+'),
            re.compile(r'ë‹ˆë‹¤\.\s+')
        ]

        self.logger.info(f"íŒ¨í„´ ì»´íŒŒì¼ ì™„ë£Œ: {len(self.compiled_important)}ê°œ ì¤‘ìš” íŒ¨í„´, {len(self.compiled_noise)}ê°œ ë…¸ì´ì¦ˆ íŒ¨í„´")

    def compress_documents(
        self, 
        documents: List[Dict[str, Any]], 
        query: str,
        target_length: int = None,
        compression_ratio: float = None
    ) -> Dict[str, Any]:
        """ë¬¸ì„œ ì••ì¶• ìˆ˜í–‰"""
        start_time = time.time()
        
        compression_result = {
            'original_documents': documents,
            'compressed_documents': [],
            'compression_stats': {},
            'processing_time': 0.0
        }
        
        try:
            if not documents:
                return compression_result
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            if target_length is None:
                target_length = self.DEFAULT_TARGET_LENGTH
            if compression_ratio is None:
                compression_ratio = self.DEFAULT_COMPRESSION_RATIO
            
            # ê° ë¬¸ì„œë³„ë¡œ ì••ì¶• ìˆ˜í–‰
            for doc in documents:
                compressed_doc = self._compress_single_document(
                    doc, query, target_length, compression_ratio
                )
                compression_result['compressed_documents'].append(compressed_doc)
            
            # ì••ì¶• í†µê³„ ê³„ì‚°
            compression_result['compression_stats'] = self._calculate_compression_stats(
                documents, compression_result['compressed_documents']
            )
            
            compression_result['processing_time'] = time.time() - start_time

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.compression_count += len(documents)
            self.total_compression_time += compression_result['processing_time']
            self.total_original_chars += compression_result['compression_stats']['original_total_chars']
            self.total_compressed_chars += compression_result['compression_stats']['compressed_total_chars']

            self.logger.info(
                f"ë¬¸ì„œ ì••ì¶• ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ "
                f"(ì›ë³¸: {compression_result['compression_stats']['original_total_chars']}ì "
                f"â†’ ì••ì¶•: {compression_result['compression_stats']['compressed_total_chars']}ì, "
                f"ë¹„ìœ¨: {compression_result['compression_stats']['compression_ratio']:.2f})"
            )

            return compression_result
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì••ì¶• ì‹¤íŒ¨: {e}")
            compression_result['processing_time'] = time.time() - start_time
            return compression_result
    
    def _compress_single_document(
        self, 
        document: Dict[str, Any], 
        query: str,
        target_length: int,
        compression_ratio: float
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì„œ ì••ì¶•"""
        
        content = document.get('content', '')
        if not content:
            return document
        
        # 1. ë…¸ì´ì¦ˆ ì œê±°
        cleaned_content = self._remove_noise(content)
        
        # 2. ë¬¸ì¥ ë¶„í•  ë° ì¤‘ìš”ë„ ê³„ì‚°
        sentences = self._split_into_sentences(cleaned_content)
        sentence_scores = self._calculate_sentence_importance(sentences, query)
        
        # 3. ì¤‘ìš”í•œ ë¬¸ì¥ ì„ íƒ
        target_sentences = max(1, int(len(sentences) * compression_ratio))
        selected_sentences = self._select_important_sentences(
            sentences, sentence_scores, target_sentences, target_length
        )
        
        # 4. ì••ì¶•ëœ ë¬¸ì„œ ìƒì„±
        compressed_doc = document.copy()
        compressed_doc['content'] = ' '.join(selected_sentences)
        compressed_doc['original_length'] = len(content)
        compressed_doc['compressed_length'] = len(compressed_doc['content'])
        compressed_doc['compression_ratio'] = compressed_doc['compressed_length'] / len(content) if content else 1.0
        compressed_doc['sentences_kept'] = len(selected_sentences)
        compressed_doc['sentences_total'] = len(sentences)
        
        return compressed_doc
    
    def _remove_noise(self, content: str) -> str:
        """ë…¸ì´ì¦ˆ íŒ¨í„´ ì œê±° (ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)"""
        cleaned = content

        for pattern in self.compiled_noise:
            cleaned = pattern.sub('', cleaned)

        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()

        return cleaned
    
    def _split_into_sentences(self, content: str) -> List[str]:
        """ë¬¸ì¥ ë¶„í•  (ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)"""
        sentences = [content]

        for pattern in self.sentence_split_patterns:
            new_sentences = []
            for sentence in sentences:
                parts = pattern.split(sentence)
                new_sentences.extend([p.strip() for p in parts if p.strip()])
            sentences = new_sentences
        
        # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
        sentences = [s for s in sentences if len(s) >= self.MIN_SENTENCE_LENGTH]
        
        return sentences
    
    def _calculate_sentence_importance(self, sentences: List[str], query: str) -> List[float]:
        """ë¬¸ì¥ë³„ ì¤‘ìš”ë„ ê³„ì‚° (ìµœì í™”)"""
        scores = []
        query_words = set(self.compiled_word_pattern.findall(query.lower()))

        for idx, sentence in enumerate(sentences):
            score = 0.0
            sentence_lower = sentence.lower()

            # 1. ì¿¼ë¦¬ ë‹¨ì–´ ë§¤ì¹­ ì ìˆ˜
            sentence_words = set(self.compiled_word_pattern.findall(sentence_lower))
            query_match_score = len(query_words.intersection(sentence_words)) / len(query_words) if query_words else 0
            score += query_match_score * self.QUERY_MATCH_WEIGHT

            # 2. ì¤‘ìš” íŒ¨í„´ ì ìˆ˜ (ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)
            for pattern in self.compiled_important:
                matches = pattern.findall(sentence)
                score += len(matches) * self.PATTERN_MATCH_WEIGHT

            # 3. ê³ ê°€ì¹˜ í‚¤ì›Œë“œ ì ìˆ˜
            for keyword, weight in self.high_value_keywords.items():
                if keyword.lower() in sentence_lower:
                    score += weight

            # 4. ë¬¸ì¥ ìœ„ì¹˜ ì ìˆ˜ (ì¸ë±ìŠ¤ ì§ì ‘ ì‚¬ìš©)
            position_score = 0.0
            if idx == 0:  # ì²« ë¬¸ì¥
                position_score = self.FIRST_SENTENCE_BONUS
            elif idx == len(sentences) - 1:  # ë§ˆì§€ë§‰ ë¬¸ì¥
                position_score = self.LAST_SENTENCE_BONUS
            score += position_score
            
            # 5. ë¬¸ì¥ ê¸¸ì´ ì •ê·œí™” (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë¬¸ì¥ í˜ë„í‹°)
            length_score = 1.0
            if len(sentence) < self.SHORT_SENTENCE_LENGTH:
                length_score = self.SHORT_SENTENCE_PENALTY
            elif len(sentence) > self.LONG_SENTENCE_LENGTH:
                length_score = self.LONG_SENTENCE_PENALTY
            score *= length_score
            
            scores.append(score)
        
        return scores
    
    def _select_important_sentences(
        self, 
        sentences: List[str], 
        scores: List[float], 
        target_count: int,
        target_length: int
    ) -> List[str]:
        """ì¤‘ìš”í•œ ë¬¸ì¥ ì„ íƒ"""
        
        if not sentences:
            return []
        
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sentence_score_pairs = list(zip(sentences, scores, range(len(sentences))))
        sentence_score_pairs.sort(key=lambda x: x[1], reverse=True)
        
        selected_sentences = []
        selected_indices = []
        total_length = 0
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ë¬¸ì¥ ì„ íƒ
        for sentence, score, idx in sentence_score_pairs:
            if len(selected_sentences) >= target_count:
                break
            if total_length + len(sentence) > target_length:
                break
            
            selected_sentences.append(sentence)
            selected_indices.append(idx)
            total_length += len(sentence)
        
        # ì›ë³¸ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬
        selected_indices.sort()
        ordered_sentences = [sentences[i] for i in selected_indices]
        
        return ordered_sentences if ordered_sentences else sentences[:1]  # ìµœì†Œ 1ê°œëŠ” ë°˜í™˜
    
    def _calculate_compression_stats(
        self, 
        original_docs: List[Dict[str, Any]], 
        compressed_docs: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ì••ì¶• í†µê³„ ê³„ì‚°"""
        
        original_total = sum(len(doc.get('content', '')) for doc in original_docs)
        compressed_total = sum(len(doc.get('content', '')) for doc in compressed_docs)
        
        stats = {
            'total_documents': len(original_docs),
            'original_total_chars': original_total,
            'compressed_total_chars': compressed_total,
            'compression_ratio': compressed_total / original_total if original_total > 0 else 1.0,
            'average_compression_per_doc': compressed_total / len(compressed_docs) if compressed_docs else 0,
            'sentences_stats': {
                'original_total': sum(doc.get('sentences_total', 0) for doc in compressed_docs),
                'kept_total': sum(doc.get('sentences_kept', 0) for doc in compressed_docs),
                'kept_ratio': 0.0
            }
        }
        
        if stats['sentences_stats']['original_total'] > 0:
            stats['sentences_stats']['kept_ratio'] = (
                stats['sentences_stats']['kept_total'] / stats['sentences_stats']['original_total']
            )
        
        return stats

    def get_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        avg_compression_ratio = (self.total_compressed_chars / self.total_original_chars * 100
                                if self.total_original_chars > 0 else 0.0)

        stats = {
            'compression_count': self.compression_count,
            'total_compression_time': self.total_compression_time,
            'avg_compression_time': self.total_compression_time / self.compression_count if self.compression_count > 0 else 0.0,
            'total_original_chars': self.total_original_chars,
            'total_compressed_chars': self.total_compressed_chars,
            'overall_compression_ratio': avg_compression_ratio,
            'compiled_patterns': {
                'important': len(self.compiled_important),
                'noise': len(self.compiled_noise),
                'sentence_split': len(self.sentence_split_patterns)
            }
        }
        return stats

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_document_compression():
    """ë¬¸ì„œ ì••ì¶• í…ŒìŠ¤íŠ¸"""
    print("ğŸ“„ ë¬¸ì„œ ì••ì¶• í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # ë¬¸ì„œ ì••ì¶•ê¸° ì´ˆê¸°í™”
        compressor = DocumentCompression()
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤
        test_documents = [
            {
                'content': """
                HP Z8 ì›Œí¬ìŠ¤í…Œì´ì…˜ ì´ ê¸ˆì•¡ì€ 179,300,000ì›ì…ë‹ˆë‹¤. ì´ ì¥ë¹„ëŠ” ì˜ìƒí¸ì§‘íŒ€ì˜ ê¸°ì¡´ ì¥ë¹„ êµì²´ë¥¼ ìœ„í•´ ê²€í† ë˜ì—ˆìŠµë‹ˆë‹¤.
                ê¸°ì¡´ ì›Œí¬ìŠ¤í…Œì´ì…˜ì˜ ë…¸í›„í™”ë¡œ ì¸í•œ ì•ˆì •ì„± ì €í•˜ ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì›Œí¬ìŠ¤í…Œì´ì…˜ì€ ê³ ì„±ëŠ¥ ì˜ìƒ í¸ì§‘ ì‘ì—…ì— ì í•©í•©ë‹ˆë‹¤.
                php?mode=getPrint&idx=214683&menu_depth=001004003&is_mark=&is_comm... ì´ëŸ° ë¶ˆí•„ìš”í•œ ì •ë³´ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
                gw.channela-mt.com/groupware/approval/approval_form_popup ê°™ì€ URLë„ ìˆìŠµë‹ˆë‹¤.
                [í˜ì´ì§€ 1] ì´ëŸ° í˜ì´ì§€ ì •ë³´ë„ ìˆìŠµë‹ˆë‹¤.
                """,
                'filename': 'hp_workstation.pdf',
                'score': 5.2
            },
            {
                'content': """
                ê´‘í™”ë¬¸ ìŠ¤íŠœë””ì˜¤ ëª¨ë‹ˆí„° êµì²´ ê²€í†  ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ì˜ˆì‚°ì€ 9,760,000ì›ìœ¼ë¡œ ê³„íšë˜ì—ˆìŠµë‹ˆë‹¤.
                ëª¨ë‹ˆí„° 3ëŒ€ì™€ ê´€ë ¨ ë¶€í’ˆë“¤ì„ í¬í•¨í•œ ê²¬ì ì…ë‹ˆë‹¤. LG 55ì¸ì¹˜ ëª¨ë‹ˆí„°ë¥¼ ì„ ì •í•˜ì˜€ìŠµë‹ˆë‹¤.
                ì„¤ì¹˜ ì‘ì—…ì€ 2025ë…„ 1ì›”ì— ì§„í–‰ë  ì˜ˆì •ì…ë‹ˆë‹¤. ê¸°ì¡´ ëª¨ë‹ˆí„°ëŠ” íê¸° ì²˜ë¦¬ë©ë‹ˆë‹¤.
                ì•„ì£¼ ì§§ì€ ë¬¸ì¥. ì´ëŸ° ë¬¸ì¥ë“¤ì€ ì œê±°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                ë§¤ìš° ê¸¸ê³  ë¶ˆí•„ìš”í•œ ë‚´ìš©ì´ í¬í•¨ëœ ë¬¸ì¥ìœ¼ë¡œì„œ ì‹¤ì œ ì˜ë¯¸ìˆëŠ” ì •ë³´ëŠ” ë³„ë¡œ ì—†ì§€ë§Œ ê¸¸ì´ë§Œ ê¸´ ë¬¸ì¥ì…ë‹ˆë‹¤ ì´ëŸ° ê²½ìš°ì—ëŠ” ì••ì¶• ê³¼ì •ì—ì„œ í˜ë„í‹°ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                """,
                'filename': 'monitor_replacement.pdf',
                'score': 4.8
            }
        ]
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_query = "HP Z8 ì›Œí¬ìŠ¤í…Œì´ì…˜ ê°€ê²©"
        
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{test_query}'")
        print(f"ğŸ“„ ë¬¸ì„œ ìˆ˜: {len(test_documents)}")
        
        # ë¬¸ì„œ ì••ì¶• ìˆ˜í–‰
        result = compressor.compress_documents(
            documents=test_documents,
            query=test_query,
            target_length=300,
            compression_ratio=0.6
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ì••ì¶• ê²°ê³¼:")
        print(f"  ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.3f}ì´ˆ")
        print(f"  ì›ë³¸ ì´ ê¸€ì ìˆ˜: {result['compression_stats']['original_total_chars']}ì")
        print(f"  ì••ì¶• í›„ ê¸€ì ìˆ˜: {result['compression_stats']['compressed_total_chars']}ì")
        print(f"  ì••ì¶• ë¹„ìœ¨: {result['compression_stats']['compression_ratio']:.2f}")
        
        print(f"\nğŸ“„ ë¬¸ì„œë³„ ì••ì¶• ê²°ê³¼:")
        for i, compressed_doc in enumerate(result['compressed_documents']):
            print(f"  ë¬¸ì„œ {i+1}: {compressed_doc['filename']}")
            print(f"    ì›ë³¸: {compressed_doc['original_length']}ì â†’ ì••ì¶•: {compressed_doc['compressed_length']}ì")
            print(f"    ë¬¸ì¥: {compressed_doc['sentences_total']}ê°œ â†’ {compressed_doc['sentences_kept']}ê°œ")
            print(f"    ì••ì¶• ë‚´ìš©: {compressed_doc['content'][:100]}...")
            print()
        
        print("âœ… ë¬¸ì„œ ì••ì¶• í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ì••ì¶• í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)
    test_document_compression()