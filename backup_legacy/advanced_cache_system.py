#!/usr/bin/env python3
"""
ê³ ê¸‰ ìºì‹± ì‹œìŠ¤í…œ
- ë¬¸ì„œ ë‚´ìš© ìºì‹±
- ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
- ìë™ ê°±ì‹ 
"""

import json
import hashlib
import time
from pathlib import Path
from typing import Dict, Any, Optional, List
import pickle
import sqlite3
from datetime import datetime, timedelta

class AdvancedCacheSystem:
    def __init__(self, cache_dir: str = "rag_system/cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ì—¬ëŸ¬ ë ˆë²¨ì˜ ìºì‹œ
        self.memory_cache = {}  # ë©”ëª¨ë¦¬ ìºì‹œ (ê°€ì¥ ë¹ ë¦„)
        self.disk_cache_path = self.cache_dir / "advanced_cache.db"
        self.vector_cache_path = self.cache_dir / "vector_cache.pkl"

        # TTL ì„¤ì •
        self.default_ttl = 3600  # 1ì‹œê°„
        self.max_memory_items = 100

        # DB ì´ˆê¸°í™”
        self._init_db()

        # í†µê³„
        self.stats = {
            'hits': 0,
            'misses': 0,
            'memory_hits': 0,
            'disk_hits': 0
        }

    def _init_db(self):
        """SQLite ìºì‹œ DB ì´ˆê¸°í™”"""
        with sqlite3.connect(self.disk_cache_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS cache (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    created_at REAL,
                    expires_at REAL,
                    hit_count INTEGER DEFAULT 0,
                    last_hit REAL
                )
            """)

            conn.execute("""
                CREATE TABLE IF NOT EXISTS document_cache (
                    filepath TEXT PRIMARY KEY,
                    content TEXT,
                    metadata TEXT,
                    embeddings BLOB,
                    file_hash TEXT,
                    indexed_at REAL
                )
            """)

            conn.execute("""
                CREATE TABLE IF NOT EXISTS search_cache (
                    query_hash TEXT PRIMARY KEY,
                    query TEXT,
                    results TEXT,
                    search_type TEXT,
                    created_at REAL,
                    expires_at REAL
                )
            """)
            conn.commit()

    def get_cache_key(self, *args) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        combined = "_".join(str(arg) for arg in args)
        return hashlib.md5(combined.encode()).hexdigest()

    def get(self, key: str, cache_type: str = "general") -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        # 1. ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if key in self.memory_cache:
            entry = self.memory_cache[key]
            if time.time() < entry['expires_at']:
                self.stats['memory_hits'] += 1
                self.stats['hits'] += 1
                return entry['value']
            else:
                del self.memory_cache[key]

        # 2. ë””ìŠ¤í¬ ìºì‹œ í™•ì¸
        with sqlite3.connect(self.disk_cache_path) as conn:
            if cache_type == "document":
                cursor = conn.execute(
                    "SELECT content, metadata FROM document_cache WHERE filepath = ?",
                    (key,)
                )
            elif cache_type == "search":
                cursor = conn.execute(
                    "SELECT results FROM search_cache WHERE query_hash = ? AND expires_at > ?",
                    (key, time.time())
                )
            else:
                cursor = conn.execute(
                    "SELECT value FROM cache WHERE key = ? AND expires_at > ?",
                    (key, time.time())
                )

            row = cursor.fetchone()
            if row:
                self.stats['disk_hits'] += 1
                self.stats['hits'] += 1

                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                if len(self.memory_cache) < self.max_memory_items:
                    self.memory_cache[key] = {
                        'value': json.loads(row[0]) if cache_type != "document" else row[0],
                        'expires_at': time.time() + 300  # 5ë¶„ê°„ ë©”ëª¨ë¦¬ ìºì‹œ
                    }

                return json.loads(row[0]) if cache_type != "document" else {'content': row[0], 'metadata': json.loads(row[1]) if row[1] else {}}

        self.stats['misses'] += 1
        return None

    def set(self, key: str, value: Any, ttl: Optional[int] = None, cache_type: str = "general"):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        if ttl is None:
            ttl = self.default_ttl

        expires_at = time.time() + ttl

        # 1. ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥
        if len(self.memory_cache) >= self.max_memory_items:
            # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = min(self.memory_cache.keys(),
                           key=lambda k: self.memory_cache[k].get('last_access', 0))
            del self.memory_cache[oldest_key]

        self.memory_cache[key] = {
            'value': value,
            'expires_at': expires_at,
            'last_access': time.time()
        }

        # 2. ë””ìŠ¤í¬ ìºì‹œ ì €ì¥
        with sqlite3.connect(self.disk_cache_path) as conn:
            if cache_type == "document":
                conn.execute("""
                    INSERT OR REPLACE INTO document_cache
                    (filepath, content, metadata, file_hash, indexed_at)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    key,
                    value.get('content', ''),
                    json.dumps(value.get('metadata', {}), ensure_ascii=False),
                    value.get('file_hash', ''),
                    time.time()
                ))
            elif cache_type == "search":
                conn.execute("""
                    INSERT OR REPLACE INTO search_cache
                    (query_hash, query, results, search_type, created_at, expires_at)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    key,
                    value.get('query', ''),
                    json.dumps(value.get('results', []), ensure_ascii=False),
                    value.get('search_type', 'general'),
                    time.time(),
                    expires_at
                ))
            else:
                conn.execute("""
                    INSERT OR REPLACE INTO cache
                    (key, value, created_at, expires_at, hit_count)
                    VALUES (?, ?, ?, ?, 0)
                """, (
                    key,
                    json.dumps(value, ensure_ascii=False),
                    time.time(),
                    expires_at
                ))
            conn.commit()

    def cache_document(self, filepath: Path, content: str, metadata: Dict):
        """ë¬¸ì„œ ìºì‹±"""
        file_hash = hashlib.md5(f"{filepath.stat().st_size}_{filepath.stat().st_mtime}".encode()).hexdigest()

        self.set(
            str(filepath),
            {
                'content': content,
                'metadata': metadata,
                'file_hash': file_hash
            },
            ttl=86400,  # 24ì‹œê°„
            cache_type="document"
        )

    def cache_search_result(self, query: str, results: List, search_type: str = "general"):
        """ê²€ìƒ‰ ê²°ê³¼ ìºì‹±"""
        query_hash = self.get_cache_key(query, search_type)

        self.set(
            query_hash,
            {
                'query': query,
                'results': results,
                'search_type': search_type
            },
            ttl=1800,  # 30ë¶„
            cache_type="search"
        )

    def get_search_result(self, query: str, search_type: str = "general") -> Optional[List]:
        """ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        query_hash = self.get_cache_key(query, search_type)
        result = self.get(query_hash, cache_type="search")
        return result.get('results') if result else None

    def clear_expired(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        current_time = time.time()

        # ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
        expired_keys = [k for k, v in self.memory_cache.items()
                       if v['expires_at'] < current_time]
        for key in expired_keys:
            del self.memory_cache[key]

        # ë””ìŠ¤í¬ ìºì‹œ ì •ë¦¬
        with sqlite3.connect(self.disk_cache_path) as conn:
            conn.execute("DELETE FROM cache WHERE expires_at < ?", (current_time,))
            conn.execute("DELETE FROM search_cache WHERE expires_at < ?", (current_time,))
            conn.commit()

    def get_stats(self) -> Dict:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        hit_rate = (self.stats['hits'] / (self.stats['hits'] + self.stats['misses']) * 100) if (self.stats['hits'] + self.stats['misses']) > 0 else 0

        with sqlite3.connect(self.disk_cache_path) as conn:
            cursor = conn.execute("SELECT COUNT(*) FROM cache")
            general_count = cursor.fetchone()[0]

            cursor = conn.execute("SELECT COUNT(*) FROM document_cache")
            doc_count = cursor.fetchone()[0]

            cursor = conn.execute("SELECT COUNT(*) FROM search_cache")
            search_count = cursor.fetchone()[0]

        return {
            'hit_rate': f"{hit_rate:.1f}%",
            'total_hits': self.stats['hits'],
            'total_misses': self.stats['misses'],
            'memory_hits': self.stats['memory_hits'],
            'disk_hits': self.stats['disk_hits'],
            'memory_cache_size': len(self.memory_cache),
            'disk_cache_items': {
                'general': general_count,
                'documents': doc_count,
                'searches': search_count
            }
        }

    def warm_up_cache(self, docs_dir: Path):
        """ìºì‹œ ì˜ˆì—´ (ìì£¼ ì‚¬ìš©í•˜ëŠ” ë¬¸ì„œ ë¯¸ë¦¬ ë¡œë“œ)"""
        print("ğŸ”¥ ìºì‹œ ì˜ˆì—´ ì‹œì‘...")

        # ìµœê·¼ ìˆ˜ì •ëœ ë¬¸ì„œ ìš°ì„ 
        pdf_files = sorted(docs_dir.glob("*.pdf"),
                          key=lambda x: x.stat().st_mtime,
                          reverse=True)[:20]

        for pdf_file in pdf_files:
            # ì—¬ê¸°ì„œ ì‹¤ì œ PDF ì²˜ë¦¬ ë¡œì§ í˜¸ì¶œ
            print(f"  - {pdf_file.name} ìºì‹±...")

        print("âœ… ìºì‹œ ì˜ˆì—´ ì™„ë£Œ")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    cache = AdvancedCacheSystem()

    # ìºì‹œ ì €ì¥
    cache.set("test_key", {"data": "test_value"}, ttl=60)

    # ìºì‹œ ì¡°íšŒ
    result = cache.get("test_key")
    print(f"ìºì‹œ ê²°ê³¼: {result}")

    # í†µê³„
    print(f"ìºì‹œ í†µê³„: {cache.get_stats()}")