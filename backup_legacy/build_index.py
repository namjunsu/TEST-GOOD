#!/usr/bin/env python3
"""
PDF ë¬¸ì„œë“¤ì„ RAG ì‹œìŠ¤í…œì— ì¸ë±ì‹±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import warnings
import logging
from pathlib import Path
from typing import List
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ê¸°
current_dir = Path(__file__).parent.absolute()
sys.path.insert(0, str(current_dir))

# ì„¤ì • íŒŒì¼ import
import config

# PDF ê´€ë ¨ ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", message=".*FontBBox.*")
warnings.filterwarnings("ignore", category=UserWarning, module="pdfplumber")
warnings.filterwarnings("ignore", category=UserWarning, module="pdfminer")

from rag_system.hybrid_search import HybridSearch
from rag_system.metadata_extractor import MetadataExtractor
from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor  # OCR í”„ë¡œì„¸ì„œ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# pdfminer/pdfplumberì˜ ë¶ˆí•„ìš”í•œ ê²½ê³  ìˆ¨ê¸°ê¸°
logging.getLogger('pdfminer').setLevel(logging.ERROR)
logging.getLogger('pdfminer.pdffont').setLevel(logging.ERROR)
logging.getLogger('pdfminer.pdfinterp').setLevel(logging.ERROR)
logging.getLogger('pdfplumber').setLevel(logging.ERROR)

def clean_extracted_text(text: str) -> str:
    """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì •ë¦¬ - OCR ì˜¤ë¥˜ íŒ¨í„´ ê¸°ë°˜ ë™ì  ì²˜ë¦¬"""
    import re
    
    # 1. ë¶„ë¦¬ëœ ëª¨ë¸ëª… íŒ¨í„´ ë³µì› (XXX-999XX í˜•ì‹)
    # ëŒ€ë¬¸ìë“¤ì´ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ í›„ í•˜ì´í”ˆê³¼ ìˆ«ìê°€ ì˜¤ëŠ” íŒ¨í„´
    text = re.sub(r'([A-Z])\s+([A-Z])\s+([A-Z])\s*-?\s*(\d+)\s*([A-Z]+)', r'\1\2\3-\4\5', text)
    
    # 2. ìˆ«ì ë¶„ë¦¬ ë³µì› (ì²œë‹¨ìœ„ ì½¤ë§ˆê°€ ìˆëŠ” ìˆ«ì)
    # 3 , 0 0 0 -> 3,000 í˜•ì‹ ë³µì›
    text = re.sub(r'(\d)\s*,\s*(\d)\s*(\d)\s*(\d)', r'\1,\2\3\4', text)
    text = re.sub(r'(\d)\s+(\d)\s+(\d)\s+(\d)', r'\1\2\3\4', text)  # 4ìë¦¬
    text = re.sub(r'(\d)\s+(\d)\s+(\d)', r'\1\2\3', text)  # 3ìë¦¬
    text = re.sub(r'(\d)\s+(\d)', r'\1\2', text)  # 2ìë¦¬
    
    # 3. OCRë¡œ ë¶„ë¦¬ëœ í•œê¸€ ë³µì› (ì¼ë°˜ íŒ¨í„´)
    # 2-4ê¸€ì í•œê¸€ì´ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ê²½ìš° í•©ì¹˜ê¸°
    def merge_korean(match):
        parts = match.group(0).split()
        merged = ''.join(parts)
        # ì¼ë°˜ì ì¸ í•œê¸€ ë‹¨ì–´ ê¸¸ì´ì¸ 2-5ìë©´ í•©ì¹˜ê¸°
        if 2 <= len(merged) <= 5:
            return merged
        return match.group(0)
    
    text = re.sub(r'([ê°€-í£])\s+([ê°€-í£])(?:\s+([ê°€-í£]))?(?:\s+([ê°€-í£]))?', merge_korean, text)
    
    # 4. ì˜ë¬¸ ë¸Œëœë“œëª… ë³µì› (ëŒ€ë¬¸ì+ìˆ«ì ì¡°í•©)
    text = re.sub(r'([A-Z]{2,})\s+([A-Z]*\d+)', r'\1\2', text)
    
    # 5. ë¶ˆí•„ìš”í•œ ë‹¤ì¤‘ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    return text

def extract_text_from_txt(txt_path: str) -> List[str]:
    """TXT íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    chunks = []
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            full_text = f.read()
        
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunk_size = 2048
        overlap = 256
        
        for i in range(0, len(full_text), chunk_size - overlap):
            chunk = full_text[i:i + chunk_size]
            if chunk.strip():
                chunks.append(chunk.strip())
        
        return chunks
    except Exception as e:
        logger.error(f"TXT í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ {txt_path}: {e}")
        return []

def extract_text_from_pdf(pdf_path: str, use_enhanced_ocr: bool = True) -> List[str]:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ - í–¥ìƒëœ OCR í¬í•¨"""
    try:
        chunks = []
        full_text = ""
        
        if use_enhanced_ocr:
            # í–¥ìƒëœ OCR í”„ë¡œì„¸ì„œ ì‚¬ìš© (Tesseract í¬í•¨)
            try:
                enhanced_ocr = EnhancedOCRProcessor()
                full_text, metadata = enhanced_ocr.extract_text_with_ocr(pdf_path)
                
                # ë©”íƒ€ë°ì´í„° ë¡œê¹…
                if metadata.get('ocr_performed'):
                    logger.info(f"  OCR ìˆ˜í–‰: {metadata['image_count']}ê°œ ì´ë¯¸ì§€, {metadata['ocr_text_length']}ì ì¶”ì¶œ")
            except Exception as e:
                logger.warning(f"í–¥ìƒëœ OCR ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜: {e}")
                use_enhanced_ocr = False
        
        # ê¸°ë³¸ ë°©ì‹ í´ë°± ë˜ëŠ” í–¥ìƒëœ OCR ì‹¤íŒ¨ ì‹œ
        if not use_enhanced_ocr or not full_text:
            import pdfplumber
            # EnhancedOCRProcessorì˜ í›„ì²˜ë¦¬ ë©”ì„œë“œ ì‚¬ìš©
            enhanced_ocr = EnhancedOCRProcessor()
            
            with pdfplumber.open(pdf_path) as pdf:
                full_text = ""
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    if page_text and page_text.strip():
                        # OCR í›„ì²˜ë¦¬ ì ìš© (HP Z8, ê¸ˆì•¡ ë“± ìˆ˜ì •)
                        corrected_text = enhanced_ocr._post_process_ocr(page_text)
                        # ì¶”ê°€ í…ìŠ¤íŠ¸ ì •ë¦¬ ì ìš©
                        cleaned_text = clean_extracted_text(corrected_text)
                        full_text += f"\n[í˜ì´ì§€ {page_num + 1}]\n{cleaned_text}\n"
                    
            # PyPDF2ë¡œ fallback ì‹œë„ (OCR ê²€ì¶œê¸° í¬í•¨)
            if not full_text.strip():
                logger.warning(f"pdfplumber ì¶”ì¶œ ì‹¤íŒ¨, PyPDF2ë¡œ ì¬ì‹œë„: {pdf_path}")
                import PyPDF2
                with open(pdf_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    for page_num, page in enumerate(pdf_reader.pages):
                        page_text = page.extract_text()
                        if page_text.strip():
                            # OCR í›„ì²˜ë¦¬ ì ìš© (HP Z8, ê¸ˆì•¡ ë“± ìˆ˜ì •)
                            corrected_text = enhanced_ocr._post_process_ocr(page_text)
                            cleaned_text = clean_extracted_text(corrected_text)
                            full_text += f"\n[í˜ì´ì§€ {page_num + 1}]\n{cleaned_text}\n"
            
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ë” í° ì²­í¬ë¡œ ê°œì„ )
        chunk_size = 2048  # 1024 -> 2048ë¡œ ì¦ê°€ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        overlap = 256      # 128 -> 256ìœ¼ë¡œ ì¦ê°€ (ë¬¸ë§¥ ì—°ê²°ì„± í–¥ìƒ)
        
        for i in range(0, len(full_text), chunk_size - overlap):
            chunk = full_text[i:i + chunk_size]
            if chunk.strip():
                chunks.append(chunk.strip())
        
        return chunks
        
    except Exception as e:
        logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ {pdf_path}: {e}")
        return []

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("RAG System Index Building Started")
    print("=" * 50)
    
    try:
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        hybrid_search = HybridSearch()
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° ì´ˆê¸°í™”
        metadata_extractor = MetadataExtractor()
        
        # PDFì™€ TXT íŒŒì¼ë“¤ ì°¾ê¸° - config ì‚¬ìš©
        docs_dir = Path(config.DOCS_DIR)
        pdf_files = list(docs_dir.glob('*.pdf'))
        txt_files = list(docs_dir.glob('*.txt'))
        all_files = pdf_files + txt_files
        
        print(f"ğŸ“„ ë°œê²¬ëœ ë¬¸ì„œ íŒŒì¼: {len(all_files)}ê°œ (PDF: {len(pdf_files)}, TXT: {len(txt_files)})")
        
        total_chunks = 0
        processed_files = 0
        
        for doc_file in all_files:
            print(f"\nğŸ”„ ì²˜ë¦¬ ì¤‘: {doc_file.name}")
            
            try:
                # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if doc_file.suffix.lower() == '.pdf':
                    chunks = extract_text_from_pdf(str(doc_file))
                elif doc_file.suffix.lower() == '.txt':
                    chunks = extract_text_from_txt(str(doc_file))
                else:
                    continue
                
                if chunks:
                    # ì „ì²´ í…ìŠ¤íŠ¸ë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    full_text = ' '.join(chunks)
                    extracted_metadata = metadata_extractor.extract_metadata(full_text, str(doc_file))
                    
                    # ê²€ìƒ‰ ê°€ëŠ¥í•œ ì²­í¬ ìƒì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)
                    enhanced_chunks = []
                    metadatas = []
                    
                    for i, chunk in enumerate(chunks):
                        # ë©”íƒ€ë°ì´í„°ë¥¼ ê²€ìƒ‰ í…ìŠ¤íŠ¸ì— í¬í•¨ (í•µì‹¬!)
                        metadata_text = f"\n[ë©”íƒ€ë°ì´í„°] íŒŒì¼ëª…: {extracted_metadata.filename}"
                        if extracted_metadata.author:
                            metadata_text += f" ê¸°ì•ˆì: {extracted_metadata.author}"
                        if extracted_metadata.date:
                            metadata_text += f" ì‘ì„±ì¼: {extracted_metadata.date}"
                        if extracted_metadata.amount:
                            metadata_text += f" ê¸ˆì•¡: {extracted_metadata.amount:,}ì›"
                        if extracted_metadata.department:
                            metadata_text += f" ë¶€ì„œ: {extracted_metadata.department}"
                        
                        # ì²­í¬ì— ë©”íƒ€ë°ì´í„° í…ìŠ¤íŠ¸ ì¶”ê°€
                        enhanced_chunk = chunk + metadata_text
                        enhanced_chunks.append(enhanced_chunk)
                        
                        # ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±
                        chunk_metadata = {
                            'source': extracted_metadata.filename,  # source í•„ë“œ ì¶”ê°€ (ì¤‘ìš”!)
                            'filename': extracted_metadata.filename,
                            'file_path': extracted_metadata.file_path,
                            'doc_type': extracted_metadata.doc_type,
                            'date': extracted_metadata.date,
                            'author': extracted_metadata.author,
                            'department': extracted_metadata.department,
                            'amount': extracted_metadata.amount,
                            'chunk_id': f"{doc_file.stem}_{i}",
                            'chunk_index': i,
                            'content': chunk
                        }
                        metadatas.append(chunk_metadata)
                    
                    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì— ì¶”ê°€ (ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©)
                    hybrid_search.add_documents(enhanced_chunks, metadatas)
                    
                    total_chunks += len(chunks)
                    processed_files += 1
                    
                    print(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ë¨")
                else:
                    print(f"   âš ï¸  í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"   âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        print(f"\nğŸ“Š ì¸ë±ì‹± ì™„ë£Œ!")
        print(f"   - ì²˜ë¦¬ëœ íŒŒì¼: {processed_files}/{len(all_files)}ê°œ")
        print(f"   - ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
        
        # ì¸ë±ìŠ¤ ì €ì¥
        print(f"\nğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
        hybrid_search.save_indexes()
        print(f"   âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ê²€ìƒ‰...")
        test_results = hybrid_search.search("í•€ë§ˆì´í¬ ê°€ê²©", top_k=3)
        print(f"   âœ… í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼: {len(test_results.get('fused_results', []))}ê°œ")
        
        if test_results.get('fused_results'):
            first_result = test_results['fused_results'][0]
            print(f"   ğŸ“„ ì²« ë²ˆì§¸ ê²°ê³¼: {first_result.get('filename', 'N/A')}")
        
        print(f"\nğŸ‰ ì¸ë±ìŠ¤ êµ¬ì¶•ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"\nâŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()