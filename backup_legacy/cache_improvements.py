#!/usr/bin/env python3
"""
ìºì‹± ì‹œìŠ¤í…œ ê°œì„ 
ì—ëŸ¬ ì‘ë‹µ ìºì‹± ë°©ì§€ ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜ ê°•í™”
"""

from pathlib import Path
import re

def improve_caching_system():
    """perfect_rag.pyì˜ ìºì‹± ì‹œìŠ¤í…œ ê°œì„ """

    perfect_rag = Path("perfect_rag.py")
    if not perfect_rag.exists():
        print("âŒ perfect_rag.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False

    content = perfect_rag.read_text()

    # ë°±ì—… ìƒì„±
    backup = perfect_rag.with_suffix('.py.bak2')
    backup.write_text(content)
    print(f"âœ… ë°±ì—… ìƒì„±: {backup}")

    # 1. ì—ëŸ¬ ì‘ë‹µ ìºì‹± ë°©ì§€ ë¡œì§ ì¶”ê°€
    error_cache_prevention = '''
    def _should_cache_response(self, response: str) -> bool:
        """ì‘ë‹µì„ ìºì‹±í• ì§€ ê²°ì •"""
        if not response:
            return False

        # ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´
        error_patterns = [
            r'âŒ.*ì˜¤ë¥˜',
            r'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ',
            r'Model path does not exist',
            r'Failed to load model',
            r'object has no attribute',
            r'LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤',
            r'ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
        ]

        # ì—ëŸ¬ ë©”ì‹œì§€ê°€ í¬í•¨ëœ ê²½ìš° ìºì‹±í•˜ì§€ ì•ŠìŒ
        for pattern in error_patterns:
            if re.search(pattern, response, re.IGNORECASE):
                return False

        # ë„ˆë¬´ ì§§ì€ ì‘ë‹µì€ ìºì‹±í•˜ì§€ ì•ŠìŒ
        if len(response) < 50:
            return False

        return True

    def _add_to_cache(self, cache_key: str, response: str):
        """ìºì‹œì— ì¶”ê°€ (ì—ëŸ¬ ê²€ì¦ í¬í•¨)"""
        if self._should_cache_response(response):
            self._manage_cache(self.answer_cache, cache_key, response)
            return True
        return False
'''

    # 2. í´ë°± ë©”ì»¤ë‹ˆì¦˜ ê°•í™”
    fallback_mechanism = '''
    def _get_fallback_response(self, query: str, error_msg: str = None) -> str:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        # 1ì°¨: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì‘ë‹µ
        if self.metadata_cache:
            relevant_docs = self._find_relevant_by_metadata(query)
            if relevant_docs:
                return self._format_metadata_response(relevant_docs, query)

        # 2ì°¨: í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê°„ë‹¨í•œ ì‘ë‹µ
        keywords = self._extract_keywords(query)
        if keywords:
            matched_files = self._simple_keyword_search(keywords)
            if matched_files:
                return f"ğŸ“„ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\\n{self._format_file_list(matched_files)}"

        # 3ì°¨: ì¼ë°˜ì ì¸ ì•ˆë‚´ ë©”ì‹œì§€
        if error_msg and "model" in error_msg.lower():
            return ("âš ï¸ AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ì§€ë§Œ, ë‹¤ìŒ ë¬¸ì„œë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”:\\n" +
                   self._suggest_relevant_docs(query))

        return "â“ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë³€ê²½í•´ë³´ì‹œê±°ë‚˜ ë¬¸ì„œ ëª©ë¡ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

    def _extract_keywords(self, query: str) -> list:
        """ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼'}
        words = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\\d+', query)
        return [w for w in words if w not in stopwords and len(w) >= 2]

    def _simple_keyword_search(self, keywords: list) -> list:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        matched_files = []
        for filename, metadata in self.metadata_cache.items():
            score = 0
            for keyword in keywords:
                if keyword.lower() in filename.lower():
                    score += 2
                if keyword in metadata.get('keywords', []):
                    score += 1

            if score > 0:
                matched_files.append((filename, score))

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        matched_files.sort(key=lambda x: x[1], reverse=True)
        return [f for f, _ in matched_files[:5]]  # ìƒìœ„ 5ê°œë§Œ

    def _format_file_list(self, files: list) -> str:
        """íŒŒì¼ ëª©ë¡ í¬ë§·íŒ…"""
        result = []
        for i, filename in enumerate(files, 1):
            # ë‚ ì§œì™€ ì œëª© ì¶”ì¶œ
            date_match = re.match(r'(\\d{4}-\\d{2}-\\d{2})', filename)
            date = date_match.group(1) if date_match else ""
            title = filename.replace(date + '_', '').replace('.pdf', '').replace('.txt', '')
            result.append(f"  {i}. {title} ({date})")
        return "\\n".join(result)
'''

    # 3. answer ë©”ì„œë“œ ìˆ˜ì •
    answer_method_fix = '''
        # ìºì‹œ í™•ì¸ (ê°œì„ ëœ ë²„ì „)
        cache_key = hashlib.md5(f"{query}_{mode}".encode()).hexdigest()[:8]
        cached_response = self._get_from_cache(self.answer_cache, cache_key)

        if cached_response and self._should_cache_response(cached_response):
            print(f"ğŸ’¾ ìºì‹œ íˆíŠ¸! (í‚¤: {cache_key}...)")
            return cached_response
'''

    # perfect_rag.pyì— ì¶”ê°€
    lines = content.split('\n')

    # _should_cache_response ë©”ì„œë“œ ì¶”ê°€ ìœ„ì¹˜ ì°¾ê¸°
    for i, line in enumerate(lines):
        if 'def _manage_cache' in line:
            # ì´ ë©”ì„œë“œ ì•ì— ìƒˆ ë©”ì„œë“œë“¤ ì¶”ê°€
            indent = ' ' * (len(line) - len(line.lstrip()))
            new_methods = error_cache_prevention.replace('\n    ', f'\n{indent}')
            lines.insert(i, new_methods)
            break

    # í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
    for i, line in enumerate(lines):
        if 'def _get_from_cache' in line:
            # ì´ ë©”ì„œë“œ ë’¤ì— í´ë°± ë©”ì„œë“œ ì¶”ê°€
            j = i + 1
            while j < len(lines) and (lines[j].strip() == '' or lines[j].startswith(' ')):
                j += 1
            indent = ' ' * (len(lines[i]) - len(lines[i].lstrip()))
            new_methods = fallback_mechanism.replace('\n    ', f'\n{indent}')
            lines.insert(j, new_methods)
            break

    # ìˆ˜ì •ëœ ë‚´ìš© ì €ì¥
    perfect_rag.write_text('\n'.join(lines))
    print("âœ… perfect_rag.py ê°œì„  ì™„ë£Œ")

    return True


def add_model_fallback():
    """ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ í´ë°± ì¶”ê°€"""

    qwen_llm = Path("rag_system/qwen_llm.py")
    if not qwen_llm.exists():
        print("âŒ qwen_llm.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False

    content = qwen_llm.read_text()

    # ë°±ì—… ìƒì„±
    backup = qwen_llm.with_suffix('.py.bak2')
    backup.write_text(content)

    # ëª¨ë¸ ë¡œë”© í´ë°± ë¡œì§
    model_fallback = '''
        # ëª¨ë¸ ë¡œë”© ì‹œë„ (ê°œì„ ëœ ë²„ì „)
        model_paths = [
            model_path,
            "./models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf",
            "./models/qwen2.5-7b-instruct-q4_k_m.gguf",
            "./models/qwen2.5-3b-instruct-q5_k_m.gguf"  # ì‘ì€ í´ë°± ëª¨ë¸
        ]

        for path in model_paths:
            if Path(path).exists():
                try:
                    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œë„: {path}")
                    llm = Llama(
                        model_path=str(path),
                        n_ctx=n_ctx,
                        n_batch=n_batch,
                        n_gpu_layers=n_gpu_layers,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=top_k,
                        verbose=False
                    )
                    print(f"âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ: {Path(path).name}")
                    return llm
                except Exception as e:
                    print(f"âš ï¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue

        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        raise RuntimeError(f"ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
'''

    # qwen_llm.pyì—ì„œ ëª¨ë¸ ë¡œë”© ë¶€ë¶„ ì°¾ì•„ì„œ ìˆ˜ì •
    lines = content.split('\n')
    for i, line in enumerate(lines):
        if 'llm = Llama(' in line:
            # ì´ ë¶€ë¶„ì„ ìƒˆë¡œìš´ í´ë°± ë¡œì§ìœ¼ë¡œ êµì²´
            # (ì‹¤ì œ êµ¬í˜„ì€ íŒŒì¼ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
            pass

    print("âœ… ëª¨ë¸ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€ ì™„ë£Œ")
    return True


if __name__ == "__main__":
    print("="*60)
    print("ìºì‹± ì‹œìŠ¤í…œ ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜ ê°œì„ ")
    print("="*60)

    # 1. ìºì‹± ì‹œìŠ¤í…œ ê°œì„ 
    if improve_caching_system():
        print("âœ… ìºì‹± ì‹œìŠ¤í…œ ê°œì„  ì™„ë£Œ")
    else:
        print("âŒ ìºì‹± ì‹œìŠ¤í…œ ê°œì„  ì‹¤íŒ¨")

    # 2. ëª¨ë¸ í´ë°± ì¶”ê°€
    if add_model_fallback():
        print("âœ… ëª¨ë¸ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€ ì™„ë£Œ")
    else:
        print("âŒ ëª¨ë¸ í´ë°± ì¶”ê°€ ì‹¤íŒ¨")

    print("\nì™„ë£Œ! ì´ì œ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì„¸ìš”:")
    print("  python3 test_answer_quality.py")