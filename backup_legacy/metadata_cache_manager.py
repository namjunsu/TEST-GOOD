#!/usr/bin/env python3
"""
ë©”íƒ€ë°ì´í„° ìºì‹œ ê´€ë¦¬ì
ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ì¬ì‹œì‘ ì‹œ ë¹ ë¥¸ ë¡œë”© ì§€ì›
"""

import json
import hashlib
import time
from pathlib import Path
from typing import Dict, Any

class MetadataCacheManager:
    def __init__(self, cache_dir: str = "rag_system/cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file = self.cache_dir / "document_metadata.json"
        self.index_file = self.cache_dir / "document_index.json"

    def get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ì˜ í•´ì‹œê°’ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)"""
        stat = file_path.stat()
        # íŒŒì¼ëª…, í¬ê¸°, ìˆ˜ì •ì‹œê°„ì„ ì¡°í•©
        unique_str = f"{file_path.name}_{stat.st_size}_{stat.st_mtime}"
        return hashlib.md5(unique_str.encode()).hexdigest()

    def load_cache(self) -> Dict[str, Any]:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_cache(self, metadata: Dict[str, Any]):
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        # Path ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        serializable_metadata = {}
        for key, value in metadata.items():
            if isinstance(value, dict):
                serializable_value = {}
                for k, v in value.items():
                    if isinstance(v, Path):
                        serializable_value[k] = str(v)
                    else:
                        serializable_value[k] = v
                serializable_metadata[key] = serializable_value
            else:
                serializable_metadata[key] = value

        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_metadata, f, ensure_ascii=False, indent=2)

    def needs_update(self, file_path: Path, cached_metadata: Dict) -> bool:
        """íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        if not cached_metadata:
            return True

        current_hash = self.get_file_hash(file_path)
        cached_hash = cached_metadata.get('file_hash')

        return current_hash != cached_hash

    def update_file_metadata(self, file_path: Path, metadata: Dict):
        """ë‹¨ì¼ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        metadata['file_hash'] = self.get_file_hash(file_path)
        metadata['last_indexed'] = time.time()
        return metadata

    def build_quick_index(self, docs_dir: Path) -> Dict:
        """ë¹ ë¥¸ ì¸ë±ìŠ¤ ìƒì„± (íŒŒì¼ ëª©ë¡ë§Œ)"""
        index = {
            'pdf_files': [],
            'txt_files': [],
            'total_size': 0,
            'last_updated': time.time()
        }

        for pdf in docs_dir.glob('*.pdf'):
            index['pdf_files'].append({
                'name': pdf.name,
                'size': pdf.stat().st_size,
                'modified': pdf.stat().st_mtime
            })
            index['total_size'] += pdf.stat().st_size

        for txt in docs_dir.glob('*.txt'):
            index['txt_files'].append({
                'name': txt.name,
                'size': txt.stat().st_size,
                'modified': txt.stat().st_mtime
            })
            index['total_size'] += txt.stat().st_size

        # ì¸ë±ìŠ¤ ì €ì¥
        with open(self.index_file, 'w') as f:
            json.dump(index, f, indent=2)

        return index

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    manager = MetadataCacheManager()

    print("ğŸ“Š ìºì‹œ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    print("="*50)

    # ë¹ ë¥¸ ì¸ë±ìŠ¤ ìƒì„±
    docs_dir = Path("docs")
    index = manager.build_quick_index(docs_dir)

    print(f"âœ… PDF íŒŒì¼: {len(index['pdf_files'])}ê°œ")
    print(f"âœ… TXT íŒŒì¼: {len(index['txt_files'])}ê°œ")
    print(f"âœ… ì „ì²´ í¬ê¸°: {index['total_size'] / 1024 / 1024:.1f} MB")
    print(f"âœ… ì¸ë±ìŠ¤ íŒŒì¼: {manager.index_file}")