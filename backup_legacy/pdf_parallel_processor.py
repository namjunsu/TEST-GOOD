"""
PDF Parallel Processor - PDF íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“ˆ
ì—¬ëŸ¬ PDF íŒŒì¼ê³¼ í˜ì´ì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
"""

import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import pdfplumber
import time
import logging
from functools import partial
import hashlib

# OCR ì²˜ë¦¬ë¥¼ ìœ„í•œ import
try:
    from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False

logger = logging.getLogger(__name__)


class PDFParallelProcessor:
    """PDF ë³‘ë ¬ ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, config_manager=None):
        """
        ì´ˆê¸°í™”
        Args:
            config_manager: ConfigManager ì¸ìŠ¤í„´ìŠ¤
        """
        self.config = config_manager

        # ì„¤ì •ê°’ ë¡œë“œ
        if self.config:
            self.max_workers = self.config.get('parallel_processing.pdf.max_workers', 4)
            self.page_chunk_size = self.config.get('parallel_processing.pdf.page_chunk_size', 5)
            self.timeout_per_file = self.config.get('parallel_processing.pdf.timeout_per_file', 30)
            self.enable_ocr = self.config.get('error_handling.fallback.enable_ocr_fallback', True)
        else:
            # ê¸°ë³¸ê°’
            self.max_workers = min(mp.cpu_count(), 4)
            self.page_chunk_size = 5
            self.timeout_per_file = 30
            self.enable_ocr = True

        # OCR í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.ocr_processor = EnhancedOCRProcessor() if OCR_AVAILABLE and self.enable_ocr else None

        # ìºì‹œ
        self.extraction_cache = {}

    def process_multiple_pdfs(self, pdf_paths: List[Path],
                            progress_callback=None) -> Dict[str, Dict]:
        """
        ì—¬ëŸ¬ PDFë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬

        Args:
            pdf_paths: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜

        Returns:
            {íŒŒì¼ê²½ë¡œ: {text, page_count, metadata, error}} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        total_files = len(pdf_paths)
        completed = 0

        logger.info(f"ğŸš€ {total_files}ê°œ PDF íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì›Œì»¤: {self.max_workers})")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ê° PDFì— ëŒ€í•œ Future ìƒì„±
            future_to_path = {
                executor.submit(self._process_single_pdf_safe, path): path
                for path in pdf_paths
            }

            # Futureê°€ ì™„ë£Œë˜ë©´ ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_path):
                path = future_to_path[future]
                try:
                    result = future.result(timeout=self.timeout_per_file)
                    results[str(path)] = result
                    completed += 1

                    if progress_callback:
                        progress_callback(completed, total_files)

                    logger.debug(f"âœ… ì™„ë£Œ ({completed}/{total_files}): {path.name}")

                except TimeoutError:
                    results[str(path)] = {
                        "error": "ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼",
                        "text": "",
                        "page_count": 0
                    }
                    logger.warning(f"â±ï¸ íƒ€ì„ì•„ì›ƒ: {path.name}")

                except Exception as e:
                    results[str(path)] = {
                        "error": str(e),
                        "text": "",
                        "page_count": 0
                    }
                    logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {path.name} - {e}")

        logger.info(f"âœ… PDF ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {completed}/{total_files} ì„±ê³µ")
        return results

    def _process_single_pdf_safe(self, pdf_path: Path) -> Dict:
        """
        ë‹¨ì¼ PDF ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ (ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)
        """
        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(pdf_path)
        if cache_key in self.extraction_cache:
            logger.debug(f"ğŸ“‹ ìºì‹œ íˆíŠ¸: {pdf_path.name}")
            return self.extraction_cache[cache_key]

        try:
            result = self._process_single_pdf(pdf_path)
            # ìºì‹œ ì €ì¥
            self.extraction_cache[cache_key] = result
            return result
        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {pdf_path.name} - {e}")
            # OCR í´ë°± ì‹œë„
            if self.enable_ocr and self.ocr_processor:
                return self._try_ocr_fallback(pdf_path)
            raise

    def _process_single_pdf(self, pdf_path: Path) -> Dict:
        """
        ë‹¨ì¼ PDFì˜ í˜ì´ì§€ë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬
        """
        start_time = time.time()

        with pdfplumber.open(pdf_path) as pdf:
            total_pages = len(pdf.pages)

            # í˜ì´ì§€ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
            page_chunks = [
                pdf.pages[i:i + self.page_chunk_size]
                for i in range(0, total_pages, self.page_chunk_size)
            ]

            # ì²­í¬ë³„ë¡œ ë³‘ë ¬ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            all_texts = []
            with ThreadPoolExecutor(max_workers=min(2, self.max_workers)) as executor:
                chunk_results = list(executor.map(
                    self._extract_chunk_text,
                    page_chunks
                ))
                all_texts = [text for text in chunk_results if text]

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self._extract_metadata(pdf)

            processing_time = time.time() - start_time

            result = {
                "text": "\n\n".join(all_texts),
                "page_count": total_pages,
                "metadata": metadata,
                "processing_time": processing_time,
                "method": "pdfplumber"
            }

            logger.debug(f"ğŸ“„ ì²˜ë¦¬ ì™„ë£Œ: {pdf_path.name} "
                        f"({total_pages}í˜ì´ì§€, {processing_time:.2f}ì´ˆ)")

            return result

    def _extract_chunk_text(self, pages) -> str:
        """
        í˜ì´ì§€ ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        """
        texts = []
        for page in pages:
            try:
                text = page.extract_text()
                if text:
                    texts.append(text.strip())
            except Exception as e:
                logger.warning(f"í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                continue

        return "\n".join(texts)

    def _extract_metadata(self, pdf) -> Dict:
        """
        PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        """
        try:
            metadata = pdf.metadata or {}
            return {
                "title": metadata.get('Title', ''),
                "author": metadata.get('Author', ''),
                "subject": metadata.get('Subject', ''),
                "creator": metadata.get('Creator', ''),
                "producer": metadata.get('Producer', ''),
                "creation_date": str(metadata.get('CreationDate', '')),
                "modification_date": str(metadata.get('ModDate', '')),
                "pages": len(pdf.pages)
            }
        except Exception as e:
            logger.warning(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def _try_ocr_fallback(self, pdf_path: Path) -> Dict:
        """
        OCRì„ ì‚¬ìš©í•œ í´ë°± ì²˜ë¦¬
        """
        if not self.ocr_processor:
            return {
                "error": "OCR í”„ë¡œì„¸ì„œ ì‚¬ìš© ë¶ˆê°€",
                "text": "",
                "page_count": 0
            }

        try:
            logger.info(f"ğŸ” OCR í´ë°± ì‹œë„: {pdf_path.name}")
            text = self.ocr_processor.process_pdf(str(pdf_path))

            return {
                "text": text,
                "page_count": 0,  # OCRì—ì„œëŠ” ì •í™•í•œ í˜ì´ì§€ ìˆ˜ë¥¼ ì•Œê¸° ì–´ë ¤ì›€
                "metadata": {},
                "method": "ocr"
            }
        except Exception as e:
            logger.error(f"OCR í´ë°± ì‹¤íŒ¨: {e}")
            return {
                "error": f"OCR ì‹¤íŒ¨: {str(e)}",
                "text": "",
                "page_count": 0
            }

    def _get_cache_key(self, pdf_path: Path) -> str:
        """
        íŒŒì¼ì˜ ìºì‹œ í‚¤ ìƒì„±
        """
        stat = pdf_path.stat()
        key_string = f"{pdf_path}_{stat.st_size}_{stat.st_mtime}"
        return hashlib.md5(key_string.encode()).hexdigest()

    def process_pages_in_parallel(self, pdf_path: Path,
                                 page_numbers: List[int]) -> Dict[int, str]:
        """
        íŠ¹ì • í˜ì´ì§€ë“¤ë§Œ ë³‘ë ¬ë¡œ ì²˜ë¦¬

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            page_numbers: ì²˜ë¦¬í•  í˜ì´ì§€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (0-indexed)

        Returns:
            {í˜ì´ì§€ë²ˆí˜¸: í…ìŠ¤íŠ¸} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        results = {}

        with pdfplumber.open(pdf_path) as pdf:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # ê° í˜ì´ì§€ì— ëŒ€í•œ ì‘ì—… ì œì¶œ
                future_to_page = {
                    executor.submit(self._extract_single_page, pdf, page_num): page_num
                    for page_num in page_numbers
                    if 0 <= page_num < len(pdf.pages)
                }

                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_page):
                    page_num = future_to_page[future]
                    try:
                        text = future.result(timeout=10)
                        results[page_num] = text
                    except Exception as e:
                        logger.error(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        results[page_num] = ""

        return results

    def _extract_single_page(self, pdf, page_number: int) -> str:
        """
        ë‹¨ì¼ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        """
        try:
            page = pdf.pages[page_number]
            text = page.extract_text()
            return text.strip() if text else ""
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {page_number} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""

    def get_cache_stats(self) -> Dict:
        """
        ìºì‹œ í†µê³„ ë°˜í™˜
        """
        return {
            "cache_size": len(self.extraction_cache),
            "cached_files": list(self.extraction_cache.keys())
        }

    def clear_cache(self):
        """
        ìºì‹œ ì´ˆê¸°í™”
        """
        self.extraction_cache.clear()
        logger.info("PDF ì¶”ì¶œ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def batch_process_pdfs(pdf_paths: List[Path],
                       batch_size: int = 10,
                       config_manager=None) -> Dict[str, Dict]:
    """
    ëŒ€ëŸ‰ì˜ PDFë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬

    Args:
        pdf_paths: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ í¬ê¸°
        config_manager: ConfigManager ì¸ìŠ¤í„´ìŠ¤

    Returns:
        ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    processor = PDFParallelProcessor(config_manager)
    all_results = {}

    # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    for i in range(0, len(pdf_paths), batch_size):
        batch = pdf_paths[i:i + batch_size]
        logger.info(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)")

        batch_results = processor.process_multiple_pdfs(batch)
        all_results.update(batch_results)

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ ìºì‹œ ì •ë¦¬
        if len(processor.extraction_cache) > 50:
            processor.clear_cache()

    return all_results