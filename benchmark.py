"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
=============

ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
"""

import time
import psutil
import numpy as np
from pathlib import Path
import json

from rag_core.config import RAGConfig
from rag_core.search import HybridSearch
from rag_core.document.pdf_processor import PDFProcessor
from rag_core.cache.lru_cache import LRUCache


class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.config = RAGConfig()
        self.results = {}

    def measure_pdf_processing(self, num_files=10):
        """PDF ì²˜ë¦¬ ì„±ëŠ¥ ì¸¡ì •"""
        processor = PDFProcessor(self.config)
        pdf_files = list(Path("docs").glob("**/*.pdf"))[:num_files]

        if not pdf_files:
            return None

        times = []
        for pdf_file in pdf_files:
            start = time.time()
            text = processor.extract_text(pdf_file)
            elapsed = time.time() - start
            times.append(elapsed)

        return {
            'avg_time': np.mean(times),
            'min_time': np.min(times),
            'max_time': np.max(times),
            'total_files': len(pdf_files)
        }

    def measure_search_performance(self, num_queries=20):
        """ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •"""
        search = HybridSearch(self.config)

        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒì„±
        test_docs = []
        for i in range(100):
            test_docs.append({
                'id': f'doc_{i}',
                'text': f'í…ŒìŠ¤íŠ¸ ë¬¸ì„œ {i}. ì¥ë¹„ êµ¬ë§¤ ìˆ˜ë¦¬ íê¸° ê²€í†  ë‚´ìš©.',
                'metadata': {'year': str(2020 + i % 5)}
            })

        # ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œê°„
        start = time.time()
        search.build_index(test_docs)
        index_time = time.time() - start

        # ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
        queries = ['ì¥ë¹„ êµ¬ë§¤', 'ìˆ˜ë¦¬ ìš”ì²­', '2023ë…„', 'íê¸° ì‹ ì²­', 'ê²€í†  ë³´ê³ ì„œ']
        search_times = []

        for _ in range(num_queries):
            query = queries[_ % len(queries)]
            start = time.time()
            results = search.search(query, top_k=10)
            elapsed = time.time() - start
            search_times.append(elapsed)

        return {
            'index_time': index_time,
            'avg_search_time': np.mean(search_times),
            'min_search_time': np.min(search_times),
            'max_search_time': np.max(search_times),
            'num_docs': len(test_docs),
            'num_queries': num_queries
        }

    def measure_cache_performance(self, num_operations=1000):
        """ìºì‹œ ì„±ëŠ¥ ì¸¡ì •"""
        cache = LRUCache(self.config)

        # Write ì„±ëŠ¥
        write_times = []
        for i in range(num_operations):
            key = f'key_{i}'
            value = {'data': f'value_{i}' * 100}
            start = time.time()
            cache.set(key, value, raw_key=True)
            write_times.append(time.time() - start)

        # Read ì„±ëŠ¥ (hit)
        read_hit_times = []
        for i in range(num_operations // 2):
            key = f'key_{i}'
            start = time.time()
            result = cache.get(key, raw_key=True)
            read_hit_times.append(time.time() - start)

        # Read ì„±ëŠ¥ (miss)
        read_miss_times = []
        for i in range(num_operations // 2):
            key = f'nonexistent_{i}'
            start = time.time()
            result = cache.get(key, raw_key=True)
            read_miss_times.append(time.time() - start)

        stats = cache.get_stats()

        return {
            'avg_write_time': np.mean(write_times) * 1000,  # ms
            'avg_read_hit_time': np.mean(read_hit_times) * 1000,
            'avg_read_miss_time': np.mean(read_miss_times) * 1000,
            'cache_stats': stats
        }

    def measure_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        process = psutil.Process()
        memory_info = process.memory_info()

        return {
            'rss_mb': memory_info.rss / 1024 / 1024,
            'vms_mb': memory_info.vms / 1024 / 1024,
            'percent': process.memory_percent(),
            'available_mb': psutil.virtual_memory().available / 1024 / 1024
        }

    def run_all_benchmarks(self):
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        print("="*50)

        # PDF ì²˜ë¦¬
        print("\nğŸ“„ PDF ì²˜ë¦¬ ì„±ëŠ¥ ì¸¡ì •...")
        pdf_results = self.measure_pdf_processing(5)
        if pdf_results:
            self.results['pdf_processing'] = pdf_results
            print(f"  í‰ê· : {pdf_results['avg_time']:.3f}ì´ˆ")

        # ê²€ìƒ‰ ì„±ëŠ¥
        print("\nğŸ” ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •...")
        search_results = self.measure_search_performance(20)
        self.results['search'] = search_results
        print(f"  ì¸ë±ì‹±: {search_results['index_time']:.3f}ì´ˆ")
        print(f"  í‰ê·  ê²€ìƒ‰: {search_results['avg_search_time']:.4f}ì´ˆ")

        # ìºì‹œ ì„±ëŠ¥
        print("\nğŸ’¾ ìºì‹œ ì„±ëŠ¥ ì¸¡ì •...")
        cache_results = self.measure_cache_performance(1000)
        self.results['cache'] = cache_results
        print(f"  ì“°ê¸°: {cache_results['avg_write_time']:.3f}ms")
        print(f"  ì½ê¸°(hit): {cache_results['avg_read_hit_time']:.3f}ms")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        print("\nğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰...")
        memory_results = self.measure_memory_usage()
        self.results['memory'] = memory_results
        print(f"  RSS: {memory_results['rss_mb']:.1f}MB")
        print(f"  ì‚¬ìš©ë¥ : {memory_results['percent']:.1f}%")

        # ê²°ê³¼ ì €ì¥
        self.save_results()

        print("\n"+"="*50)
        print("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

        return self.results

    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        filename = f'benchmark_results_{timestamp}.json'

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“Š ê²°ê³¼ ì €ì¥: {filename}")


if __name__ == "__main__":
    benchmark = PerformanceBenchmark()
    results = benchmark.run_all_benchmarks()

    # ì„±ëŠ¥ ê¸°ì¤€ ì²´í¬
    print("\nğŸ¯ ì„±ëŠ¥ ê¸°ì¤€ í‰ê°€:")
    if results.get('search', {}).get('avg_search_time', 1) < 0.1:
        print("  âœ… ê²€ìƒ‰ ì†ë„: PASS (< 100ms)")
    else:
        print("  âŒ ê²€ìƒ‰ ì†ë„: FAIL (> 100ms)")

    if results.get('cache', {}).get('avg_read_hit_time', 10) < 1:
        print("  âœ… ìºì‹œ ì†ë„: PASS (< 1ms)")
    else:
        print("  âŒ ìºì‹œ ì†ë„: FAIL (> 1ms)")

    if results.get('memory', {}).get('rss_mb', 10000) < 8000:
        print("  âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©: PASS (< 8GB)")
    else:
        print("  âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©: WARNING (> 8GB)")