"""
AI-CHAT-V3 Configuration File
ëª¨ë“  ê²½ë¡œì™€ ì„¤ì •ì„ ì¤‘ì•™ ê´€ë¦¬ - ê°œì„ ëœ ë²„ì „
"""

import os
from pathlib import Path
from typing import Any, Dict, Optional
import json
import warnings
import multiprocessing

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬
PROJECT_ROOT = Path(__file__).parent.absolute()

# í™˜ê²½ë³„ ì„¤ì •
ENVIRONMENT = os.getenv('ENVIRONMENT', 'production')  # development, staging, production

# í—¬í¼ í•¨ìˆ˜ë“¤
def get_env_int(key: str, default: int) -> int:
    """í™˜ê²½ ë³€ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì •ìˆ˜ë¡œ ë³€í™˜"""
    try:
        return int(os.getenv(key, str(default)))
    except (ValueError, TypeError):
        warnings.warn(f"Invalid integer value for {key}, using default: {default}")
        return default

def get_env_float(key: str, default: float) -> float:
    """í™˜ê²½ ë³€ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤ìˆ˜ë¡œ ë³€í™˜"""
    try:
        return float(os.getenv(key, str(default)))
    except (ValueError, TypeError):
        warnings.warn(f"Invalid float value for {key}, using default: {default}")
        return default

def get_env_bool(key: str, default: bool) -> bool:
    """í™˜ê²½ ë³€ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ë¶ˆë¦°ìœ¼ë¡œ ë³€í™˜"""
    value = os.getenv(key, str(default)).lower()
    return value in ('true', '1', 'yes', 'on')

def get_env_path(key: str, default: Path) -> Path:
    """í™˜ê²½ ë³€ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ Pathë¡œ ë³€í™˜"""
    value = os.getenv(key, str(default))
    return Path(value) if value else default

def validate_threshold(value: float, name: str) -> float:
    """ì„ê³„ê°’ ë²”ìœ„ ê²€ì¦ (0.0 ~ 1.0)"""
    if not 0.0 <= value <= 1.0:
        warnings.warn(f"{name} = {value} is out of range [0.0, 1.0], clamping...")
        return max(0.0, min(1.0, value))
    return value

# ê²½ë¡œ ì„¤ì • (ê²€ì¦ ë° ìƒì„±)
MODELS_DIR = get_env_path('MODELS_DIR', PROJECT_ROOT / 'models')
DOCS_DIR = get_env_path('DOCS_DIR', PROJECT_ROOT / 'docs')
CACHE_DIR = get_env_path('CACHE_DIR', PROJECT_ROOT / 'rag_system' / 'cache')
DB_DIR = get_env_path('DB_DIR', PROJECT_ROOT / 'rag_system' / 'db')

# ë””ë ‰í„°ë¦¬ ìë™ ìƒì„±
for dir_path in [MODELS_DIR, DOCS_DIR, CACHE_DIR, DB_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

# ëª¨ë¸ ê²½ë¡œ
QWEN_MODEL_PATH = get_env_path('QWEN_MODEL_PATH',
    MODELS_DIR / 'qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf')
SENTENCE_TRANSFORMERS_CACHE = get_env_path('SENTENCE_TRANSFORMERS_CACHE',
    MODELS_DIR / 'sentence_transformers')

# ì„ë² ë”© ëª¨ë¸
KOREAN_EMBEDDING_MODEL = os.getenv('KOREAN_EMBEDDING_MODEL',
    'jhgan/ko-sroberta-multitask')

# ê²€ìƒ‰ ì„¤ì •
VECTOR_WEIGHT = get_env_float('VECTOR_WEIGHT', 0.2)
BM25_WEIGHT = get_env_float('BM25_WEIGHT', 0.8)
RRF_K = get_env_int('RRF_K', 20)

# ê°€ì¤‘ì¹˜ í•©ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”
weight_sum = VECTOR_WEIGHT + BM25_WEIGHT
if abs(weight_sum - 1.0) > 0.01:
    warnings.warn(f"VECTOR_WEIGHT + BM25_WEIGHT = {weight_sum} != 1.0, normalizing...")
    VECTOR_WEIGHT = VECTOR_WEIGHT / weight_sum
    BM25_WEIGHT = BM25_WEIGHT / weight_sum

# í’ˆì§ˆ ì„ê³„ê°’
QUALITY_THRESHOLD = validate_threshold(get_env_float('QUALITY_THRESHOLD', 0.7), 'QUALITY_THRESHOLD')
RELEVANCE_THRESHOLD = validate_threshold(get_env_float('RELEVANCE_THRESHOLD', 0.6), 'RELEVANCE_THRESHOLD')
CONFIDENCE_THRESHOLD = validate_threshold(get_env_float('CONFIDENCE_THRESHOLD', 0.7), 'CONFIDENCE_THRESHOLD')
COMPLETENESS_THRESHOLD = validate_threshold(get_env_float('COMPLETENESS_THRESHOLD', 0.6), 'COMPLETENESS_THRESHOLD')

# LLM ìƒì„± ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
# í™˜ê²½ë³„ ê¸°ë³¸ê°’
LLM_DEFAULTS = {
    'development': {'temperature': 0.7, 'max_tokens': 800, 'top_p': 0.9},
    'staging': {'temperature': 0.5, 'max_tokens': 600, 'top_p': 0.87},
    'production': {'temperature': 0.3, 'max_tokens': 512, 'top_p': 0.85}  # 800â†’512 (-36%)
}

defaults = LLM_DEFAULTS.get(ENVIRONMENT, LLM_DEFAULTS['production'])

TEMPERATURE = validate_threshold(get_env_float('TEMPERATURE', defaults['temperature']), 'TEMPERATURE')
MAX_TOKENS = max(1, min(4096, get_env_int('MAX_TOKENS', defaults['max_tokens'])))  # 1-4096 ë²”ìœ„
TOP_P = validate_threshold(get_env_float('TOP_P', defaults['top_p']), 'TOP_P')
TOP_K = max(1, min(100, get_env_int('TOP_K', 30)))  # 1-100 ë²”ìœ„
REPEAT_PENALTY = max(1.0, min(2.0, get_env_float('REPEAT_PENALTY', 1.15)))  # 1.0-2.0 ë²”ìœ„

# GPU ìµœì í™” ì„¤ì • (NVIDIA RTX PRO 4000 - 16GB VRAM)
# ğŸ”¥ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • (14GB â†’ 8GB ëª©í‘œ)
N_THREADS = max(1, min(32, get_env_int('N_THREADS', 4)))  # GPU ì‚¬ìš© ì‹œ CPU ìŠ¤ë ˆë“œ ìµœì†Œí™” (20â†’4ë¡œ ë³€ê²½)
N_CTX = max(512, min(32768, get_env_int('N_CTX', 16384)))  # ì»¨í…ìŠ¤íŠ¸ í™•ì¥ìœ¼ë¡œ ë” ë§ì€ ë¬¸ì„œ ì²˜ë¦¬
N_BATCH = max(1, min(2048, get_env_int('N_BATCH', 1024)))  # ë°°ì¹˜ í¬ê¸° ì¦ê°€ë¡œ ì²˜ë¦¬ ì†ë„ í–¥ìƒ
USE_MLOCK = get_env_bool('USE_MLOCK', False)
USE_MMAP = get_env_bool('USE_MMAP', True)
LOW_VRAM = get_env_bool('LOW_VRAM', False)  # RTX 4000 16GB VRAM ì¶©ë¶„

# GPU ì„¤ì • (í™œì„±í™”ë¨!)
N_GPU_LAYERS = get_env_int('N_GPU_LAYERS', -1)  # -1 = ëª¨ë“  ë ˆì´ì–´ GPU ì‚¬ìš©
MAIN_GPU = max(0, get_env_int('MAIN_GPU', 0))  # GPU ID (ìŒìˆ˜ ë°©ì§€)
F16_KV = get_env_bool('F16_KV', True)  # GPU ë©”ëª¨ë¦¬ ìµœì í™”

# ì„±ëŠ¥ ìµœì í™” ì„¤ì • (2025-01-18 ì¶”ê°€)
# ë¬¸ì„œ ê²€ìƒ‰ ìµœì í™”
MAX_DOCUMENTS_TO_PROCESS = max(1, min(50, get_env_int('MAX_DOCUMENTS_TO_PROCESS', 20)))  # ë” ë§ì€ ë¬¸ì„œ ë™ì‹œ ì²˜ë¦¬
MAX_PAGES_PER_PDF = max(1, min(100, get_env_int('MAX_PAGES_PER_PDF', 50)))  # PDF í˜ì´ì§€ ì²˜ë¦¬ ì¦ê°€
PDF_TIMEOUT_SECONDS = max(1, min(60, get_env_int('PDF_TIMEOUT_SECONDS', 5)))
SEARCH_TIMEOUT_SECONDS = max(5, min(300, get_env_int('SEARCH_TIMEOUT_SECONDS', 20)))

# ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
max_workers = multiprocessing.cpu_count()
PARALLEL_WORKERS = max(1, min(max_workers, get_env_int('PARALLEL_WORKERS', min(12, max_workers))))  # ë³‘ë ¬ ì›Œì»¤ ì¦ê°€
BATCH_SIZE = max(1, min(20, get_env_int('BATCH_SIZE', 10)))  # ë°°ì¹˜ í¬ê¸° ì¦ê°€

# ìºì‹± ì„¤ì • ê°•í™”
PDF_TEXT_CACHE_SIZE = max(10, min(1000, get_env_int('PDF_TEXT_CACHE_SIZE', 100)))
RESPONSE_CACHE_TTL = max(60, min(86400, get_env_int('RESPONSE_CACHE_TTL', 7200)))  # 1ë¶„-24ì‹œê°„
CACHE_MAX_SIZE = max(50, min(5000, get_env_int('CACHE_MAX_SIZE', 500)))

# ë””ë²„ê·¸ ì„¤ì •
DEBUG_MODE = get_env_bool('DEBUG_MODE', False)
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO').upper()

# ë¡œê·¸ ë ˆë²¨ ê²€ì¦
VALID_LOG_LEVELS = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']
if LOG_LEVEL not in VALID_LOG_LEVELS:
    warnings.warn(f"Invalid LOG_LEVEL: {LOG_LEVEL}, using INFO")
    LOG_LEVEL = 'INFO'

# ëª¨ë“  ê²½ë¡œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
MODELS_DIR = str(MODELS_DIR)
DOCS_DIR = str(DOCS_DIR)
CACHE_DIR = str(CACHE_DIR)
DB_DIR = str(DB_DIR)
QWEN_MODEL_PATH = str(QWEN_MODEL_PATH)
SENTENCE_TRANSFORMERS_CACHE = str(SENTENCE_TRANSFORMERS_CACHE)

# ì„¤ì • ìš”ì•½ í‘œì‹œ
def get_config_summary() -> Dict[str, Any]:
    """í˜„ì¬ ì„¤ì • ìš”ì•½ ë°˜í™˜"""
    return {
        'environment': ENVIRONMENT,
        'gpu': {
            'enabled': N_GPU_LAYERS != 0,
            'layers': N_GPU_LAYERS,
            'main_gpu': MAIN_GPU
        },
        'llm': {
            'temperature': TEMPERATURE,
            'max_tokens': MAX_TOKENS,
            'context_size': N_CTX
        },
        'search': {
            'vector_weight': VECTOR_WEIGHT,
            'bm25_weight': BM25_WEIGHT,
            'max_docs': MAX_DOCUMENTS_TO_PROCESS
        },
        'performance': {
            'parallel_workers': PARALLEL_WORKERS,
            'cache_size': CACHE_MAX_SIZE,
            'cache_ttl': RESPONSE_CACHE_TTL
        },
        'debug': DEBUG_MODE
    }

# ì„¤ì • íŒŒì¼ ì €ì¥/ë¡œë“œ
CONFIG_FILE = PROJECT_ROOT / 'config.json'

def save_config(config: Optional[Dict[str, Any]] = None):
    """í˜„ì¬ ì„¤ì •ì„ íŒŒì¼ë¡œ ì €ì¥"""
    if config is None:
        config = get_config_summary()

    with open(CONFIG_FILE, 'w') as f:
        json.dump(config, f, indent=2)

    return config

def load_config_from_file():
    """ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ"""
    if CONFIG_FILE.exists():
        try:
            with open(CONFIG_FILE, 'r') as f:
                return json.load(f)
        except Exception as e:
            warnings.warn(f"Failed to load config file: {e}")
    return None

def reload_config():
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë‹¤ì‹œ ë¡œë“œ"""
    global VECTOR_WEIGHT, BM25_WEIGHT, TEMPERATURE, MAX_TOKENS
    # ì£¼ìš” ì„¤ì •ë“¤ì„ ë‹¤ì‹œ ë¡œë“œ
    VECTOR_WEIGHT = get_env_float('VECTOR_WEIGHT', 0.2)
    BM25_WEIGHT = get_env_float('BM25_WEIGHT', 0.8)
    TEMPERATURE = get_env_float('TEMPERATURE', defaults['temperature'])
    MAX_TOKENS = get_env_int('MAX_TOKENS', defaults['max_tokens'])
    return get_config_summary()

def validate_config() -> Dict[str, bool]:
    """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
    validations = {}

    # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
    validations['model_exists'] = Path(QWEN_MODEL_PATH).exists()

    # ë””ë ‰í„°ë¦¬ ì“°ê¸° ê¶Œí•œ í™•ì¸
    validations['cache_writable'] = os.access(CACHE_DIR, os.W_OK)
    validations['db_writable'] = os.access(DB_DIR, os.W_OK)

    # GPU ì„¤ì • í™•ì¸
    validations['gpu_configured'] = N_GPU_LAYERS != 0

    # ê°€ì¤‘ì¹˜ í•© í™•ì¸
    validations['weights_normalized'] = abs((VECTOR_WEIGHT + BM25_WEIGHT) - 1.0) < 0.01

    return validations

# ì´ˆê¸°í™” ì‹œ ì„¤ì • ê²€ì¦
if ENVIRONMENT == 'development' and DEBUG_MODE:
    print("\n=== Configuration Summary ===")
    for key, value in get_config_summary().items():
        print(f"{key}: {value}")
    print("\n=== Configuration Validation ===")
    for check, passed in validate_config().items():
        status = "âœ…" if passed else "âŒ"
        print(f"{status} {check}: {passed}")
    print("============================\n")