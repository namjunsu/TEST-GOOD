#!/usr/bin/env python3
"""
ê³ ê¸‰ OCR ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ

ì£¼ìš” ê¸°ëŠ¥:
- ìŠ¤ìº” PDF ìë™ ê°ì§€ ë° ë¶„ë¥˜
- ë³‘ë ¬ OCR ì²˜ë¦¬ with ì§„í–‰ë¥  í‘œì‹œ
- ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ë° ì˜¤ë¥˜ ë³µêµ¬
- ì¦ë¶„ ì²˜ë¦¬ (ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ê±´ë„ˆë›°ê¸°)
- ìƒì„¸í•œ í†µê³„ ë° ë¦¬í¬íŠ¸ ìƒì„±
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
"""

import os
import sys
import json
import hashlib
import argparse
import psutil
import pickle
from pathlib import Path
import time
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass, field, asdict
from collections import defaultdict
import pdfplumber
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor
import logging
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ê²½ë¡œ ë™ì  ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

try:
    from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    class tqdm:  # Fallback
        def __init__(self, iterable=None, total=None, desc="", **kwargs):
            self.iterable = iterable or []
            self.total = total or len(self.iterable)
            self.desc = desc
        def __iter__(self):
            return iter(self.iterable)
        def update(self, n=1):
            pass
        def close(self):
            pass

# ì„¤ì • ìƒìˆ˜
DEFAULT_CACHE_FILE = "ocr_cache.json"
DEFAULT_STATE_FILE = "ocr_state.pkl"
DEFAULT_BATCH_SIZE = 10
DEFAULT_MAX_WORKERS = 4
MIN_TEXT_LENGTH = 50  # ìŠ¤ìº” PDF íŒë³„ ê¸°ì¤€
MAX_RETRIES = 3
MEMORY_THRESHOLD_MB = 500
CACHE_VERSION = "2.0"

# ë¡œê¹… ì„¤ì •
class LogLevel(Enum):
    DEBUG = logging.DEBUG
    INFO = logging.INFO
    WARNING = logging.WARNING
    ERROR = logging.ERROR

def setup_logging(level: str = "INFO", log_file: Optional[Path] = None):
    """í–¥ìƒëœ ë¡œê¹… ì„¤ì •"""
    log_level = getattr(logging, level.upper(), logging.INFO)

    handlers = [logging.StreamHandler()]
    if log_file:
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))

    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers,
        force=True
    )
    return logging.getLogger(__name__)

@dataclass
class PDFInfo:
    """PDF íŒŒì¼ ì •ë³´"""
    path: Path
    size_mb: float
    page_count: int
    text_length: int
    is_scanned: bool
    hash: str
    error: Optional[str] = None
    processing_time: float = 0.0
    ocr_success: bool = False
    extracted_text: str = ""

@dataclass
class OCRStats:
    """OCR ì²˜ë¦¬ í†µê³„"""
    total_files: int = 0
    scanned_files: int = 0
    processed_files: int = 0
    successful_files: int = 0
    failed_files: int = 0
    skipped_files: int = 0
    total_time: float = 0.0
    total_size_mb: float = 0.0
    errors: List[str] = field(default_factory=list)

    def get_summary(self) -> Dict:
        """í†µê³„ ìš”ì•½ ë°˜í™˜"""
        return {
            "total_files": self.total_files,
            "scanned_files": self.scanned_files,
            "processed_files": self.processed_files,
            "success_rate": f"{(self.successful_files/self.processed_files*100):.1f}%" if self.processed_files else "0%",
            "average_time": f"{self.total_time/self.processed_files:.1f}s" if self.processed_files else "0s",
            "total_size_mb": f"{self.total_size_mb:.1f}MB",
            "errors_count": len(self.errors)
        }

class OCRProcessor:
    """í–¥ìƒëœ OCR ì²˜ë¦¬ê¸°"""

    def __init__(self, cache_file: Path, state_file: Path,
                 batch_size: int = DEFAULT_BATCH_SIZE,
                 max_workers: int = DEFAULT_MAX_WORKERS,
                 verbose: bool = True):
        self.cache_file = cache_file
        self.state_file = state_file
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.verbose = verbose
        self.logger = logging.getLogger(self.__class__.__name__)

        # OCR í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.ocr_processor = EnhancedOCRProcessor()

        # ìºì‹œ ë° ìƒíƒœ ë¡œë“œ
        self.cache = self.load_cache()
        self.processed_hashes = self.load_state()

        # í†µê³„
        self.stats = OCRStats()

    def load_cache(self) -> Dict:
        """ê¸°ì¡´ ìºì‹œ ë¡œë“œ"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    if cache_data.get('version') == CACHE_VERSION:
                        self.logger.info(f"âœ… ìºì‹œ ë¡œë“œ: {len(cache_data.get('ocr_texts', {}))}ê°œ í•­ëª©")
                        return cache_data
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {'version': CACHE_VERSION, 'ocr_texts': {}, 'metadata': {}}

    def load_state(self) -> Set[str]:
        """ì²˜ë¦¬ ìƒíƒœ ë¡œë“œ"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'rb') as f:
                    state = pickle.load(f)
                    self.logger.info(f"âœ… ìƒíƒœ ë¡œë“œ: {len(state)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
                    return state
            except Exception as e:
                self.logger.warning(f"âš ï¸ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return set()

    def save_cache(self):
        """ìºì‹œ ì €ì¥"""
        self.cache['updated_at'] = datetime.now().isoformat()
        self.cache['stats'] = asdict(self.stats)

        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)
        self.logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {self.cache_file}")

    def save_state(self):
        """ì²˜ë¦¬ ìƒíƒœ ì €ì¥"""
        with open(self.state_file, 'wb') as f:
            pickle.dump(self.processed_hashes, f)
        self.logger.info(f"ğŸ’¾ ìƒíƒœ ì €ì¥: {self.state_file}")

    def get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        hash_md5.update(str(file_path).encode())
        hash_md5.update(str(file_path.stat().st_mtime).encode())
        return hash_md5.hexdigest()

    def analyze_pdf(self, pdf_path: Path) -> PDFInfo:
        """PDF ë¶„ì„ ë° ìŠ¤ìº” ì—¬ë¶€ íŒë³„"""
        try:
            file_hash = self.get_file_hash(pdf_path)
            size_mb = pdf_path.stat().st_size / (1024 * 1024)

            with pdfplumber.open(pdf_path) as pdf:
                page_count = len(pdf.pages)

                # ì²˜ìŒ 3í˜ì´ì§€ ê²€ì‚¬
                total_text = ""
                for page in pdf.pages[:3]:
                    text = page.extract_text() or ""
                    total_text += text

                text_length = len(total_text.strip())
                is_scanned = text_length < MIN_TEXT_LENGTH

                return PDFInfo(
                    path=pdf_path,
                    size_mb=size_mb,
                    page_count=page_count,
                    text_length=text_length,
                    is_scanned=is_scanned,
                    hash=file_hash
                )

        except Exception as e:
            return PDFInfo(
                path=pdf_path,
                size_mb=0,
                page_count=0,
                text_length=0,
                is_scanned=True,  # ì˜¤ë¥˜ ì‹œ ìŠ¤ìº”ìœ¼ë¡œ ê°„ì£¼
                hash=self.get_file_hash(pdf_path),
                error=str(e)
            )

    def identify_scanned_pdfs(self, docs_dir: Path, skip_processed: bool = True) -> List[PDFInfo]:
        """ìŠ¤ìº” PDF íŒŒì¼ ì‹ë³„ ë° ë¶„ì„"""
        all_pdfs = list(docs_dir.glob('**/*.pdf'))
        self.stats.total_files = len(all_pdfs)

        self.logger.info(f"ğŸ” ì „ì²´ {len(all_pdfs)}ê°œ PDF ë¶„ì„ ì‹œì‘...")

        pdf_infos = []
        scanned_count = 0

        # ì§„í–‰ë¥  í‘œì‹œ
        with tqdm(total=len(all_pdfs), desc="PDF ë¶„ì„", disable=not self.verbose) as pbar:
            for pdf_path in all_pdfs:
                info = self.analyze_pdf(pdf_path)

                # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ê±´ë„ˆë›°ê¸°
                if skip_processed and info.hash in self.processed_hashes:
                    self.stats.skipped_files += 1
                    pbar.update(1)
                    continue

                pdf_infos.append(info)
                if info.is_scanned:
                    scanned_count += 1

                pbar.update(1)
                pbar.set_postfix(scanned=scanned_count)

        self.stats.scanned_files = scanned_count

        scanned_pdfs = [info for info in pdf_infos if info.is_scanned]
        self.logger.info(f"ğŸ“Š ê²°ê³¼: {len(scanned_pdfs)}ê°œ ìŠ¤ìº” PDF ë°œê²¬ ({scanned_count*100//len(all_pdfs)}%)")

        return scanned_pdfs

    def process_single_pdf(self, pdf_info: PDFInfo, retry_count: int = 0) -> PDFInfo:
        """ë‹¨ì¼ PDF OCR ì²˜ë¦¬ with ì¬ì‹œë„"""
        start_time = time.time()

        try:
            if self.verbose:
                self.logger.info(f"ğŸ”„ OCR ì²˜ë¦¬: {pdf_info.path.name}")

            text, metadata = self.ocr_processor.extract_text_with_ocr(str(pdf_info.path))

            if metadata.get('ocr_performed') and len(text) > 100:
                pdf_info.ocr_success = True
                pdf_info.extracted_text = text
                pdf_info.processing_time = time.time() - start_time

                # ìºì‹œ ì—…ë°ì´íŠ¸
                self.cache['ocr_texts'][str(pdf_info.path)] = text
                self.cache['metadata'][str(pdf_info.path)] = {
                    'hash': pdf_info.hash,
                    'processing_time': pdf_info.processing_time,
                    'text_length': len(text),
                    'timestamp': datetime.now().isoformat()
                }

                # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                self.processed_hashes.add(pdf_info.hash)

            else:
                pdf_info.ocr_success = False
                pdf_info.error = "OCR í…ìŠ¤íŠ¸ ë¶€ì¡±"

        except Exception as e:
            pdf_info.error = str(e)
            pdf_info.ocr_success = False

            # ì¬ì‹œë„
            if retry_count < MAX_RETRIES:
                time.sleep(2 ** retry_count)  # Exponential backoff
                return self.process_single_pdf(pdf_info, retry_count + 1)

            self.logger.error(f"âŒ OCR ì‹¤íŒ¨ ({retry_count+1}/{MAX_RETRIES}): {pdf_info.path.name} - {e}")

        pdf_info.processing_time = time.time() - start_time
        return pdf_info

    def check_memory(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬"""
        memory = psutil.Process().memory_info().rss / 1024 / 1024
        if memory > MEMORY_THRESHOLD_MB:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory:.1f}MB")
            return False
        return True

    def process_batch(self, pdf_infos: List[PDFInfo]) -> List[PDFInfo]:
        """ë°°ì¹˜ ë‹¨ìœ„ ë³‘ë ¬ OCR ì²˜ë¦¬"""
        processed_infos = []

        # ë©”ëª¨ë¦¬ ì²´í¬
        if not self.check_memory():
            import gc
            gc.collect()

        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ê¸° ì„ íƒ (íŒŒì¼ í¬ê¸°ì— ë”°ë¼)
        total_size_mb = sum(info.size_mb for info in pdf_infos)
        use_process = total_size_mb > 100  # 100MB ì´ìƒì´ë©´ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©

        executor_class = ProcessPoolExecutor if use_process else ThreadPoolExecutor
        executor_name = "í”„ë¡œì„¸ìŠ¤" if use_process else "ìŠ¤ë ˆë“œ"

        with executor_class(max_workers=self.max_workers) as executor:
            if self.verbose:
                self.logger.info(f"ğŸš€ {self.max_workers}ê°œ {executor_name}ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")

            futures = {
                executor.submit(self.process_single_pdf, pdf_info): pdf_info
                for pdf_info in pdf_infos
            }

            # ì§„í–‰ë¥  í‘œì‹œ
            with tqdm(total=len(futures), desc="OCR ì²˜ë¦¬", disable=not self.verbose) as pbar:
                for future in as_completed(futures):
                    pdf_info = futures[future]

                    try:
                        result = future.result(timeout=60)
                        processed_infos.append(result)

                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.stats.processed_files += 1
                        if result.ocr_success:
                            self.stats.successful_files += 1
                            status = "âœ…"
                        else:
                            self.stats.failed_files += 1
                            status = "âŒ"
                            if result.error:
                                self.stats.errors.append(f"{result.path.name}: {result.error}")

                        self.stats.total_time += result.processing_time
                        self.stats.total_size_mb += result.size_mb

                        pbar.update(1)
                        pbar.set_postfix(
                            success=self.stats.successful_files,
                            failed=self.stats.failed_files
                        )

                        if self.verbose:
                            self.logger.info(
                                f"{status} {result.path.name}: "
                                f"{len(result.extracted_text)}ì, "
                                f"{result.processing_time:.1f}ì´ˆ"
                            )

                    except Exception as e:
                        self.logger.error(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {pdf_info.path.name} - {e}")
                        self.stats.failed_files += 1
                        self.stats.errors.append(f"{pdf_info.path.name}: {e}")
                        pbar.update(1)

        return processed_infos

    def process_all(self, pdf_infos: List[PDFInfo], save_interval: int = 10) -> List[PDFInfo]:
        """ì „ì²´ PDF ë°°ì¹˜ ì²˜ë¦¬"""
        all_results = []
        total_batches = (len(pdf_infos) + self.batch_size - 1) // self.batch_size

        self.logger.info(f"ğŸ“¦ ì´ {total_batches}ê°œ ë°°ì¹˜ë¡œ ì²˜ë¦¬ ì‹œì‘...")

        for i in range(0, len(pdf_infos), self.batch_size):
            batch = pdf_infos[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1

            self.logger.info(f"\nğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘...")

            results = self.process_batch(batch)
            all_results.extend(results)

            # ì£¼ê¸°ì  ì €ì¥
            if batch_num % save_interval == 0:
                self.save_cache()
                self.save_state()
                self.logger.info(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ (ë°°ì¹˜ {batch_num})")

        # ìµœì¢… ì €ì¥
        self.save_cache()
        self.save_state()

        return all_results

    def generate_report(self, output_file: Optional[Path] = None) -> str:
        """ì²˜ë¦¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        stats = self.stats.get_summary()

        report = []
        report.append("\n" + "="*60)
        report.append("ğŸ“Š OCR ì²˜ë¦¬ ê²°ê³¼ ë¦¬í¬íŠ¸")
        report.append("="*60)
        report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        report.append("ğŸ“ˆ í†µê³„:")
        for key, value in stats.items():
            report.append(f"  - {key}: {value}")

        if self.stats.errors:
            report.append("\nâŒ ì˜¤ë¥˜ ëª©ë¡:")
            for error in self.stats.errors[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                report.append(f"  - {error}")
            if len(self.stats.errors) > 10:
                report.append(f"  ... ì™¸ {len(self.stats.errors)-10}ê°œ")

        report.append("\nğŸ’¾ íŒŒì¼ ì •ë³´:")
        report.append(f"  - ìºì‹œ íŒŒì¼: {self.cache_file}")
        report.append(f"  - ìƒíƒœ íŒŒì¼: {self.state_file}")

        report_text = "\n".join(report)

        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
            self.logger.info(f"ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")

        return report_text

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="AI-CHAT OCR ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  %(prog)s --batch-size 20 --workers 8  # ëŒ€ëŸ‰ ì²˜ë¦¬
  %(prog)s --limit 10 --verbose        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
  %(prog)s --skip-processed            # ì¦ë¶„ ì²˜ë¦¬
  %(prog)s --report report.txt         # ë¦¬í¬íŠ¸ ìƒì„±
        """
    )

    parser.add_argument(
        "--docs-dir", type=Path, default=Path("docs"),
        help="ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸: docs)"
    )
    parser.add_argument(
        "--cache-file", type=Path, default=Path(DEFAULT_CACHE_FILE),
        help=f"ìºì‹œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: {DEFAULT_CACHE_FILE})"
    )
    parser.add_argument(
        "--state-file", type=Path, default=Path(DEFAULT_STATE_FILE),
        help=f"ìƒíƒœ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: {DEFAULT_STATE_FILE})"
    )
    parser.add_argument(
        "--batch-size", type=int, default=DEFAULT_BATCH_SIZE,
        help=f"ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: {DEFAULT_BATCH_SIZE})"
    )
    parser.add_argument(
        "--workers", type=int, default=DEFAULT_MAX_WORKERS,
        help=f"ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: {DEFAULT_MAX_WORKERS})"
    )
    parser.add_argument(
        "--limit", type=int, default=None,
        help="ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)"
    )
    parser.add_argument(
        "--skip-processed", action="store_true",
        help="ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ê±´ë„ˆë›°ê¸°"
    )
    parser.add_argument(
        "--force", action="store_true",
        help="ëª¨ë“  íŒŒì¼ ê°•ì œ ì¬ì²˜ë¦¬"
    )
    parser.add_argument(
        "--report", type=Path, default=None,
        help="ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--verbose", action="store_true",
        help="ìì„¸í•œ ì¶œë ¥"
    )
    parser.add_argument(
        "--log-level", choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO', help="ë¡œê·¸ ë ˆë²¨"
    )
    parser.add_argument(
        "--log-file", type=Path, default=None,
        help="ë¡œê·¸ íŒŒì¼ ê²½ë¡œ"
    )

    args = parser.parse_args()

    # ë¡œê¹… ì„¤ì •
    logger = setup_logging(args.log_level, args.log_file)

    print("\n" + "="*60)
    print("ğŸ¤– AI-CHAT OCR ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ v2.0")
    print("="*60)

    # OCR í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = OCRProcessor(
        cache_file=args.cache_file,
        state_file=args.state_file,
        batch_size=args.batch_size,
        max_workers=args.workers,
        verbose=args.verbose
    )

    # ë¬¸ì„œ ë””ë ‰í† ë¦¬ í™•ì¸
    if not args.docs_dir.exists():
        logger.error(f"âŒ ë¬¸ì„œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.docs_dir}")
        return 1

    # 1. ìŠ¤ìº” PDF ì‹ë³„
    print(f"\nğŸ“ ë””ë ‰í† ë¦¬: {args.docs_dir}")

    skip_processed = args.skip_processed and not args.force
    scanned_pdfs = processor.identify_scanned_pdfs(args.docs_dir, skip_processed)

    if not scanned_pdfs:
        print("\nâœ… ì²˜ë¦¬í•  ìŠ¤ìº” PDFê°€ ì—†ìŠµë‹ˆë‹¤!")
        if processor.stats.skipped_files > 0:
            print(f"   (ì´ë¯¸ ì²˜ë¦¬ë¨: {processor.stats.skipped_files}ê°œ)")
        return 0

    # ì²˜ë¦¬ ëŒ€ìƒ ì œí•œ
    if args.limit:
        scanned_pdfs = scanned_pdfs[:args.limit]
        print(f"\nâš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {args.limit}ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬")

    print(f"\nğŸ“‹ ì²˜ë¦¬ ëŒ€ìƒ: {len(scanned_pdfs)}ê°œ ìŠ¤ìº” PDF")

    # í¬ê¸°ë³„ ì •ë ¬ (ì‘ì€ íŒŒì¼ë¶€í„°)
    scanned_pdfs.sort(key=lambda x: x.size_mb)

    # ì˜ˆì‹œ ì¶œë ¥
    print(f"\nğŸ“„ ì²˜ë¦¬í•  íŒŒì¼ ì˜ˆì‹œ:")
    for info in scanned_pdfs[:5]:
        print(f"  - {info.path.name} ({info.size_mb:.1f}MB, {info.page_count}í˜ì´ì§€)")
    if len(scanned_pdfs) > 5:
        print(f"  ... ì™¸ {len(scanned_pdfs)-5}ê°œ")

    # ì‚¬ìš©ì í™•ì¸
    if not args.force and len(scanned_pdfs) > 50:
        response = input(f"\nâš ï¸ {len(scanned_pdfs)}ê°œ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if response.lower() != 'y':
            print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return 0

    # 2. OCR ì²˜ë¦¬
    print(f"\nğŸš€ OCR ì²˜ë¦¬ ì‹œì‘...")
    start_time = time.time()

    try:
        results = processor.process_all(scanned_pdfs)

        elapsed = time.time() - start_time

        # 3. ê²°ê³¼ ë¦¬í¬íŠ¸
        print("\n" + "="*60)
        print("âœ… OCR ì²˜ë¦¬ ì™„ë£Œ!")
        print("="*60)

        # ë¦¬í¬íŠ¸ ìƒì„± ë° ì¶œë ¥
        report = processor.generate_report(args.report)
        print(report)

        # ì„±ê³µ ì˜ˆì‹œ ì¶œë ¥
        successful_results = [r for r in results if r.ocr_success and r.extracted_text]
        if successful_results:
            example = successful_results[0]
            print(f"\nğŸ“„ OCR ì„±ê³µ ì˜ˆì‹œ ({example.path.name}):")
            text_preview = example.extracted_text[:300]
            print(f"{text_preview}..." if len(example.extracted_text) > 300 else text_preview)

        return 0

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        processor.save_cache()
        processor.save_state()
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        return 1

    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())