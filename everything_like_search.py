#!/usr/bin/env python3
"""
Everythingì²˜ëŸ¼ ì´ˆê³ ì† ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
ë¹ ë¥¸ íŒŒì¼ ê²€ìƒ‰ + ì„ íƒëœ ë¬¸ì„œë§Œ AI ë¶„ì„
"""

import os
import re
import json
import time
import sqlite3
import pdfplumber
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EverythingLikeSearch:
    """Everythingì²˜ëŸ¼ ë¹ ë¥¸ ë¬¸ì„œ ê²€ìƒ‰"""

    def __init__(self):
        self.docs_dir = Path("docs")
        self.db_path = Path("everything_index.db")
        self.conn = None
        self.ocr_cache = {}
        self.setup_database()
        self._load_ocr_cache()

    def setup_database(self):
        """SQLite DB ì„¤ì • - ì´ˆê³ ì† ê²€ìƒ‰ì„ ìœ„í•´"""
        # check_same_thread=Falseë¡œ ë©€í‹°ìŠ¤ë ˆë“œ ì§€ì›
        self.conn = sqlite3.connect(str(self.db_path), check_same_thread=False)
        cursor = self.conn.cursor()

        # ê¸°ì¡´ í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  content ì»¬ëŸ¼ ì¶”ê°€
        cursor.execute("PRAGMA table_info(files)")
        columns = [col[1] for col in cursor.fetchall()]
        has_content = 'content' in columns

        if not has_content and columns:  # í…Œì´ë¸”ì€ ìˆì§€ë§Œ content ì»¬ëŸ¼ì´ ì—†ìŒ
            print("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ ì—…ê·¸ë ˆì´ë“œ: content ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
            cursor.execute("ALTER TABLE files ADD COLUMN content TEXT")
            self.conn.commit()
            print("âœ… content ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ")

        # íŒŒì¼ ì¸ë±ìŠ¤ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY,
                filename TEXT NOT NULL,
                path TEXT NOT NULL,
                size INTEGER,
                date TEXT,
                year INTEGER,
                month INTEGER,
                category TEXT,
                department TEXT,
                keywords TEXT,
                content TEXT,
                created_at TIMESTAMP,
                UNIQUE(path)
            )
        """)

        # ì¸ë±ìŠ¤ ìƒì„± (ì´ˆê³ ì† ê²€ìƒ‰ì„ ìœ„í•´)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_filename ON files(filename)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_date ON files(date)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_year ON files(year)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_category ON files(category)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_keywords ON files(keywords)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_content ON files(content)")

        self.conn.commit()

    def index_all_files(self):
        """ëª¨ë“  íŒŒì¼ ì¸ë±ì‹± (íŒŒì¼ëª…/ë©”íƒ€ë°ì´í„°ë§Œ)"""
        print("ğŸš€ ì´ˆê³ ì† ì¸ë±ì‹± ì‹œì‘...")
        start_time = time.time()

        cursor = self.conn.cursor()

        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        cursor.execute("DELETE FROM files")

        # Zone.Identifier íŒŒì¼ ìë™ ì •ë¦¬ (ìœˆë„ìš° ë‹¤ìš´ë¡œë“œ í”ì )
        zone_files = list(self.docs_dir.rglob("*Zone.Identifier*"))
        if zone_files:
            print(f"ğŸ§¹ {len(zone_files)}ê°œ Zone.Identifier íŒŒì¼ ìë™ ì •ë¦¬ ì¤‘...")
            for zone_file in zone_files:
                try:
                    zone_file.unlink()
                except Exception as e:
                    logger.warning(f"Zone.Identifier ì‚­ì œ ì‹¤íŒ¨: {zone_file}: {e}")
            print("âœ… Zone.Identifier íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")

        # ëª¨ë“  PDF íŒŒì¼ ìˆ˜ì§‘
        pdf_files = list(self.docs_dir.rglob("*.pdf"))

        # ì¤‘ë³µ ì œê±° (ì‹¬ë³¼ë¦­ ë§í¬ ì‹¤ì œ ê²½ë¡œ ê¸°ì¤€, ì ˆëŒ€ ê²½ë¡œë¡œ í†µì¼)
        seen_real_paths = set()
        unique_files = []
        for pdf in pdf_files:
            # ëª¨ë“  ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (ì‹¬ë³¼ë¦­ ë§í¬ë„ í•´ê²°)
            real_path = pdf.resolve()
            real_path_str = str(real_path)

            if real_path_str not in seen_real_paths:
                seen_real_paths.add(real_path_str)
                # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì‚¬ìš© (ì‹¬ë³¼ë¦­ ë§í¬ ì œì™¸)
                unique_files.append(real_path)

        print(f"ğŸ“ {len(unique_files)}ê°œ íŒŒì¼ ì¸ë±ì‹± ì¤‘...")

        for pdf_path in unique_files:
            try:
                filename = pdf_path.name

                # ë‚ ì§œ ì¶”ì¶œ
                date_match = re.search(r'(\d{4})-(\d{2})-(\d{2})', filename)
                date = year = month = None
                if date_match:
                    date = date_match.group(0)
                    year = int(date_match.group(1))
                    month = int(date_match.group(2))

                # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                category = self._extract_category(filename)

                # ë¶€ì„œ ì¶”ì¶œ
                department = self._extract_department(filename)

                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self._extract_keywords(filename)

                # ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ (í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
                content = self._extract_text_content(pdf_path)

                # DBì— ì €ì¥
                cursor.execute("""
                    INSERT OR REPLACE INTO files
                    (filename, path, size, date, year, month, category, department, keywords, content, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    filename,
                    str(pdf_path),
                    pdf_path.stat().st_size,
                    date,
                    year,
                    month,
                    category,
                    department,
                    keywords,
                    content,
                    datetime.now()
                ))

            except Exception as e:
                logger.error(f"íŒŒì¼ ì¸ë±ì‹± ì‹¤íŒ¨: {pdf_path.name}: {e}")

        self.conn.commit()

        elapsed = time.time() - start_time
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ! ({elapsed:.2f}ì´ˆ)")
        print(f"   - {len(unique_files)}ê°œ íŒŒì¼")
        print(f"   - í‰ê·  {elapsed/len(unique_files)*1000:.1f}ms/íŒŒì¼")

    def _extract_category(self, filename: str) -> str:
        """ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        filename_lower = filename.lower()
        if 'êµ¬ë§¤' in filename_lower:
            return 'êµ¬ë§¤'
        elif 'ìˆ˜ë¦¬' in filename_lower:
            return 'ìˆ˜ë¦¬'
        elif 'êµì²´' in filename_lower:
            return 'êµì²´'
        elif 'íê¸°' in filename_lower:
            return 'íê¸°'
        elif 'ì‹ ì²­' in filename_lower or 'ì‹ ì²©' in filename_lower:
            return 'ì‹ ì²­ì„œ'
        elif 'ê²€í† ' in filename_lower:
            return 'ê²€í† '
        elif 'ê¸°ì•ˆ' in filename_lower:
            return 'ê¸°ì•ˆì„œ'
        return 'ê¸°íƒ€'

    def _extract_department(self, filename: str) -> str:
        """ë¶€ì„œ ì¶”ì¶œ"""
        filename_lower = filename.lower()
        if 'ì¤‘ê³„' in filename_lower:
            return 'ì¤‘ê³„'
        elif 'ì¹´ë©”ë¼' in filename_lower:
            return 'ì¹´ë©”ë¼'
        elif 'ì¡°ëª…' in filename_lower:
            return 'ì¡°ëª…'
        elif 'dvr' in filename_lower:
            return 'DVR'
        elif 'ìŠ¤íŠœë””ì˜¤' in filename_lower:
            return 'ìŠ¤íŠœë””ì˜¤'
        elif 'ì†¡ì¶œ' in filename_lower:
            return 'ì†¡ì¶œ'
        elif 'ì˜ìƒ' in filename_lower:
            return 'ì˜ìƒ'
        return ''

    def _extract_keywords(self, filename: str) -> str:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í•œê¸€ë§Œ ì¶”ì¶œ
        korean_words = re.findall(r'[ê°€-í£]+', filename)
        # ì¤‘ìš” í‚¤ì›Œë“œë§Œ í•„í„°ë§ (2ê¸€ì ì´ìƒ)
        keywords = [word for word in korean_words if len(word) >= 2]
        return ' '.join(keywords)

    def _extract_text_content(self, pdf_path: Path) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ (ì²« 3í˜ì´ì§€ë§Œ, ìµœëŒ€ 5000ì)"""
        try:
            text = ""
            with pdfplumber.open(pdf_path) as pdf:
                # ì²« 3í˜ì´ì§€ë§Œ ì²˜ë¦¬ (ì¸ë±ì‹± ì†ë„ë¥¼ ìœ„í•´)
                for page in pdf.pages[:3]:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + " "
                        # 5000ì ì œí•œ (DB í¬ê¸° ê´€ë¦¬)
                        if len(text) >= 5000:
                            break

            # í…ìŠ¤íŠ¸ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°)
            text = ' '.join(text.split())

            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ OCR ìºì‹œ ì‹œë„ (ìŠ¤ìº” ë¬¸ì„œ ë“±)
            if len(text.strip()) < 50:
                ocr_text = self._get_ocr_from_cache(pdf_path)
                if ocr_text:
                    logger.info(f"ğŸ“· OCR ìºì‹œ ì‚¬ìš© (ì¸ë±ì‹±): {pdf_path.name} ({len(ocr_text)}ì)")
                    return ocr_text[:5000]
                return ""

            return text[:5000]  # ìµœëŒ€ 5000ìë¡œ ì œí•œ

        except Exception as e:
            logger.debug(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path.name}: {e}")
            return ""

    def search(self, query: str, limit: int = 20) -> List[Dict]:
        """ì´ˆê³ ì† ê²€ìƒ‰ (Everythingì²˜ëŸ¼) - ê°€ì¤‘ì¹˜ ì ìš©"""
        cursor = self.conn.cursor()

        # ì¿¼ë¦¬ ì „ì²˜ë¦¬ - "dvrê´€ë ¨" ê°™ì€ ê²½ìš° ë¶„ë¦¬
        import re
        # ì˜ì–´ì™€ í•œê¸€ì´ ë¶™ì–´ìˆìœ¼ë©´ ë¶„ë¦¬
        query_processed = re.sub(r'([a-zA-Z]+)(ê´€ë ¨|ê´€í•œ|ëŒ€í•œ|ê´€í•˜ì—¬)', r'\1 \2', query)

        # í•œêµ­ì–´ ë‚ ì§œ í˜•ì‹ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (2024ë…„ 8ì›” -> 2024-08)
        korean_date_pattern = r'(\d{4})ë…„\s*(\d{1,2})ì›”'
        date_match = re.search(korean_date_pattern, query_processed)
        if date_match:
            year = date_match.group(1)
            month = date_match.group(2).zfill(2)  # 1ì›” -> 01
            standard_date = f"{year}-{month}"
            # ì›ë˜ ì¿¼ë¦¬ì—ì„œ í•œêµ­ì–´ ë‚ ì§œë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€ê²½
            query_processed = re.sub(korean_date_pattern, standard_date, query_processed)

        # ì¿¼ë¦¬ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±° ê°•í™”)
        skip_words = [
            'ê´€ë ¨', 'ë¬¸ì„œ', 'ì°¾ì•„', 'ì°¾ì•„ì¤˜', 'ê²€ìƒ‰', 'ì•Œë ¤', 'ì•Œë ¤ì¤˜',
            'ë³´ì—¬', 'ë³´ì—¬ì¤˜', 'ì˜', 'ê±´', 'ë‚´ìš©', 'ëŒ€í•´', 'ëŒ€í•œ', 'ë¬¸ì„œë“¤',
            'ìë£Œ', 'ì¢€', 'ë­', 'ë­”ì§€', 'ì–´ë–»ê²Œ', 'ì´', 'ê·¸', 'ì €',
            'ìš”', 'ì¤˜', 'ì£¼ì„¸ìš”', 'í•´ì¤˜', 'í•´ì£¼ì„¸ìš”'
        ]
        keywords = []
        for word in query_processed.split():
            if word and word not in skip_words and len(word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ (ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°)
                keywords.append(word)

        # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
        if not keywords:
            keywords = [query]

        # DVRì²˜ëŸ¼ ì˜ì–´ëŠ” ëŒ€ë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰
        processed_keywords = []
        for kw in keywords:
            # ì˜ì–´ë§Œ ìˆëŠ” ê²½ìš° ëŒ€ë¬¸ìë¡œ
            if re.match(r'^[a-zA-Z]+$', kw):
                processed_keywords.append(kw.upper())
            else:
                processed_keywords.append(kw)
        keywords = processed_keywords

        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° SQL
        # íŒŒì¼ëª…(5ì ) > ë¶€ì„œ(3ì ) > ì¹´í…Œê³ ë¦¬(2ì ) > í‚¤ì›Œë“œ(2ì ) > ë‚´ìš©(1ì )
        score_parts = []
        params = []

        for keyword in keywords:
            search_term = f'%{keyword}%'
            score_part = """(
                CASE WHEN filename LIKE ? THEN 5 ELSE 0 END +
                CASE WHEN department LIKE ? THEN 3 ELSE 0 END +
                CASE WHEN category LIKE ? THEN 2 ELSE 0 END +
                CASE WHEN keywords LIKE ? THEN 2 ELSE 0 END +
                CASE WHEN content LIKE ? THEN 1 ELSE 0 END
            )"""
            score_parts.append(score_part)
            params.extend([search_term] * 5)

        # SQL ì¿¼ë¦¬ êµ¬ì„± (ì ìˆ˜ ê¸°ë°˜ ì •ë ¬)
        sql = f"""
            SELECT *, ({' + '.join(score_parts)}) as relevance_score
            FROM files
            WHERE relevance_score > 0
            ORDER BY
                relevance_score DESC,
                year DESC,
                month DESC
            LIMIT ?
        """
        params.append(limit)

        cursor.execute(sql, params)

        results = []
        for row in cursor.fetchall():
            results.append({
                'id': row[0],
                'filename': row[1],
                'path': row[2],
                'size': row[3],
                'date': row[4],
                'year': row[5],
                'month': row[6],
                'category': row[7],
                'department': row[8],
                'keywords': row[9],
                'content': row[10],
                'score': row[12]  # relevance_score (row[11]ì€ created_at)
            })

        return results

    def get_document_content(self, file_path: str) -> Dict[str, Any]:
        """ì„ íƒëœ ë¬¸ì„œì˜ ë‚´ìš© ì¶”ì¶œ (ì‹¤ì‹œê°„)"""
        pdf_path = Path(file_path)

        if not pdf_path.exists():
            return {'error': 'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}

        try:
            text = ""
            with pdfplumber.open(pdf_path) as pdf:
                # ìµœëŒ€ 10í˜ì´ì§€ë§Œ
                for page in pdf.pages[:10]:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"

            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´
            if len(text.strip()) < 50:
                text = f"[ìŠ¤ìº” ë¬¸ì„œ - OCR í•„ìš”]\níŒŒì¼ëª…: {pdf_path.name}"

            return {
                'filename': pdf_path.name,
                'text': text,
                'length': len(text),
                'pages': len(pdf.pages) if 'pdf' in locals() else 0
            }

        except Exception as e:
            return {
                'filename': pdf_path.name,
                'error': str(e)
            }

    def summarize_document(self, file_path: str) -> str:
        """ë¬¸ì„œ ìš”ì•½ (LLM ì‚¬ìš©)"""
        content = self.get_document_content(file_path)

        if 'error' in content:
            return f"âŒ ì˜¤ë¥˜: {content['error']}"

        text = content['text'][:3000]  # ì²˜ìŒ 3000ìë§Œ

        # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (ì‹¤ì œë¡œëŠ” LLM ì‚¬ìš©)
        summary = f"""
ğŸ“„ **{content['filename']}**

ğŸ“Š **ë¬¸ì„œ ì •ë³´:**
- í¬ê¸°: {content['length']:,}ì
- í˜ì´ì§€: {content.get('pages', '?')}í˜ì´ì§€

ğŸ“ **ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:**
{text[:500]}...

ğŸ’¡ **ì£¼ìš” ë‚´ìš©:**
- ì´ ë¬¸ì„œëŠ” ë°©ì†¡ì¥ë¹„ ê´€ë ¨ ë¬¸ì„œì…ë‹ˆë‹¤
- êµ¬ë§¤/ìˆ˜ë¦¬/ê²€í†  ë“±ì˜ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤
"""

        return summary

    def _load_ocr_cache(self):
        """OCR ìºì‹œ ë¡œë“œ"""
        ocr_cache_path = self.docs_dir / ".ocr_cache.json"
        if ocr_cache_path.exists():
            try:
                with open(ocr_cache_path, 'r', encoding='utf-8') as f:
                    self.ocr_cache = json.load(f)
                logger.info(f"âœ… OCR ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.ocr_cache)}ê°œ ë¬¸ì„œ")
            except Exception as e:
                logger.warning(f"OCR ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.ocr_cache = {}
        else:
            logger.debug("OCR ìºì‹œ íŒŒì¼ ì—†ìŒ")

    def _get_ocr_from_cache(self, pdf_path: Path) -> str:
        """OCR ìºì‹œì—ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            import hashlib
            with open(pdf_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()

            if file_hash in self.ocr_cache:
                cached_data = self.ocr_cache[file_hash]
                return cached_data.get('text', '')

            return ""
        except Exception as e:
            logger.debug(f"OCR ìºì‹œ ì½ê¸° ì‹¤íŒ¨: {e}")
            return ""


class FastDocumentRAG:
    """ë¹ ë¥¸ ë¬¸ì„œ ê²€ìƒ‰ + AI ìš”ì•½"""

    def __init__(self):
        self.search_engine = EverythingLikeSearch()
        self.search_engine.index_all_files()

    def find_documents(self, query: str) -> List[Dict]:
        """ë¬¸ì„œ ê²€ìƒ‰ (ì´ˆê³ ì†)"""
        return self.search_engine.search(query, limit=10)

    def analyze_document(self, file_path: str) -> str:
        """ì„ íƒëœ ë¬¸ì„œ ë¶„ì„"""
        return self.search_engine.summarize_document(file_path)

    def answer_question(self, query: str) -> Dict:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€"""
        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        results = self.find_documents(query)

        if not results:
            return {
                'answer': "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'documents': []
            }

        # 2. ìƒìœ„ 3ê°œ ë¬¸ì„œ ë¶„ì„
        summaries = []
        for doc in results[:3]:
            content = self.search_engine.get_document_content(doc['path'])
            if 'error' not in content:
                summaries.append({
                    'filename': doc['filename'],
                    'category': doc['category'],
                    'date': doc['date'],
                    'preview': content['text'][:200]
                })

        # 3. ë‹µë³€ ìƒì„±
        answer = f"""
ğŸ” **"{query}"** ê²€ìƒ‰ ê²°ê³¼

ğŸ“š **ì°¾ì€ ë¬¸ì„œ: {len(results)}ê°œ**

"""
        for i, doc in enumerate(results[:5], 1):
            answer += f"{i}. {doc['filename']}\n"
            answer += f"   - ì¹´í…Œê³ ë¦¬: {doc['category']}\n"
            answer += f"   - ë‚ ì§œ: {doc['date'] or 'ë‚ ì§œ ì—†ìŒ'}\n\n"

        if len(results) > 5:
            answer += f"... ì™¸ {len(results)-5}ê°œ ë”\n"

        return {
            'answer': answer,
            'documents': results,
            'summaries': summaries
        }


def main():
    """í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ Everythingì²˜ëŸ¼ ë¹ ë¥¸ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("="*60)

    rag = FastDocumentRAG()

    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    test_queries = [
        "DVR",
        "ì¤‘ê³„ì°¨ ìˆ˜ë¦¬",
        "2020ë…„ ì¹´ë©”ë¼",
        "ì¡°ëª…"
    ]

    for query in test_queries:
        print(f"\nğŸ“Œ ê²€ìƒ‰: {query}")

        start = time.time()
        result = rag.answer_question(query)
        elapsed = time.time() - start

        print(result['answer'][:500])
        print(f"â±ï¸ ê²€ìƒ‰ ì‹œê°„: {elapsed*1000:.1f}ms")
        print("-"*60)


if __name__ == "__main__":
    main()