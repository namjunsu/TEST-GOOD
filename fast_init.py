#!/usr/bin/env python3
"""
ì´ˆê³ ì† ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
OCR ì™„ì „ ìƒëµ, ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ
"""

import time
import logging
from pathlib import Path
import pickle
import json
from concurrent.futures import ThreadPoolExecutor
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

def extract_fast_metadata(pdf_path):
    """ë¹ ë¥¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (OCR ì—†ì´)"""
    try:
        filename = pdf_path.name

        # ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ
        date_pattern = r'(\d{4})[_-](\d{2})[_-](\d{2})'
        date_match = re.search(date_pattern, filename)
        date = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}" if date_match else "unknown"

        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        if any(k in filename.lower() for k in ['êµ¬ë§¤', 'buy', 'purchase']):
            category = "êµ¬ë§¤"
        elif any(k in filename.lower() for k in ['ìˆ˜ë¦¬', 'repair', 'fix']):
            category = "ìˆ˜ë¦¬"
        elif any(k in filename.lower() for k in ['ë³´ê³ ', 'report']):
            category = "ë³´ê³ ì„œ"
        elif any(k in filename.lower() for k in ['ê²€í† ', 'review']):
            category = "ê²€í† "
        else:
            category = "ê¸°íƒ€"

        # ì œëª© ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ)
        title = filename.replace('.pdf', '').replace('_', ' ')

        return {
            'filename': filename,
            'path': str(pdf_path),
            'date': date,
            'category': category,
            'title': title,
            'content': '',  # OCR ì—†ì´ ë¹ˆ ë‚´ìš©
            'size': pdf_path.stat().st_size
        }
    except Exception as e:
        logger.error(f"Error processing {pdf_path.name}: {e}")
        return None

def build_fast_cache():
    """ì´ˆê³ ì† ìºì‹œ êµ¬ì¶•"""
    logger.info("âš¡ ì´ˆê³ ì† ìºì‹œ êµ¬ì¶• ì‹œì‘")
    logger.info("=" * 60)

    start_time = time.time()

    # PDF íŒŒì¼ ì°¾ê¸°
    pdf_dir = Path("docs")
    pdf_files = list(pdf_dir.glob("**/*.pdf"))
    logger.info(f"ğŸ“„ {len(pdf_files)}ê°œ PDF ë°œê²¬")

    # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    cache_dir = Path("config/cache")
    cache_dir.mkdir(parents=True, exist_ok=True)

    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    logger.info("ğŸš€ ë³‘ë ¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    metadata_cache = {}

    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = []
        for pdf_file in pdf_files:
            future = executor.submit(extract_fast_metadata, pdf_file)
            futures.append((pdf_file.name, future))

        completed = 0
        for filename, future in futures:
            result = future.result()
            if result:
                metadata_cache[filename] = result
                completed += 1
                if completed % 50 == 0:
                    logger.info(f"  ì²˜ë¦¬: {completed}/{len(pdf_files)}")

    # ìºì‹œ ì €ì¥
    cache_file = cache_dir / "metadata_cache.pkl"
    with open(cache_file, 'wb') as f:
        pickle.dump(metadata_cache, f)

    # JSON ë²„ì „ë„ ì €ì¥ (ë””ë²„ê¹…ìš©)
    json_file = cache_dir / "metadata_cache.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(metadata_cache, f, ensure_ascii=False, indent=2, default=str)

    elapsed = time.time() - start_time

    # ê²°ê³¼ ì¶œë ¥
    logger.info("=" * 60)
    logger.info(f"âœ… ìºì‹œ êµ¬ì¶• ì™„ë£Œ!")
    logger.info(f"  â€¢ ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
    logger.info(f"  â€¢ ì²˜ë¦¬ëœ ë¬¸ì„œ: {len(metadata_cache)}ê°œ")
    logger.info(f"  â€¢ ìºì‹œ íŒŒì¼: {cache_file}")
    logger.info(f"  â€¢ ìºì‹œ í¬ê¸°: {cache_file.stat().st_size / 1024 / 1024:.1f} MB")
    logger.info("=" * 60)

    return True

if __name__ == "__main__":
    build_fast_cache()