#!/usr/bin/env python3
"""
ë¹ ë¥¸ ì‹œì‘ ìµœì í™” ì‹œìŠ¤í…œ
========================

ë¬¸ì„œ ë¡œë”© ì‹œê°„ì„ ëŒ€í­ ë‹¨ì¶•
"""

import os
import pickle
import time
import concurrent.futures
from pathlib import Path
from typing import Dict, List, Tuple, Any
import hashlib
import json
from datetime import datetime
import multiprocessing as mp

# ìƒ‰ìƒ ì½”ë“œ
GREEN = '\033[92m'
YELLOW = '\033[93m'
RED = '\033[91m'
CYAN = '\033[96m'
RESET = '\033[0m'
BOLD = '\033[1m'


class FastStartupOptimizer:
    """ì´ˆê³ ì† ì‹œì‘ ìµœì í™”"""

    def __init__(self):
        self.docs_dir = Path("docs")
        self.cache_dir = Path(".cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.index_cache_path = self.cache_dir / "document_index.pkl"
        self.metadata_cache_path = self.cache_dir / "metadata_cache.pkl"
        self.stats = {
            'total_files': 0,
            'processed': 0,
            'cached': 0,
            'errors': 0
        }

    def get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ë¹ ë¥¸ ì²´í¬ì„¬)"""
        stat = file_path.stat()
        # íŒŒì¼ëª…, í¬ê¸°, ìˆ˜ì •ì‹œê°„ìœ¼ë¡œ ë¹ ë¥¸ í•´ì‹œ ìƒì„±
        hash_str = f"{file_path.name}_{stat.st_size}_{stat.st_mtime}"
        return hashlib.md5(hash_str.encode()).hexdigest()[:8]

    def process_single_pdf(self, pdf_path: Path) -> Tuple[str, Dict]:
        """ë‹¨ì¼ PDF ë¹ ë¥¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ (ë¹ ë¥¸ ì²˜ë¦¬)
            filename = pdf_path.name
            parts = filename.replace('.pdf', '').split('_')

            # ë‚ ì§œ ì¶”ì¶œ
            date = parts[0] if parts else "unknown"

            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            category = "unknown"
            if "êµ¬ë§¤" in filename:
                category = "purchase"
            elif "ìˆ˜ë¦¬" in filename:
                category = "repair"
            elif "ê²€í† " in filename:
                category = "review"
            elif "íê¸°" in filename:
                category = "disposal"

            metadata = {
                'path': str(pdf_path),
                'filename': filename,
                'date': date,
                'category': category,
                'size': pdf_path.stat().st_size,
                'hash': self.get_file_hash(pdf_path),
                'year': date[:4] if len(date) >= 4 else "unknown"
            }

            return str(pdf_path), metadata

        except Exception as e:
            self.stats['errors'] += 1
            return str(pdf_path), {'error': str(e)}

    def build_fast_index(self, max_files: int = None) -> Dict:
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print(f"{CYAN}{BOLD}âš¡ ë¹ ë¥¸ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘{RESET}")
        start_time = time.time()

        # PDF íŒŒì¼ ëª©ë¡
        pdf_files = list(self.docs_dir.rglob("*.pdf"))

        if max_files:
            pdf_files = pdf_files[:max_files]
            print(f"  {YELLOW}ì œí•œ ëª¨ë“œ: ìµœëŒ€ {max_files}ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬{RESET}")

        self.stats['total_files'] = len(pdf_files)
        print(f"  ë°œê²¬ëœ íŒŒì¼: {len(pdf_files)}ê°œ")

        # ë³‘ë ¬ ì²˜ë¦¬
        index = {}
        cpu_count = mp.cpu_count()
        workers = min(cpu_count * 2, 16)  # ìµœëŒ€ 16ê°œ ì›Œì»¤

        print(f"  {GREEN}ë³‘ë ¬ ì²˜ë¦¬: {workers}ê°œ ì›Œì»¤ ì‚¬ìš©{RESET}")

        with concurrent.futures.ProcessPoolExecutor(max_workers=workers) as executor:
            # ë°°ì¹˜ ì²˜ë¦¬
            batch_size = 50
            for i in range(0, len(pdf_files), batch_size):
                batch = pdf_files[i:i+batch_size]
                futures = {executor.submit(self.process_single_pdf, pdf): pdf for pdf in batch}

                for future in concurrent.futures.as_completed(futures):
                    path, metadata = future.result()
                    index[path] = metadata
                    self.stats['processed'] += 1

                    # ì§„í–‰ ìƒí™©
                    if self.stats['processed'] % 100 == 0:
                        print(f"    ì²˜ë¦¬ ì¤‘: {self.stats['processed']}/{self.stats['total_files']}")

        elapsed = time.time() - start_time
        print(f"\n  {GREEN}âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!{RESET}")
        print(f"    ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print(f"    ì²˜ë¦¬ ì†ë„: {len(pdf_files)/elapsed:.1f} íŒŒì¼/ì´ˆ")

        return index

    def save_cache(self, index: Dict):
        """ìºì‹œ ì €ì¥"""
        print(f"\n{CYAN}ğŸ’¾ ìºì‹œ ì €ì¥ ì¤‘...{RESET}")

        # ì¸ë±ìŠ¤ ìºì‹œ
        with open(self.index_cache_path, 'wb') as f:
            pickle.dump(index, f, protocol=pickle.HIGHEST_PROTOCOL)

        # ë©”íƒ€ë°ì´í„° ìºì‹œ (JSON í˜•ì‹)
        json_cache = self.cache_dir / "metadata.json"
        with open(json_cache, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)

        print(f"  {GREEN}âœ… ìºì‹œ ì €ì¥ ì™„ë£Œ{RESET}")
        print(f"    ì¸ë±ìŠ¤: {self.index_cache_path}")
        print(f"    ë©”íƒ€ë°ì´í„°: {json_cache}")

    def load_cache(self) -> Dict:
        """ìºì‹œ ë¡œë“œ"""
        if self.index_cache_path.exists():
            print(f"{CYAN}ğŸ“‚ ìºì‹œ ë¡œë“œ ì¤‘...{RESET}")
            start = time.time()

            with open(self.index_cache_path, 'rb') as f:
                index = pickle.load(f)

            elapsed = time.time() - start
            print(f"  {GREEN}âœ… ìºì‹œ ë¡œë“œ ì™„ë£Œ: {elapsed:.3f}ì´ˆ{RESET}")
            return index
        return {}

    def optimize_config(self):
        """config.py ìµœì í™”"""
        print(f"\n{CYAN}âš™ï¸  ì„¤ì • ìµœì í™”{RESET}")

        config_content = '''"""
ìµœì í™”ëœ ì„¤ì • íŒŒì¼
==================

ë¹ ë¥¸ ì‹œì‘ì„ ìœ„í•œ ì„¤ì •
"""

import os

# ============ ì„±ëŠ¥ ìµœì í™” ì„¤ì • ============

# ë¬¸ì„œ ë¡œë”© ì œí•œ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
MAX_DOCUMENTS = int(os.getenv('MAX_DOCUMENTS', '100'))  # ìµœëŒ€ 100ê°œë§Œ ë¡œë“œ
LAZY_LOAD = True  # ì§€ì—° ë¡œë”© í™œì„±í™”
PARALLEL_WORKERS = 8  # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜

# ìºì‹± ì„¤ì •
USE_CACHE = True
CACHE_DIR = ".cache"
CACHE_TTL = 3600 * 24  # 24ì‹œê°„

# ëª¨ë¸ ì„¤ì • (ê²½ëŸ‰í™”)
MODEL_NAME = "qwen2.5-7b-instruct-q4_k_m.gguf"
N_CTX = 2048  # ì»¨í…ìŠ¤íŠ¸ ê°ì†Œ (4096 -> 2048)
N_BATCH = 128  # ë°°ì¹˜ í¬ê¸° ê°ì†Œ (256 -> 128)
MAX_TOKENS = 256  # ìµœëŒ€ í† í° ê°ì†Œ (512 -> 256)
N_GPU_LAYERS = 20  # GPU ë ˆì´ì–´ ê°ì†Œ

# ë©”ëª¨ë¦¬ ìµœì í™”
LOW_VRAM = True
OFFLOAD_LAYERS = True
USE_MMAP = True
USE_MLOCK = False

# ê²€ìƒ‰ ì„¤ì •
SEARCH_TOP_K = 3  # ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
MIN_RELEVANCE_SCORE = 0.3

# ë¡œê¹…
LOG_LEVEL = "INFO"
VERBOSE = False

print(f"âš¡ ìµœì í™” ëª¨ë“œ: ìµœëŒ€ {MAX_DOCUMENTS}ê°œ ë¬¸ì„œ ë¡œë“œ")
'''

        # config.py ë°±ì—…
        config_path = Path("config.py")
        if config_path.exists():
            backup_path = Path("config_backup.py")
            config_path.rename(backup_path)
            print(f"  ê¸°ì¡´ ì„¤ì • ë°±ì—…: {backup_path}")

        # ìƒˆ ì„¤ì • ì €ì¥
        config_path.write_text(config_content)
        print(f"  {GREEN}âœ… ì„¤ì • ìµœì í™” ì™„ë£Œ{RESET}")

    def create_quick_start_script(self):
        """ë¹ ë¥¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        print(f"\n{CYAN}ğŸš€ ë¹ ë¥¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±{RESET}")

        script_content = '''#!/bin/bash
# ë¹ ë¥¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸

echo "âš¡ AI-CHAT ë¹ ë¥¸ ì‹œì‘ ëª¨ë“œ"
echo "========================="

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì œí•œ ëª¨ë“œ)
export MAX_DOCUMENTS=50
export USE_CACHE=true
export LOW_VRAM=true
export LOG_LEVEL=WARNING

# ìºì‹œ í™•ì¸
if [ -d ".cache" ]; then
    echo "âœ… ìºì‹œ ë°œê²¬ - ë¹ ë¥¸ ë¡œë”© ê°€ëŠ¥"
else
    echo "âš ï¸  ìºì‹œ ì—†ìŒ - ì´ˆê¸° êµ¬ì¶• í•„ìš”"
    python3 fast_startup_optimizer.py --build-cache
fi

# Streamlit ì‹¤í–‰
echo ""
echo "ğŸš€ ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹œì‘..."
streamlit run web_interface.py

'''

        script_path = Path("quick_start.sh")
        script_path.write_text(script_content)
        script_path.chmod(0o755)
        print(f"  {GREEN}âœ… ë¹ ë¥¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: {script_path}{RESET}")

    def print_summary(self):
        """ìµœì í™” ìš”ì•½"""
        print(f"\n{'='*60}")
        print(f"{BOLD}ğŸ“Š ìµœì í™” ì™„ë£Œ ìš”ì•½{RESET}")
        print(f"{'='*60}")
        print(f"  ì²˜ë¦¬ëœ íŒŒì¼: {self.stats['processed']}ê°œ")
        print(f"  ìºì‹œ ìƒì„±: {self.stats['cached']}ê°œ")
        print(f"  ì˜¤ë¥˜: {self.stats['errors']}ê°œ")
        print(f"\n{GREEN}ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì‹œì‘:{RESET}")
        print(f"  ./quick_start.sh")
        print(f"\n{YELLOW}ğŸ“Œ ì²« ì‹¤í–‰ ì˜ˆìƒ ì‹œê°„:{RESET}")
        print(f"  ê¸°ì¡´: 2-3ë¶„")
        print(f"  ìµœì í™” í›„: 10-15ì´ˆ")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse

    parser = argparse.ArgumentParser(description='ë¹ ë¥¸ ì‹œì‘ ìµœì í™”')
    parser.add_argument('--build-cache', action='store_true', help='ìºì‹œ êµ¬ì¶•')
    parser.add_argument('--max-files', type=int, default=100, help='ìµœëŒ€ íŒŒì¼ ìˆ˜')
    parser.add_argument('--optimize-config', action='store_true', help='ì„¤ì • ìµœì í™”')
    args = parser.parse_args()

    optimizer = FastStartupOptimizer()

    print(f"{BOLD}âš¡ ë¹ ë¥¸ ì‹œì‘ ìµœì í™” ì‹œìŠ¤í…œ{RESET}")
    print("="*60)

    # ìºì‹œ êµ¬ì¶•
    if args.build_cache or not optimizer.index_cache_path.exists():
        index = optimizer.build_fast_index(max_files=args.max_files)
        optimizer.save_cache(index)
        optimizer.stats['cached'] = len(index)
    else:
        index = optimizer.load_cache()
        print(f"  ìºì‹œëœ ë¬¸ì„œ: {len(index)}ê°œ")

    # ì„¤ì • ìµœì í™”
    if args.optimize_config:
        optimizer.optimize_config()

    # ë¹ ë¥¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
    optimizer.create_quick_start_script()

    # ìš”ì•½
    optimizer.print_summary()


if __name__ == "__main__":
    main()