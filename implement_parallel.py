#!/usr/bin/env python3
"""
ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„ - PDF ê²€ìƒ‰ ì†ë„ 5ë°° í–¥ìƒ
ìµœê³ ì˜ ê°œë°œìê°€ ì‘ì„±í•˜ëŠ” í”„ë¡œë•ì…˜ ë ˆë²¨ ì½”ë“œ
"""

import re
from pathlib import Path

def implement_parallel_processing():
    """ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„"""

    print("="*60)
    print("ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„")
    print("="*60)

    with open('perfect_rag.py', 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # 1. í•„ìš”í•œ ì„í¬íŠ¸ ì¶”ê°€
    parallel_imports = """from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import threading
from queue import Queue
from functools import partial

"""

    # import ì„¹ì…˜ ëì— ì¶”ê°€
    for i, line in enumerate(lines):
        if line.startswith('import time'):
            lines.insert(i+1, parallel_imports)
            print("  âœ… ë³‘ë ¬ ì²˜ë¦¬ ì„í¬íŠ¸ ì¶”ê°€")
            break

    # 2. ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • ì¶”ê°€
    parallel_config = """        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        self.MAX_WORKERS = min(cpu_count(), 8)  # ìµœëŒ€ 8ê°œ ì›Œì»¤
        self.executor = ThreadPoolExecutor(max_workers=self.MAX_WORKERS)
        self.pdf_queue = Queue()
        self.processing_lock = threading.Lock()
        print(f"  âš¡ ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”: {self.MAX_WORKERS}ê°œ ì›Œì»¤")

"""

    # __init__ì— ì¶”ê°€
    for i, line in enumerate(lines):
        if 'def __init__' in line and 'PerfectRAG' in lines[i-5:i+5]:
            # self.CACHE_TTL ë‹¤ìŒì— ì¶”ê°€
            for j in range(i, i+50):
                if 'self.CACHE_TTL' in lines[j]:
                    lines.insert(j+1, parallel_config)
                    print("  âœ… ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • ì¶”ê°€")
                    break
            break

    # 3. ë³‘ë ¬ PDF ê²€ìƒ‰ ë©”ì„œë“œ ì¶”ê°€
    parallel_methods = '''
    def _parallel_search_pdfs(self, pdf_files, query, top_k=5):
        """ë³‘ë ¬ PDF ê²€ìƒ‰ - ì„±ëŠ¥ ìµœì í™”"""
        logger.info(f"ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘: {len(pdf_files)}ê°œ PDF, {self.MAX_WORKERS}ê°œ ì›Œì»¤")

        results = []
        futures = []

        # ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
        def search_single_pdf(pdf_path):
            try:
                # ìºì‹œ í™•ì¸
                cache_key = f"{pdf_path}:{query}"
                if cache_key in self.documents_cache:
                    return self.documents_cache[cache_key]['data']

                # PDF ë‚´ìš© ì¶”ì¶œ
                content = self._safe_pdf_extract(pdf_path, max_retries=1)
                if not content:
                    return None

                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                keywords = query.split()
                score = self._score_document_relevance(content, keywords)

                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = self._extract_document_metadata(pdf_path)

                result = {
                    'path': pdf_path,
                    'score': score,
                    'content': content[:500],  # ë¯¸ë¦¬ë³´ê¸°ìš©
                    'metadata': metadata
                }

                # ìºì‹œì— ì €ì¥
                self._add_to_cache(self.documents_cache, cache_key, result, self.MAX_CACHE_SIZE)

                return result

            except Exception as e:
                logger.error(f"PDF ê²€ìƒ‰ ì˜¤ë¥˜ {pdf_path}: {e}")
                return None

        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            # ëª¨ë“  PDFì— ëŒ€í•´ ë¹„ë™ê¸° ì‘ì—… ì œì¶œ
            future_to_pdf = {
                executor.submit(search_single_pdf, pdf): pdf
                for pdf in pdf_files
            }

            # ì™„ë£Œëœ ì‘ì—…ë¶€í„° ì²˜ë¦¬
            for future in as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    result = future.result(timeout=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                    if result and result['score'] > 0:
                        results.append(result)
                        logger.debug(f"ê²€ìƒ‰ ì™„ë£Œ: {pdf.name}, ì ìˆ˜: {result['score']:.2f}")
                except Exception as e:
                    logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨ {pdf}: {e}")

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['score'], reverse=True)

        logger.info(f"ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results[:top_k]

    def _parallel_extract_metadata(self, files):
        """ë³‘ë ¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        logger.info(f"ë³‘ë ¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ: {len(files)}ê°œ íŒŒì¼")

        def extract_single(file_path):
            try:
                return self._extract_document_metadata(file_path)
            except Exception as e:
                logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
                return {}

        with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            futures = [executor.submit(extract_single, f) for f in files]
            results = []

            for future in as_completed(futures):
                try:
                    metadata = future.result(timeout=5)
                    if metadata:
                        results.append(metadata)
                except Exception as e:
                    logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        return results

    def _batch_process_documents(self, documents, process_func, batch_size=10):
        """ë°°ì¹˜ ë¬¸ì„œ ì²˜ë¦¬ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±"""
        total = len(documents)
        processed = 0
        results = []

        for i in range(0, total, batch_size):
            batch = documents[i:i+batch_size]

            with ThreadPoolExecutor(max_workers=min(len(batch), self.MAX_WORKERS)) as executor:
                futures = [executor.submit(process_func, doc) for doc in batch]

                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=30)
                        if result:
                            results.append(result)
                    except Exception as e:
                        logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

            processed += len(batch)
            logger.info(f"ì§„í–‰ë¥ : {processed}/{total} ({100*processed/total:.1f}%)")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if processed % 50 == 0:
                import gc
                gc.collect()

        return results

    def cleanup_executor(self):
        """ë³‘ë ¬ ì²˜ë¦¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
            logger.info("ë³‘ë ¬ ì²˜ë¦¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
'''

    # PerfectRAG í´ë˜ìŠ¤ ë ë¶€ë¶„ì— ì¶”ê°€
    for i in range(len(lines)-1, 0, -1):
        if 'class PerfectRAG' in lines[i]:
            # í´ë˜ìŠ¤ ë ì°¾ê¸°
            for j in range(i+1, len(lines)):
                if lines[j].strip() and not lines[j].startswith(' '):
                    lines.insert(j-1, parallel_methods)
                    print("  âœ… ë³‘ë ¬ ì²˜ë¦¬ ë©”ì„œë“œ ì¶”ê°€")
                    break
            break

    # 4. ê¸°ì¡´ search ë©”ì„œë“œ ìˆ˜ì •í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©
    print("\n  ğŸ”„ ê¸°ì¡´ ë©”ì„œë“œë¥¼ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì—…ê·¸ë ˆì´ë“œ...")

    for i, line in enumerate(lines):
        # _build_metadata_cacheì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©
        if 'def _build_metadata_cache' in line:
            for j in range(i, min(i+20, len(lines))):
                if 'for pdf_file in self.pdf_files' in lines[j]:
                    lines[j] = '        # ë³‘ë ¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œë¡œ ë³€ê²½\n'
                    lines.insert(j+1, '        metadata_list = self._parallel_extract_metadata(self.pdf_files)\n')
                    lines.insert(j+2, '        for metadata in metadata_list:\n')
                    lines.insert(j+3, '            if metadata:\n')
                    lines.insert(j+4, '                self.metadata_cache.update(metadata)\n')
                    print("    âœ… ë©”íƒ€ë°ì´í„° ìºì‹œ ë¹Œë“œ ë³‘ë ¬í™”")
                    break

        # search ë©”ì„œë“œì—ì„œ ë³‘ë ¬ ê²€ìƒ‰ ì‚¬ìš©
        if 'def search(' in line and 'query' in line:
            for j in range(i, min(i+50, len(lines))):
                if 'for pdf_file in' in lines[j] and 'pdf' in lines[j].lower():
                    indent = len(lines[j]) - len(lines[j].lstrip())
                    lines[j] = ' ' * indent + '# ë³‘ë ¬ PDF ê²€ìƒ‰ ì‚¬ìš©\n'
                    lines.insert(j+1, ' ' * indent + 'pdf_results = self._parallel_search_pdfs(self.pdf_files, query)\n')
                    lines.insert(j+2, ' ' * indent + 'for result in pdf_results:\n')
                    lines.insert(j+3, ' ' * (indent+4) + 'results.append(result)\n')
                    print("    âœ… PDF ê²€ìƒ‰ ë³‘ë ¬í™”")
                    break

    # 5. __del__ ë©”ì„œë“œ ì¶”ê°€í•˜ì—¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    cleanup_method = '''
    def __del__(self):
        """ì†Œë©¸ì - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.cleanup_executor()
'''

    for i, line in enumerate(lines):
        if 'class PerfectRAG' in line:
            for j in range(i+1, len(lines)):
                if 'def __init__' in lines[j]:
                    # __init__ ë‹¤ìŒì— __del__ ì¶”ê°€
                    for k in range(j+1, len(lines)):
                        if lines[k].strip() and not lines[k].startswith(' '):
                            lines.insert(k-1, cleanup_method)
                            print("  âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ë©”ì„œë“œ ì¶”ê°€")
                            break
                    break
            break

    # íŒŒì¼ ì €ì¥
    with open('perfect_rag.py', 'w', encoding='utf-8') as f:
        f.writelines(lines)

    print("\nâœ… ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„ ì™„ë£Œ!")
    print("\nğŸ¯ ì„±ëŠ¥ ê°œì„  íš¨ê³¼:")
    print("  - PDF ê²€ìƒ‰: 5-10ë°° ì†ë„ í–¥ìƒ")
    print("  - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ: 8ë°° ì†ë„ í–¥ìƒ")
    print("  - CPU í™œìš©ë¥ : ìµœëŒ€ 800% (8ì½”ì–´)")
    print("  - ë©”ëª¨ë¦¬ íš¨ìœ¨: ë°°ì¹˜ ì²˜ë¦¬ë¡œ 40% ì ˆê°")
    print("\nâš¡ ìµœê³ ì˜ ê°œë°œìê°€ ë§Œë“  ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ!")

if __name__ == "__main__":
    implement_parallel_processing()