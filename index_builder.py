#!/usr/bin/env python3
"""
ë¬¸ì„œ ì¸ë±ìŠ¤ ë¹Œë”
ëª¨ë“  PDFë¥¼ ì²­í‚¹í•˜ê³  BM25/Vector ì¸ë±ìŠ¤ ìƒì„±
"""

import pickle
import json
from pathlib import Path
from typing import List, Dict, Any
import pdfplumber
import logging
from tqdm import tqdm

from rag_system.bm25_store import BM25Store
from rag_system.korean_vector_store import KoreanVectorStore

logger = logging.getLogger(__name__)

class IndexBuilder:
    """ë¬¸ì„œ ì¸ë±ìŠ¤ êµ¬ì¶•"""

    def __init__(self):
        self.docs_dir = Path("docs")
        self.index_dir = Path("indexes")
        self.index_dir.mkdir(exist_ok=True)

        # ì²­í¬ ì„¤ì •
        self.chunk_size = 1000  # ê¸€ì ìˆ˜
        self.chunk_overlap = 200  # ì¤‘ì²©

        logger.info("IndexBuilder ì´ˆê¸°í™”")

    def build_all_indexes(self):
        """ëª¨ë“  ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ğŸ“š ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘...")

        # 1. ëª¨ë“  ë¬¸ì„œ ì²­í‚¹
        chunks = self.chunk_all_documents()
        print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")

        # 2. BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
        self.build_bm25_index(chunks)
        print("âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")

        # 3. Vector ì¸ë±ìŠ¤ êµ¬ì¶•
        self.build_vector_index(chunks)
        print("âœ… Vector ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")

        # 4. ë©”íƒ€ë°ì´í„° ì €ì¥
        self.save_metadata(chunks)
        print("âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")

        print(f"ğŸ‰ ì¸ë±ì‹± ì™„ë£Œ! ì´ {len(chunks)}ê°œ ì²­í¬")

    def chunk_all_documents(self) -> List[Dict]:
        """ëª¨ë“  ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        chunk_id = 0

        # PDF íŒŒì¼ ì°¾ê¸°
        pdf_files = list(self.docs_dir.glob("**/*.pdf"))
        print(f"ğŸ“„ {len(pdf_files)}ê°œ PDF ë°œê²¬")

        for pdf_path in tqdm(pdf_files, desc="ë¬¸ì„œ ì²­í‚¹"):
            try:
                # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = self._extract_pdf_text(pdf_path)
                if not text:
                    continue

                # ì²­í‚¹
                doc_chunks = self._create_chunks(text, pdf_path)

                for chunk_text in doc_chunks:
                    chunk = {
                        'id': f"chunk_{chunk_id}",
                        'content': chunk_text,
                        'metadata': {
                            'source': pdf_path.name,
                            'path': str(pdf_path),
                            'chunk_id': chunk_id
                        }
                    }
                    chunks.append(chunk)
                    chunk_id += 1

            except Exception as e:
                logger.error(f"ì²­í‚¹ ì‹¤íŒ¨: {pdf_path.name} - {e}")
                continue

        return chunks

    def _extract_pdf_text(self, pdf_path: Path) -> str:
        """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text = ""

        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page in pdf.pages[:50]:  # ìµœëŒ€ 50í˜ì´ì§€
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
        except Exception as e:
            logger.error(f"PDF ì½ê¸° ì‹¤íŒ¨: {pdf_path.name} - {e}")

        return text

    def _create_chunks(self, text: str, source_path: Path) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = text.replace('\n', ' ').split('.')

        current_chunk = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            # ì²­í¬ í¬ê¸° ì²´í¬
            if len(current_chunk) + len(sentence) < self.chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence + ". "

        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    def build_bm25_index(self, chunks: List[Dict]):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        bm25 = BM25Store()

        # í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        texts = [chunk['content'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]

        # ì¸ë±ìŠ¤ êµ¬ì¶• (add_documents ì‚¬ìš©)
        bm25.add_documents(texts, metadatas)

        # ì €ì¥
        index_path = self.index_dir / "bm25_index.pkl"
        with open(index_path, 'wb') as f:
            pickle.dump(bm25, f)

        logger.info(f"BM25 ì¸ë±ìŠ¤ ì €ì¥: {index_path}")

    def build_vector_index(self, chunks: List[Dict]):
        """Vector ì¸ë±ìŠ¤ êµ¬ì¶•"""
        vector_store = KoreanVectorStore()

        # ì„ë² ë”© ìƒì„± ë° ì¸ë±ì‹±
        vector_store.add_documents(chunks)

        # ì €ì¥
        vector_store.save_index(str(self.index_dir / "vector_index"))

        logger.info("Vector ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")

    def save_metadata(self, chunks: List[Dict]):
        """ì²­í¬ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata_path = self.index_dir / "chunks_metadata.json"

        # ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ (contentëŠ” ë„ˆë¬´ í¬ë¯€ë¡œ ì œì™¸)
        metadata = []
        for chunk in chunks:
            metadata.append({
                'id': chunk['id'],
                'source': chunk['metadata']['source'],
                'path': chunk['metadata']['path'],
                'content_preview': chunk['content'][:100]  # ë¯¸ë¦¬ë³´ê¸°ë§Œ
            })

        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        logger.info(f"ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

    def load_indexes(self) -> Dict:
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        indexes = {}

        # BM25 ë¡œë“œ
        bm25_path = self.index_dir / "bm25_index.pkl"
        if bm25_path.exists():
            with open(bm25_path, 'rb') as f:
                indexes['bm25'] = pickle.load(f)

        # Vector ë¡œë“œ
        vector_store = KoreanVectorStore()
        vector_index_path = self.index_dir / "vector_index"
        if vector_index_path.exists():
            vector_store.load_index(str(vector_index_path))
            indexes['vector'] = vector_store

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = self.index_dir / "chunks_metadata.json"
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                indexes['metadata'] = json.load(f)

        return indexes

if __name__ == "__main__":
    # ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤í–‰
    builder = IndexBuilder()
    builder.build_all_indexes()