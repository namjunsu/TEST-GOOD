#!/usr/bin/env python3
"""
Lazy Loading System - ê³ ê¸‰ ì§€ì—° ë¡œë”© ì‹œìŠ¤í…œ

ì£¼ìš” ê¸°ëŠ¥:
- í•„ìš”í•œ ë¬¸ì„œë§Œ ì˜¨ë””ë§¨ë“œ ë¡œë“œ
- LRU ìºì‹±ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
- íŒŒì¼ ë³€ê²½ ìë™ ê°ì§€
- ë©”íƒ€ë°ì´í„° ì¸ë±ì‹±
- ìŠ¤ë ˆë“œ ì•ˆì „ ë³´ì¥
- ë°°ì¹˜ ë¡œë”© ì§€ì›
- ë¹„ë™ê¸° í”„ë¦¬í˜ì¹­
"""
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any, Callable
from collections import OrderedDict, defaultdict
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, Future
import threading
import hashlib
import json
import time
import logging
import re
import weakref
import gc

try:
    import pdfplumber
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False
    logging.warning("pdfplumber not installed - PDF support disabled")

try:
    from functools import lru_cache
except ImportError:
    # Python 2 fallback (shouldn't happen but just in case)
    def lru_cache(maxsize=128):
        def decorator(func):
            cache = OrderedDict()
            def wrapper(*args, **kwargs):
                key = str(args) + str(kwargs)
                if key in cache:
                    cache.move_to_end(key)
                    return cache[key]
                result = func(*args, **kwargs)
                cache[key] = result
                if len(cache) > maxsize:
                    cache.popitem(last=False)
                return result
            return wrapper
        return decorator

@dataclass
class DocumentInfo:
    """ë¬¸ì„œ ì •ë³´ ë°ì´í„°í´ë˜ìŠ¤"""
    path: Path
    name: str
    size: int
    modified: float
    hash: Optional[str] = None
    metadata: Optional[Dict] = None
    content: Optional[str] = None
    loaded: bool = False
    load_time: Optional[float] = None

class LRUCache:
    """Thread-safe LRU Cache implementation"""

    def __init__(self, maxsize: int = 100):
        self.cache = OrderedDict()
        self.maxsize = maxsize
        self.lock = threading.RLock()
        self.hits = 0
        self.misses = 0

    def get(self, key: str) -> Optional[Any]:
        with self.lock:
            if key in self.cache:
                self.cache.move_to_end(key)
                self.hits += 1
                return self.cache[key]
            self.misses += 1
            return None

    def put(self, key: str, value: Any) -> None:
        with self.lock:
            if key in self.cache:
                self.cache.move_to_end(key)
            self.cache[key] = value
            if len(self.cache) > self.maxsize:
                self.cache.popitem(last=False)

    def invalidate(self, key: str) -> None:
        with self.lock:
            self.cache.pop(key, None)

    def clear(self) -> None:
        with self.lock:
            self.cache.clear()

    def get_stats(self) -> Dict[str, Any]:
        with self.lock:
            total = self.hits + self.misses
            return {
                'hits': self.hits,
                'misses': self.misses,
                'hit_rate': self.hits / total if total > 0 else 0,
                'size': len(self.cache),
                'maxsize': self.maxsize
            }

class LazyDocumentLoader:
    """ê³ ê¸‰ ì§€ì—° ë¬¸ì„œ ë¡œë”"""

    def __init__(self, docs_dir: Path, cache_size: int = 100,
                 prefetch_size: int = 10, max_workers: int = 4):
        self.docs_dir = Path(docs_dir)
        self.cache_size = cache_size
        self.prefetch_size = prefetch_size
        self.max_workers = max_workers

        # ë°ì´í„° êµ¬ì¡°
        self.file_index = {}  # filename -> DocumentInfo
        self.path_index = {}  # full_path -> DocumentInfo
        self.metadata_index = defaultdict(lambda: defaultdict(set))  # field -> value -> filenames

        # ìºì‹±
        self.content_cache = LRUCache(cache_size)
        self.hash_cache = {}  # path -> (hash, mtime)

        # ìŠ¤ë ˆë“œ ê´€ë¦¬
        self.lock = threading.RLock()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.prefetch_queue = set()
        self.prefetch_futures = {}

        # í†µê³„
        self.stats = {
            'files_scanned': 0,
            'files_loaded': 0,
            'total_load_time': 0.0,
            'cache_invalidations': 0,
            'prefetch_hits': 0
        }

        # ë¡œê±°
        self.logger = logging.getLogger(__name__)

    def quick_scan(self, pattern: Optional[str] = None) -> int:
        """ë¹ ë¥¸ íŒŒì¼ ìŠ¤ìº” (ë‚´ìš© ë¡œë“œ ì—†ì´)

        Args:
            pattern: íŒŒì¼ëª… íŒ¨í„´ í•„í„° (ì •ê·œì‹)

        Returns:
            ìŠ¤ìº”ëœ íŒŒì¼ ê°œìˆ˜
        """
        start_time = time.time()

        with self.lock:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
            self.file_index.clear()
            self.path_index.clear()
            self.metadata_index.clear()

            # ê²€ìƒ‰ ê²½ë¡œ êµ¬ì„±
            search_paths = self._get_search_paths()
            pattern_re = re.compile(pattern) if pattern else None

            # ë³‘ë ¬ ìŠ¤ìº”
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = []
                for path in search_paths:
                    if path.exists():
                        future = executor.submit(self._scan_directory, path, pattern_re)
                        futures.append(future)

                # ê²°ê³¼ ìˆ˜ì§‘
                for future in futures:
                    docs = future.result()
                    for doc in docs:
                        self.file_index[doc.name] = doc
                        self.path_index[str(doc.path)] = doc
                        self.stats['files_scanned'] += 1

            # ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ êµ¬ì¶•
            self._build_metadata_index()

        elapsed = time.time() - start_time
        count = len(self.file_index)
        self.logger.info(f"âš¡ {count}ê°œ íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
        return count

    def _get_search_paths(self) -> List[Path]:
        """ê²€ìƒ‰ ê²½ë¡œ ëª©ë¡ ìƒì„±"""
        paths = [self.docs_dir]

        # ì—°ë„ë³„ í´ë”
        for year in range(2014, 2026):
            paths.append(self.docs_dir / f"year_{year}")

        # ì¹´í…Œê³ ë¦¬ í´ë”
        categories = ['category_purchase', 'category_repair', 'category_review',
                     'category_disposal', 'category_consumables', 'recent', 'archive', 'assets']
        for cat in categories:
            paths.append(self.docs_dir / cat)

        return paths

    def _scan_directory(self, path: Path, pattern_re: Optional[re.Pattern]) -> List[DocumentInfo]:
        """ë””ë ‰í† ë¦¬ ìŠ¤ìº” (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        docs = []

        for ext in ['*.pdf', '*.txt']:
            for file_path in path.glob(ext):
                if pattern_re and not pattern_re.search(file_path.name):
                    continue

                try:
                    stat = file_path.stat()
                    doc = DocumentInfo(
                        path=file_path,
                        name=file_path.name,
                        size=stat.st_size,
                        modified=stat.st_mtime
                    )

                    # ê°„ë‹¨í•œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (íŒŒì¼ëª… ê¸°ë°˜)
                    doc.metadata = self._extract_quick_metadata(file_path)
                    docs.append(doc)

                except Exception as e:
                    self.logger.warning(f"Failed to scan {file_path}: {e}")

        return docs

    def _extract_quick_metadata(self, file_path: Path) -> Dict[str, Any]:
        """íŒŒì¼ëª…ì—ì„œ ë¹ ë¥¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {
            'extension': file_path.suffix.lower(),
            'folder': file_path.parent.name
        }

        # ì—°ë„ ì¶”ì¶œ
        year_match = re.search(r'(20\d{2})', file_path.name)
        if year_match:
            metadata['year'] = int(year_match.group(1))

        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        if 'êµ¬ë§¤' in file_path.name or 'purchase' in file_path.parent.name:
            metadata['category'] = 'purchase'
        elif 'ìˆ˜ë¦¬' in file_path.name or 'repair' in file_path.parent.name:
            metadata['category'] = 'repair'
        elif 'íê¸°' in file_path.name or 'disposal' in file_path.parent.name:
            metadata['category'] = 'disposal'

        return metadata

    def _build_metadata_index(self) -> None:
        """ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        for filename, doc in self.file_index.items():
            if doc.metadata:
                for field, value in doc.metadata.items():
                    self.metadata_index[field][str(value)].add(filename)

    def load_document(self, filename: str, prefetch_related: bool = True) -> Optional[Dict]:
        """í•„ìš”í•  ë•Œë§Œ ë¬¸ì„œ ë¡œë“œ

        Args:
            filename: ë¡œë“œí•  íŒŒì¼ëª…
            prefetch_related: ê´€ë ¨ ë¬¸ì„œ í”„ë¦¬í˜ì¹­ ì—¬ë¶€

        Returns:
            ë¬¸ì„œ ë°ì´í„° ë˜ëŠ” None
        """
        # ìºì‹œ í™•ì¸
        cached = self.content_cache.get(filename)
        if cached:
            # íŒŒì¼ ë³€ê²½ í™•ì¸
            doc = self.file_index.get(filename)
            if doc and not self._is_file_changed(doc):
                return cached
            else:
                self.content_cache.invalidate(filename)
                self.stats['cache_invalidations'] += 1

        # í”„ë¦¬í˜ì¹˜ íì— ìˆëŠ”ì§€ í™•ì¸
        if filename in self.prefetch_futures:
            future = self.prefetch_futures.pop(filename)
            try:
                result = future.result(timeout=5.0)
                self.stats['prefetch_hits'] += 1
                return result
            except Exception as e:
                self.logger.warning(f"Prefetch failed for {filename}: {e}")

        # ë¬¸ì„œ ë¡œë“œ
        with self.lock:
            doc = self.file_index.get(filename)
            if not doc:
                return None

            start_time = time.time()
            result = self._load_single_document(doc)
            load_time = time.time() - start_time

            if result:
                doc.loaded = True
                doc.load_time = load_time
                self.stats['files_loaded'] += 1
                self.stats['total_load_time'] += load_time

                # ìºì‹œì— ì €ì¥
                self.content_cache.put(filename, result)

                # ê´€ë ¨ ë¬¸ì„œ í”„ë¦¬í˜ì¹­
                if prefetch_related:
                    self._prefetch_related(doc)

                return result

        return None

    def load_batch(self, filenames: List[str], parallel: bool = True) -> Dict[str, Dict]:
        """ì—¬ëŸ¬ ë¬¸ì„œ ë°°ì¹˜ ë¡œë“œ

        Args:
            filenames: ë¡œë“œí•  íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
            parallel: ë³‘ë ¬ ì²˜ë¦¬ ì—¬ë¶€

        Returns:
            filename -> ë¬¸ì„œ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        results = {}

        if parallel:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {}
                for filename in filenames:
                    # ìºì‹œì— ìˆìœ¼ë©´ ìŠ¤í‚µ
                    cached = self.content_cache.get(filename)
                    if cached:
                        results[filename] = cached
                    else:
                        future = executor.submit(self.load_document, filename, False)
                        futures[filename] = future

                # ê²°ê³¼ ìˆ˜ì§‘
                for filename, future in futures.items():
                    try:
                        result = future.result(timeout=10.0)
                        if result:
                            results[filename] = result
                    except Exception as e:
                        self.logger.error(f"Failed to load {filename}: {e}")
        else:
            for filename in filenames:
                result = self.load_document(filename, False)
                if result:
                    results[filename] = result

        return results

    def _load_single_document(self, doc: DocumentInfo) -> Optional[Dict]:
        """ë‹¨ì¼ ë¬¸ì„œ ë¡œë“œ (ì‹¤ì œ ë‚´ìš© ì¶”ì¶œ)"""
        try:
            file_path = doc.path

            # íŒŒì¼ í•´ì‹œ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)
            doc.hash = self._calculate_file_hash(file_path)

            # í™•ì¥ìë³„ ì²˜ë¦¬
            if file_path.suffix.lower() == '.pdf':
                content = self._extract_pdf_content(file_path)
            elif file_path.suffix.lower() == '.txt':
                content = self._extract_text_content(file_path)
            else:
                self.logger.warning(f"Unsupported file type: {file_path.suffix}")
                return None

            # ê²°ê³¼ êµ¬ì„±
            result = {
                'filename': doc.name,
                'path': str(doc.path),
                'content': content,
                'size': doc.size,
                'modified': doc.modified,
                'hash': doc.hash,
                'metadata': doc.metadata or {},
                'loaded_at': time.time()
            }

            # ì „ì²´ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë‚´ìš© ê¸°ë°˜)
            if content:
                result['metadata'].update(self._extract_content_metadata(content))
                doc.content = content[:1000]  # ë¯¸ë¦¬ë³´ê¸°ìš© ì¼ë¶€ë§Œ ì €ì¥

            return result

        except Exception as e:
            self.logger.error(f"Failed to load document {doc.path}: {e}")
            return None

    def _extract_pdf_content(self, file_path: Path) -> str:
        """PDF ë‚´ìš© ì¶”ì¶œ"""
        if not PDF_SUPPORT:
            return "[PDF support not available]"

        try:
            content = []
            with pdfplumber.open(file_path) as pdf:
                # ì²˜ìŒ 10í˜ì´ì§€ë§Œ ì¶”ì¶œ (ì„±ëŠ¥ ê³ ë ¤)
                max_pages = min(10, len(pdf.pages))
                for i in range(max_pages):
                    page = pdf.pages[i]
                    text = page.extract_text()
                    if text:
                        content.append(text)

            return '\n'.join(content)

        except Exception as e:
            self.logger.warning(f"PDF extraction failed for {file_path}: {e}")
            return ""

    def _extract_text_content(self, file_path: Path) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš© ì¶”ì¶œ"""
        try:
            # ì¸ì½”ë”© ìë™ ê°ì§€
            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1']

            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        return f.read()
                except UnicodeDecodeError:
                    continue

            # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ì‹œ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê¸°
            with open(file_path, 'rb') as f:
                return f.read().decode('utf-8', errors='ignore')

        except Exception as e:
            self.logger.warning(f"Text extraction failed for {file_path}: {e}")
            return ""

    def _extract_content_metadata(self, content: str) -> Dict[str, Any]:
        """ë‚´ìš©ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}

        # ê¸ˆì•¡ ì¶”ì¶œ
        amounts = re.findall(r'([\d,]+)\s*(?:ì›|ì²œì›|ë§Œì›|ì–µì›)', content)
        if amounts:
            metadata['amounts'] = amounts[:5]  # ì²˜ìŒ 5ê°œë§Œ

        # ë‚ ì§œ ì¶”ì¶œ
        dates = re.findall(r'(20\d{2}[-./]\d{1,2}[-./]\d{1,2})', content)
        if dates:
            metadata['dates'] = list(set(dates))[:5]

        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë¹ˆë„ ê¸°ë°˜)
        words = re.findall(r'[ê°€-í£]{2,}', content)
        if words:
            from collections import Counter
            word_freq = Counter(words)
            metadata['keywords'] = [w for w, _ in word_freq.most_common(10)]

        return metadata

    def _calculate_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ìºì‹± í¬í•¨)"""
        cache_key = str(file_path)
        current_mtime = file_path.stat().st_mtime

        # ìºì‹œ í™•ì¸
        if cache_key in self.hash_cache:
            cached_hash, cached_mtime = self.hash_cache[cache_key]
            if cached_mtime == current_mtime:
                return cached_hash

        # í•´ì‹œ ê³„ì‚°
        hash_md5 = hashlib.md5()
        with open(file_path, 'rb') as f:
            # í° íŒŒì¼ì€ ìƒ˜í”Œë§
            if file_path.stat().st_size > 10 * 1024 * 1024:  # 10MB
                f.read(1024 * 1024)  # ì²˜ìŒ 1MB
                hash_md5.update(f.read(1024 * 1024))
                f.seek(-1024 * 1024, 2)  # ë§ˆì§€ë§‰ 1MB
                hash_md5.update(f.read())
            else:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)

        file_hash = hash_md5.hexdigest()
        self.hash_cache[cache_key] = (file_hash, current_mtime)
        return file_hash

    def _is_file_changed(self, doc: DocumentInfo) -> bool:
        """íŒŒì¼ ë³€ê²½ ì—¬ë¶€ í™•ì¸"""
        try:
            current_mtime = doc.path.stat().st_mtime
            return current_mtime != doc.modified
        except:
            return True

    def _prefetch_related(self, doc: DocumentInfo) -> None:
        """ê´€ë ¨ ë¬¸ì„œ í”„ë¦¬í˜ì¹­"""
        if not doc.metadata:
            return

        related_files = set()

        # ê°™ì€ ì—°ë„ ë¬¸ì„œ
        if 'year' in doc.metadata:
            year_files = self.metadata_index['year'].get(str(doc.metadata['year']), set())
            related_files.update(list(year_files)[:self.prefetch_size // 2])

        # ê°™ì€ ì¹´í…Œê³ ë¦¬ ë¬¸ì„œ
        if 'category' in doc.metadata:
            cat_files = self.metadata_index['category'].get(doc.metadata['category'], set())
            related_files.update(list(cat_files)[:self.prefetch_size // 2])

        # í˜„ì¬ íŒŒì¼ê³¼ ì´ë¯¸ íì— ìˆëŠ” íŒŒì¼ ì œì™¸
        related_files.discard(doc.name)
        related_files -= self.prefetch_queue

        # í”„ë¦¬í˜ì¹­ ì‹œì‘
        for filename in list(related_files)[:self.prefetch_size]:
            if filename not in self.content_cache.cache:
                self.prefetch_queue.add(filename)
                future = self.executor.submit(self.load_document, filename, False)
                self.prefetch_futures[filename] = future

    def get_file_list(self, filter_func: Optional[Callable] = None) -> List[str]:
        """íŒŒì¼ëª… ëª©ë¡ ë°˜í™˜

        Args:
            filter_func: í•„í„° í•¨ìˆ˜ (DocumentInfo -> bool)

        Returns:
            íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
        """
        with self.lock:
            if filter_func:
                return [name for name, doc in self.file_index.items()
                       if filter_func(doc)]
            return list(self.file_index.keys())

    def search_by_metadata(self, field: str, value: str) -> List[str]:
        """ë©”íƒ€ë°ì´í„°ë¡œ ê²€ìƒ‰

        Args:
            field: ë©”íƒ€ë°ì´í„° í•„ë“œëª…
            value: ê²€ìƒ‰í•  ê°’

        Returns:
            ë§¤ì¹­ë˜ëŠ” íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
        """
        with self.lock:
            return list(self.metadata_index.get(field, {}).get(str(value), set()))

    def get_document_info(self, filename: str) -> Optional[DocumentInfo]:
        """ë¬¸ì„œ ì •ë³´ ë°˜í™˜ (ë‚´ìš© ë¡œë“œ ì—†ì´)"""
        with self.lock:
            return self.file_index.get(filename)

    def invalidate_cache(self, filename: Optional[str] = None) -> None:
        """ìºì‹œ ë¬´íš¨í™”

        Args:
            filename: íŠ¹ì • íŒŒì¼ë§Œ ë¬´íš¨í™”, Noneì´ë©´ ì „ì²´ ë¬´íš¨í™”
        """
        if filename:
            self.content_cache.invalidate(filename)
            self.logger.info(f"Cache invalidated for {filename}")
        else:
            self.content_cache.clear()
            self.logger.info("All cache cleared")

    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        with self.lock:
            cache_stats = self.content_cache.get_stats()

            return {
                'files_scanned': self.stats['files_scanned'],
                'files_loaded': self.stats['files_loaded'],
                'avg_load_time': (self.stats['total_load_time'] / self.stats['files_loaded']
                                 if self.stats['files_loaded'] > 0 else 0),
                'cache_stats': cache_stats,
                'cache_invalidations': self.stats['cache_invalidations'],
                'prefetch_hits': self.stats['prefetch_hits'],
                'memory_usage': self._estimate_memory_usage()
            }

    def _estimate_memory_usage(self) -> int:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (bytes)"""
        total = 0

        # ì¸ë±ìŠ¤ ë©”ëª¨ë¦¬
        for doc in self.file_index.values():
            total += 1024  # DocumentInfo ê¸°ë³¸ í¬ê¸°
            if doc.content:
                total += len(doc.content.encode('utf-8'))

        # ìºì‹œ ë©”ëª¨ë¦¬
        for key, value in self.content_cache.cache.items():
            total += len(str(key).encode('utf-8'))
            if isinstance(value, dict) and 'content' in value:
                total += len(str(value['content']).encode('utf-8'))

        return total

    def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.executor.shutdown(wait=False)
        self.content_cache.clear()
        self.hash_cache.clear()
        self.prefetch_futures.clear()
        gc.collect()
        self.logger.info("LazyDocumentLoader cleanup completed")

if __name__ == "__main__":
    import argparse

    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    parser = argparse.ArgumentParser(description="Lazy Document Loader Test")
    parser.add_argument('--docs-dir', default='docs', help='Documents directory')
    parser.add_argument('--cache-size', type=int, default=100, help='Cache size')
    parser.add_argument('--test-load', action='store_true', help='Test document loading')
    parser.add_argument('--test-batch', action='store_true', help='Test batch loading')
    parser.add_argument('--test-search', action='store_true', help='Test metadata search')
    args = parser.parse_args()

    # í…ŒìŠ¤íŠ¸
    loader = LazyDocumentLoader(
        Path(args.docs_dir),
        cache_size=args.cache_size
    )

    print("ğŸš€ LazyDocumentLoader Test")
    print("=" * 50)

    # 1. ë¹ ë¥¸ ìŠ¤ìº” í…ŒìŠ¤íŠ¸
    print("\nğŸ“‚ Quick Scan Test:")
    start = time.time()
    count = loader.quick_scan()
    scan_time = time.time() - start
    print(f"âœ… Scanned {count} files in {scan_time:.2f}s")
    print(f"   Average: {scan_time/count*1000:.2f}ms per file")

    # 2. íŒŒì¼ ëª©ë¡
    print(f"\nğŸ“‹ File List (first 10):")
    for name in loader.get_file_list()[:10]:
        info = loader.get_document_info(name)
        if info:
            size_kb = info.size / 1024
            print(f"   - {name} ({size_kb:.1f}KB)")

    # 3. ë¬¸ì„œ ë¡œë”© í…ŒìŠ¤íŠ¸
    if args.test_load:
        print("\nğŸ“– Document Loading Test:")
        files = loader.get_file_list()[:5]

        for filename in files:
            start = time.time()
            doc = loader.load_document(filename)
            load_time = time.time() - start

            if doc:
                content_len = len(doc['content'])
                print(f"   âœ“ {filename}: {load_time:.3f}s ({content_len} chars)")
            else:
                print(f"   âœ— {filename}: Failed")

        # ìºì‹œ íˆíŠ¸ í…ŒìŠ¤íŠ¸
        print("\nğŸ”„ Cache Hit Test:")
        for filename in files[:2]:
            start = time.time()
            doc = loader.load_document(filename)
            cache_time = time.time() - start
            print(f"   âœ“ {filename}: {cache_time*1000:.3f}ms (cached)")

    # 4. ë°°ì¹˜ ë¡œë”© í…ŒìŠ¤íŠ¸
    if args.test_batch:
        print("\nğŸ“¦ Batch Loading Test:")
        batch_files = loader.get_file_list()[:10]

        start = time.time()
        results = loader.load_batch(batch_files, parallel=True)
        batch_time = time.time() - start

        print(f"   Loaded {len(results)}/{len(batch_files)} files in {batch_time:.2f}s")
        print(f"   Average: {batch_time/len(results):.3f}s per file")

    # 5. ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    if args.test_search:
        print("\nğŸ” Metadata Search Test:")

        # ì—°ë„ë³„ ê²€ìƒ‰
        for year in [2020, 2021, 2022]:
            files = loader.search_by_metadata('year', str(year))
            if files:
                print(f"   Year {year}: {len(files)} files")

        # ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰
        for category in ['purchase', 'repair', 'disposal']:
            files = loader.search_by_metadata('category', category)
            if files:
                print(f"   Category '{category}': {len(files)} files")

    # 6. í†µê³„ ì¶œë ¥
    print("\nğŸ“Š Statistics:")
    stats = loader.get_stats()
    print(f"   Files scanned: {stats['files_scanned']}")
    print(f"   Files loaded: {stats['files_loaded']}")
    print(f"   Avg load time: {stats['avg_load_time']:.3f}s")
    print(f"   Cache hit rate: {stats['cache_stats']['hit_rate']*100:.1f}%")
    print(f"   Memory usage: {stats['memory_usage']/1024/1024:.1f}MB")

    # ì •ë¦¬
    loader.cleanup()
    print("\nâœ… Test completed and cleaned up")