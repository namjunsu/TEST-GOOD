"""
LLM Handler Module
ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ì²˜ë¦¬ ëª¨ë“ˆ
"""

import os
import time
from typing import Dict, Any, Optional
from threading import Lock
from llama_cpp import Llama


class LLMHandler:
    """LLM í•¸ë“¤ëŸ¬ - Qwen2.5-7B"""

    _instance = None
    _lock = Lock()

    def __new__(cls, *args, **kwargs):
        """ì‹±ê¸€í†¤ íŒ¨í„´ êµ¬í˜„"""
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, config: Dict[str, Any]):
        if hasattr(self, '_initialized'):
            return

        self.config = config
        self.model_path = "models/qwen2_5-7b-instruct-q4_k_m.gguf"
        self.model = None

        # LLM íŒŒë¼ë¯¸í„°
        self.temperature = 0.3
        self.max_tokens = 800
        self.top_p = 0.85
        self.top_k = 30

        self._load_model()
        self._initialized = True

    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(self.model_path):
            print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
            print("Qwen2.5-7B ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return

        print("ğŸ”§ LLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
        start_time = time.time()

        try:
            self.model = Llama(
                model_path=self.model_path,
                n_ctx=8192,  # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°
                n_batch=512,
                n_threads=8,  # CPU ìŠ¤ë ˆë“œ
                n_gpu_layers=35,  # GPU ë ˆì´ì–´ (RTX 4000ìš©)
                verbose=False
            )

            elapsed = time.time() - start_time
            print(f"âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.model = None

    def generate(self, query: str, context: str) -> str:
        """ë‹µë³€ ìƒì„±"""
        if not self.model:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_prompt(query, context)

        # ìƒì„±
        try:
            response = self.model(
                prompt,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                top_p=self.top_p,
                top_k=self.top_k,
                stop=["</ë‹µë³€>", "\n\nì§ˆë¬¸:"],
                echo=False
            )

            answer = response['choices'][0]['text'].strip()

            # í›„ì²˜ë¦¬
            answer = self._postprocess_answer(answer)

            return answer

        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _build_prompt(self, query: str, context: str) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±"""
        prompt = f"""<ì‹œìŠ¤í…œ>
ë‹¹ì‹ ì€ í•œêµ­ ë°©ì†¡ì‚¬ì˜ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ê·œì¹™:
1. ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì„ í•˜ì§€ ë§ˆì„¸ìš”
3. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”
4. ì¤‘ìš”í•œ ì •ë³´ëŠ” ê°•ì¡°í•˜ì—¬ ì „ë‹¬í•˜ì„¸ìš”
</ì‹œìŠ¤í…œ>

<ë¬¸ì„œ>
{context}
</ë¬¸ì„œ>

<ì§ˆë¬¸>
{query}
</ì§ˆë¬¸>

<ë‹µë³€>
"""
        return prompt

    def _postprocess_answer(self, answer: str) -> str:
        """ë‹µë³€ í›„ì²˜ë¦¬"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        answer = answer.replace("</ë‹µë³€>", "").strip()

        # ì¤‘êµ­ì–´ ì œê±°
        import re
        answer = re.sub(r'[\u4e00-\u9fff]+', '', answer)

        # ë¹ˆ ë‹µë³€ ì²˜ë¦¬
        if not answer or len(answer) < 10:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return answer

    def generate_summary(self, text: str, max_length: int = 200) -> str:
        """í…ìŠ¤íŠ¸ ìš”ì•½"""
        if not self.model:
            return text[:max_length]

        prompt = f"""<ì‹œìŠ¤í…œ>
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {max_length}ì ì´ë‚´ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•˜ì„¸ìš”.
í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨í•˜ì„¸ìš”.
</ì‹œìŠ¤í…œ>

<í…ìŠ¤íŠ¸>
{text[:2000]}
</í…ìŠ¤íŠ¸>

<ìš”ì•½>
"""

        try:
            response = self.model(
                prompt,
                temperature=0.3,
                max_tokens=max_length,
                stop=["</ìš”ì•½>"],
                echo=False
            )

            summary = response['choices'][0]['text'].strip()
            return summary if summary else text[:max_length]

        except:
            return text[:max_length]