#!/usr/bin/env python3
"""
LLM ì²˜ë¦¬ ëª¨ë“ˆ - Perfect RAGì—ì„œ ë¶„ë¦¬ëœ LLM ê´€ë ¨ ê¸°ëŠ¥
2025-09-29 ë¦¬íŒ©í† ë§

ì´ ëª¨ë“ˆì€ Qwen/Llama ëª¨ë¸ ì²˜ë¦¬, í”„ë¡¬í”„íŠ¸ ê´€ë¦¬, ì‘ë‹µ ìƒì„± ë“±
LLM ê´€ë ¨ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import os
import re
import time
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List
import json

# LLM ê´€ë ¨ ëª¨ë“ˆë“¤
try:
    from rag_system.qwen_llm import QwenLLM
    from rag_system.llm_singleton import LLMSingleton
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False

logger = logging.getLogger(__name__)


class LLMModule:
    """LLM ì²˜ë¦¬ í†µí•© ëª¨ë“ˆ"""

    def __init__(self, config: Dict = None):
        """
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or {}
        self.llm = None
        self.model_path = self.config.get('model_path', './models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf')

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ë“¤
        self._init_prompts()

        # LLM ì´ˆê¸°í™” (preload ì˜µì…˜ì´ ìˆìœ¼ë©´)
        if self.config.get('preload_llm', False):
            self.load_llm()

    def _init_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""
        self.prompts = {
            'conversational_system': """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ì¤‘ìš” ì›ì¹™:
1. ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”í•˜ë“¯ ë‹µë³€í•˜ì„¸ìš”
2. í…œí”Œë¦¿ì´ë‚˜ ì •í˜•í™”ëœ í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ í•„ìš”ë¡œ í•˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
4. ì¶”ê°€ë¡œ ë„ì›€ì´ ë  ë§Œí•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì œì•ˆí•˜ì„¸ìš”
5. ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”""",

            'summary': """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ê°„ë‹¨íˆ ì„¤ëª…
- ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ì„¤ëª…
- ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì„ ì •ë³´ ì œì•ˆ
- ë”±ë”±í•œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹Œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì—°ê²°""",

            'comparison': """ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹„êµ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- ë¹„êµ ëŒ€ìƒë“¤ì˜ ì£¼ìš” ì°¨ì´ì ì„ ë¨¼ì € ì„¤ëª…
- ê°ê°ì˜ ì¥ë‹¨ì ì„ ì‹¤ìš©ì  ê´€ì ì—ì„œ ì„¤ëª…
- ìƒí™©ì— ë”°ë¥¸ ì¶”ì²œ ì œê³µ
- "ì´ëŸ° ê²½ìš°ì—” Aê°€ ì¢‹ê³ , ì €ëŸ° ê²½ìš°ì—” Bê°€ ë‚«ë‹¤"ëŠ” ì‹ìœ¼ë¡œ ì„¤ëª…""",

            'recommendation': """ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ìš©ì ì¸ ì¶”ì²œì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- ê°€ì¥ ì í•©í•œ ì„ íƒì„ ë¨¼ì € ì œì‹œ
- ê·¸ ì´ìœ ë¥¼ ì‹¤ìš©ì  ê´€ì ì—ì„œ ì„¤ëª…
- ëŒ€ì•ˆì´ ìˆë‹¤ë©´ í•¨ê»˜ ì–¸ê¸‰
- ê³ ë ¤ì‚¬í•­ì´ë‚˜ ì£¼ì˜ì  ì•ˆë‚´""",

            'analysis': """ë‹¤ìŒ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- ë°ì´í„°ë‚˜ ì •ë³´ì˜ í•µì‹¬ íŒ¨í„´ íŒŒì•…
- ì¤‘ìš”í•œ íŠ¸ë Œë“œë‚˜ ë³€í™” ì„¤ëª…
- ì‹¤ë¬´ì  ì‹œì‚¬ì  ì œê³µ
- í–¥í›„ ê³ ë ¤ì‚¬í•­ ì œì•ˆ""",

            'smart_summary': """ë‹¤ìŒ ë‚´ìš©ì„ ì½ê³  í•µì‹¬ë§Œ ê°„ë‹¨íˆ 3-5ì¤„ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©:
{content}

ìš”ì•½ ì›ì¹™:
1. ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ ìœ„ì£¼ë¡œ
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë‚ ì§œ í¬í•¨
3. ì‹¤ë¬´ì ìœ¼ë¡œ í•„ìš”í•œ í•µì‹¬ë§Œ
4. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ""",

            'fallback': """ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì§ˆë¬¸: {query}

ê°€ëŠ¥í•œ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
        }

    def load_llm(self) -> bool:
        """LLM ëª¨ë¸ ë¡œë“œ (ì‹±ê¸€í†¤ ì‚¬ìš©)"""
        if not LLM_AVAILABLE:
            logger.error("LLM modules not available")
            return False

        if self.llm is None:
            try:
                if not LLMSingleton.is_loaded():
                    logger.info("ğŸ¤– LLM ëª¨ë¸ ìµœì´ˆ ë¡œë”© ì¤‘...")
                else:
                    logger.info("â™»ï¸ LLM ëª¨ë¸ ì¬ì‚¬ìš©")

                start = time.time()
                self.llm = LLMSingleton.get_instance(model_path=self.model_path)
                elapsed = time.time() - start

                if elapsed > 1.0:
                    logger.info(f"âœ… LLM ë¡œë“œ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")

                return True
            except Exception as e:
                logger.error(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
                return False
        return True

    def generate_response(self, context: str, query: str,
                         intent_type: str = 'general',
                         temperature: float = 0.7,
                         max_tokens: int = 1500) -> str:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±

        Args:
            context: ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            query: ì‚¬ìš©ì ì§ˆë¬¸
            intent_type: ì˜ë„ ìœ í˜• (summary, comparison, recommendation ë“±)
            temperature: ìƒì„± ì˜¨ë„
            max_tokens: ìµœëŒ€ í† í° ìˆ˜

        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        # LLM ë¡œë“œ í™•ì¸
        if not self.load_llm():
            return self._generate_fallback_response_simple(context, query)

        # í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if intent_type in self.prompts:
            user_prompt = self.prompts[intent_type].format(
                context=context[:3000],  # ì»¨í…ìŠ¤íŠ¸ ì œí•œ
                query=query
            )
        else:
            user_prompt = self.prompts['fallback'].format(
                context=context[:3000],
                query=query
            )

        try:
            # LLM ì‘ë‹µ ìƒì„±
            response = self.llm.generate_response(
                question=query,
                context_chunks=[{'content': context, 'title': 'Context', 'filename': ''}]
            )

            # ì‘ë‹µ í›„ì²˜ë¦¬
            return self._format_response(response)

        except Exception as e:
            logger.error(f"LLM ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._generate_fallback_response_simple(context, query)

    def generate_smart_summary(self, text: str, title: str = "") -> str:
        """
        ìŠ¤ë§ˆíŠ¸ ìš”ì•½ ìƒì„±

        Args:
            text: ìš”ì•½í•  í…ìŠ¤íŠ¸
            title: ë¬¸ì„œ ì œëª©

        Returns:
            ìš”ì•½ëœ í…ìŠ¤íŠ¸
        """
        if not self.load_llm():
            return self._extract_key_sentences(text, 3)

        try:
            user_prompt = self.prompts['smart_summary'].format(
                title=title or "ë¬¸ì„œ",
                content=text[:2000]
            )

            response = self.llm.generate_response(
                question=user_prompt,
                context_chunks=[{'content': text[:2000], 'title': title or 'ë¬¸ì„œ', 'filename': ''}]
            )

            return self._format_response(response)

        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._extract_key_sentences(text, 3)

    def generate_conversational_response(self, context: str, query: str,
                                        intent: Dict[str, Any]) -> str:
        """
        ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”í˜• ì‘ë‹µ ìƒì„±

        Args:
            context: ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            query: ì‚¬ìš©ì ì§ˆë¬¸
            intent: ì˜ë„ ì •ë³´

        Returns:
            ëŒ€í™”í˜• ì‘ë‹µ
        """
        intent_type = intent.get('type', 'general')
        confidence = intent.get('confidence', 0.5)

        # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ì¼ë°˜ ì‘ë‹µ
        if confidence < 0.3:
            intent_type = 'fallback'

        return self.generate_response(
            context=context,
            query=query,
            intent_type=intent_type,
            temperature=0.7
        )

    def generate_analysis_response(self, data: Dict[str, Any], query: str) -> str:
        """
        ë°ì´í„° ë¶„ì„ ì‘ë‹µ ìƒì„±

        Args:
            data: ë¶„ì„í•  ë°ì´í„°
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ë¶„ì„ ì‘ë‹µ
        """
        # ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        context = self._format_data_for_analysis(data)

        return self.generate_response(
            context=context,
            query=query,
            intent_type='analysis',
            temperature=0.5
        )

    def prepare_context(self, content: str, max_length: int = 2000) -> str:
        """
        LLM ì…ë ¥ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„

        Args:
            content: ì›ë³¸ ì½˜í…ì¸ 
            max_length: ìµœëŒ€ ê¸¸ì´

        Returns:
            ì¤€ë¹„ëœ ì»¨í…ìŠ¤íŠ¸
        """
        if not content:
            return ""

        # ê¸¸ì´ ì œí•œ
        if len(content) <= max_length:
            return content

        # ì¤‘ìš”í•œ ë¶€ë¶„ ì¶”ì¶œ
        # ì‹œì‘ê³¼ ë ë¶€ë¶„ì„ í¬í•¨
        start_len = max_length // 2
        end_len = max_length - start_len

        prepared = content[:start_len] + "\n...\n" + content[-end_len:]

        return prepared

    def _format_response(self, raw_response: str) -> str:
        """ì‘ë‹µ í¬ë§·íŒ…"""
        if not raw_response:
            return ""

        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        response = raw_response.strip()

        # ì¤‘ë³µëœ ì¤„ë°”ê¿ˆ ì œê±°
        response = re.sub(r'\n{3,}', '\n\n', response)

        # í…œí”Œë¦¿ ë§ˆì»¤ ì œê±°
        markers = ['ë‹µë³€:', 'ì‘ë‹µ:', 'Answer:', 'Response:']
        for marker in markers:
            if response.startswith(marker):
                response = response[len(marker):].strip()

        return response

    def _format_data_for_analysis(self, data: Dict[str, Any]) -> str:
        """ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° í¬ë§·íŒ…"""
        lines = []

        for key, value in data.items():
            if isinstance(value, dict):
                lines.append(f"{key}:")
                for sub_key, sub_value in value.items():
                    lines.append(f"  - {sub_key}: {sub_value}")
            elif isinstance(value, list):
                lines.append(f"{key}: {', '.join(map(str, value[:10]))}")
            else:
                lines.append(f"{key}: {value}")

        return '\n'.join(lines)

    def _extract_key_sentences(self, text: str, num_sentences: int = 3) -> str:
        """í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (í´ë°±ìš©)"""
        if not text:
            return ""

        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?]\s+', text)

        # ê¸¸ì´ê°€ ê¸´ ë¬¸ì¥ ìš°ì„  ì„ íƒ
        valid_sentences = [s for s in sentences if len(s) > 20]
        valid_sentences.sort(key=len, reverse=True)

        # ìƒìœ„ Nê°œ ì„ íƒ
        key_sentences = valid_sentences[:num_sentences]

        return '. '.join(key_sentences) + '.'

    def _generate_fallback_response_simple(self, context: str, query: str) -> str:
        """ê°„ë‹¨í•œ í´ë°± ì‘ë‹µ ìƒì„±"""
        if not context:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        lines = context.split('\n')
        relevant_lines = []

        query_keywords = query.lower().split()

        for line in lines[:20]:  # ì²˜ìŒ 20ì¤„ë§Œ í™•ì¸
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in query_keywords):
                relevant_lines.append(line)

        if relevant_lines:
            response = "ì°¾ì€ ì •ë³´:\n\n"
            response += '\n'.join(relevant_lines[:5])
        else:
            response = f"ë‹¤ìŒì€ ê´€ë ¨ ë¬¸ì„œì˜ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n{context[:500]}"

        return response

    def get_prompt_template(self, intent_type: str) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë°˜í™˜"""
        return self.prompts.get(intent_type, self.prompts['fallback'])

    def update_prompt_template(self, intent_type: str, template: str):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì—…ë°ì´íŠ¸"""
        self.prompts[intent_type] = template

    def clear_llm(self):
        """LLM ì¸ìŠ¤í„´ìŠ¤ ì •ë¦¬"""
        self.llm = None
        logger.info("LLM instance cleared")