#!/usr/bin/env python3
"""
LLM ì‘ë‹µ ì†ë„ ìƒì„¸ ë¶„ì„ ë„êµ¬
ê° ë‹¨ê³„ë³„ ì‹œê°„ì„ ì •ë°€í•˜ê²Œ ì¸¡ì •í•˜ì—¬ ë³‘ëª© ì§€ì ì„ ì°¾ìŠµë‹ˆë‹¤.
"""

import time
import logging
import psutil
import torch
from pathlib import Path
import json
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class LLMPerformanceAnalyzer:
    def __init__(self):
        self.metrics = {
            'initialization': {},
            'model_loading': {},
            'prompt_processing': {},
            'response_generation': {},
            'memory_usage': {},
            'gpu_usage': {}
        }

    def measure_initialization(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œê°„ ì¸¡ì •"""
        logger.info("ğŸ“Š ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë¶„ì„ ì‹œì‘...")

        # 1. Import ì‹œê°„ ì¸¡ì •
        import_start = time.time()
        from perfect_rag import PerfectRAG
        import_time = time.time() - import_start
        self.metrics['initialization']['import_time'] = import_time
        logger.info(f"  â€¢ Import ì‹œê°„: {import_time:.3f}ì´ˆ")

        # 2. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œê°„
        init_start = time.time()
        rag = PerfectRAG()
        init_time = time.time() - init_start
        self.metrics['initialization']['instance_creation'] = init_time
        logger.info(f"  â€¢ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±: {init_time:.3f}ì´ˆ")

        # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        self.metrics['memory_usage']['after_init'] = memory_mb
        logger.info(f"  â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_mb:.1f} MB")

        return rag

    def measure_model_loading(self):
        """LLM ëª¨ë¸ ë¡œë”© ì„±ëŠ¥ ì¸¡ì •"""
        logger.info("\nğŸ¤– LLM ë¡œë”© ì„±ëŠ¥ ë¶„ì„...")

        from rag_system.qwen_llm import QwenLLM

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
        gpu_available = torch.cuda.is_available()
        if gpu_available:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"  â€¢ GPU: {gpu_name} ({gpu_memory:.1f} GB)")
            self.metrics['gpu_usage']['device'] = gpu_name
            self.metrics['gpu_usage']['total_memory_gb'] = gpu_memory
        else:
            logger.warning("  â€¢ GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œ")
            self.metrics['gpu_usage']['device'] = "CPU"

        # ëª¨ë¸ ë¡œë”© ì‹œê°„
        load_start = time.time()
        llm = QwenLLM()
        load_time = time.time() - load_start
        self.metrics['model_loading']['total_time'] = load_time
        logger.info(f"  â€¢ ëª¨ë¸ ë¡œë”©: {load_time:.3f}ì´ˆ")

        # ë¡œë”© í›„ ë©”ëª¨ë¦¬
        if gpu_available:
            gpu_memory_used = torch.cuda.memory_allocated(0) / 1024**3
            self.metrics['gpu_usage']['memory_used_gb'] = gpu_memory_used
            logger.info(f"  â€¢ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: {gpu_memory_used:.2f} GB")

        return llm

    def measure_prompt_processing(self, rag):
        """í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì„±ëŠ¥ ì¸¡ì •"""
        logger.info("\nğŸ“ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì„±ëŠ¥ ë¶„ì„...")

        test_queries = [
            "ê°„ë‹¨í•œ ì§ˆë¬¸",
            "2017ë…„ì— êµ¬ë§¤í•œ ì¹´ë©”ë¼ ì¥ë¹„ì˜ êµ¬ë§¤ ëª©ì ê³¼ ê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "ë¬¸ì ë°œìƒê¸°ì™€ ê´€ë ¨ëœ ëª¨ë“  ë¬¸ì„œë¥¼ ì°¾ì•„ì„œ êµ¬ë§¤ ë‚ ì§œì™€ ìš©ë„ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”. íŠ¹íˆ ë‰´ìŠ¤ë£¸ê³¼ ë¶€ì¡°ì •ì‹¤ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì¥ë¹„ë¥¼ êµ¬ë¶„í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        ]

        for i, query in enumerate(test_queries, 1):
            logger.info(f"\n  í…ŒìŠ¤íŠ¸ {i}: {len(query)}ì ì¿¼ë¦¬")

            # ê²€ìƒ‰ ì‹œê°„
            search_start = time.time()
            # ì‹¤ì œ ê²€ìƒ‰ ë¡œì§ (perfect_rag.pyì˜ search ë¶€ë¶„)
            search_time = time.time() - search_start

            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹œê°„
            context_start = time.time()
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë¡œì§
            context_time = time.time() - context_start

            self.metrics['prompt_processing'][f'query_{i}'] = {
                'query_length': len(query),
                'search_time': search_time,
                'context_time': context_time
            }

            logger.info(f"    - ê²€ìƒ‰: {search_time:.3f}ì´ˆ")
            logger.info(f"    - ì»¨í…ìŠ¤íŠ¸: {context_time:.3f}ì´ˆ")

    def measure_response_generation(self, llm):
        """ì‘ë‹µ ìƒì„± ì„±ëŠ¥ ì¸¡ì •"""
        logger.info("\nğŸ”„ ì‘ë‹µ ìƒì„± ì„±ëŠ¥ ë¶„ì„...")

        # ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì»¨í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
        context_sizes = [500, 1000, 2000, 4000]

        for size in context_sizes:
            logger.info(f"\n  ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: {size}í† í°")

            # ìƒ˜í”Œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ë‚´ìš©. " * (size // 10)
            query = "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”."

            # í† í°í™” ì‹œê°„
            token_start = time.time()
            # í† í°í™” ë¡œì§
            token_time = time.time() - token_start

            # ìƒì„± ì‹œê°„
            gen_start = time.time()
            # ì‹¤ì œ ìƒì„± ë¡œì§ (ìŠ¤íŠ¸ë¦¬ë° vs ì¼ë°˜)
            gen_time = time.time() - gen_start

            self.metrics['response_generation'][f'context_{size}'] = {
                'tokenization_time': token_time,
                'generation_time': gen_time,
                'tokens_per_second': size / gen_time if gen_time > 0 else 0
            }

            logger.info(f"    - í† í°í™”: {token_time:.3f}ì´ˆ")
            logger.info(f"    - ìƒì„±: {gen_time:.3f}ì´ˆ")
            logger.info(f"    - ì†ë„: {size/gen_time if gen_time > 0 else 0:.1f} tokens/s")

    def analyze_bottlenecks(self):
        """ë³‘ëª© ì§€ì  ë¶„ì„ ë° ê°œì„  ì œì•ˆ"""
        logger.info("\nğŸ” ë³‘ëª© ì§€ì  ë¶„ì„...")

        # ê°€ì¥ ëŠë¦° ë‹¨ê³„ ì°¾ê¸°
        total_init = sum(self.metrics['initialization'].values())
        total_load = self.metrics['model_loading'].get('total_time', 0)

        logger.info(f"\nğŸ“Š ì „ì²´ ì‹œê°„ ë¶„ì„:")
        logger.info(f"  â€¢ ì´ˆê¸°í™”: {total_init:.3f}ì´ˆ")
        logger.info(f"  â€¢ ëª¨ë¸ ë¡œë”©: {total_load:.3f}ì´ˆ")

        # ê°œì„  ì œì•ˆ
        suggestions = []

        if total_init > 3:
            suggestions.append("âš ï¸ ì´ˆê¸°í™” ì‹œê°„ì´ 3ì´ˆ ì´ìƒ - ëª¨ë“ˆ lazy loading ê¶Œì¥")

        if total_load > 5:
            suggestions.append("âš ï¸ ëª¨ë¸ ë¡œë”©ì´ 5ì´ˆ ì´ìƒ - ëª¨ë¸ ì‚¬ì „ ë¡œë”© ë˜ëŠ” ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©")

        if self.metrics['gpu_usage'].get('device') == "CPU":
            suggestions.append("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ - GPU ì‚¬ìš© ê¶Œì¥")

        logger.info("\nğŸ’¡ ê°œì„  ì œì•ˆ:")
        for suggestion in suggestions:
            logger.info(f"  {suggestion}")

        return suggestions

    def save_report(self):
        """ë¶„ì„ ë³´ê³ ì„œ ì €ì¥"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'metrics': self.metrics,
            'analysis': self.analyze_bottlenecks()
        }

        output_file = Path("llm_performance_report.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"\nğŸ“„ ë³´ê³ ì„œ ì €ì¥: {output_file}")
        return report

def main():
    """ë©”ì¸ ë¶„ì„ ì‹¤í–‰"""
    logger.info("=" * 60)
    logger.info("ğŸš€ LLM ì‘ë‹µ ì†ë„ ìƒì„¸ ë¶„ì„ ì‹œì‘")
    logger.info("=" * 60)

    analyzer = LLMPerformanceAnalyzer()

    try:
        # 1. ì´ˆê¸°í™” ì¸¡ì •
        rag = analyzer.measure_initialization()

        # 2. ëª¨ë¸ ë¡œë”© ì¸¡ì •
        llm = analyzer.measure_model_loading()

        # 3. í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¸¡ì •
        analyzer.measure_prompt_processing(rag)

        # 4. ì‘ë‹µ ìƒì„± ì¸¡ì •
        analyzer.measure_response_generation(llm)

        # 5. ë³‘ëª© ë¶„ì„
        analyzer.analyze_bottlenecks()

        # 6. ë³´ê³ ì„œ ì €ì¥
        analyzer.save_report()

        logger.info("\nâœ… ë¶„ì„ ì™„ë£Œ!")

    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()