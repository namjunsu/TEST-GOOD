#!/usr/bin/env python3
"""
ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ
- JSON ê¸°ë°˜ ê²½ëŸ‰ DB
- ë¹ ë¥¸ ê²€ìƒ‰ ì§€ì›
- ìë™ ì—…ë°ì´íŠ¸ ê¸°ëŠ¥
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple
import re
from datetime import datetime
import threading
import hashlib
from collections import defaultdict
import time
import os
from functools import lru_cache
import fcntl  # For file locking on Unix systems

logger = logging.getLogger(__name__)

class MetadataManager:
    # ìƒìˆ˜ ì •ì˜
    DEFAULT_DB_PATH = "document_metadata.json"
    DEFAULT_CACHE_TTL = 300  # 5ë¶„
    AUTO_SAVE_INTERVAL = 10  # 10ì´ˆë§ˆë‹¤ ìë™ ì €ì¥
    NAME_MIN_LENGTH = 2
    NAME_MAX_LENGTH = 4

    def __init__(self, db_path: str = None, cache_ttl: int = None):
        self.db_path = Path(db_path) if db_path else Path(self.DEFAULT_DB_PATH)
        self.cache_ttl = cache_ttl if cache_ttl is not None else self.DEFAULT_CACHE_TTL

        # Performance optimization
        self.metadata = self.load_metadata()
        self._last_load_time = time.time()
        self._cache = {}  # Query result cache
        self._index = self._build_indexes()  # Build indexes for fast search

        # Thread safety
        self._lock = threading.RLock()
        self._write_lock = threading.Lock()

        # Change tracking
        self._dirty = False  # Track if data needs saving
        self._last_save_time = time.time()

        # Performance metrics
        self._search_count = 0
        self._cache_hits = 0
        self._save_count = 0

        # Compile patterns
        self._compile_patterns()

    def load_metadata(self) -> Dict:
        """ë©”íƒ€ë°ì´í„° DB ë¡œë“œ - ì—ëŸ¬ ì²˜ë¦¬ ë° ë°±ì—… ì§€ì›"""
        if not self.db_path.exists():
            return {}

        try:
            # íŒŒì¼ ì ê¸ˆìœ¼ë¡œ ë™ì‹œ ì ‘ê·¼ ë°©ì§€
            with open(self.db_path, 'r', encoding='utf-8') as f:
                try:
                    # Unix ì‹œìŠ¤í…œì—ì„œ íŒŒì¼ ì ê¸ˆ
                    fcntl.flock(f.fileno(), fcntl.LOCK_SH)
                    data = json.load(f)
                    fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                    return data
                except (ImportError, AttributeError):
                    # Windows ë˜ëŠ” fcntl ì—†ëŠ” í™˜ê²½
                    return json.load(f)
        except json.JSONDecodeError as e:
            # ì†ìƒëœ JSON ì²˜ë¦¬
            logger.warning(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ ì†ìƒ ê°ì§€: {e}")
            backup_path = self.db_path.with_suffix('.backup')
            if backup_path.exists():
                logger.info("ë°±ì—… íŒŒì¼ì—ì„œ ë³µêµ¬ ì‹œë„...")
                with open(backup_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}

    def save_metadata(self, force: bool = False):
        """ë©”íƒ€ë°ì´í„° DB ì €ì¥ - ë°±ì—… ë° ì›ìì  ì“°ê¸°"""
        if not self._dirty and not force:
            return  # ë³€ê²½ì‚¬í•­ì´ ì—†ìœ¼ë©´ ì €ì¥ ìŠ¤í‚µ

        with self._write_lock:
            try:
                # ê¸°ì¡´ íŒŒì¼ ë°±ì—…
                if self.db_path.exists():
                    backup_path = self.db_path.with_suffix('.backup')
                    self.db_path.rename(backup_path)

                # ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì“°ê¸° (ì›ìì  ì“°ê¸°)
                temp_path = self.db_path.with_suffix('.tmp')
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(self.metadata, f, ensure_ascii=False, indent=2)

                # ì›ìì  ì´ë™
                temp_path.rename(self.db_path)

                self._dirty = False
                self._last_save_time = time.time()
                self._save_count += 1

            except Exception as e:
                logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
                # ë°±ì—…ì—ì„œ ë³µêµ¬
                backup_path = self.db_path.with_suffix('.backup')
                if backup_path.exists():
                    backup_path.rename(self.db_path)
                raise

    def add_document(self, filename: str, **kwargs):
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ê°€/ì—…ë°ì´íŠ¸ - ê²€ì¦ ë° ì¸ë±ì‹±"""
        with self._lock:
            # ì…ë ¥ ê²€ì¦
            kwargs = self._validate_metadata(kwargs)

            if filename not in self.metadata:
                self.metadata[filename] = {}

            # ë³€ê²½ ì‚¬í•­ ì¶”ì 
            old_data = self.metadata[filename].copy()

            # ê¸°ë³¸ ì •ë³´ ì¶”ê°€
            self.metadata[filename].update(kwargs)
            self.metadata[filename]['last_updated'] = datetime.now().isoformat()

            # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
            self._update_indexes(filename, old_data, self.metadata[filename])

            # ë³€ê²½ í”Œë˜ê·¸ ì„¤ì •
            self._dirty = True

            # ìºì‹œ ë¬´íš¨í™”
            self._invalidate_cache()

            # ìë™ ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì§€ì—°)
            if time.time() - self._last_save_time > self.AUTO_SAVE_INTERVAL:
                self.save_metadata()

    def get_document(self, filename: str) -> Optional[Dict]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        return self.metadata.get(filename)

    def search_by_drafter(self, drafter_name: str, fuzzy: bool = True) -> List[str]:
        """ê¸°ì•ˆìë¡œ ê²€ìƒ‰ - ì¸ë±ìŠ¤ ê¸°ë°˜ ë¹ ë¥¸ ê²€ìƒ‰"""
        self._search_count += 1

        # ìºì‹œ í™•ì¸
        cache_key = f"drafter:{drafter_name}:{fuzzy}"
        if cache_key in self._cache:
            cache_time, results = self._cache[cache_key]
            if time.time() - cache_time < self.cache_ttl:
                self._cache_hits += 1
                return results

        with self._lock:
            results = set()

            # ì¸ë±ìŠ¤ ì‚¬ìš©
            if 'drafter' in self._index:
                # ì •í™•í•œ ë§¤ì¹­
                if drafter_name in self._index['drafter']:
                    results.update(self._index['drafter'][drafter_name])

                # í¼ì§€ ë§¤ì¹­
                if fuzzy:
                    for indexed_name, files in self._index['drafter'].items():
                        if drafter_name.lower() in indexed_name.lower() or \
                           indexed_name.lower() in drafter_name.lower():
                            results.update(files)

            results = list(results)
            # ìºì‹œ ì €ì¥
            self._cache[cache_key] = (time.time(), results)
            return results

    def search_by_field(self, field: str, value: str, fuzzy: bool = True) -> List[str]:
        """íŠ¹ì • í•„ë“œë¡œ ê²€ìƒ‰ - ì¸ë±ìŠ¤ ê¸°ë°˜"""
        self._search_count += 1

        # ìºì‹œ í™•ì¸
        cache_key = f"field:{field}:{value}:{fuzzy}"
        if cache_key in self._cache:
            cache_time, results = self._cache[cache_key]
            if time.time() - cache_time < self.cache_ttl:
                self._cache_hits += 1
                return results

        with self._lock:
            results = set()

            # ì¸ë±ìŠ¤ ì‚¬ìš©
            if field in self._index:
                # ì •í™•í•œ ë§¤ì¹­
                if value in self._index[field]:
                    results.update(self._index[field][value])

                # í¼ì§€ ë§¤ì¹­
                if fuzzy:
                    value_lower = value.lower()
                    for indexed_value, files in self._index[field].items():
                        if value_lower in str(indexed_value).lower():
                            results.update(files)

            results = list(results)
            # ìºì‹œ ì €ì¥
            self._cache[cache_key] = (time.time(), results)
            return results

    def _compile_patterns(self):
        """ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼"""
        self._patterns = {
            'drafter': [
                re.compile(r'ê¸°ì•ˆì[\s:ï¼š]*([ê°€-í£]{2,4})'),
                re.compile(r'ì‘ì„±ì[\s:ï¼š]*([ê°€-í£]{2,4})'),
                re.compile(r'ë‹´ë‹¹ì[\s:ï¼š]*([ê°€-í£]{2,4})')
            ],
            'department': [
                re.compile(r'ê¸°ì•ˆë¶€ì„œ[\s:ï¼š]*([^\n]+)'),
                re.compile(r'ë¶€ì„œ[\s:ï¼š]*([^\n]+)'),
                re.compile(r'ì†Œì†[\s:ï¼š]*([^\n]+)')
            ],
            'date': [
                re.compile(r'ê¸°ì•ˆì¼ì[\s:ï¼š]*(\d{4}[-./]\d{1,2}[-./]\d{1,2})'),
                re.compile(r'ì‘ì„±ì¼[\s:ï¼š]*(\d{4}[-./]\d{1,2}[-./]\d{1,2})'),
                re.compile(r'ë‚ ì§œ[\s:ï¼š]*(\d{4}[-./]\d{1,2}[-./]\d{1,2})')
            ],
            'amount': [
                re.compile(r'ê¸ˆì•¡[\s:ï¼š]*([0-9,]+)ì›'),
                re.compile(r'ì´ì•¡[\s:ï¼š]*([0-9,]+)ì›'),
                re.compile(r'([0-9,]+)ì›')  # ê¸ˆì•¡ íŒ¨í„´
            ]
        }

        # ê²€ì¦ íŒ¨í„´ë„ ì»´íŒŒì¼
        self._name_pattern = re.compile(f'^[ê°€-í£]{{{self.NAME_MIN_LENGTH},{self.NAME_MAX_LENGTH}}}$')
        self._date_pattern = re.compile(r'\d{4}[-./]\d{1,2}[-./]\d{1,2}')

    def extract_from_text(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„° ìë™ ì¶”ì¶œ (ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)"""
        metadata = {}

        # ì»´íŒŒì¼ëœ íŒ¨í„´ìœ¼ë¡œ ì¶”ì¶œ
        for field, pattern_list in self._patterns.items():
            for pattern in pattern_list:
                match = pattern.search(text)
                if match:
                    metadata[field] = match.group(1).strip()
                    break

        # ë¬¸ì„œ íƒ€ì… ìë™ ë¶„ë¥˜
        if 'ê¸´ê¸‰' in text:
            metadata['priority'] = 'high'
        elif 'ê²€í† ' in text:
            metadata['type'] = 'ê²€í† ì„œ'
        elif 'êµ¬ë§¤' in text:
            metadata['type'] = 'êµ¬ë§¤ê¸°ì•ˆ'
        elif 'ìˆ˜ë¦¬' in text or 'ë³´ìˆ˜' in text:
            metadata['type'] = 'ìˆ˜ë¦¬/ë³´ìˆ˜'

        return metadata

    def _build_indexes(self) -> Dict[str, Dict[str, Set[str]]]:
        """ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        indexes = defaultdict(lambda: defaultdict(set))

        for filename, data in self.metadata.items():
            for field, value in data.items():
                if field not in ['last_updated']:  # ì¸ë±ì‹± ì œì™¸ í•„ë“œ
                    indexes[field][str(value)].add(filename)

        return indexes

    def _update_indexes(self, filename: str, old_data: Dict, new_data: Dict):
        """ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        # ê¸°ì¡´ ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        for field, value in old_data.items():
            if field in self._index and str(value) in self._index[field]:
                self._index[field][str(value)].discard(filename)

        # ìƒˆ ì¸ë±ìŠ¤ ì¶”ê°€
        for field, value in new_data.items():
            if field not in ['last_updated']:
                if field not in self._index:
                    self._index[field] = defaultdict(set)
                self._index[field][str(value)].add(filename)

    def _validate_metadata(self, metadata: Dict) -> Dict:
        """ë©”íƒ€ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”"""
        validated = {}

        for key, value in metadata.items():
            # ê¸°ë³¸ ê²€ì¦
            if value is None or (isinstance(value, str) and not value.strip()):
                continue

            # íƒ€ì…ë³„ ê²€ì¦ (ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)
            if key == 'drafter':
                # ì´ë¦„ ê²€ì¦ (2-4ì í•œê¸€)
                if isinstance(value, str) and self._name_pattern.match(value):
                    validated[key] = value.strip()
            elif key == 'amount':
                # ê¸ˆì•¡ ì •ê·œí™”
                if isinstance(value, (str, int)):
                    amount_str = str(value).replace(',', '')
                    if amount_str.isdigit():
                        validated[key] = int(amount_str)
            elif key == 'date':
                # ë‚ ì§œ í˜•ì‹ ê²€ì¦
                if isinstance(value, str):
                    date_match = self._date_pattern.match(value)
                    if date_match:
                        validated[key] = date_match.group()
            else:
                validated[key] = value

        return validated

    def _invalidate_cache(self, pattern: Optional[str] = None):
        """ìºì‹œ ë¬´íš¨í™”"""
        if pattern:
            # íŠ¹ì • íŒ¨í„´ë§Œ ë¬´íš¨í™”
            keys_to_remove = [k for k in self._cache.keys() if pattern in k]
            for key in keys_to_remove:
                del self._cache[key]
        else:
            # ì „ì²´ ìºì‹œ ë¬´íš¨í™”
            self._cache.clear()

    def multi_field_search(self, criteria: Dict[str, str], operator: str = 'AND') -> List[str]:
        """ë‹¤ì¤‘ í•„ë“œ ê²€ìƒ‰"""
        results_sets = []

        for field, value in criteria.items():
            field_results = set(self.search_by_field(field, value))
            results_sets.append(field_results)

        if not results_sets:
            return []

        if operator.upper() == 'AND':
            # êµì§‘í•©
            result_set = results_sets[0]
            for s in results_sets[1:]:
                result_set = result_set.intersection(s)
        else:  # OR
            # í•©ì§‘í•©
            result_set = results_sets[0]
            for s in results_sets[1:]:
                result_set = result_set.union(s)

        return list(result_set)

    def bulk_update(self, updates: Dict[str, Dict]):
        """ëŒ€ëŸ‰ ì—…ë°ì´íŠ¸ - ì„±ëŠ¥ ìµœì í™”"""
        with self._lock:
            for filename, metadata in updates.items():
                if filename not in self.metadata:
                    self.metadata[filename] = {}

                old_data = self.metadata[filename].copy()
                validated = self._validate_metadata(metadata)

                self.metadata[filename].update(validated)
                self.metadata[filename]['last_updated'] = datetime.now().isoformat()

                self._update_indexes(filename, old_data, self.metadata[filename])

            self._dirty = True
            self._invalidate_cache()
            self.save_metadata(force=True)  # ëŒ€ëŸ‰ ì—…ë°ì´íŠ¸ëŠ” ì¦‰ì‹œ ì €ì¥

    def get_statistics(self) -> Dict:
        """ì „ì²´ í†µê³„"""
        stats = {
            'total_documents': len(self.metadata),
            'documents_with_drafter': 0,
            'documents_with_amount': 0,
            'drafters': {},
            'departments': {},
            'types': {}
        }

        for filename, data in self.metadata.items():
            if data.get('drafter'):
                stats['documents_with_drafter'] += 1
                drafter = data['drafter']
                stats['drafters'][drafter] = stats['drafters'].get(drafter, 0) + 1

            if data.get('amount'):
                stats['documents_with_amount'] += 1

            if data.get('department'):
                dept = data['department']
                stats['departments'][dept] = stats['departments'].get(dept, 0) + 1

            if data.get('type'):
                doc_type = data['type']
                stats['types'][doc_type] = stats['types'].get(doc_type, 0) + 1

        return stats

    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        cache_hit_rate = (self._cache_hits / self._search_count * 100
                         if self._search_count > 0 else 0)

        return {
            'search_count': self._search_count,
            'cache_hits': self._cache_hits,
            'cache_hit_rate': cache_hit_rate,
            'save_count': self._save_count,
            'cache_size': len(self._cache),
            'index_size': sum(len(idx) for idx in self._index.values())
        }


# í…ŒìŠ¤íŠ¸ ë° ì´ˆê¸° ë°ì´í„°
if __name__ == "__main__":
    manager = MetadataManager()

    # ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ (ì‹¤ì œ ë¬¸ì„œ ê¸°ë°˜)
    sample_data = [
        {
            "filename": "2025-03-20_ì±„ë„ì—ì´_ì¤‘ê³„ì°¨_ì¹´ë©”ë¼_ë…¸í›„í™”_ì¥ì• _ê¸´ê¸‰_ë³´ìˆ˜ê±´.pdf",
            "drafter": "ìµœìƒˆë¦„",
            "department": "ê¸°ìˆ ê´€ë¦¬íŒ€-ë³´ë„ê¸°ìˆ ê´€ë¦¬íŒŒíŠ¸",
            "type": "ê¸´ê¸‰ë³´ìˆ˜",
            "amount": "2,446,000",
            "priority": "high"
        },
        {
            "filename": "2025-01-14_ì±„ë„A_ë¶ˆìš©_ë°©ì†¡_ì¥ë¹„_íê¸°_ìš”ì²­ì˜_ê±´.pdf",
            "drafter": "ë‚¨ì¤€ìˆ˜",
            "department": "ê¸°ìˆ ê´€ë¦¬íŒ€",
            "type": "íê¸°ìš”ì²­",
            "date": "2025-01-14"
        },
        {
            "filename": "2023-12-06_ì˜¤í”ˆìŠ¤íŠœë””ì˜¤_ë¬´ì„ ë§ˆì´í¬_ìˆ˜ì‹ _ì¥ì• _ì¡°ì¹˜_ê¸°ì•ˆì„œ.pdf",
            "drafter": "ìµœìƒˆë¦„",
            "department": "ê¸°ìˆ ê´€ë¦¬íŒ€",
            "type": "ì¥ì• ì¡°ì¹˜",
            "date": "2023-12-06"
        },
        {
            "filename": "2023-11-02_ì˜ìƒì·¨ì¬íŒ€_íŠ¸ë¼ì´í¬ë“œ_ìˆ˜ë¦¬_ê±´.pdf",
            "drafter": "ìœ ì¸í˜",
            "department": "ì˜ìƒì·¨ì¬íŒ€",
            "type": "ìˆ˜ë¦¬/ë³´ìˆ˜",
            "date": "2023-11-02"
        },
        {
            "filename": "2024-11-14_ë‰´ìŠ¤_ìŠ¤íŠœë””ì˜¤_ì§€ë¯¸ì§‘_Control_Box_ìˆ˜ë¦¬_ê±´.pdf",
            "drafter": "ë‚¨ì¤€ìˆ˜",
            "department": "ê¸°ìˆ ê´€ë¦¬íŒ€",
            "type": "ìˆ˜ë¦¬/ë³´ìˆ˜",
            "date": "2024-11-14"
        },
        {
            "filename": "2019-05-31_Audio_Patch_Cable_êµ¬ë§¤.pdf",
            "drafter": "ìœ ì¸í˜",
            "department": "ê¸°ìˆ ê´€ë¦¬íŒ€",
            "type": "êµ¬ë§¤ê¸°ì•ˆ",
            "date": "2019-05-31"
        }
    ]

    # ë°ì´í„° ì¶”ê°€
    for data in sample_data:
        filename = data.pop('filename')
        manager.add_document(filename, **data)

    logger.info("ë©”íƒ€ë°ì´í„° DB ìƒì„± ì™„ë£Œ!")
    logger.info(f"ì´ {len(manager.metadata)}ê°œ ë¬¸ì„œ ì •ë³´ ì €ì¥")

    # í†µê³„ ì¶œë ¥
    stats = manager.get_statistics()
    print("\nğŸ“ˆ í†µê³„:")
    print(f"  - ê¸°ì•ˆì ì •ë³´ ìˆëŠ” ë¬¸ì„œ: {stats['documents_with_drafter']}ê°œ")
    print(f"  - ê¸ˆì•¡ ì •ë³´ ìˆëŠ” ë¬¸ì„œ: {stats['documents_with_amount']}ê°œ")

    print("\nğŸ‘¥ ê¸°ì•ˆìë³„ ë¬¸ì„œ:")
    for drafter, count in stats['drafters'].items():
        print(f"  - {drafter}: {count}ê°œ")

    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print("\nğŸ” ìµœìƒˆë¦„ ê¸°ì•ˆì ë¬¸ì„œ ê²€ìƒ‰:")
    results = manager.search_by_drafter("ìµœìƒˆë¦„")
    for doc in results:
        print(f"  - {doc}")

    # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
    perf_stats = manager.get_performance_stats()
    print("\nâš¡ ì„±ëŠ¥ í†µê³„:")
    print(f"  - ê²€ìƒ‰ íšŸìˆ˜: {perf_stats['search_count']}")
    print(f"  - ìºì‹œ íˆíŠ¸ìœ¨: {perf_stats['cache_hit_rate']:.1f}%")