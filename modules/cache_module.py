#!/usr/bin/env python3
"""
from app.core.logging import get_logger
ìºì‹œ ê´€ë¦¬ ëª¨ë“ˆ - Perfect RAGì—ì„œ ë¶„ë¦¬ëœ ìºì‹œ ê´€ë ¨ ê¸°ëŠ¥
2025-09-29 ë¦¬íŒ©í† ë§

ì´ ëª¨ë“ˆì€ LRU ìºì‹œ, TTL ê´€ë¦¬, ë‹¤ì¤‘ ìºì‹œ ì‹œìŠ¤í…œ ë“±
ìºì‹± ê´€ë ¨ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import os
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
from collections import OrderedDict
import pickle

logger = get_logger(__name__)


class CacheModule:
    """ìºì‹œ ê´€ë¦¬ í†µí•© ëª¨ë“ˆ"""

    def __init__(self, config: Dict = None):
        """
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or {}

        # ìºì‹œ ì„¤ì •
        self.max_cache_size = self.config.get('max_cache_size', 100)
        self.max_metadata_cache = self.config.get('max_metadata_cache', 500)
        self.max_pdf_cache = self.config.get('max_pdf_cache', 50)
        self.cache_ttl = self.config.get('cache_ttl', 3600)  # 1ì‹œê°„

        # ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = Path(self.config.get('cache_dir', './cache'))
        self.cache_dir.mkdir(exist_ok=True)

        # ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.documents_cache = OrderedDict()  # ë¬¸ì„œ ìºì‹œ
        self.metadata_cache = OrderedDict()   # ë©”íƒ€ë°ì´í„° ìºì‹œ
        self.answer_cache = OrderedDict()     # ì‘ë‹µ ìºì‹œ
        self.pdf_text_cache = OrderedDict()   # PDF í…ìŠ¤íŠ¸ ìºì‹œ
        self.search_cache = OrderedDict()     # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ

        # í†µê³„
        self.cache_hits = 0
        self.cache_misses = 0

        # ìºì‹œ íŒŒì¼ ê²½ë¡œ
        self.cache_file = self.cache_dir / 'cache_state.pkl'

        # ìºì‹œ ë¡œë“œ
        self.load_cache_from_disk()

    def manage_cache(self, cache_dict: OrderedDict, key: str, value: Any,
                    max_size: Optional[int] = None) -> None:
        """
        ìºì‹œ í¬ê¸° ê´€ë¦¬ - LRU ë°©ì‹

        Args:
            cache_dict: ê´€ë¦¬í•  ìºì‹œ ë”•ì…”ë„ˆë¦¬
            key: ìºì‹œ í‚¤
            value: ì €ì¥í•  ê°’
            max_size: ìµœëŒ€ í¬ê¸° (ê¸°ë³¸ê°’: self.max_cache_size)
        """
        if max_size is None:
            max_size = self.max_cache_size

        if key in cache_dict:
            # ê¸°ì¡´ í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™ (ê°€ì¥ ìµœê·¼ ì‚¬ìš©)
            cache_dict.move_to_end(key)
        else:
            # ìƒˆ í•­ëª© ì¶”ê°€
            if len(cache_dict) >= max_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                removed_key = cache_dict.popitem(last=False)[0]
                logger.debug(f"ìºì‹œ ì œê±° (LRU): {removed_key}")

        # ê°’ê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
        cache_dict[key] = (value, time.time())

    def get_from_cache(self, cache_dict: OrderedDict, key: str) -> Optional[Any]:
        """
        ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸° (TTL ì²´í¬ ë° íƒ€ì„ìŠ¤íƒ¬í”„ ê°±ì‹ )

        Args:
            cache_dict: ìºì‹œ ë”•ì…”ë„ˆë¦¬
            key: ìºì‹œ í‚¤

        Returns:
            ìºì‹œëœ ê°’ ë˜ëŠ” None
        """
        if key in cache_dict:
            cache_value = cache_dict[key]
            current_time = time.time()

            # íŠœí”Œ í˜•ì‹ (value, timestamp) ì²´í¬
            if isinstance(cache_value, tuple) and len(cache_value) == 2:
                value, timestamp = cache_value

                if current_time - timestamp < self.cache_ttl:
                    # LRU: ì‚¬ìš©í•œ í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™
                    cache_dict.move_to_end(key)
                    # íƒ€ì„ìŠ¤íƒ¬í”„ ê°±ì‹  (ì‚¬ìš© ì‹œê°„ ì—°ì¥)
                    cache_dict[key] = (value, current_time)
                    self.cache_hits += 1
                    return value
                else:
                    # TTL ë§Œë£Œ - ì‚­ì œ
                    del cache_dict[key]
                    logger.debug(f"ìºì‹œ ë§Œë£Œ: {key}")
                    self.cache_misses += 1
                    return None
            else:
                # ì´ì „ í˜•ì‹ í˜¸í™˜ (íŠœí”Œ ì•„ë‹Œ ê²½ìš°)
                cache_dict.move_to_end(key)
                self.cache_hits += 1
                return cache_value

        self.cache_misses += 1
        return None

    def set_document_cache(self, key: str, value: Any) -> None:
        """ë¬¸ì„œ ìºì‹œì— ì €ì¥"""
        self.manage_cache(self.documents_cache, key, value, self.max_cache_size)

    def get_document_cache(self, key: str) -> Optional[Any]:
        """ë¬¸ì„œ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        return self.get_from_cache(self.documents_cache, key)

    def set_metadata_cache(self, key: str, value: Any) -> None:
        """ë©”íƒ€ë°ì´í„° ìºì‹œì— ì €ì¥"""
        self.manage_cache(self.metadata_cache, key, value, self.max_metadata_cache)

    def get_metadata_cache(self, key: str) -> Optional[Any]:
        """ë©”íƒ€ë°ì´í„° ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        return self.get_from_cache(self.metadata_cache, key)

    def set_answer_cache(self, query: str, answer: str) -> None:
        """ì‘ë‹µ ìºì‹œì— ì €ì¥"""
        self.manage_cache(self.answer_cache, query, answer, self.max_cache_size)

    def get_answer_cache(self, query: str) -> Optional[str]:
        """ì‘ë‹µ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        return self.get_from_cache(self.answer_cache, query)

    def set_pdf_cache(self, pdf_path: str, text_data: Dict) -> None:
        """PDF í…ìŠ¤íŠ¸ ìºì‹œì— ì €ì¥"""
        self.manage_cache(self.pdf_text_cache, pdf_path, text_data, self.max_pdf_cache)

    def get_pdf_cache(self, pdf_path: str) -> Optional[Dict]:
        """PDF í…ìŠ¤íŠ¸ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        return self.get_from_cache(self.pdf_text_cache, pdf_path)

    def set_search_cache(self, query: str, results: List) -> None:
        """ê²€ìƒ‰ ê²°ê³¼ ìºì‹œì— ì €ì¥"""
        self.manage_cache(self.search_cache, query, results, self.max_cache_size)

    def get_search_cache(self, query: str) -> Optional[List]:
        """ê²€ìƒ‰ ê²°ê³¼ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        return self.get_from_cache(self.search_cache, query)

    def clear_all_cache(self) -> None:
        """ëª¨ë“  ìºì‹œ ì´ˆê¸°í™”"""
        self.documents_cache.clear()
        self.metadata_cache.clear()
        self.answer_cache.clear()
        self.pdf_text_cache.clear()
        self.search_cache.clear()

        # í†µê³„ ì´ˆê¸°í™”
        self.cache_hits = 0
        self.cache_misses = 0

        logger.info("â™»ï¸ ëª¨ë“  ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def clear_cache(self, cache_type: str = 'all') -> None:
        """
        íŠ¹ì • ìºì‹œ ì´ˆê¸°í™”

        Args:
            cache_type: 'documents', 'metadata', 'answer', 'pdf', 'search', 'all'
        """
        if cache_type == 'all':
            self.clear_all_cache()
        elif cache_type == 'documents':
            self.documents_cache.clear()
        elif cache_type == 'metadata':
            self.metadata_cache.clear()
        elif cache_type == 'answer':
            self.answer_cache.clear()
        elif cache_type == 'pdf':
            self.pdf_text_cache.clear()
        elif cache_type == 'search':
            self.search_cache.clear()
        else:
            logger.warning(f"Unknown cache type: {cache_type}")

    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        total_size = (
            len(self.documents_cache) +
            len(self.metadata_cache) +
            len(self.answer_cache) +
            len(self.pdf_text_cache) +
            len(self.search_cache)
        )

        hit_rate = 0
        if self.cache_hits + self.cache_misses > 0:
            hit_rate = self.cache_hits / (self.cache_hits + self.cache_misses)

        return {
            'total_size': total_size,
            'hits': self.cache_hits,
            'misses': self.cache_misses,
            'hit_rate': f"{hit_rate:.2%}",
            'document_cache_size': len(self.documents_cache),
            'metadata_cache_size': len(self.metadata_cache),
            'answer_cache_size': len(self.answer_cache),
            'pdf_cache_size': len(self.pdf_text_cache),
            'search_cache_size': len(self.search_cache),
            'ttl': self.cache_ttl,
            'max_sizes': {
                'cache': self.max_cache_size,
                'metadata': self.max_metadata_cache,
                'pdf': self.max_pdf_cache
            }
        }

    def save_cache_to_disk(self) -> bool:
        """
        ìºì‹œë¥¼ ë””ìŠ¤í¬ì— ì €ì¥

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            cache_data = {
                'documents': dict(self.documents_cache),
                'metadata': dict(self.metadata_cache),
                'answer': dict(self.answer_cache),
                'pdf': dict(self.pdf_text_cache),
                'search': dict(self.search_cache),
                'stats': {
                    'hits': self.cache_hits,
                    'misses': self.cache_misses,
                    'saved_at': time.time()
                }
            }

            with open(self.cache_file, 'wb') as f:
                pickle.dump(cache_data, f)

            logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {self.cache_file}")
            return True

        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def load_cache_from_disk(self) -> bool:
        """
        ë””ìŠ¤í¬ì—ì„œ ìºì‹œ ë¡œë“œ

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self.cache_file.exists():
            logger.info("ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            return False

        try:
            with open(self.cache_file, 'rb') as f:
                cache_data = pickle.load(f)

            # ìºì‹œ ë³µì›
            self.documents_cache = OrderedDict(cache_data.get('documents', {}))
            self.metadata_cache = OrderedDict(cache_data.get('metadata', {}))
            self.answer_cache = OrderedDict(cache_data.get('answer', {}))
            self.pdf_text_cache = OrderedDict(cache_data.get('pdf', {}))
            self.search_cache = OrderedDict(cache_data.get('search', {}))

            # í†µê³„ ë³µì›
            stats = cache_data.get('stats', {})
            self.cache_hits = stats.get('hits', 0)
            self.cache_misses = stats.get('misses', 0)

            # TTL ì²´í¬ ë° ë§Œë£Œ í•­ëª© ì œê±°
            self._cleanup_expired_items()

            logger.info(f"ğŸ“‚ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {self.cache_file}")
            return True

        except Exception as e:
            logger.error(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def _cleanup_expired_items(self) -> int:
        """
        ë§Œë£Œëœ ìºì‹œ í•­ëª© ì œê±°

        Returns:
            ì œê±°ëœ í•­ëª© ìˆ˜
        """
        current_time = time.time()
        removed_count = 0

        for cache_dict in [
            self.documents_cache,
            self.metadata_cache,
            self.answer_cache,
            self.pdf_text_cache,
            self.search_cache
        ]:
            expired_keys = []
            for key, value in cache_dict.items():
                if isinstance(value, tuple) and len(value) == 2:
                    _, timestamp = value
                    if current_time - timestamp >= self.cache_ttl:
                        expired_keys.append(key)

            for key in expired_keys:
                del cache_dict[key]
                removed_count += 1

        if removed_count > 0:
            logger.info(f"ğŸ§¹ ë§Œë£Œëœ ìºì‹œ {removed_count}ê°œ ì œê±°")

        return removed_count

    def optimize_cache(self) -> Dict[str, int]:
        """
        ìºì‹œ ìµœì í™” (ë§Œë£Œ í•­ëª© ì œê±°, í¬ê¸° ì¡°ì •)

        Returns:
            ìµœì í™” ê²°ê³¼
        """
        stats = {
            'expired_removed': self._cleanup_expired_items(),
            'before_size': sum(len(c) for c in [
                self.documents_cache, self.metadata_cache,
                self.answer_cache, self.pdf_text_cache, self.search_cache
            ])
        }

        # í¬ê¸° ì´ˆê³¼ í•­ëª© ì œê±° (LRU)
        for cache_dict, max_size in [
            (self.documents_cache, self.max_cache_size),
            (self.metadata_cache, self.max_metadata_cache),
            (self.answer_cache, self.max_cache_size),
            (self.pdf_text_cache, self.max_pdf_cache),
            (self.search_cache, self.max_cache_size)
        ]:
            while len(cache_dict) > max_size:
                cache_dict.popitem(last=False)

        stats['after_size'] = sum(len(c) for c in [
            self.documents_cache, self.metadata_cache,
            self.answer_cache, self.pdf_text_cache, self.search_cache
        ])

        stats['removed_total'] = stats['before_size'] - stats['after_size']

        logger.info(f"âœ¨ ìºì‹œ ìµœì í™” ì™„ë£Œ: {stats['removed_total']}ê°œ í•­ëª© ì œê±°")

        return stats