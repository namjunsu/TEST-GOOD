#!/usr/bin/env python3
"""
Phase 1.2: ë©”íƒ€ë°ì´í„° DB êµ¬ì¶•
SQLiteë¥¼ ì‚¬ìš©í•œ PDF ë©”íƒ€ë°ì´í„° ê´€ë¦¬
"""

from app.core.logging import get_logger
import sqlite3
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import re
from functools import lru_cache
import threading
from contextlib import contextmanager

logger = get_logger(__name__)


class MetadataDB:
    """PDF ë©”íƒ€ë°ì´í„° SQLite DB ê´€ë¦¬"""

    def __init__(self, db_path: str = "metadata.db"):
        self._db_path = db_path
        self._local = threading.local()
        self._docs_root = Path("docs").resolve()  # ë¬¸ì„œ ë£¨íŠ¸ ê²½ë¡œ
        self.init_database()
        logger.info(f"MetadataDB ì´ˆê¸°í™” (thread-safe): {db_path}")

    def _new_conn(self) -> sqlite3.Connection:
        """ìƒˆ SQLite ì—°ê²° ìƒì„± (ìŠ¤ë ˆë“œë³„)"""
        conn = sqlite3.connect(
            f"file:{self._db_path}?cache=shared",
            uri=True,
            check_same_thread=False,
            isolation_level=None,
            timeout=30
        )
        conn.row_factory = sqlite3.Row

        # Performance optimizations (WAL + memory tuning)
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA busy_timeout=5000;")
        conn.execute("PRAGMA synchronous=NORMAL;")
        conn.execute("PRAGMA temp_store=MEMORY;")
        conn.execute("PRAGMA mmap_size=30000000000;")  # 30GB mmap (OS limits apply)
        conn.execute("PRAGMA cache_size=-524288;")     # ~512MB cache (negative = KB)
        conn.execute("PRAGMA page_size=4096;")
        conn.execute("PRAGMA analysis_limit=400;")

        return conn

    def _get_conn(self) -> sqlite3.Connection:
        """í˜„ì¬ ìŠ¤ë ˆë“œì˜ ì—°ê²° ë°˜í™˜ (ì—†ìœ¼ë©´ ìƒì„±)"""
        conn = getattr(self._local, "conn", None)
        if conn is None:
            conn = self._new_conn()
            self._local.conn = conn
        return conn

    @contextmanager
    def _cursor(self):
        """ì»¤ì„œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (ì•ˆì „í•œ commit/close)"""
        conn = self._get_conn()
        cur = conn.cursor()
        try:
            yield cur
            conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            cur.close()

    def _normalize_path(self, path: str) -> str:
        """
        ìƒëŒ€ ê²½ë¡œ ì •ê·œí™” í—¬í¼
        docs/foo.pdf â†’ foo.pdf
        /abs/path/docs/foo.pdf â†’ foo.pdf
        """
        try:
            p = Path(path).resolve()
            rel = p.relative_to(self._docs_root)
            return str(rel)
        except ValueError:
            # docs ë°–: ê·¸ëŒ€ë¡œ ë°˜í™˜ + ê²½ê³ 
            logger.warning(f"âš ï¸ PDF path outside docs: {path}")
            return path

    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° í…Œì´ë¸” ìƒì„±"""
        # ì´ˆê¸° ì—°ê²° ìƒì„± (ìŠ¤ë ˆë“œ ë¡œì»¬)
        conn = self._get_conn()
        logger.info(f"DB WAL mode enabled: {self._db_path}")

        with self._cursor() as cur:
            # ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ìƒì„±
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS documents (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    path TEXT UNIQUE NOT NULL,
                    filename TEXT NOT NULL,
                    title TEXT,
                    date TEXT,
                    year TEXT,
                    month TEXT,
                    category TEXT,
                    drafter TEXT,
                    amount INTEGER,
                    file_size INTEGER,
                    page_count INTEGER,
                    text_preview TEXT,
                    keywords TEXT,  -- JSON array
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """
            )

            # ì¸ë±ìŠ¤ ìƒì„± (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_year ON documents(year)")
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_category ON documents(category)"
            )
            cur.execute("CREATE INDEX IF NOT EXISTS idx_date ON documents(date)")
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_filename ON documents(filename)"
            )

            # ì „ë¬¸ ê²€ìƒ‰ì„ ìœ„í•œ FTS í…Œì´ë¸” (Full-Text Search)
            cur.execute(
                """
                CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts
                USING fts5(
                    path UNINDEXED,
                    title,
                    text_preview,
                    keywords,
                    content=documents,
                    content_rowid=id
                )
            """
            )

            # FTS íŠ¸ë¦¬ê±° ì„¤ì • (ìë™ ë™ê¸°í™”)
            cur.execute(
                """
                CREATE TRIGGER IF NOT EXISTS documents_ai
                AFTER INSERT ON documents
                BEGIN
                    INSERT INTO documents_fts(rowid, path, title, text_preview, keywords)
                    VALUES (new.id, new.path, new.title, new.text_preview, new.keywords);
                END
            """
            )

            # FTS5 external content ëª¨ë“œì—ì„œëŠ” UPDATE ëŒ€ì‹  DELETE+INSERT íŒ¨í„´ ê¶Œì¥
            cur.execute("DROP TRIGGER IF EXISTS documents_au")
            cur.execute(
                """
                CREATE TRIGGER documents_au
                AFTER UPDATE ON documents
                BEGIN
                    DELETE FROM documents_fts WHERE rowid = new.id;
                    INSERT INTO documents_fts(rowid, path, title, text_preview, keywords)
                    VALUES (new.id, new.path, new.title, new.text_preview, new.keywords);
                END
            """
            )

            cur.execute(
                """
                CREATE TRIGGER IF NOT EXISTS documents_ad
                AFTER DELETE ON documents
                BEGIN
                    DELETE FROM documents_fts WHERE rowid = old.id;
                END
            """
            )

        # ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜: doctype, display_date, claimed_total, sum_match ì»¬ëŸ¼ ì¶”ê°€
        self._migrate_schema()

    def _migrate_schema(self):
        """ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜: ì‹ ê·œ ì»¬ëŸ¼ ì¶”ê°€"""
        try:
            # ë°±ì—… ìƒì„±
            backup_path = f"{self._db_path}.bak"
            import shutil

            if Path(self._db_path).exists() and not Path(backup_path).exists():
                shutil.copy2(self._db_path, backup_path)
                logger.info(f"DB ë°±ì—… ìƒì„±: {backup_path}")

            with self._cursor() as cur:
                # doctype ì»¬ëŸ¼ ì¶”ê°€ (ì¡´ì¬ í™•ì¸ í›„ ì¶”ê°€)
                cur.execute("PRAGMA table_info(documents)")
                columns = [col[1] for col in cur.fetchall()]

                if "doctype" not in columns:
                    cur.execute(
                        'ALTER TABLE documents ADD COLUMN doctype TEXT DEFAULT "proposal"'
                    )
                    logger.info("âœ“ doctype ì»¬ëŸ¼ ì¶”ê°€")

                if "display_date" not in columns:
                    cur.execute("ALTER TABLE documents ADD COLUMN display_date TEXT")
                    logger.info("âœ“ display_date ì»¬ëŸ¼ ì¶”ê°€")

                if "claimed_total" not in columns:
                    cur.execute(
                        "ALTER TABLE documents ADD COLUMN claimed_total INTEGER"
                    )
                    logger.info("âœ“ claimed_total ì»¬ëŸ¼ ì¶”ê°€")

                if "sum_match" not in columns:
                    cur.execute("ALTER TABLE documents ADD COLUMN sum_match BOOLEAN")
                    logger.info("âœ“ sum_match ì»¬ëŸ¼ ì¶”ê°€")

        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")

    def add_document(self, metadata: Dict[str, Any]) -> int:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        try:
            # í‚¤ì›Œë“œë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            keywords = metadata.get("keywords", [])
            if isinstance(keywords, list):
                keywords = json.dumps(keywords, ensure_ascii=False)

            with self._cursor() as cur:
                cur.execute(
                    """
                    INSERT OR REPLACE INTO documents (
                        path, filename, title, date, year, month, category,
                        drafter, amount, file_size, page_count, text_preview, keywords,
                        doctype, display_date, claimed_total, sum_match
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        self._normalize_path(str(metadata.get("path", ""))),
                        metadata.get("filename", ""),
                        metadata.get("title", ""),
                        metadata.get("date", ""),
                        metadata.get("year", ""),
                        metadata.get("month", ""),
                        metadata.get("category", ""),
                        metadata.get("drafter", ""),
                        metadata.get("amount", 0),
                        metadata.get("file_size", 0),
                        metadata.get("page_count", 0),
                        metadata.get("text_preview", ""),
                        keywords,
                        metadata.get("doctype", "proposal"),
                        metadata.get("display_date", ""),
                        metadata.get("claimed_total", None),
                        metadata.get("sum_match", None),
                    ),
                )
                return cur.lastrowid

        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return -1

    def search_by_year(self, year: str) -> List[Dict[str, Any]]:
        """ì—°ë„ë³„ ê²€ìƒ‰"""
        conn = self._get_conn()
        cursor = conn.execute(
            "SELECT * FROM documents WHERE year = ? ORDER BY date DESC", (year,)
        )
        return [dict(row) for row in cursor.fetchall()]

    def search_documents(
        self, drafter: Optional[str] = None, year: Optional[str] = None, limit: int = 20
    ) -> List[Dict[str, Any]]:
        """ë‹¤ì¤‘ í•„í„° ê²€ìƒ‰ (drafter, year ì¡°í•© ì§€ì›)

        Args:
            drafter: ê¸°ì•ˆìëª… (ë¶€ë¶„ ì¼ì¹˜)
            year: ì—°ë„ (display_date ê¸°ì¤€, ì˜ˆ: "2024")
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ë‚ ì§œ ë‚´ë¦¼ì°¨ìˆœ)
        """
        query = "SELECT * FROM documents WHERE 1=1"
        params = []

        if drafter:
            query += " AND drafter LIKE ?"
            params.append(f"%{drafter}%")

        if year:
            # display_date ë˜ëŠ” date í•„ë“œì—ì„œ ì—°ë„ ì¶”ì¶œ
            query += " AND (display_date LIKE ? OR date LIKE ?)"
            params.append(f"{year}%")
            params.append(f"{year}%")

        query += " ORDER BY date DESC"
        if limit is not None:
            query += " LIMIT ?"
            params.append(limit)

        conn = self._get_conn()
        cursor = conn.execute(query, params)
        return [dict(row) for row in cursor.fetchall()]

    def count_documents(
        self, drafter: Optional[str] = None, year: Optional[str] = None
    ) -> int:
        """ë¬¸ì„œ ê°œìˆ˜ ì¡°íšŒ (í•„í„° ì ìš©)

        Args:
            drafter: ê¸°ì•ˆìëª… (ë¶€ë¶„ ì¼ì¹˜)
            year: ì—°ë„ (display_date ê¸°ì¤€, ì˜ˆ: "2024")

        Returns:
            ì¡°ê±´ì— ë§ëŠ” ë¬¸ì„œ ê°œìˆ˜
        """
        query = "SELECT COUNT(*) as count FROM documents WHERE 1=1"
        params = []

        if drafter:
            query += " AND drafter LIKE ?"
            params.append(f"%{drafter}%")

        if year:
            # display_date ë˜ëŠ” date í•„ë“œì—ì„œ ì—°ë„ ì¶”ì¶œ
            query += " AND (display_date LIKE ? OR date LIKE ?)"
            params.append(f"{year}%")
            params.append(f"{year}%")

        conn = self._get_conn()
        cursor = conn.execute(query, params)
        result = cursor.fetchone()
        return result["count"] if result else 0

    def search_by_category(self, category: str) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰"""
        conn = self._get_conn()
        cursor = conn.execute(
            "SELECT * FROM documents WHERE category LIKE ? ORDER BY date DESC",
            (f"%{category}%",),
        )
        return [dict(row) for row in cursor.fetchall()]

    def search_by_keyword(self, keyword: str, limit: int = 20) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ (FTS5 BM25 ìŠ¤ì½”ì–´ë§ ì‚¬ìš©)"""
        conn = self._get_conn()
        cursor = conn.execute(
            """
            SELECT d.*, bm25(documents_fts) AS score
            FROM documents d
            JOIN documents_fts f ON d.id = f.rowid
            WHERE documents_fts MATCH ?
            ORDER BY score ASC
            LIMIT ?
        """,
            (keyword, limit),
        )
        return [dict(row) for row in cursor.fetchall()]

    def search_by_date_range(
        self, start_date: str, end_date: str
    ) -> List[Dict[str, Any]]:
        """ë‚ ì§œ ë²”ìœ„ ê²€ìƒ‰"""
        conn = self._get_conn()
        cursor = conn.execute(
            "SELECT * FROM documents WHERE date BETWEEN ? AND ? ORDER BY date DESC",
            (start_date, end_date),
        )
        return [dict(row) for row in cursor.fetchall()]

    def get_document_by_path(self, path: str) -> Optional[Dict[str, Any]]:
        """ê²½ë¡œë¡œ ë¬¸ì„œ ì¡°íšŒ"""
        conn = self._get_conn()
        cursor = conn.execute(
            "SELECT * FROM documents WHERE path = ?", (str(path),)
        )
        row = cursor.fetchone()
        return dict(row) if row else None

    def get_document(self, filename: str) -> Optional[Dict[str, Any]]:
        """íŒŒì¼ëª…ìœ¼ë¡œ ë¬¸ì„œ ì¡°íšŒ (perfect_rag.py í˜¸í™˜ìš©)"""
        # íŒŒì¼ëª…ë§Œìœ¼ë¡œ ê²€ìƒ‰
        conn = self._get_conn()
        cursor = conn.execute(
            "SELECT * FROM documents WHERE filename = ? LIMIT 1", (filename,)
        )
        row = cursor.fetchone()
        return dict(row) if row else None

    def get_by_filename(self, filename: str) -> Optional[Dict[str, Any]]:
        """íŒŒì¼ëª…ìœ¼ë¡œ ë¬¸ì„œ ì¡°íšŒ (claimed_total í¬í•¨, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)

        Args:
            filename: íŒŒì¼ëª… (í™•ì¥ì í¬í•¨ ê°€ëŠ¥)

        Returns:
            ë¬¸ì„œ ë”•ì…”ë„ˆë¦¬ (claimed_total í¬í•¨) ë˜ëŠ” None
        """
        conn = self._get_conn()
        cursor = conn.execute(
            "SELECT * FROM documents WHERE filename = ? COLLATE NOCASE LIMIT 1",
            (filename,)
        )
        row = cursor.fetchone()
        return dict(row) if row else None

    def get_by_filename_fuzzy(self, name: str) -> Optional[Dict[str, Any]]:
        """í¼ì§€ ë§¤ì¹­ìœ¼ë¡œ íŒŒì¼ëª… ê²€ìƒ‰ (ì–¸ë”ìŠ¤ì½”ì–´/ê³µë°±/íŠ¹ìˆ˜ê¸°í˜¸ ë¬´ì‹œ)

        Args:
            name: ê²€ìƒ‰í•  íŒŒì¼ëª… (ì¼ë¶€ë§Œ ì…ë ¥ ê°€ëŠ¥)

        Returns:
            ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        # ì´ëª¨ì§€ì™€ ë©”íƒ€ë°ì´í„° ì œê±° (ğŸ·, ğŸ“…, âœ ë“±)
        # "ë‰´ìŠ¤ ìŠ¤íŠœë””ì˜¤ ì§€ë¯¸ì§‘ Control Box ìˆ˜ë¦¬ ê±´ ğŸ· proposal Â· ğŸ“… 2024-11-25 Â· âœ ë‚¨ì¤€ìˆ˜"
        # -> "ë‰´ìŠ¤ ìŠ¤íŠœë””ì˜¤ ì§€ë¯¸ì§‘ Control Box ìˆ˜ë¦¬ ê±´"
        clean_name = re.sub(r'[ğŸ·ğŸ“…âœÂ·].*$', '', name).strip()

        def slug(s):
            """ë¬¸ìì—´ ì •ê·œí™”: ì†Œë¬¸ì + íŠ¹ìˆ˜ê¸°í˜¸ ì œê±°"""
            s = s.lower().replace("&", "and")
            # ê³µë°±ê³¼ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ëª¨ë‘ ì œê±°í•˜ì—¬ ë¹„êµ
            return re.sub(r"[^0-9a-zê°€-í£]", "", s)

        s = slug(clean_name)

        # ë¹ˆ ë¬¸ìì—´ì´ë©´ None ë°˜í™˜
        if not s:
            return None

        conn = self._get_conn()
        cur = conn.cursor()
        # SQLì—ì„œë„ ëª¨ë“  íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ì—¬ ë¹„êµ
        cur.execute(
            """
            SELECT *,
              ABS(LENGTH(filename) - ?) AS len_diff
            FROM documents
            WHERE LOWER(REPLACE(REPLACE(REPLACE(REPLACE(filename, '_',''), ' ',''), '.pdf',''), '-','')) LIKE ?
            ORDER BY len_diff ASC
            LIMIT 1
            """,
            (len(clean_name), f"%{s}%"),
        )
        row = cur.fetchone()
        return dict(row) if row else None

    def get_text_preview(self, filename: str) -> Optional[str]:
        """íŒŒì¼ëª…ìœ¼ë¡œ text_preview ì¡°íšŒ (snippet ë³´ê°•ìš©)

        Args:
            filename: íŒŒì¼ëª… (í™•ì¥ì í¬í•¨ ê°€ëŠ¥)

        Returns:
            text_preview ë¬¸ìì—´ ë˜ëŠ” None
        """
        conn = self._get_conn()
        cursor = conn.execute(
            "SELECT text_preview FROM documents WHERE filename = ? COLLATE NOCASE LIMIT 1",
            (filename,)
        )
        row = cursor.fetchone()
        return row["text_preview"] if row and row["text_preview"] else None

    def get_page_text(self, doc_id: str, page: int) -> Optional[str]:
        """íŠ¹ì • ë¬¸ì„œì˜ íŠ¹ì • í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìºì‹œ ì§€ì›)

        Args:
            doc_id: ë¬¸ì„œ ID (filename ë˜ëŠ” path)
            page: í˜ì´ì§€ ë²ˆí˜¸ (1-based)

        Returns:
            í˜ì´ì§€ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        try:
            # ë¬¸ì„œ ê²½ë¡œ ì¡°íšŒ
            conn = self._get_conn()
            cursor = conn.execute(
                "SELECT path FROM documents WHERE filename = ? OR path = ? LIMIT 1",
                (doc_id, self._normalize_path(doc_id))
            )
            row = cursor.fetchone()

            if not row:
                logger.debug(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {doc_id}")
                return None

            pdf_path = Path(row["path"])

            if not pdf_path.exists():
                logger.debug(f"PDF íŒŒì¼ ì—†ìŒ: {pdf_path}")
                return None

            # PDFì—ì„œ í˜ì´ì§€ ì¶”ì¶œ (pdfplumber ì‚¬ìš©)
            try:
                import pdfplumber

                with pdfplumber.open(pdf_path) as pdf:
                    if page < 1 or page > len(pdf.pages):
                        logger.debug(f"í˜ì´ì§€ ë²”ìœ„ ì´ˆê³¼: {page} (ì´ {len(pdf.pages)}ìª½)")
                        return None

                    page_obj = pdf.pages[page - 1]  # 0-based index
                    text = page_obj.extract_text() or ""

                    logger.debug(f"í˜ì´ì§€ ì¶”ì¶œ ì„±ê³µ: {doc_id} page={page}, len={len(text)}")
                    return text.strip()

            except Exception as e:
                logger.error(f"PDF í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path} page={page}, error={e}")
                return None

        except Exception as e:
            logger.error(f"get_page_text ì‹¤íŒ¨: doc_id={doc_id}, page={page}, error={e}")
            return None

    def update_document(self, filename: str, **kwargs):
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê°„í¸ ì—…ë°ì´íŠ¸ (perfect_rag.py í˜¸í™˜ìš©)"""
        # ë¨¼ì € ë¬¸ì„œ ì°¾ê¸°
        doc = self.get_document(filename)

        if not doc:
            # ìƒˆ ë¬¸ì„œë©´ ì¶”ê°€
            metadata = {"filename": filename}
            metadata.update(kwargs)
            return self.add_document(metadata)

        # ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸
        fields = []
        values = []
        for key, value in kwargs.items():
            if key in [
                "title",
                "date",
                "year",
                "month",
                "category",
                "drafter",
                "amount",
                "file_size",
                "page_count",
                "text_preview",
                "keywords",
                "doctype",
                "display_date",
                "claimed_total",
                "sum_match",
            ]:
                fields.append(f"{key} = ?")
                if key == "keywords" and isinstance(value, list):
                    value = json.dumps(value, ensure_ascii=False)
                values.append(value)

        if fields:
            values.append(doc["id"])
            query = f"UPDATE documents SET {', '.join(fields)}, updated_at = CURRENT_TIMESTAMP WHERE id = ?"
            with self._cursor() as cur:
                cur.execute(query, values)

    def update_text_preview(self, path: str, text_preview: str):
        """í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° ì—…ë°ì´íŠ¸"""
        with self._cursor() as cur:
            cur.execute(
                "UPDATE documents SET text_preview = ?, updated_at = CURRENT_TIMESTAMP WHERE path = ?",
                (text_preview[:1000], self._normalize_path(str(path))),  # ìµœëŒ€ 1000ì
            )

    def get_statistics(self) -> Dict[str, Any]:
        """DB í†µê³„ ì •ë³´"""
        conn = self._get_conn()
        cursor = conn.execute("SELECT COUNT(*) as total FROM documents")
        total = cursor.fetchone()["total"]

        cursor = conn.execute(
            """
            SELECT year, COUNT(*) as count
            FROM documents
            GROUP BY year
            ORDER BY year DESC
        """
        )
        by_year = {row["year"]: row["count"] for row in cursor.fetchall()}

        cursor = conn.execute(
            """
            SELECT category, COUNT(*) as count
            FROM documents
            GROUP BY category
            ORDER BY count DESC
        """
        )
        by_category = {row["category"]: row["count"] for row in cursor.fetchall()}

        return {
            "total_documents": total,
            "by_year": by_year,
            "by_category": by_category,
        }

    def rebuild_fts_index(self):
        """FTS ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        with self._cursor() as cur:
            cur.execute('INSERT INTO documents_fts(documents_fts) VALUES("rebuild")')
        logger.info("FTS ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ")

    def list_unique_drafters(self) -> set:
        """DBì— ì¡´ì¬í•˜ëŠ” ê³ ìœ  ê¸°ì•ˆì ëª©ë¡ ë°˜í™˜ (Closed-World Validationìš©)

        Returns:
            set: ê³ ìœ  ê¸°ì•ˆì ì´ë¦„ ì§‘í•©
        """
        try:
            conn = self._get_conn()
            cursor = conn.execute("""
                SELECT DISTINCT drafter
                FROM documents
                WHERE drafter IS NOT NULL
                  AND drafter != ''
                  AND drafter != 'ë¯¸ìƒ'
                  AND drafter != 'ì‘ì„±ì ë¯¸ìƒ'
            """)
            drafters = {row['drafter'] for row in cursor.fetchall()}
            logger.info(f"âœ… ê³ ìœ  ê¸°ì•ˆì {len(drafters)}ëª… ë¡œë“œ")
            return drafters
        except Exception as e:
            logger.error(f"ê¸°ì•ˆì ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return set()

    def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ (ëª¨ë“  ìŠ¤ë ˆë“œ ë¡œì»¬ ì—°ê²° ì¢…ë£Œ)"""
        conn = getattr(self._local, "conn", None)
        if conn:
            conn.close()
            self._local.conn = None

    def count_unique_documents(self, allowed_ext=('pdf', 'txt')) -> int:
        """ê³ ìœ  ë¬¸ì„œ ìˆ˜ ì¹´ìš´íŠ¸ (ì¤‘ë³µ ì œê±°, í™•ì¥ì í•„í„°)

        Args:
            allowed_ext: í—ˆìš© í™•ì¥ì íŠœí”Œ (ê¸°ë³¸: pdf, txt)

        Returns:
            ê³ ìœ  ë¬¸ì„œ ìˆ˜
        """
        try:
            # í™•ì¥ì ì¡°ê±´ ìƒì„±
            ext_conditions = []
            for ext in allowed_ext:
                ext_conditions.append(f"LOWER(filename) LIKE '%.{ext}'")
            ext_where = f"({' OR '.join(ext_conditions)})" if ext_conditions else "1=1"

            # ê³ ìœ  ë¬¸ì„œ ì¹´ìš´íŠ¸ (ì¤‘ë³µ ì œê±°)
            query = f"""
                SELECT COUNT(DISTINCT filename) as count
                FROM documents
                WHERE {ext_where}
            """

            conn = self._get_conn()
            cursor = conn.execute(query)
            result = cursor.fetchone()
            return result['count'] if result else 0

        except Exception as e:
            logger.error(f"ê³ ìœ  ë¬¸ì„œ ì¹´ìš´íŠ¸ ì‹¤íŒ¨: {e}")
            return 0

    def count_by_extension(self) -> dict:
        """í™•ì¥ìë³„ ë¬¸ì„œ ìˆ˜ ì¹´ìš´íŠ¸ (ë¬¼ë¦¬ íŒŒì¼ ê¸°ì¤€)

        Returns:
            {'pdf': N, 'txt': M, 'others': K} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        try:
            import json
            import os
            from config.indexing import DB_PATHS

            # file_index.jsonì—ì„œ ë¬¼ë¦¬ íŒŒì¼ ì •ë³´ ì½ê¸°
            file_index_path = DB_PATHS.get("file_index", "file_index.json")

            if os.path.exists(file_index_path):
                with open(file_index_path, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)

                counts = {'pdf': 0, 'txt': 0, 'others': 0}

                # íŒŒì¼ë³„ë¡œ í™•ì¥ì ì¹´ìš´íŠ¸
                for file_info in file_data.get('files', []):
                    filename = file_info.get('name', '').lower()
                    if filename.endswith('.pdf'):
                        counts['pdf'] += 1
                    elif filename.endswith('.txt'):
                        counts['txt'] += 1
                    else:
                        counts['others'] += 1

                return counts
            else:
                # file_index.jsonì´ ì—†ìœ¼ë©´ DBì—ì„œ ì¹´ìš´íŠ¸
                counts = {'pdf': 0, 'txt': 0, 'others': 0}

                conn = self._get_conn()
                cursor = conn.execute("""
                    SELECT
                        LOWER(filename) as fname
                    FROM documents
                """)

                for row in cursor:
                    fname = row['fname']
                    if fname.endswith('.pdf'):
                        counts['pdf'] += 1
                    elif fname.endswith('.txt'):
                        counts['txt'] += 1
                    else:
                        counts['others'] += 1

                return counts

        except Exception as e:
            logger.error(f"í™•ì¥ìë³„ ì¹´ìš´íŠ¸ ì‹¤íŒ¨: {e}")
            return {'pdf': 0, 'txt': 0, 'others': 0}

    def count_search_index(self) -> int:
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ì— ë“±ë¡ëœ ê³ ìœ  ë¬¸ì„œ ìˆ˜

        Returns:
            ê²€ìƒ‰ ê°€ëŠ¥í•œ ê³ ìœ  ë¬¸ì„œ ìˆ˜
        """
        try:
            import sqlite3
            import os
            from config.indexing import DB_PATHS

            # everything_index.dbì—ì„œ ê²€ìƒ‰ ê°€ëŠ¥ ë¬¸ì„œ ìˆ˜ ì¡°íšŒ
            index_db_path = DB_PATHS.get("everything_index", "everything_index.db")

            if os.path.exists(index_db_path):
                index_conn = sqlite3.connect(index_db_path)
                index_conn.row_factory = sqlite3.Row

                # ê³ ìœ  íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ì¹´ìš´íŠ¸
                cursor = index_conn.execute("""
                    SELECT COUNT(DISTINCT filename) as count
                    FROM files
                """)

                result = cursor.fetchone()
                index_conn.close()

                return result['count'] if result else 0
            else:
                # everything_index.dbê°€ ì—†ìœ¼ë©´ metadata.db ì‚¬ìš©
                return self.count_unique_documents()

        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¹´ìš´íŠ¸ ì‹¤íŒ¨: {e}")
            # í´ë°±: metadata.dbì˜ ê³ ìœ  ë¬¸ì„œ ìˆ˜ ë°˜í™˜
            return self.count_unique_documents()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


def extract_metadata_from_filename(filename: str) -> Dict[str, Any]:
    """íŒŒì¼ëª…ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    metadata = {
        "filename": filename,
        "title": "",
        "date": "",
        "year": "",
        "month": "",
        "category": "",
        "drafter": "",
    }

    # ë‚ ì§œ ì¶”ì¶œ (YYYY-MM-DD or YYYY-MM or YYYY)
    date_match = re.search(r"(\d{4})[-_]?(\d{2})?[-_]?(\d{2})?", filename)
    if date_match:
        year = date_match.group(1)
        month = date_match.group(2) or ""
        day = date_match.group(3) or ""

        metadata["year"] = year
        metadata["month"] = month

        if day:
            metadata["date"] = f"{year}-{month}-{day}"
        elif month:
            metadata["date"] = f"{year}-{month}"
        else:
            metadata["date"] = year

    # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
    categories = ["êµ¬ë§¤", "ìˆ˜ë¦¬", "ë³´ìˆ˜", "êµì²´", "íê¸°", "ê²€í† ", "ê¸°ìˆ ", "ì†Œëª¨í’ˆ"]
    for cat in categories:
        if cat in filename:
            metadata["category"] = cat
            break

    # ì œëª© ì¶”ì¶œ (ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ)
    title_part = filename.replace(".pdf", "").replace(".PDF", "")
    # ë‚ ì§œ ë¶€ë¶„ ì œê±°
    title_part = re.sub(r"\d{4}[-_]?\d{2}[-_]?\d{2}[-_]?", "", title_part)
    title_part = title_part.replace("_", " ").strip()
    metadata["title"] = title_part

    return metadata
