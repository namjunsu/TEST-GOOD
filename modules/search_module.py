#!/usr/bin/env python3
"""
ê²€ìƒ‰ ëª¨ë“ˆ - Perfect RAGì—ì„œ ë¶„ë¦¬ëœ ê²€ìƒ‰ ê¸°ëŠ¥
2025-09-29 ë¦¬íŒ©í† ë§

ì´ ëª¨ë“ˆì€ perfect_rag.pyì—ì„œ ê²€ìƒ‰ ê´€ë ¨ ê¸°ëŠ¥ì„ ë¶„ë¦¬í•˜ì—¬
ìœ ì§€ë³´ìˆ˜ì„±ê³¼ ê°€ë…ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import os
import re
import time
import logging
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import pdfplumber

# ê²€ìƒ‰ ê´€ë ¨ ëª¨ë“ˆë“¤
from everything_like_search import EverythingLikeSearch
from modules.metadata_extractor import MetadataExtractor
from modules.metadata_db import MetadataDB

logger = logging.getLogger(__name__)


class SearchModule:
    """ê²€ìƒ‰ ê¸°ëŠ¥ í†µí•© ëª¨ë“ˆ"""

    def __init__(self, docs_dir: str = "docs", config: Dict = None):
        """
        Args:
            docs_dir: ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.docs_dir = Path(docs_dir)
        self.config = config or {}

        # Everything-like ê²€ìƒ‰ ì´ˆê¸°í™”
        self.everything_search = None
        try:
            self.everything_search = EverythingLikeSearch()
            logger.info("Everything-like search initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Everything search: {e}")

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° ì´ˆê¸°í™”
        self.metadata_extractor = None
        try:
            self.metadata_extractor = MetadataExtractor()
            logger.info("âœ… MetadataExtractor ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ MetadataExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # ë©”íƒ€ë°ì´í„° DB ì´ˆê¸°í™”
        self.metadata_db = None
        try:
            self.metadata_db = MetadataDB()
            logger.info("âœ… MetadataDB ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ MetadataDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # ìºì‹œ
        self.search_cache = {}
        self.cache_ttl = 3600  # 1ì‹œê°„

        # OCR ìºì‹œ ë¡œë“œ
        self.ocr_cache = {}
        self._load_ocr_cache()

    def search_by_drafter(self, drafter_name: str, top_k: int = 20) -> List[Dict[str, Any]]:
        """
        ê¸°ì•ˆìë³„ ë¬¸ì„œ ê²€ìƒ‰ - department í•„ë“œì—ì„œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë§Œ ë°˜í™˜

        Args:
            drafter_name: ê¸°ì•ˆì ì´ë¦„
            top_k: ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜

        Returns:
            ê¸°ì•ˆìê°€ ì‘ì„±í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if self.everything_search:
            try:
                # ì§ì ‘ SQL ì¿¼ë¦¬ë¡œ department í•„ë“œì—ì„œ ì •í™•í•œ ê¸°ì•ˆì ê²€ìƒ‰
                import sqlite3
                conn = sqlite3.connect('everything_index.db')
                cursor = conn.cursor()

                cursor.execute("""
                    SELECT * FROM files
                    WHERE department LIKE ?
                    ORDER BY year DESC, month DESC
                    LIMIT ?
                """, (f'%{drafter_name}%', top_k))

                results = []
                for row in cursor.fetchall():
                    results.append({
                        'filename': row[1],
                        'path': row[2],
                        'score': 2.0,  # ë†’ì€ ì ìˆ˜ (ì •í™•í•œ ë§¤ì¹­)
                        'date': row[4],
                        'category': row[7],
                        'department': row[8],
                        'keywords': row[9]
                    })

                conn.close()
                logger.info(f"Found {len(results)} documents by drafter: {drafter_name}")
                return results

            except Exception as e:
                logger.error(f"Drafter search failed: {e}")
                return []

        return []

    def search_by_content(self, query: str, top_k: int = 20) -> List[Dict[str, Any]]:
        """
        ë‚´ìš© ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ - Everything-like ì´ˆê³ ì† ê²€ìƒ‰ ì‚¬ìš©

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # Everything-like ê²€ìƒ‰ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
        if self.everything_search:
            try:
                # ì´ˆê³ ì† SQLite ê²€ìƒ‰
                search_results = self.everything_search.search(query, limit=top_k)

                results = []
                for doc in search_results:
                    result = {
                        'filename': doc['filename'],
                        'path': doc['path'],
                        'score': doc.get('score', 1.0),
                        'date': doc.get('date', ''),
                        'category': doc.get('category', ''),
                        'keywords': doc.get('keywords', ''),
                        'department': doc.get('department', '')  # ê¸°ì•ˆì ì •ë³´ í¬í•¨
                    }

                    # ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    if doc['path']:
                        try:
                            pdf_path = Path(doc['path'])
                            if pdf_path.exists() and pdf_path.suffix.lower() == '.pdf':
                                with pdfplumber.open(pdf_path) as pdf:
                                    # ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ ë° í‘œ ì¶”ì¶œ (ìµœëŒ€ 5000ì)
                                    full_text = ""
                                    for page in pdf.pages[:5]:  # ìµœëŒ€ 5í˜ì´ì§€
                                        # ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                        page_text = page.extract_text() or ""
                                        full_text += page_text + "\n\n"

                                        # í‘œ ì¶”ì¶œ ë° ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë³€í™˜
                                        tables = page.extract_tables()
                                        if tables:
                                            for table in tables:
                                                table_md = self._format_table_as_markdown(table)
                                                if table_md:
                                                    full_text += "\nğŸ“Š **í‘œ ë°ì´í„°**\n" + table_md + "\n\n"

                                        # ì ì‘í˜• í˜ì´ì§€ ì œí•œ: ë¬¸ì„œ ê¸¸ì´ì— ë”°ë¼ ì¡°ì ˆ
                                        if len(full_text) > 15000:  # ì¶©ë¶„í•œ ë‚´ìš© í™•ë³´
                                            break

                                    # pdfplumber ì‹¤íŒ¨ì‹œ OCR ìºì‹œ ì‹œë„
                                    # 1) í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜
                                    # 2) í—¤ë”/í‘¸í„°ë§Œ ìˆê³  ì‹¤ì œ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°
                                    text_lines = [line for line in full_text.split('\n') if line.strip()]
                                    is_mostly_headers = len(text_lines) < 10 or full_text.count('gw.channela-mt.com') > 2

                                    if len(full_text.strip()) < 500 or is_mostly_headers:
                                        ocr_text = self._get_ocr_text(pdf_path)
                                        if ocr_text and len(ocr_text) > len(full_text):
                                            full_text = ocr_text
                                            logger.info(f"ğŸ“· OCR ìºì‹œ ì‚¬ìš©: {pdf_path.name} ({len(ocr_text)}ì)")

                                    # ì ì‘í˜• ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬ (ë¬¸ì„œ ê¸¸ì´ì— ë”°ë¼ ìë™ ì¡°ì ˆ)
                                    result['content'] = full_text  # ì „ì²´ ë‚´ìš© ì €ì¥ (ê¸¸ì´ ì œí•œ ì—†ìŒ)

                                    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì²« í˜ì´ì§€ ê¸°ì¤€)
                                    if self.metadata_extractor and pdf.pages:
                                        first_page_text = pdf.pages[0].extract_text() or ""
                                        metadata = self.metadata_extractor.extract_all(
                                            first_page_text[:2000],
                                            doc['filename']
                                        )

                                        # ì¶”ì¶œëœ ì •ë³´ ì¶”ê°€
                                        summary = metadata.get('summary', {})
                                        if summary.get('date'):
                                            result['extracted_date'] = summary['date']
                                        if summary.get('amount'):
                                            result['extracted_amount'] = summary['amount']
                                        if summary.get('department'):
                                            result['extracted_dept'] = summary['department']
                                        if summary.get('doc_type'):
                                            result['extracted_type'] = summary['doc_type']
                                        if summary.get('drafter'):
                                            result['drafter'] = summary['drafter']
                        except Exception as e:
                            logger.debug(f"í…ìŠ¤íŠ¸/ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                            result['content'] = ""  # ì‹¤íŒ¨ì‹œ ë¹ˆ ë¬¸ìì—´

                    results.append(result)

                logger.info(f"Everything search found {len(results)} documents for query: {query}")
                return results

            except Exception as e:
                logger.error(f"Everything search failed: {e}, falling back to legacy search")
                return self._legacy_search(query, top_k)
        else:
            return self._legacy_search(query, top_k)

    def _legacy_search(self, query: str, top_k: int = 20) -> List[Dict[str, Any]]:
        """ë ˆê±°ì‹œ ê²€ìƒ‰ (Everything ì‚¬ìš© ë¶ˆê°€ì‹œ í´ë°±)"""
        pdf_files = list(self.docs_dir.rglob("*.pdf"))
        if not pdf_files:
            logger.warning("No PDF files found in docs directory")
            return []

        # ë³‘ë ¬ ê²€ìƒ‰
        results = self._parallel_search_pdfs(pdf_files, query, top_k)
        return results[:top_k]

    def _parallel_search_pdfs(self, pdf_files: List[Path], query: str, top_k: int = 5) -> List[Dict]:
        """PDF íŒŒì¼ë“¤ì„ ë³‘ë ¬ë¡œ ê²€ìƒ‰"""
        results = []

        def search_single_pdf(pdf_path):
            try:
                # ê°„ë‹¨í•œ íŒŒì¼ëª… ë§¤ì¹­
                filename_lower = pdf_path.name.lower()
                query_lower = query.lower()

                # íŒŒì¼ëª…ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
                score = 0
                for keyword in query_lower.split():
                    if keyword in filename_lower:
                        score += 1

                if score > 0:
                    return {
                        'filename': pdf_path.name,
                        'path': str(pdf_path),
                        'score': score,
                        'date': self._extract_date_from_filename(pdf_path.name),
                        'category': self._extract_category_from_path(pdf_path)
                    }
            except Exception as e:
                logger.debug(f"Error searching {pdf_path}: {e}")

            return None

        # ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(search_single_pdf, pdf): pdf for pdf in pdf_files}

            for future in as_completed(futures):
                result = future.result()
                if result:
                    results.append(result)

        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        results.sort(key=lambda x: x['score'], reverse=True)
        return results

    def find_best_document(self, query: str) -> Optional[Path]:
        """ì¿¼ë¦¬ì— ê°€ì¥ ì í•©í•œ ë‹¨ì¼ ë¬¸ì„œ ì°¾ê¸°"""
        results = self.search_by_content(query, top_k=1)
        if results:
            return Path(results[0]['path'])
        return None

    def search_multiple_documents(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ë¬¸ì„œ ë™ì‹œ ê²€ìƒ‰ ë° í†µí•© ê²°ê³¼ ë°˜í™˜"""
        results = self.search_by_content(query, top_k=top_k)

        # ê° ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ
        for result in results:
            try:
                pdf_path = Path(result['path'])
                if pdf_path.exists():
                    with pdfplumber.open(pdf_path) as pdf:
                        # ì²« í˜ì´ì§€ ë‚´ìš© ì¶”ê°€
                        if pdf.pages:
                            text = pdf.pages[0].extract_text() or ""
                            result['preview'] = text[:500]  # ë¯¸ë¦¬ë³´ê¸°
            except Exception as e:
                logger.debug(f"Error extracting preview: {e}")
                result['preview'] = ""

        return results

    def search_by_date_range(self, start_date: str, end_date: str) -> List[Dict[str, Any]]:
        """ë‚ ì§œ ë²”ìœ„ë¡œ ë¬¸ì„œ ê²€ìƒ‰"""
        all_files = list(self.docs_dir.rglob("*.pdf"))
        results = []

        for pdf_path in all_files:
            date = self._extract_date_from_filename(pdf_path.name)
            if date and start_date <= date <= end_date:
                results.append({
                    'filename': pdf_path.name,
                    'path': str(pdf_path),
                    'date': date
                })

        results.sort(key=lambda x: x['date'])
        return results

    def search_by_category(self, category: str) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ê²€ìƒ‰"""
        category_path = self.docs_dir / f"category_{category.lower()}"
        if not category_path.exists():
            return []

        pdf_files = list(category_path.rglob("*.pdf"))
        return [
            {
                'filename': pdf.name,
                'path': str(pdf),
                'category': category
            }
            for pdf in pdf_files
        ]

    def get_search_statistics(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ í†µê³„ ë°˜í™˜"""
        stats = {
            'total_documents': len(list(self.docs_dir.rglob("*.pdf"))),
            'categories': len([d for d in self.docs_dir.iterdir() if d.is_dir() and d.name.startswith('category_')]),
            'years': len([d for d in self.docs_dir.iterdir() if d.is_dir() and d.name.startswith('year_')]),
            'cache_size': len(self.search_cache)
        }

        if self.metadata_db:
            stats['indexed_documents'] = self.metadata_db.get_document_count()

        return stats

    # í‘œ í˜•ì‹ ë³€í™˜ ë©”ì„œë“œ
    def _format_table_as_markdown(self, table):
        """í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not table or not table[0]:
            return ""

        lines = []

        # í—¤ë” (ì²« ë²ˆì§¸ í–‰)
        header = " | ".join([str(cell or '').strip() for cell in table[0]])
        lines.append(header)

        # êµ¬ë¶„ì„ 
        separator = " | ".join(["---"] * len(table[0]))
        lines.append(separator)

        # ë°ì´í„° í–‰
        for row in table[1:]:
            row_text = " | ".join([str(cell or '').strip() for cell in row])
            lines.append(row_text)

        return "\n".join(lines)

    # OCR ìºì‹œ ê´€ë ¨ ë©”ì„œë“œ
    def _load_ocr_cache(self):
        """OCR ìºì‹œ ë¡œë“œ"""
        ocr_cache_path = self.docs_dir / ".ocr_cache.json"
        if ocr_cache_path.exists():
            try:
                with open(ocr_cache_path, 'r', encoding='utf-8') as f:
                    self.ocr_cache = json.load(f)
                logger.info(f"âœ… OCR ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.ocr_cache)}ê°œ ë¬¸ì„œ")
            except Exception as e:
                logger.warning(f"OCR ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.ocr_cache = {}
        else:
            logger.debug("OCR ìºì‹œ íŒŒì¼ ì—†ìŒ")

    def _get_ocr_text(self, pdf_path: Path) -> str:
        """PDFì—ì„œ OCR í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ìš°ì„ )"""
        try:
            # íŒŒì¼ í•´ì‹œ ê³„ì‚°
            import hashlib
            with open(pdf_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()

            # ìºì‹œì—ì„œ ì°¾ê¸°
            if file_hash in self.ocr_cache:
                cached_data = self.ocr_cache[file_hash]
                return cached_data.get('text', '')

            return ""
        except Exception as e:
            logger.debug(f"OCR í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return ""

    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def _extract_date_from_filename(self, filename: str) -> Optional[str]:
        """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        # 2024-08-13 í˜•ì‹
        date_pattern = r'(\d{4}-\d{2}-\d{2})'
        match = re.search(date_pattern, filename)
        if match:
            return match.group(1)

        # 2024_08_13 í˜•ì‹
        date_pattern2 = r'(\d{4}_\d{2}_\d{2})'
        match = re.search(date_pattern2, filename)
        if match:
            return match.group(1).replace('_', '-')

        return None

    def _extract_category_from_path(self, path: Path) -> str:
        """ê²½ë¡œì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        parts = path.parts
        for part in parts:
            if part.startswith('category_'):
                return part.replace('category_', '')
        return 'general'

    def clear_cache(self):
        """ê²€ìƒ‰ ìºì‹œ ì´ˆê¸°í™”"""
        self.search_cache.clear()
        logger.info("Search cache cleared")