#!/usr/bin/env python3
"""
ê²€ìƒ‰ ëª¨ë“ˆ - Perfect RAGì—ì„œ ë¶„ë¦¬ëœ ê²€ìƒ‰ ê¸°ëŠ¥
2025-09-29 ë¦¬íŒ©í† ë§

ì´ ëª¨ë“ˆì€ perfect_rag.pyì—ì„œ ê²€ìƒ‰ ê´€ë ¨ ê¸°ëŠ¥ì„ ë¶„ë¦¬í•˜ì—¬
ìœ ì§€ë³´ìˆ˜ì„±ê³¼ ê°€ë…ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

from app.core.logging import get_logger
import os
import re
import time
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import pdfplumber

# ê²€ìƒ‰ ê´€ë ¨ ëª¨ë“ˆë“¤
from everything_like_search import EverythingLikeSearch
from modules.metadata_extractor import MetadataExtractor
from modules.metadata_db import MetadataDB

logger = get_logger(__name__)


class SearchModule:
    """ê²€ìƒ‰ ê¸°ëŠ¥ í†µí•© ëª¨ë“ˆ"""

    def __init__(self, docs_dir: str = "docs", config: Dict = None):
        """
        Args:
            docs_dir: ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.docs_dir = Path(docs_dir)
        self.config = config or {}

        # Everything-like ê²€ìƒ‰ ì´ˆê¸°í™”
        self.everything_search = None
        try:
            self.everything_search = EverythingLikeSearch()
            logger.info("Everything-like search initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Everything search: {e}")

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° ì´ˆê¸°í™”
        self.metadata_extractor = None
        try:
            self.metadata_extractor = MetadataExtractor()
            logger.info("âœ… MetadataExtractor ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ MetadataExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # ë©”íƒ€ë°ì´í„° DB ì´ˆê¸°í™”
        self.metadata_db = None
        try:
            self.metadata_db = MetadataDB()
            logger.info("âœ… MetadataDB ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ MetadataDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # ìºì‹œ
        self.search_cache = {}
        self.cache_ttl = 3600  # 1ì‹œê°„

        # OCR ìºì‹œ ë¡œë“œ
        self.ocr_cache = {}
        self._load_ocr_cache()

    def search_by_drafter(self, drafter_name: str, top_k: int = 20) -> List[Dict[str, Any]]:
        """
        ê¸°ì•ˆìë³„ ë¬¸ì„œ ê²€ìƒ‰ - ì •í™• ì»¬ëŸ¼: metadata.db.documents.drafter

        Args:
            drafter_name: ê¸°ì•ˆì ì´ë¦„
            top_k: ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜

        Returns:
            ê¸°ì•ˆìê°€ ì‘ì„±í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        # 1ìˆœìœ„: ë©”íƒ€DB FTS/ì •ë ¬ ê¸°ë°˜
        if self.metadata_db:
            try:
                # ë©”íƒ€DB ì§ì ‘ ì¡°íšŒ (COALESCEë¡œ ìµœì‹ ìˆœ)
                import sqlite3
                conn = sqlite3.connect("metadata.db")
                conn.row_factory = sqlite3.Row
                cur = conn.execute("""
                    SELECT *
                    FROM documents
                    WHERE drafter LIKE ?
                    ORDER BY COALESCE(display_date, date) DESC
                    LIMIT ?
                """, (f"%{drafter_name}%", top_k))
                rows = [dict(r) for r in cur.fetchall()]
                conn.close()

                return [{
                    'id': r['id'],  # doc_id for deduplication
                    'filename': r['filename'],
                    'path': r['path'],
                    'score': 2.0,  # í•„ë“œ ì¼ì¹˜ ê°€ì¤‘
                    'date': r.get('display_date') or r.get('date') or '',
                    'category': r.get('category') or '',
                    'drafter': r.get('drafter') or '',
                    'keywords': r.get('keywords') or ''
                } for r in rows]
            except Exception as e:
                logger.error(f"Drafter search via metadata.db failed: {e}")

        # 2ìˆœìœ„(ì˜µì…˜): everything_indexì— drafter ì»¬ëŸ¼ì´ ì¡´ì¬í•  ë•Œë§Œ ì‚¬ìš©
        if self.everything_search:
            try:
                import sqlite3
                conn = sqlite3.connect('everything_index.db')
                cur = conn.cursor()
                # ë“œë¡­ì¸ í˜¸í™˜: ì¡´ì¬ ì»¬ëŸ¼ í™•ì¸
                cols = [c[1] for c in cur.execute("PRAGMA table_info(files)")]
                if 'drafter' in cols:
                    cur.execute("""
                        SELECT id, filename, path, date, category, drafter, keywords
                        FROM files
                        WHERE drafter LIKE ?
                        ORDER BY date DESC
                        LIMIT ?
                    """, (f"%{drafter_name}%", top_k))
                    results = [{
                        'id': r[0], 'filename': r[1], 'path': r[2], 'score': 1.5,
                        'date': r[3] or '', 'category': r[4] or '',
                        'drafter': r[5] or '', 'keywords': r[6] or ''
                    } for r in cur.fetchall()]
                    conn.close()
                    return results
                conn.close()
            except Exception as e:
                logger.error(f"Drafter search via everything_index failed: {e}")

        return []

    def search_by_drafter_and_year(self, drafter_name: str, year: str, top_k: int = 20) -> List[Dict[str, Any]]:
        """
        ê¸°ì•ˆì + ì—°ë„ ì¡°í•© ê²€ìƒ‰ - ë©”íƒ€ë°ì´í„° ê¸°ë°˜

        Args:
            drafter_name: ê¸°ì•ˆì ì´ë¦„
            year: ì—°ë„ (ì˜ˆ: '2021', '2022')
            top_k: ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜

        Returns:
            í•´ë‹¹ ê¸°ì•ˆìì˜ í•´ë‹¹ ì—°ë„ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.metadata_db:
            return []

        try:
            import sqlite3
            conn = sqlite3.connect("metadata.db")
            conn.row_factory = sqlite3.Row

            # ì—°ë„ ì¡°ê±´: year ì»¬ëŸ¼ ë˜ëŠ” date ì»¬ëŸ¼ì—ì„œ ì¶”ì¶œ
            cur = conn.execute("""
                SELECT *
                FROM documents
                WHERE drafter LIKE ?
                  AND (year = ? OR year = CAST(? AS INTEGER) OR date LIKE ?)
                ORDER BY COALESCE(display_date, date) DESC
                LIMIT ?
            """, (f"%{drafter_name}%", year, year, f"{year}%", top_k))

            rows = [dict(r) for r in cur.fetchall()]
            conn.close()

            return [{
                'id': r['id'],
                'filename': r['filename'],
                'path': r['path'],
                'score': 3.0,  # ë©”íƒ€ë°ì´í„° 2ê°œ ì¼ì¹˜ (ë†’ì€ ê°€ì¤‘ì¹˜)
                'date': r.get('display_date') or r.get('date') or '',
                'category': r.get('category') or '',
                'drafter': r.get('drafter') or '',
                'keywords': r.get('keywords') or '',
                'year': r.get('year') or ''
            } for r in rows]

        except Exception as e:
            logger.error(f"Drafter+Year search failed: {e}")
            return []

    def search_by_content(self, query: str, top_k: int = 20) -> List[Dict[str, Any]]:
        """
        ë‚´ìš© ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ - Everything-like ì´ˆê³ ì† ê²€ìƒ‰ ì‚¬ìš©

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # 0) ì¿¼ë¦¬ ì •ê·œí™”
        q = (query or '').strip()
        if not q:
            return []

        # 1) Everything ìš°ì„ 
        if self.everything_search:
            try:
                raw = self.everything_search.search(q, limit=top_k)
                return self._enrich_results(raw, preview_mode='lite', limit=top_k)
            except Exception as e:
                logger.error(f"Everything search failed: {e}")

        # 2) í´ë°±: ë©”íƒ€DB FTS5 (bm25)
        if self.metadata_db:
            try:
                fts_hits = self.metadata_db.search_by_keyword(q)
                raw = [{
                    'id': r.get('id', 0),  # doc_id for deduplication
                    'filename': r.get('filename',''),
                    'path': r.get('path',''),
                    'score': r.get('score', 1.0) if 'score' in r else 1.0,
                    'date': r.get('display_date') or r.get('date') or '',
                    'category': r.get('category') or '',
                    'keywords': r.get('keywords') or '',
                    'department': r.get('drafter') or ''
                } for r in fts_hits][:top_k]
                return self._enrich_results(raw, preview_mode='lite', limit=top_k)
            except Exception as e:
                logger.error(f"FTS fallback failed: {e}")

        # 3) ìµœí›„ í´ë°±: íŒŒì¼ëª… ë³‘ë ¬ ë§¤ì¹­
        return self._legacy_search(q, top_k)

    def _legacy_search(self, query: str, top_k: int = 20) -> List[Dict[str, Any]]:
        """ë ˆê±°ì‹œ ê²€ìƒ‰ (Everything ì‚¬ìš© ë¶ˆê°€ì‹œ í´ë°±)"""
        pdf_files = list(self.docs_dir.rglob("*.pdf"))
        if not pdf_files:
            logger.warning("No PDF files found in docs directory")
            return []

        # ë³‘ë ¬ ê²€ìƒ‰
        results = self._parallel_search_pdfs(pdf_files, query, top_k)
        return results[:top_k]

    def _parallel_search_pdfs(self, pdf_files: List[Path], query: str, top_k: int = 5) -> List[Dict]:
        """PDF íŒŒì¼ë“¤ì„ ë³‘ë ¬ë¡œ ê²€ìƒ‰"""
        results = []

        def search_single_pdf(pdf_path):
            try:
                # ê°„ë‹¨í•œ íŒŒì¼ëª… ë§¤ì¹­
                filename_lower = pdf_path.name.lower()
                query_lower = query.lower()

                # íŒŒì¼ëª…ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
                score = 0
                for keyword in query_lower.split():
                    if keyword in filename_lower:
                        score += 1

                if score > 0:
                    return {
                        'filename': pdf_path.name,
                        'path': str(pdf_path),
                        'score': score,
                        'date': self._extract_date_from_filename(pdf_path.name),
                        'category': self._extract_category_from_path(pdf_path)
                    }
            except Exception as e:
                logger.debug(f"Error searching {pdf_path}: {e}")

            return None

        # ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(search_single_pdf, pdf): pdf for pdf in pdf_files}

            for future in as_completed(futures):
                result = future.result()
                if result:
                    results.append(result)

        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        results.sort(key=lambda x: x['score'], reverse=True)
        return results

    def find_best_document(self, query: str) -> Optional[Path]:
        """ì¿¼ë¦¬ì— ê°€ì¥ ì í•©í•œ ë‹¨ì¼ ë¬¸ì„œ ì°¾ê¸°"""
        results = self.search_by_content(query, top_k=1)
        if results:
            return Path(results[0]['path'])
        return None

    def search_multiple_documents(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ë¬¸ì„œ ë™ì‹œ ê²€ìƒ‰ ë° í†µí•© ê²°ê³¼ ë°˜í™˜"""
        results = self.search_by_content(query, top_k=top_k)

        # ê° ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ
        for result in results:
            try:
                pdf_path = Path(result['path'])
                if pdf_path.exists():
                    with pdfplumber.open(pdf_path) as pdf:
                        # ì²« í˜ì´ì§€ ë‚´ìš© ì¶”ê°€
                        if pdf.pages:
                            text = pdf.pages[0].extract_text() or ""
                            result['preview'] = text[:500]  # ë¯¸ë¦¬ë³´ê¸°
            except Exception as e:
                logger.debug(f"Error extracting preview: {e}")
                result['preview'] = ""

        return results

    def search_by_date_range(self, start_date: str, end_date: str) -> List[Dict[str, Any]]:
        """ë‚ ì§œ ë²”ìœ„ë¡œ ë¬¸ì„œ ê²€ìƒ‰"""
        all_files = list(self.docs_dir.rglob("*.pdf"))
        results = []

        for pdf_path in all_files:
            date = self._extract_date_from_filename(pdf_path.name)
            if date and start_date <= date <= end_date:
                results.append({
                    'filename': pdf_path.name,
                    'path': str(pdf_path),
                    'date': date
                })

        results.sort(key=lambda x: x['date'])
        return results

    def search_by_category(self, category: str) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ê²€ìƒ‰"""
        category_path = self.docs_dir / f"category_{category.lower()}"
        if not category_path.exists():
            return []

        pdf_files = list(category_path.rglob("*.pdf"))
        return [
            {
                'filename': pdf.name,
                'path': str(pdf),
                'category': category
            }
            for pdf in pdf_files
        ]

    def get_search_statistics(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ í†µê³„ ë°˜í™˜"""
        stats = {'total_documents': 0, 'categories': 0, 'years': 0, 'cache_size': len(self.search_cache)}
        try:
            if self.docs_dir.exists():
                stats['total_documents'] = len(list(self.docs_dir.rglob("*.pdf")))
                stats['categories'] = sum(1 for d in self.docs_dir.iterdir() if d.is_dir() and d.name.startswith('category_'))
                stats['years'] = sum(1 for d in self.docs_dir.iterdir() if d.is_dir() and d.name.startswith('year_'))
        except Exception as e:
            logger.debug(f"Stats dir scan skipped: {e}")

        if self.metadata_db:
            try:
                # ìš°ì„ ìˆœìœ„ 1: ë©”íƒ€DBì˜ ì´ ë¬¸ì„œ ìˆ˜
                stats_db = self.metadata_db.get_statistics()
                stats['indexed_documents'] = stats_db.get('total_documents', 0)
            except Exception:
                # ëŒ€ì²´: ê³ ìœ  ë¬¸ì„œ ì¹´ìš´íŠ¸
                stats['indexed_documents'] = getattr(self.metadata_db, 'count_unique_documents', lambda: 0)()

        return stats

    def _enrich_results(self, docs: List[Dict[str, Any]], preview_mode: str = 'lite', limit: int = 20) -> List[Dict[str, Any]]:
        """
        ê²°ê³¼ í›„ì²˜ë¦¬:
          - preview_mode='lite' : ì²« í˜ì´ì§€ 800~1200ìë§Œ, í‘œ íŒŒì‹±/ OCR ë¯¸ìˆ˜í–‰
          - preview_mode='full' : í‘œâ†’MD ë³€í™˜ + OCR ìºì‹œ(ëŠë¦¼)
        """
        out = []
        for doc in docs[:limit]:
            res = dict(doc)
            pdf_path = Path(doc.get('path',''))
            # ì•ˆì „ ê°€ë“œ
            if not (pdf_path.suffix.lower() == '.pdf' and pdf_path.exists()):
                res['content'] = ''
                out.append(res)
                continue

            try:
                with pdfplumber.open(pdf_path) as pdf:
                    if not pdf.pages:
                        res['content'] = ''
                        out.append(res)
                        continue

                    if preview_mode == 'lite':
                        txt = (pdf.pages[0].extract_text() or '')[:1200]
                        res['content'] = txt
                    else:
                        full_text = ''
                        for page in pdf.pages[:5]:
                            page_txt = page.extract_text() or ''
                            full_text += page_txt + '\n\n'

                            # í‘œ ì¶”ì¶œ ë° ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë³€í™˜
                            tables = page.extract_tables()
                            if tables:
                                for table in tables:
                                    table_md = self._format_table_as_markdown(table)
                                    if table_md:
                                        full_text += "\nğŸ“Š **í‘œ ë°ì´í„°**\n" + table_md + "\n\n"

                            if len(full_text) > 15000:
                                break

                        # OCR í´ë°± (full ëª¨ë“œì—ì„œë§Œ)
                        text_lines = [line for line in full_text.split('\n') if line.strip()]
                        is_mostly_headers = len(text_lines) < 10 or full_text.count('gw.channela-mt.com') > 2

                        if len(full_text.strip()) < 500 or is_mostly_headers:
                            ocr_text = self._get_ocr_text(pdf_path)
                            if ocr_text and len(ocr_text) > len(full_text):
                                full_text = ocr_text
                                logger.info(f"ğŸ“· OCR ìºì‹œ ì‚¬ìš©: {pdf_path.name} ({len(ocr_text)}ì)")

                        res['content'] = full_text

                    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì²« í˜ì´ì§€ ê¸°ì¤€)
                    if self.metadata_extractor and pdf.pages:
                        first_page_text = pdf.pages[0].extract_text() or ""
                        metadata = self.metadata_extractor.extract_all(
                            first_page_text[:2000],
                            doc.get('filename', '')
                        )

                        # ì¶”ì¶œëœ ì •ë³´ ì¶”ê°€
                        summary = metadata.get('summary', {})
                        if summary.get('date'):
                            res['extracted_date'] = summary['date']
                        if summary.get('amount'):
                            res['extracted_amount'] = summary['amount']
                        if summary.get('department'):
                            res['extracted_dept'] = summary['department']
                        if summary.get('doc_type'):
                            res['extracted_type'] = summary['doc_type']
                        if summary.get('drafter'):
                            res['drafter'] = summary['drafter']

            except Exception as e:
                logger.debug(f"Preview extract failed: {pdf_path.name} - {e}")
                res['content'] = ''

            out.append(res)
        return out

    # í‘œ í˜•ì‹ ë³€í™˜ ë©”ì„œë“œ
    def _format_table_as_markdown(self, table):
        """í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not table or not table[0]:
            return ""

        lines = []

        # í—¤ë” (ì²« ë²ˆì§¸ í–‰)
        header = " | ".join([str(cell or '').strip() for cell in table[0]])
        lines.append(header)

        # êµ¬ë¶„ì„ 
        separator = " | ".join(["---"] * len(table[0]))
        lines.append(separator)

        # ë°ì´í„° í–‰
        for row in table[1:]:
            row_text = " | ".join([str(cell or '').strip() for cell in row])
            lines.append(row_text)

        return "\n".join(lines)

    # OCR ìºì‹œ ê´€ë ¨ ë©”ì„œë“œ
    def _load_ocr_cache(self):
        """OCR ìºì‹œ ë¡œë“œ"""
        ocr_cache_path = self.docs_dir / ".ocr_cache.json"
        if ocr_cache_path.exists():
            try:
                with open(ocr_cache_path, 'r', encoding='utf-8') as f:
                    self.ocr_cache = json.load(f)
                logger.info(f"âœ… OCR ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.ocr_cache)}ê°œ ë¬¸ì„œ")
            except Exception as e:
                logger.warning(f"OCR ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.ocr_cache = {}
        else:
            logger.debug("OCR ìºì‹œ íŒŒì¼ ì—†ìŒ")

    def _fast_md5(self, pdf_path: Path, chunk: int = 2*1024*1024) -> str:
        """ë¹ ë¥¸ MD5 í•´ì‹œ ê³„ì‚° (ìƒ˜í”Œ ê¸°ë°˜)"""
        import hashlib
        size = pdf_path.stat().st_size
        h = hashlib.md5()
        with open(pdf_path, 'rb') as f:
            h.update(f.read(chunk))
            if size > chunk:
                f.seek(max(0, size - chunk))
                h.update(f.read(chunk))
        h.update(str((size, pdf_path.stat().st_mtime_ns)).encode())
        return h.hexdigest()

    def _get_ocr_text(self, pdf_path: Path) -> str:
        """PDFì—ì„œ OCR í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ìš°ì„ )"""
        try:
            # ë¹ ë¥¸ íŒŒì¼ í•´ì‹œ ê³„ì‚° (ìƒ˜í”Œ ê¸°ë°˜)
            file_hash = self._fast_md5(pdf_path)

            # ìºì‹œì—ì„œ ì°¾ê¸°
            if file_hash in self.ocr_cache:
                cached_data = self.ocr_cache[file_hash]
                return cached_data.get('text', '')

            return ""
        except Exception as e:
            logger.debug(f"OCR í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return ""

    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def _extract_date_from_filename(self, filename: str) -> Optional[str]:
        """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        # 2024-08-13 í˜•ì‹
        date_pattern = r'(\d{4}-\d{2}-\d{2})'
        match = re.search(date_pattern, filename)
        if match:
            return match.group(1)

        # 2024_08_13 í˜•ì‹
        date_pattern2 = r'(\d{4}_\d{2}_\d{2})'
        match = re.search(date_pattern2, filename)
        if match:
            return match.group(1).replace('_', '-')

        return None

    def _extract_category_from_path(self, path: Path) -> str:
        """ê²½ë¡œì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        parts = path.parts
        for part in parts:
            if part.startswith('category_'):
                return part.replace('category_', '')
        return 'general'

    def clear_cache(self):
        """ê²€ìƒ‰ ìºì‹œ ì´ˆê¸°í™”"""
        self.search_cache.clear()
        logger.info("Search cache cleared")