#!/usr/bin/env python3
"""
ë‹¤ì¤‘ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í†µí•© ê²€ìƒ‰í•˜ê³  ì¢…í•© ë‹µë³€ ìƒì„±
"""

import re
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict
import logging

# RAG ëª¨ë“ˆ ì„í¬íŠ¸
from rag_system.bm25_store import BM25Store
from rag_system.korean_vector_store import KoreanVectorStore
from rag_system.hybrid_search import HybridSearch
from rag_system.korean_reranker import KoreanReranker
from rag_system.qwen_llm import QwenLLM
from rag_system.llm_singleton import LLMSingleton

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class MultiDocumentSearch:
    """ë‹¤ì¤‘ ë¬¸ì„œ ê²€ìƒ‰ ë° í†µí•© ë‹µë³€ ìƒì„±"""

    def __init__(self):
        self.docs_dir = Path("docs")
        self.index_dir = Path("indexes")
        self.index_dir.mkdir(exist_ok=True)

        # RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.bm25 = BM25Store()
        self.vector_store = KoreanVectorStore()
        self.hybrid_search = HybridSearch()
        self.reranker = KoreanReranker()

        # LLM ì´ˆê¸°í™” - model_path í•„ìš”
        import os
        model_path = os.getenv('MODEL_PATH', 'models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf')
        self.llm = LLMSingleton.get_instance(model_path=model_path)

        # ë¬¸ì„œ ì²­í¬ ìºì‹œ
        self.chunks_cache = {}

        logger.info("MultiDocumentSearch ì´ˆê¸°í™” ì™„ë£Œ")

    def search_multiple_docs(self, query: str, top_k: int = 5) -> List[Dict]:
        """ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""

        # ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜
        query_type = self._classify_query_type(query)

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
        results = self.hybrid_search.search(
            query=query,
            top_k=top_k * 2,  # Rerankingì„ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
            search_type=query_type
        )

        # Reranking
        if results:
            results = self.reranker.rerank(results, query, top_k)

        return results

    def aggregate_answer(self, query: str, documents: List[Dict]) -> str:
        """ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ì¢…í•©í•œ ë‹µë³€ ìƒì„±"""

        query_type = self._classify_query_type(query)

        if query_type == "aggregation":
            return self._aggregate_total(query, documents)
        elif query_type == "similarity":
            return self._find_similar_cases(query, documents)
        elif query_type == "timeline":
            return self._create_timeline(query, documents)
        elif query_type == "comparison":
            return self._compare_documents(query, documents)
        else:
            return self._generate_comprehensive_answer(query, documents)

    def _classify_query_type(self, query: str) -> str:
        """ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜"""
        query_lower = query.lower()

        if any(word in query_lower for word in ["ì´ì•¡", "í•©ê³„", "ì´", "ëª¨ë“ "]):
            return "aggregation"
        elif any(word in query_lower for word in ["ë¹„ìŠ·í•œ", "ìœ ì‚¬í•œ", "ê°™ì€"]):
            return "similarity"
        elif any(word in query_lower for word in ["ì—°ë„ë³„", "ì›”ë³„", "ê¸°ê°„ë³„", "ì¶”ì´"]):
            return "timeline"
        elif any(word in query_lower for word in ["ë¹„êµ", "ì°¨ì´", "ëŒ€ë¹„"]):
            return "comparison"
        else:
            return "general"

    def _aggregate_total(self, query: str, documents: List[Dict]) -> str:
        """ê¸ˆì•¡ ì´í•© ê³„ì‚°"""
        total = 0
        details = []

        for doc in documents:
            # ê¸ˆì•¡ ì¶”ì¶œ
            amounts = self._extract_amounts(doc['content'])
            if amounts:
                doc_total = sum(amounts)
                total += doc_total
                details.append({
                    'source': doc['metadata']['source'],
                    'amount': doc_total
                })

        # ì‘ë‹µ ìƒì„±
        response = f"ğŸ“Š **ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ ë¬¸ì„œ**\n\n"

        for detail in details:
            response += f"â€¢ {detail['source']}: {detail['amount']:,}ì›\n"

        response += f"\nğŸ’° **ì´ì•¡: {total:,}ì›**"

        return response

    def _find_similar_cases(self, query: str, documents: List[Dict]) -> str:
        """ìœ ì‚¬ ì‚¬ë¡€ ì°¾ê¸°"""
        cases = []

        for doc in documents:
            # ì£¼ìš” ì •ë³´ ì¶”ì¶œ
            case_info = {
                'source': doc['metadata']['source'],
                'content': doc['content'][:500],  # ìš”ì•½
                'score': doc.get('score', 0)
            }
            cases.append(case_info)

        # ì‘ë‹µ ìƒì„±
        response = f"ğŸ” **ìœ ì‚¬ ì‚¬ë¡€ {len(cases)}ê±´ ë°œê²¬**\n\n"

        for i, case in enumerate(cases, 1):
            response += f"**[ì‚¬ë¡€ {i}]** {case['source']}\n"
            response += f"{case['content']}...\n"
            response += f"ìœ ì‚¬ë„: {case['score']:.2f}\n\n"

        return response

    def _create_timeline(self, query: str, documents: List[Dict]) -> str:
        """ì‹œê°„ìˆœ ì •ë ¬ ë° í‘œì‹œ"""
        timeline = defaultdict(list)

        for doc in documents:
            # ë‚ ì§œ ì¶”ì¶œ
            date = self._extract_date(doc['metadata']['source'])
            if date:
                year = date[:4]
                timeline[year].append({
                    'date': date,
                    'source': doc['metadata']['source'],
                    'summary': doc['content'][:200]
                })

        # ì‘ë‹µ ìƒì„±
        response = "ğŸ“… **ì‹œê°„ìˆœ ì •ë¦¬**\n\n"

        for year in sorted(timeline.keys()):
            response += f"**{year}ë…„**\n"
            for item in sorted(timeline[year], key=lambda x: x['date']):
                response += f"â€¢ {item['date']}: {item['source']}\n"
                response += f"  {item['summary']}...\n\n"

        return response

    def _compare_documents(self, query: str, documents: List[Dict]) -> str:
        """ë¬¸ì„œ ë¹„êµ"""
        comparison = []

        for doc in documents:
            # ì£¼ìš” í•­ëª© ì¶”ì¶œ
            comparison.append({
                'source': doc['metadata']['source'],
                'key_points': self._extract_key_points(doc['content'])
            })

        # LLMì„ ì‚¬ìš©í•œ ë¹„êµ ë¶„ì„
        prompt = f"""
        ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”:

        {json.dumps(comparison, ensure_ascii=False, indent=2)}

        ì§ˆë¬¸: {query}

        ì£¼ìš” ì°¨ì´ì ê³¼ ê³µí†µì ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
        """

        response = self.llm.generate_response(prompt)
        return response.answer if hasattr(response, 'answer') else str(response)

    def _generate_comprehensive_answer(self, query: str, documents: List[Dict]) -> str:
        """ì¢…í•©ì ì¸ ë‹µë³€ ìƒì„±"""

        # ëª¨ë“  ë¬¸ì„œ ë‚´ìš© í†µí•©
        combined_context = "\n\n---\n\n".join([
            f"[{doc['metadata']['source']}]\n{doc['content']}"
            for doc in documents
        ])

        # LLM í”„ë¡¬í”„íŠ¸
        prompt = f"""
        ë‹¤ìŒ {len(documents)}ê°œ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

        ë¬¸ì„œ ë‚´ìš©:
        {combined_context[:10000]}  # í† í° ì œí•œ

        ì§ˆë¬¸: {query}

        ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ê° ì •ë³´ì˜ ì¶œì²˜ë„ ëª…ì‹œí•´ì£¼ì„¸ìš”.
        """

        response = self.llm.generate_response(prompt)
        answer = response.answer if hasattr(response, 'answer') else str(response)

        # ì¶œì²˜ ì¶”ê°€
        answer += "\n\nğŸ“š **ì°¸ê³  ë¬¸ì„œ:**\n"
        for doc in documents:
            answer += f"â€¢ {doc['metadata']['source']}\n"

        return answer

    def _extract_amounts(self, text: str) -> List[int]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê¸ˆì•¡ ì¶”ì¶œ"""
        amounts = []

        # ê¸ˆì•¡ íŒ¨í„´ ë§¤ì¹­
        patterns = [
            r'(\d{1,3}(?:,\d{3})*(?:\.\d+)?)\s*ì›',
            r'(\d+)\s*ì–µ\s*(?:(\d+)\s*ì²œ)?(?:\s*ë§Œ)?(?:\s*ì›)?',
            r'(\d+)\s*ì²œ\s*(?:ë§Œ)?(?:\s*ì›)?',
            r'(\d+)\s*ë§Œ\s*ì›'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    if isinstance(match, tuple):
                        # ì–µ ë‹¨ìœ„ ì²˜ë¦¬
                        if 'ì–µ' in pattern:
                            amount = int(match[0]) * 100000000
                            if len(match) > 1 and match[1]:
                                amount += int(match[1]) * 10000000
                        # ì²œë§Œ ë‹¨ìœ„ ì²˜ë¦¬
                        elif 'ì²œ' in pattern and 'ë§Œ' in pattern:
                            amount = int(match[0]) * 10000000
                        # ë§Œ ë‹¨ìœ„ ì²˜ë¦¬
                        elif 'ë§Œ' in pattern:
                            amount = int(match[0]) * 10000
                        else:
                            amount = int(match[0].replace(',', ''))
                    else:
                        amount = int(match.replace(',', ''))

                    if amount > 0:
                        amounts.append(amount)
                except (ValueError, AttributeError):
                    continue

        return amounts

    def _extract_date(self, filename: str) -> Optional[str]:
        """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        date_match = re.search(r'(\d{4})-(\d{2})-(\d{2})', filename)
        if date_match:
            return date_match.group(0)

        year_match = re.search(r'(20\d{2})', filename)
        if year_match:
            return year_match.group(1)

        return None

    def _extract_key_points(self, text: str) -> List[str]:
        """í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        points = []

        # ë¶ˆë¦¿ í¬ì¸íŠ¸ ì°¾ê¸°
        bullet_patterns = [
            r'[â€¢Â·â–ªâ–«â—¦â€£âƒ]\s*(.+)',
            r'\d+\.\s*(.+)',
            r'-\s*(.+)'
        ]

        for pattern in bullet_patterns:
            matches = re.findall(pattern, text)
            points.extend(matches[:5])  # ìƒìœ„ 5ê°œë§Œ

        # ì—†ìœ¼ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¶”ì¶œ
        if not points:
            sentences = text.split('.')[:5]
            points = [s.strip() for s in sentences if len(s.strip()) > 20]

        return points