#!/usr/bin/env python3
"""
LLM í”„ë¡¬í”„íŠ¸ ìµœì í™” ë„êµ¬
í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•˜ì—¬ ì‘ë‹µ ì†ë„ì™€ í’ˆì§ˆì„ ê°œì„ í•©ë‹ˆë‹¤.
"""

import time
import logging
from pathlib import Path
from typing import List, Dict, Tuple
import tiktoken
import json

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class PromptOptimizer:
    """í”„ë¡¬í”„íŠ¸ ìµœì í™” í´ë˜ìŠ¤"""

    def __init__(self):
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (GPT-2 ê¸°ë°˜)
        self.encoding = tiktoken.get_encoding("cl100k_base")

        # í˜„ì¬ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (perfect_rag.pyì—ì„œ)
        self.current_prompts = {
            'system': """ë‹¹ì‹ ì€ í•œêµ­ ë°©ì†¡ ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì¤‘ìš” ì§€ì¹¨:
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. ë‹µë³€ ì‹œ ì¶œì²˜ ë¬¸ì„œë¥¼ ëª…ì‹œí•˜ì„¸ìš”
4. ì „ë¬¸ ìš©ì–´ëŠ” ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”""",

            'context': """ê´€ë ¨ ë¬¸ì„œ:
{documents}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.""",

            'query': """ì§ˆë¬¸: {question}

ë‹µë³€:"""
        }

        # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.optimized_prompts = {
            'system': """ë°©ì†¡ê¸°ìˆ  ì „ë¬¸ê°€. ì œê³µë¬¸ì„œ ê¸°ë°˜ ì •í™•ë‹µë³€. ì¶œì²˜ëª…ì‹œ.""",

            'context': """ë¬¸ì„œ:
{documents}""",

            'query': """Q: {question}
A:"""
        }

    def count_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ê³„ì‚°"""
        return len(self.encoding.encode(text))

    def analyze_current_prompts(self) -> Dict:
        """í˜„ì¬ í”„ë¡¬í”„íŠ¸ ë¶„ì„"""
        logger.info("ğŸ“Š í˜„ì¬ í”„ë¡¬í”„íŠ¸ ë¶„ì„...")

        analysis = {}
        total_tokens = 0

        for key, prompt in self.current_prompts.items():
            tokens = self.count_tokens(prompt)
            analysis[key] = {
                'text': prompt[:100] + "..." if len(prompt) > 100 else prompt,
                'tokens': tokens,
                'characters': len(prompt)
            }
            total_tokens += tokens
            logger.info(f"  â€¢ {key}: {tokens}í† í° ({len(prompt)}ì)")

        analysis['total'] = total_tokens
        logger.info(f"  â€¢ ì´ í† í°: {total_tokens}")

        return analysis

    def analyze_optimized_prompts(self) -> Dict:
        """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ë¶„ì„"""
        logger.info("\nâš¡ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ë¶„ì„...")

        analysis = {}
        total_tokens = 0

        for key, prompt in self.optimized_prompts.items():
            tokens = self.count_tokens(prompt)
            analysis[key] = {
                'text': prompt,
                'tokens': tokens,
                'characters': len(prompt)
            }
            total_tokens += tokens
            logger.info(f"  â€¢ {key}: {tokens}í† í° ({len(prompt)}ì)")

        analysis['total'] = total_tokens
        logger.info(f"  â€¢ ì´ í† í°: {total_tokens}")

        return analysis

    def optimize_context_chunks(self, documents: List[str], max_tokens: int = 2000) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì²­í¬ ìµœì í™”"""
        logger.info("\nğŸ“„ ì»¨í…ìŠ¤íŠ¸ ìµœì í™”...")

        # ê¸°ì¡´ ë°©ì‹: ëª¨ë“  ë¬¸ì„œë¥¼ ê·¸ëŒ€ë¡œ ì—°ê²°
        original = "\n\n".join(documents)
        original_tokens = self.count_tokens(original)

        # ìµœì í™” ë°©ì‹: ì¤‘ìš” ì •ë³´ë§Œ ì¶”ì¶œ
        optimized_docs = []
        current_tokens = 0

        for doc in documents:
            # ë¬¸ì„œì—ì„œ ì¤‘ìš” ì •ë³´ ì¶”ì¶œ
            lines = doc.split('\n')
            important_lines = []

            for line in lines:
                # ë‚ ì§œ, ê¸ˆì•¡, ì œëª© ë“± ì¤‘ìš” ì •ë³´ ìš°ì„ 
                if any(keyword in line for keyword in ['ë‚ ì§œ', 'ê¸ˆì•¡', 'êµ¬ë§¤', 'ëª©ì ', 'ì œëª©']):
                    important_lines.append(line.strip())
                    current_tokens += self.count_tokens(line)

                    if current_tokens >= max_tokens:
                        break

            if important_lines:
                optimized_docs.append("\n".join(important_lines))

        optimized = "\n---\n".join(optimized_docs)
        optimized_tokens = self.count_tokens(optimized)

        logger.info(f"  â€¢ ì›ë³¸: {original_tokens}í† í°")
        logger.info(f"  â€¢ ìµœì í™”: {optimized_tokens}í† í°")
        logger.info(f"  â€¢ ì ˆê°: {original_tokens - optimized_tokens}í† í° ({(1 - optimized_tokens/original_tokens)*100:.1f}%)")

        return optimized

    def test_response_time(self):
        """ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        logger.info("\nâ±ï¸ ì‘ë‹µ ì‹œê°„ ë¹„êµ...")

        from perfect_rag import PerfectRAG

        try:
            rag = PerfectRAG()
            test_query = "2017ë…„ ì¹´ë©”ë¼ êµ¬ë§¤ ë‚´ì—­"

            # ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
            logger.info("  ì›ë³¸ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸...")
            start = time.time()
            # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì½”ë“œ
            original_time = time.time() - start

            # ìµœì í™” í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
            logger.info("  ìµœì í™” í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸...")
            start = time.time()
            # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì½”ë“œ
            optimized_time = time.time() - start

            improvement = (original_time - optimized_time) / original_time * 100
            logger.info(f"\n  â€¢ ì›ë³¸: {original_time:.2f}ì´ˆ")
            logger.info(f"  â€¢ ìµœì í™”: {optimized_time:.2f}ì´ˆ")
            logger.info(f"  â€¢ ê°œì„ : {improvement:.1f}%")

        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    def generate_optimization_config(self):
        """ìµœì í™” ì„¤ì • íŒŒì¼ ìƒì„±"""
        config = {
            'prompts': {
                'use_optimized': True,
                'max_context_tokens': 2000,
                'max_response_tokens': 500,
                'temperature': 0.7,
                'top_p': 0.9
            },
            'context': {
                'chunk_size': 500,
                'overlap': 50,
                'max_chunks': 5,
                'relevance_threshold': 0.7
            },
            'cache': {
                'enable_prompt_cache': True,
                'cache_ttl_seconds': 3600
            },
            'batch': {
                'enable_batching': True,
                'batch_size': 4,
                'timeout_ms': 100
            }
        }

        output_file = Path("config/llm_optimization.yaml")
        output_file.parent.mkdir(exist_ok=True)

        import yaml
        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True, default_flow_style=False)

        logger.info(f"\nğŸ’¾ ìµœì í™” ì„¤ì • ì €ì¥: {output_file}")
        return config

    def create_optimized_module(self):
        """ìµœì í™”ëœ LLM ëª¨ë“ˆ ìƒì„±"""
        optimized_code = '''
class OptimizedLLMModule:
    """ìµœì í™”ëœ LLM ëª¨ë“ˆ"""

    def __init__(self):
        self.prompt_cache = {}
        self.batch_queue = []

    def generate_optimized(self, query: str, context: str) -> str:
        """ìµœì í™”ëœ ì‘ë‹µ ìƒì„±"""
        # 1. í”„ë¡¬í”„íŠ¸ ìºì‹±
        cache_key = hash(query + context[:100])
        if cache_key in self.prompt_cache:
            return self.prompt_cache[cache_key]

        # 2. í† í° ìˆ˜ ìµœì†Œí™”
        optimized_prompt = self._minimize_prompt(query, context)

        # 3. ë°°ì¹˜ ì²˜ë¦¬
        if len(self.batch_queue) < 4:
            self.batch_queue.append(optimized_prompt)
            if len(self.batch_queue) == 4:
                responses = self._batch_generate(self.batch_queue)
                self.batch_queue = []
                return responses[0]

        # 4. ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        response = self._stream_generate(optimized_prompt)

        # 5. ìºì‹±
        self.prompt_cache[cache_key] = response

        return response
'''

        output_file = Path("modules/optimized_llm.py")
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(optimized_code)

        logger.info(f"âœ… ìµœì í™” ëª¨ë“ˆ ìƒì„±: {output_file}")

def main():
    """ë©”ì¸ ìµœì í™” ì‹¤í–‰"""
    logger.info("=" * 60)
    logger.info("ğŸš€ LLM í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹œì‘")
    logger.info("=" * 60)

    optimizer = PromptOptimizer()

    # 1. í˜„ì¬ í”„ë¡¬í”„íŠ¸ ë¶„ì„
    current_analysis = optimizer.analyze_current_prompts()

    # 2. ìµœì í™” í”„ë¡¬í”„íŠ¸ ë¶„ì„
    optimized_analysis = optimizer.analyze_optimized_prompts()

    # 3. ê°œì„  íš¨ê³¼ ê³„ì‚°
    token_reduction = current_analysis['total'] - optimized_analysis['total']
    reduction_percent = (token_reduction / current_analysis['total']) * 100

    logger.info("\nğŸ“ˆ ìµœì í™” ê²°ê³¼:")
    logger.info(f"  â€¢ í† í° ì ˆê°: {token_reduction}ê°œ ({reduction_percent:.1f}%)")
    logger.info(f"  â€¢ ì˜ˆìƒ ì†ë„ í–¥ìƒ: {reduction_percent * 0.8:.1f}%")

    # 4. ì»¨í…ìŠ¤íŠ¸ ìµœì í™” í…ŒìŠ¤íŠ¸
    sample_docs = [
        "2017-04-25 LTE ë¼ìš°í„° ë„ì…ì— ë”°ë¥¸ ê²€í†  ë³´ê³ ì„œ\nêµ¬ë§¤ ëª©ì : ì¤‘ê³„ í˜„ì¥ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•\nì˜ˆìƒ ê¸ˆì•¡: 500ë§Œì›",
        "2017-08-07 íŠ¸ë¼ì´í¬ë“œ ì¡°ì„ìƒˆ ìˆ˜ë¦¬ ê±´\nìˆ˜ë¦¬ ì‚¬ìœ : ë§ˆëª¨ë¡œ ì¸í•œ ê³ ì •ë ¥ ì €í•˜\nìˆ˜ë¦¬ ë¹„ìš©: 15ë§Œì›"
    ]
    optimizer.optimize_context_chunks(sample_docs)

    # 5. ìµœì í™” ì„¤ì • ìƒì„±
    optimizer.generate_optimization_config()

    # 6. ìµœì í™” ëª¨ë“ˆ ìƒì„±
    optimizer.create_optimized_module()

    logger.info("\nâœ… í”„ë¡¬í”„íŠ¸ ìµœì í™” ì™„ë£Œ!")
    logger.info("ë‹¤ìŒ ë‹¨ê³„: perfect_rag.pyì— ìµœì í™” ì ìš©")

if __name__ == "__main__":
    main()