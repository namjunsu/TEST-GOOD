#!/usr/bin/env python3
"""
ê³ ê¸‰ ë³‘ë ¬ ê²€ìƒ‰ ìµœì í™” ì‹œìŠ¤í…œ

ì£¼ìš” ê¸°ëŠ¥:
- ë©€í‹°ìŠ¤ë ˆë“œ/ë©€í‹°í”„ë¡œì„¸ìŠ¤/ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬
- ë™ì  ì›Œì»¤ í’€ ê´€ë¦¬
- ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‘ì—… ìŠ¤ì¼€ì¤„ë§
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ì¥ì•  ë³µêµ¬ ë° ì¬ì‹œë„
- ê²°ê³¼ ì§‘ê³„ ë° ë­í‚¹
"""

import os
import sys
import time
import asyncio
import hashlib
import threading
import multiprocessing
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable, Tuple, Generator, Union
from dataclasses import dataclass, field
from collections import deque, defaultdict
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, Future
from queue import Queue, PriorityQueue, Empty
from functools import partial, lru_cache
import logging
import pickle
import psutil
import warnings

# PDF ì²˜ë¦¬
try:
    import pdfplumber
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False
    warnings.warn("pdfplumber not installed - PDF support disabled")

# í…ìŠ¤íŠ¸ ê²€ìƒ‰
try:
    from rapidfuzz import fuzz
    FUZZY_SUPPORT = True
except ImportError:
    FUZZY_SUPPORT = False
    warnings.warn("rapidfuzz not installed - fuzzy search disabled")

@dataclass
class SearchTask:
    """ê²€ìƒ‰ ì‘ì—… ì •ì˜"""
    id: str
    file_path: Path
    query: str
    priority: int = 0
    timeout: float = 30.0
    retry_count: int = 0
    max_retries: int = 3
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __lt__(self, other):
        return self.priority > other.priority  # ë†’ì€ ìš°ì„ ìˆœìœ„ê°€ ë¨¼ì €

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    task_id: str
    file_path: Path
    relevance: float
    snippets: List[str]
    metadata: Dict[str, Any]
    processing_time: float
    success: bool = True
    error: Optional[str] = None

class WorkerPool:
    """ë™ì  ì›Œì»¤ í’€ ê´€ë¦¬"""

    def __init__(self, min_workers: int = 2, max_workers: int = 8,
                 pool_type: str = 'thread'):
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.pool_type = pool_type

        # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ì¡°ì •
        cpu_count = psutil.cpu_count()
        self.max_workers = min(self.max_workers, cpu_count * 2)

        # í˜„ì¬ ì›Œì»¤ ìˆ˜
        self.current_workers = min_workers

        # í’€ ìƒì„±
        self._create_pool()

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.tasks_submitted = 0
        self.tasks_completed = 0
        self.total_wait_time = 0

        self.logger = logging.getLogger(__name__)

    def _create_pool(self):
        """ì›Œì»¤ í’€ ìƒì„±"""
        if self.pool_type == 'thread':
            self.executor = ThreadPoolExecutor(max_workers=self.current_workers)
        elif self.pool_type == 'process':
            self.executor = ProcessPoolExecutor(max_workers=self.current_workers)
        else:
            raise ValueError(f"Unknown pool type: {self.pool_type}")

    def submit(self, fn: Callable, *args, **kwargs) -> Future:
        """ì‘ì—… ì œì¶œ"""
        self.tasks_submitted += 1
        future = self.executor.submit(fn, *args, **kwargs)

        # ë™ì  ìŠ¤ì¼€ì¼ë§ ì²´í¬
        self._check_scaling()

        return future

    def _check_scaling(self):
        """ë™ì  ìŠ¤ì¼€ì¼ë§ ì²´í¬"""
        # ëŒ€ê¸° ì¤‘ì¸ ì‘ì—… ìˆ˜
        pending = self.tasks_submitted - self.tasks_completed

        # ìŠ¤ì¼€ì¼ ì—…
        if pending > self.current_workers * 2 and self.current_workers < self.max_workers:
            self._scale_up()

        # ìŠ¤ì¼€ì¼ ë‹¤ìš´
        elif pending < self.current_workers // 2 and self.current_workers > self.min_workers:
            self._scale_down()

    def _scale_up(self):
        """ì›Œì»¤ ì¦ê°€"""
        new_workers = min(self.current_workers + 2, self.max_workers)
        if new_workers > self.current_workers:
            self.logger.info(f"Scaling up workers: {self.current_workers} -> {new_workers}")
            self.current_workers = new_workers
            # ìƒˆ í’€ë¡œ êµì²´
            old_executor = self.executor
            self._create_pool()
            old_executor.shutdown(wait=False)

    def _scale_down(self):
        """ì›Œì»¤ ê°ì†Œ"""
        new_workers = max(self.current_workers - 1, self.min_workers)
        if new_workers < self.current_workers:
            self.logger.info(f"Scaling down workers: {self.current_workers} -> {new_workers}")
            self.current_workers = new_workers

    def shutdown(self, wait: bool = True):
        """í’€ ì¢…ë£Œ"""
        self.executor.shutdown(wait=wait)

    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        return {
            'current_workers': self.current_workers,
            'tasks_submitted': self.tasks_submitted,
            'tasks_completed': self.tasks_completed,
            'pending_tasks': self.tasks_submitted - self.tasks_completed,
            'pool_type': self.pool_type
        }

class ParallelSearchEngine:
    """ë³‘ë ¬ ê²€ìƒ‰ ì—”ì§„"""

    def __init__(self, max_workers: int = 4, pool_type: str = 'thread'):
        self.worker_pool = WorkerPool(
            min_workers=2,
            max_workers=max_workers,
            pool_type=pool_type
        )

        # ì‘ì—… í
        self.task_queue = PriorityQueue()
        self.result_queue = Queue()

        # í™œì„± ì‘ì—… ì¶”ì 
        self.active_tasks = {}
        self.completed_tasks = {}

        # ìºì‹œ
        self.search_cache = {}

        # í†µê³„
        self.stats = {
            'total_searches': 0,
            'successful_searches': 0,
            'failed_searches': 0,
            'cache_hits': 0,
            'total_time': 0
        }

        self.logger = logging.getLogger(__name__)

    def search_files(self, file_paths: List[Path], query: str,
                    max_results: int = 10,
                    timeout: float = 30.0) -> List[SearchResult]:
        """íŒŒì¼ ëª©ë¡ ë³‘ë ¬ ê²€ìƒ‰"""
        start_time = time.time()

        # ì‘ì—… ìƒì„±
        tasks = []
        for i, file_path in enumerate(file_paths):
            task = SearchTask(
                id=f"task_{i}_{time.time()}",
                file_path=file_path,
                query=query,
                priority=self._calculate_priority(file_path),
                timeout=timeout
            )
            tasks.append(task)

        # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
        results = self._execute_parallel_search(tasks, max_results)

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['total_searches'] += len(tasks)
        self.stats['total_time'] += time.time() - start_time

        return results

    def _calculate_priority(self, file_path: Path) -> int:
        """íŒŒì¼ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        priority = 0

        # ìµœê·¼ ìˆ˜ì •ëœ íŒŒì¼ ìš°ì„ 
        stat = file_path.stat()
        days_old = (time.time() - stat.st_mtime) / 86400
        if days_old < 30:
            priority += 10
        elif days_old < 90:
            priority += 5

        # ì‘ì€ íŒŒì¼ ìš°ì„  (ë¹ ë¥¸ ì²˜ë¦¬)
        size_mb = stat.st_size / (1024 * 1024)
        if size_mb < 1:
            priority += 5
        elif size_mb < 5:
            priority += 3

        return priority

    def _execute_parallel_search(self, tasks: List[SearchTask],
                                max_results: int) -> List[SearchResult]:
        """ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰"""
        results = []
        futures = {}

        # ëª¨ë“  ì‘ì—… ì œì¶œ
        for task in tasks:
            # ìºì‹œ í™•ì¸
            cache_key = self._get_cache_key(task)
            if cache_key in self.search_cache:
                self.stats['cache_hits'] += 1
                cached_result = self.search_cache[cache_key]
                results.append(cached_result)
                continue

            # ë³‘ë ¬ ì‹¤í–‰
            future = self.worker_pool.submit(self._search_single_file, task)
            futures[future] = task

        # ê²°ê³¼ ìˆ˜ì§‘ (ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ)
        for future in as_completed(futures, timeout=30):
            task = futures[future]

            try:
                result = future.result(timeout=1)

                if result.success:
                    self.stats['successful_searches'] += 1
                    results.append(result)

                    # ìºì‹œ ì €ì¥
                    cache_key = self._get_cache_key(task)
                    self.search_cache[cache_key] = result

                    # ì¡°ê¸° ì¢…ë£Œ (ì¶©ë¶„í•œ ê²°ê³¼)
                    if len(results) >= max_results * 2:
                        break
                else:
                    self.stats['failed_searches'] += 1

                    # ì¬ì‹œë„
                    if task.retry_count < task.max_retries:
                        task.retry_count += 1
                        retry_future = self.worker_pool.submit(
                            self._search_single_file, task
                        )
                        futures[retry_future] = task

            except Exception as e:
                self.logger.error(f"Search failed for {task.file_path}: {e}")
                self.stats['failed_searches'] += 1

        # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda r: r.relevance, reverse=True)

        return results[:max_results]

    def _search_single_file(self, task: SearchTask) -> SearchResult:
        """ë‹¨ì¼ íŒŒì¼ ê²€ìƒ‰"""
        start_time = time.time()

        try:
            # íŒŒì¼ íƒ€ì…ë³„ ì²˜ë¦¬
            if task.file_path.suffix.lower() == '.pdf':
                content = self._extract_pdf_content(task.file_path)
            elif task.file_path.suffix.lower() in ['.txt', '.md']:
                content = self._extract_text_content(task.file_path)
            else:
                raise ValueError(f"Unsupported file type: {task.file_path.suffix}")

            # í…ìŠ¤íŠ¸ ê²€ìƒ‰
            relevance, snippets = self._search_content(content, task.query)

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            metadata = {
                'file_size': task.file_path.stat().st_size,
                'modified': task.file_path.stat().st_mtime,
                'extension': task.file_path.suffix
            }
            metadata.update(task.metadata)

            return SearchResult(
                task_id=task.id,
                file_path=task.file_path,
                relevance=relevance,
                snippets=snippets,
                metadata=metadata,
                processing_time=time.time() - start_time,
                success=True
            )

        except Exception as e:
            return SearchResult(
                task_id=task.id,
                file_path=task.file_path,
                relevance=0.0,
                snippets=[],
                metadata={},
                processing_time=time.time() - start_time,
                success=False,
                error=str(e)
            )

    def _extract_pdf_content(self, file_path: Path) -> str:
        """PDF ë‚´ìš© ì¶”ì¶œ"""
        if not PDF_SUPPORT:
            raise ImportError("PDF support not available")

        content = []
        with pdfplumber.open(file_path) as pdf:
            # ì²˜ìŒ 10í˜ì´ì§€ë§Œ
            for i, page in enumerate(pdf.pages[:10]):
                text = page.extract_text()
                if text:
                    content.append(text)

        return '\n'.join(content)

    def _extract_text_content(self, file_path: Path) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš© ì¶”ì¶œ"""
        encodings = ['utf-8', 'cp949', 'euc-kr']

        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read()
            except UnicodeDecodeError:
                continue

        # ì‹¤íŒ¨ì‹œ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê³  ë””ì½”ë“œ
        with open(file_path, 'rb') as f:
            return f.read().decode('utf-8', errors='ignore')

    def _search_content(self, content: str, query: str) -> Tuple[float, List[str]]:
        """ë‚´ìš©ì—ì„œ ì¿¼ë¦¬ ê²€ìƒ‰"""
        if not content:
            return 0.0, []

        content_lower = content.lower()
        query_lower = query.lower()

        # ì •í™•í•œ ë§¤ì¹­
        exact_matches = content_lower.count(query_lower)

        # í¼ì§€ ë§¤ì¹­
        if FUZZY_SUPPORT:
            fuzzy_score = fuzz.partial_ratio(query_lower, content_lower) / 100.0
        else:
            fuzzy_score = 0.5 if query_lower in content_lower else 0.0

        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        relevance = min(1.0, (exact_matches * 0.2) + (fuzzy_score * 0.8))

        # ìŠ¤ë‹ˆí« ì¶”ì¶œ
        snippets = self._extract_snippets(content, query, max_snippets=3)

        return relevance, snippets

    def _extract_snippets(self, content: str, query: str,
                         max_snippets: int = 3,
                         context_size: int = 100) -> List[str]:
        """ì¿¼ë¦¬ ì£¼ë³€ í…ìŠ¤íŠ¸ ìŠ¤ë‹ˆí« ì¶”ì¶œ"""
        snippets = []
        query_lower = query.lower()
        content_lower = content.lower()

        # ì¿¼ë¦¬ ìœ„ì¹˜ ì°¾ê¸°
        start = 0
        while len(snippets) < max_snippets:
            pos = content_lower.find(query_lower, start)
            if pos == -1:
                break

            # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            snippet_start = max(0, pos - context_size)
            snippet_end = min(len(content), pos + len(query) + context_size)

            snippet = content[snippet_start:snippet_end]
            snippet = f"...{snippet}..."

            snippets.append(snippet)
            start = pos + len(query)

        return snippets

    def _get_cache_key(self, task: SearchTask) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_str = f"{task.file_path}_{task.query}_{task.file_path.stat().st_mtime}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        stats = dict(self.stats)
        stats['worker_pool'] = self.worker_pool.get_stats()
        stats['cache_size'] = len(self.search_cache)

        if self.stats['total_searches'] > 0:
            stats['success_rate'] = self.stats['successful_searches'] / self.stats['total_searches']
            stats['cache_hit_rate'] = self.stats['cache_hits'] / self.stats['total_searches']
            stats['avg_time'] = self.stats['total_time'] / self.stats['total_searches']

        return stats

    def shutdown(self):
        """ì—”ì§„ ì¢…ë£Œ"""
        self.worker_pool.shutdown()
        self.logger.info("Parallel search engine shutdown")

class AsyncSearchEngine:
    """ë¹„ë™ê¸° ê²€ìƒ‰ ì—”ì§„"""

    def __init__(self, max_concurrent: int = 10):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.results = []
        self.logger = logging.getLogger(__name__)

    async def search_async(self, file_paths: List[Path], query: str,
                          max_results: int = 10) -> List[SearchResult]:
        """ë¹„ë™ê¸° íŒŒì¼ ê²€ìƒ‰"""
        tasks = []

        for i, file_path in enumerate(file_paths):
            task = self._search_file_async(file_path, query, f"async_{i}")
            tasks.append(task)

        # ë™ì‹œ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
        valid_results = [r for r in results if isinstance(r, SearchResult) and r.success]

        # ê´€ë ¨ì„± ìˆœ ì •ë ¬
        valid_results.sort(key=lambda r: r.relevance, reverse=True)

        return valid_results[:max_results]

    async def _search_file_async(self, file_path: Path, query: str,
                                task_id: str) -> SearchResult:
        """ë¹„ë™ê¸° ë‹¨ì¼ íŒŒì¼ ê²€ìƒ‰"""
        async with self.semaphore:  # ë™ì‹œ ì‹¤í–‰ ì œí•œ
            start_time = time.time()

            try:
                # I/O ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                content = await self._read_file_async(file_path)

                # CPU ì§‘ì•½ì  ì‘ì—…ì€ executorë¡œ
                loop = asyncio.get_event_loop()
                relevance, snippets = await loop.run_in_executor(
                    None,
                    self._search_content_sync,
                    content,
                    query
                )

                return SearchResult(
                    task_id=task_id,
                    file_path=file_path,
                    relevance=relevance,
                    snippets=snippets,
                    metadata={},
                    processing_time=time.time() - start_time,
                    success=True
                )

            except Exception as e:
                return SearchResult(
                    task_id=task_id,
                    file_path=file_path,
                    relevance=0.0,
                    snippets=[],
                    metadata={},
                    processing_time=time.time() - start_time,
                    success=False,
                    error=str(e)
                )

    async def _read_file_async(self, file_path: Path) -> str:
        """ë¹„ë™ê¸° íŒŒì¼ ì½ê¸°"""
        loop = asyncio.get_event_loop()

        def read_file():
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()

        return await loop.run_in_executor(None, read_file)

    def _search_content_sync(self, content: str, query: str) -> Tuple[float, List[str]]:
        """ë™ê¸° ë°©ì‹ ë‚´ìš© ê²€ìƒ‰"""
        if not content:
            return 0.0, []

        # ê°„ë‹¨í•œ ê²€ìƒ‰ ë¡œì§
        query_lower = query.lower()
        content_lower = content.lower()

        count = content_lower.count(query_lower)
        relevance = min(1.0, count * 0.1)

        # ìŠ¤ë‹ˆí« ì¶”ì¶œ
        snippets = []
        pos = content_lower.find(query_lower)
        if pos != -1:
            start = max(0, pos - 50)
            end = min(len(content), pos + len(query) + 50)
            snippets.append(f"...{content[start:end]}...")

        return relevance, snippets

class StreamingSearchEngine:
    """ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ ì—”ì§„ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì """

    def __init__(self, chunk_size: int = 1024 * 1024):  # 1MB chunks
        self.chunk_size = chunk_size
        self.logger = logging.getLogger(__name__)

    def search_stream(self, file_paths: List[Path], query: str,
                      callback: Optional[Callable] = None) -> Generator[SearchResult, None, None]:
        """ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜"""

        for i, file_path in enumerate(file_paths):
            try:
                # íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ìœ¼ë©´ì„œ ê²€ìƒ‰
                relevance = 0.0
                snippets = []

                with open(file_path, 'rb') as f:
                    chunk_num = 0

                    while True:
                        chunk = f.read(self.chunk_size)
                        if not chunk:
                            break

                        # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë“œ
                        try:
                            text = chunk.decode('utf-8', errors='ignore')
                        except:
                            continue

                        # ì²­í¬ì—ì„œ ê²€ìƒ‰
                        if query.lower() in text.lower():
                            relevance += 0.1

                            # ìŠ¤ë‹ˆí« ì¶”ì¶œ
                            pos = text.lower().find(query.lower())
                            if pos != -1 and len(snippets) < 3:
                                start = max(0, pos - 50)
                                end = min(len(text), pos + len(query) + 50)
                                snippets.append(f"...{text[start:end]}...")

                        chunk_num += 1

                # ê²°ê³¼ ìƒì„±
                if relevance > 0:
                    result = SearchResult(
                        task_id=f"stream_{i}",
                        file_path=file_path,
                        relevance=min(1.0, relevance),
                        snippets=snippets,
                        metadata={'chunks_processed': chunk_num},
                        processing_time=0,
                        success=True
                    )

                    # ì½œë°± í˜¸ì¶œ
                    if callback:
                        callback(result)

                    yield result

            except Exception as e:
                self.logger.error(f"Stream search failed for {file_path}: {e}")

# í¸ì˜ í•¨ìˆ˜ë“¤
def parallel_search(file_paths: List[Path], query: str,
                   max_workers: int = 4,
                   max_results: int = 10) -> List[SearchResult]:
    """ê°„ë‹¨í•œ ë³‘ë ¬ ê²€ìƒ‰"""
    engine = ParallelSearchEngine(max_workers=max_workers)
    try:
        results = engine.search_files(file_paths, query, max_results)
        return results
    finally:
        engine.shutdown()

async def async_search(file_paths: List[Path], query: str,
                      max_results: int = 10) -> List[SearchResult]:
    """ê°„ë‹¨í•œ ë¹„ë™ê¸° ê²€ìƒ‰"""
    engine = AsyncSearchEngine()
    return await engine.search_async(file_paths, query, max_results)

def stream_search(file_paths: List[Path], query: str) -> Generator[SearchResult, None, None]:
    """ê°„ë‹¨í•œ ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰"""
    engine = StreamingSearchEngine()
    yield from engine.search_stream(file_paths, query)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Parallel Search Optimizer")
    parser.add_argument('--mode', choices=['parallel', 'async', 'stream'],
                       default='parallel', help='Search mode')
    parser.add_argument('--query', default='êµ¬ë§¤', help='Search query')
    parser.add_argument('--workers', type=int, default=4, help='Number of workers')
    parser.add_argument('--max-results', type=int, default=10, help='Max results')
    parser.add_argument('--test', action='store_true', help='Run tests')
    args = parser.parse_args()

    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    if args.test:
        print("=" * 60)
        print("ğŸ§ª Parallel Search Optimizer Test")
        print("=" * 60)

        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        test_dir = Path("test_docs")
        test_dir.mkdir(exist_ok=True)

        test_files = []
        for i in range(20):
            file_path = test_dir / f"test_{i}.txt"
            content = f"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ {i}\nêµ¬ë§¤ ê´€ë ¨ ë‚´ìš©\n" * 10
            if i % 3 == 0:
                content += f"\n{args.query} ê´€ë ¨ ìƒì„¸ ë‚´ìš©\n" * 5

            file_path.write_text(content)
            test_files.append(file_path)

        # 1. ë³‘ë ¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ“Š Parallel Search Test:")
        print("-" * 40)

        engine = ParallelSearchEngine(max_workers=args.workers)

        start = time.time()
        results = engine.search_files(test_files, args.query, args.max_results)
        elapsed = time.time() - start

        print(f"Found {len(results)} results in {elapsed:.2f}s")
        for i, result in enumerate(results[:3], 1):
            print(f"  {i}. {result.file_path.name} (relevance: {result.relevance:.2f})")

        print("\nğŸ“ˆ Statistics:")
        stats = engine.get_stats()
        for key, value in stats.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for k, v in value.items():
                    print(f"    {k}: {v}")
            else:
                print(f"  {key}: {value}")

        engine.shutdown()

        # 2. ë¹„ë™ê¸° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        if args.mode == 'async':
            print("\nğŸ“Š Async Search Test:")
            print("-" * 40)

            async def test_async():
                start = time.time()
                results = await async_search(test_files, args.query, args.max_results)
                elapsed = time.time() - start

                print(f"Found {len(results)} results in {elapsed:.2f}s")
                for i, result in enumerate(results[:3], 1):
                    print(f"  {i}. {result.file_path.name} (relevance: {result.relevance:.2f})")

            asyncio.run(test_async())

        # 3. ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        if args.mode == 'stream':
            print("\nğŸ“Š Streaming Search Test:")
            print("-" * 40)

            start = time.time()
            results = list(stream_search(test_files, args.query))
            elapsed = time.time() - start

            print(f"Streamed {len(results)} results in {elapsed:.2f}s")
            for i, result in enumerate(results[:3], 1):
                print(f"  {i}. {result.file_path.name} (relevance: {result.relevance:.2f})")

        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
        import shutil
        shutil.rmtree(test_dir)

        print("\nâœ… Test completed")

    else:
        # ì‹¤ì œ ê²€ìƒ‰ ì‹¤í–‰
        docs_dir = Path("docs")
        if not docs_dir.exists():
            print(f"âŒ Directory not found: {docs_dir}")
            sys.exit(1)

        # PDF íŒŒì¼ ìˆ˜ì§‘
        pdf_files = list(docs_dir.rglob("*.pdf"))[:50]  # ìµœëŒ€ 50ê°œ

        if not pdf_files:
            print("âŒ No PDF files found")
            sys.exit(1)

        print(f"ğŸ” Searching {len(pdf_files)} files for '{args.query}'...")

        if args.mode == 'parallel':
            results = parallel_search(pdf_files, args.query, args.workers, args.max_results)
        elif args.mode == 'async':
            results = asyncio.run(async_search(pdf_files, args.query, args.max_results))
        else:  # stream
            results = list(stream_search(pdf_files, args.query))[:args.max_results]

        print(f"\nğŸ“Š Search Results ({len(results)} found):")
        for i, result in enumerate(results, 1):
            print(f"\n{i}. {result.file_path.name}")
            print(f"   Relevance: {result.relevance:.2f}")
            if result.snippets:
                print(f"   Snippet: {result.snippets[0][:100]}...")