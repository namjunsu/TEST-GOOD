#!/usr/bin/env python3
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import threading
from queue import Queue
from functools import partial

"""
"""

import re
import warnings
import logging
import pdfplumber
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import json
import sys
import os
from functools import lru_cache
import hashlib
from contextlib import nullcontext

try:
    from log_system import get_logger, TimerContext
    chat_logger = get_logger()
    logger = chat_logger
except ImportError:
    chat_logger = None
    logger = None
    TimerContext = None
    
try:
    from response_formatter import ResponseFormatter
except ImportError:
    ResponseFormatter = None

warnings.filterwarnings("ignore", message=".*FontBBox.*")
warnings.filterwarnings("ignore", category=UserWarning, module="pdfplumber")

logging.getLogger("pdfplumber").setLevel(logging.ERROR)
logging.getLogger("pdfminer").setLevel(logging.ERROR)

current_dir = Path(__file__).parent.absolute()
sys.path.insert(0, str(current_dir))

try:
    from config_manager import config_manager as cfg
    USE_YAML_CONFIG = True
except ImportError:
    import config as cfg
    USE_YAML_CONFIG = False

from rag_system.qwen_llm import QwenLLM
from rag_system.llm_singleton import LLMSingleton
from metadata_db import MetadataDB

try:
    from everything_like_search import EverythingLikeSearch
    EVERYTHING_SEARCH_AVAILABLE = True
except ImportError:
    EVERYTHING_SEARCH_AVAILABLE = False
    if logger:
        logger.warning("EverythingLikeSearch not available, using legacy search")

try:
    from metadata_extractor import MetadataExtractor
    METADATA_EXTRACTOR_AVAILABLE = True
except ImportError:
    METADATA_EXTRACTOR_AVAILABLE = False
    if logger:
        logger.warning("MetadataExtractor not available, metadata extraction disabled")

try:
    from search_module import SearchModule
    SEARCH_MODULE_AVAILABLE = True
except ImportError:
    SEARCH_MODULE_AVAILABLE = False
    if logger:
        logger.warning("SearchModule not available - using embedded search")

try:
    from document_module import DocumentModule
    DOCUMENT_MODULE_AVAILABLE = True
except ImportError:
    DOCUMENT_MODULE_AVAILABLE = False
    if logger:
        logger.warning("DocumentModule not available - using embedded document processing")

try:
    from llm_module import LLMModule
    LLM_MODULE_AVAILABLE = True
except ImportError:
    LLM_MODULE_AVAILABLE = False
    if logger:
        logger.warning("LLMModule not available - using embedded LLM handling")

try:
    from cache_module import CacheModule
    CACHE_MODULE_AVAILABLE = True
except ImportError:
    CACHE_MODULE_AVAILABLE = False
    if logger:
        logger.warning("CacheModule not available - using embedded cache management")

try:
    from statistics_module import StatisticsModule
    STATISTICS_MODULE_AVAILABLE = True
except ImportError:
    STATISTICS_MODULE_AVAILABLE = False
    if logger:
        logger.warning("StatisticsModule not available - using embedded statistics")

try:
    from intent_module import IntentModule
    INTENT_MODULE_AVAILABLE = True
except ImportError:
    INTENT_MODULE_AVAILABLE = False
    if logger:
        logger.warning("IntentModule not available - using embedded intent analysis")

import traceback

if logger is None:
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('perfect_rag.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)
class RAGException(Exception):
    """RAG ì‹œìŠ¤í…œ ê¸°ë³¸ ì˜ˆì™¸"""

class DocumentNotFoundException(RAGException):
    """ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ"""

class PDFExtractionException(RAGException):
    """PDF ì¶”ì¶œ ì‹¤íŒ¨"""

class LLMException(RAGException):
    """LLM ê´€ë ¨ ì˜¤ë¥˜"""

class CacheException(RAGException):
    """ìºì‹œ ê´€ë ¨ ì˜¤ë¥˜"""

def handle_errors(default_return=None):
    """ì—ëŸ¬ ì²˜ë¦¬ ë°ì½”ë ˆì´í„°"""

    def decorator(func):

        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except RAGPDFExtractionException as e:
                logger.error(f"{func.__name__} - RAG ì˜¤ë¥˜: {str(e)}")
                if default_return is not None:
                    return default_return
                raise
            except PDFExtractionException as e:
                logger.error(f"{func.__name__} - ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}", exc_info=True)
                if default_return is not None:
                    return default_return
                raise RAGPDFExtractionException(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return wrapper
    return decorator

class PerfectRAG:
    """ì •í™•í•˜ê³  ì‹¬í”Œí•œ RAG ì‹œìŠ¤í…œ"""

    def _load_performance_config(self):
        """performance_config.yaml ë¡œë“œ"""
        import yaml
        perf_config_path = Path(__file__).parent / 'performance_config.yaml'
        if perf_config_path.exists():
            try:
                with open(perf_config_path, 'r', encoding='utf-8') as f:
                    self.perf_config = yaml.safe_load(f)
                    if logger:
                        logger.info(f"Performance config loaded from {perf_config_path}")
            except Exception as e:
                if logger:
                    logger.warning(f"Failed to load performance config: {e}")
                self.perf_config = {}
        else:
            self.perf_config = {}

    def _get_manufacturer_pattern(self):
        if USE_YAML_CONFIG:
            manufacturers = cfg.get('patterns.manufacturers', [])
            if manufacturers:
                pattern = '|'.join(manufacturers)
                return rf'\b({pattern})\b'
        return r'\b(SONY|Sony|HARRIS|Harris|HP|TOSHIBA|Toshiba|PANASONIC|Panasonic|CANON|Canon|NIKON|Nikon|DELL|Dell|APPLE|Apple|SAMSUNG|Samsung|LG)\b'

    def _get_model_pattern(self):
        if USE_YAML_CONFIG:
            return cfg.get('patterns.model_regex', r'\b[A-Z]{2,}[-\s]?\d+')
        return r'\b[A-Z]{2,}[-\s]?\d+'
    
    def __init__(self, preload_llm=False):
        self._load_performance_config()

        if USE_YAML_CONFIG:
            self.docs_dir = Path(cfg.get('paths.documents_dir', './docs'))
            self.model_path = cfg.get('models.qwen.path', './models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf')
            self.max_cache_size = self.perf_config.get('cache', {}).get('max_size', 500)
            self.cache_ttl = self.perf_config.get('cache', {}).get('response_ttl', 7200)
            self.max_text_length = self.perf_config.get('memory', {}).get('max_document_length', 8000)
            self.max_pdf_pages = cfg.get('limits.max_pdf_pages', 10)
            self.pdf_workers = self.perf_config.get('parallel', {}).get('pdf_workers', 4)
            self.chunk_size = self.perf_config.get('parallel', {}).get('chunk_size', 5)
        else:
            self.docs_dir = Path(cfg.DOCS_DIR)
            self.model_path = cfg.QWEN_MODEL_PATH
            self.max_cache_size = self.perf_config.get('cache', {}).get('max_size', 500)
            self.cache_ttl = self.perf_config.get('cache', {}).get('response_ttl', 7200)
            self.max_text_length = self.perf_config.get('memory', {}).get('max_document_length', 8000)
            self.max_pdf_pages = getattr(cfg, 'MAX_PDF_PAGES', 10)
            self.pdf_workers = self.perf_config.get('parallel', {}).get('pdf_workers', 4)
            self.chunk_size = self.perf_config.get('parallel', {}).get('chunk_size', 5)

        self.max_pdf_cache = 50

        self.config_dir = Path(__file__).parent / 'config'
        self.config_dir.mkdir(exist_ok=True)
        self.cache_dir = self.config_dir / 'cache'
        self.cache_dir.mkdir(exist_ok=True)

        self.llm = None
        from collections import OrderedDict
        self.MAX_CACHE_SIZE = 100
        self.MAX_METADATA_CACHE = 500
        self.MAX_PDF_CACHE = 50
        self.CACHE_TTL = 3600

        self.documents_cache = OrderedDict()
        self.metadata_cache = OrderedDict()
        self.search_mode = 'document'
        self.answer_cache = OrderedDict()
        self.pdf_text_cache = OrderedDict()

        self.pdf_processor = None
        self.error_handler = None
        self.error_recovery = None

        self.search_module = None
        if SEARCH_MODULE_AVAILABLE:
            try:
                self.search_module = SearchModule(str(self.docs_dir))
                if logger:
                    logger.info("âœ… SearchModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ SearchModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.search_module = None

        self.document_module = None
        if DOCUMENT_MODULE_AVAILABLE:
            try:
                doc_config = {
                    'max_pdf_cache': self.max_pdf_cache,
                    'max_text_length': self.max_text_length,
                    'max_pdf_pages': self.max_pdf_pages,
                    'enable_ocr': False
                }
                self.document_module = DocumentModule(doc_config)
                if logger:
                    logger.info("âœ… DocumentModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ DocumentModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.document_module = None

        self.llm_module = None
        if LLM_MODULE_AVAILABLE:
            try:
                llm_config = {
                    'model_path': self.model_path,
                    'preload_llm': preload_llm
                }
                self.llm_module = LLMModule(llm_config)
                if logger:
                    logger.info("âœ… LLMModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ LLMModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.llm_module = None

        self.cache_module = None
        if CACHE_MODULE_AVAILABLE:
            try:
                cache_config = {
                    'max_cache_size': self.MAX_CACHE_SIZE,
                    'max_metadata_cache': self.MAX_METADATA_CACHE,
                    'max_pdf_cache': self.MAX_PDF_CACHE,
                    'cache_ttl': self.CACHE_TTL,
                    'cache_dir': str(self.config_dir / 'cache')
                }
                self.cache_module = CacheModule(cache_config)
                if logger:
                    logger.info("âœ… CacheModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ CacheModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.cache_module = None

        self.statistics_module = None
        if STATISTICS_MODULE_AVAILABLE:
            try:
                stats_config = {
                    'docs_dir': str(self.docs_dir)
                }
                self.statistics_module = StatisticsModule(stats_config)
                if logger:
                    logger.info("âœ… StatisticsModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ StatisticsModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.statistics_module = None

        self.intent_module = None
        if INTENT_MODULE_AVAILABLE:
            try:
                self.intent_module = IntentModule(llm_module=self.llm_module)
                if logger:
                    logger.info("âœ… IntentModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ IntentModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.intent_module = None

        self.everything_search = None
        if not self.search_module and EVERYTHING_SEARCH_AVAILABLE:
            try:
                self.everything_search = EverythingLikeSearch()
                if not hasattr(self, '_index_initialized'):
                    self.everything_search.index_all_files()
                    self.__class__._index_initialized = True
                if logger:
                    logger.info("Everything-like search initialized successfully")
            except Exception as e:
                if logger:
                    logger.error(f"Failed to initialize Everything-like search: {e}")
                self.everything_search = None

        self.formatter = ResponseFormatter() if ResponseFormatter else None

        self.metadata_extractor = None
        if METADATA_EXTRACTOR_AVAILABLE:
            try:
                self.metadata_extractor = MetadataExtractor()
                if logger:
                    logger.info("âœ… MetadataExtractor ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ MetadataExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.metadata_extractor = None

        try:
            self.metadata_db = MetadataDB(db_path=str(self.config_dir / "metadata.db"))
            logger.info("âœ… MetadataDB ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.error(f"ï¸ MetadataDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.metadata_db = None

        self.pdf_files = []
        self.txt_files = []

        self.pdf_files.extend(list(self.docs_dir.glob('*.pdf')))
        self.txt_files.extend(list(self.docs_dir.glob('*.txt')))

        for year in range(2014, 2026):
            year_folder = self.docs_dir / f"year_{year}"
            if year_folder.exists():
                self.pdf_files.extend(list(year_folder.glob('*.pdf')))
                self.txt_files.extend(list(year_folder.glob('*.txt')))

        special_folders = ['recent', 'archive']
        for folder in special_folders:
            special_folder = self.docs_dir / folder
            if special_folder.exists():
                self.pdf_files.extend(list(special_folder.glob('*.pdf')))
                self.txt_files.extend(list(special_folder.glob('*.txt')))

        self.pdf_files = list(set(self.pdf_files))
        self.txt_files = list(set(self.txt_files))
        self.all_files = self.pdf_files + self.txt_files

        if logger:
            logger.info(f"{len(self.pdf_files)}ê°œ PDF, {len(self.txt_files)}ê°œ TXT ë¬¸ì„œ ë°œê²¬")

        self._build_metadata_cache()

        self._build_metadata_index()

        if preload_llm:
            self._preload_llm()

    def _manage_cache_size(self, cache_dict, max_size, cache_name="cache"):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬ - LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°"""

    def _add_to_cache(self, cache_dict, key, value, max_size):
        """ìºì‹œì— í•­ëª© ì¶”ê°€ with í¬ê¸° ì œí•œ"""
        if key in cache_dict:
            del cache_dict[key]

        cache_dict[key] = {
            'data': value,
            'timestamp': time.time()
        }

        self._manage_cache_size(cache_dict, max_size, str(type(cache_dict)))

    def _preload_llm(self):
        """LLMì„ ë¯¸ë¦¬ ë¡œë“œ"""

        # LLMModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        if self.llm is None:
            if not LLMSingleton.is_loaded():
                if logger:
                    logger.info("LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
            elif logger:
                logger.info("LLM ëª¨ë¸ ì¬ì‚¬ìš©")

            try:
                start = time.time()
                self.llm = LLMSingleton.get_instance(model_path=self.model_path)
                elapsed = time.time() - start
                if elapsed > 1.0:  # 1ì´ˆ ì´ìƒ ê±¸ë¦° ê²½ìš°ë§Œ í‘œì‹œ
                    if logger:
                        logger.info(f"LLM ë¡œë“œ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")
            except LLMException as e:
                if logger:
                    logger.error(f"LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _build_metadata_index(self):
        """ëª¨ë“  PDFì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ DBì— ì €ì¥"""
        if not self.metadata_db:
            return

        logger.info("ğŸ“š ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘...")
        indexed = 0

        for pdf_path in self.pdf_files[:30]:
            try:
                existing = self.metadata_db.get_document(pdf_path.name)
                if existing and existing.get('title'):
                    continue

                metadata = self._extract_pdf_metadata(pdf_path)
                if metadata:
                    self.metadata_db.add_document(metadata)
                    indexed += 1

            except Exception as e:
                logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {pdf_path.name}: {e}")

        if indexed > 0:
            logger.info(f"âœ… {indexed}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¸ë±ì‹± ì™„ë£Œ")

    def _extract_pdf_metadata(self, pdf_path: Path) -> Optional[Dict[str, Any]]:
        """PDFì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            import PyPDF2
            with open(pdf_path, 'rb') as file:
                pdf = PyPDF2.PdfReader(file)

                # ì²« í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                first_page = pdf.pages[0].extract_text() if pdf.pages else ""

                # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
                metadata = {
                    'path': str(pdf_path),
                    'filename': pdf_path.name,
                    'file_size': pdf_path.stat().st_size,
                    'page_count': len(pdf.pages),
                    'text_preview': first_page[:500]
                }

                # ì œëª© ì¶”ì¶œ
                lines = first_page.split('\n')[:10]
                for line in lines:
                    if len(line) > 10 and len(line) < 100:
                        metadata['title'] = line.strip()
                        break

                # ë‚ ì§œ ì¶”ì¶œ
                date_patterns = [
                    r'(\d{4})[ë…„-]\s*(\d{1,2})[ì›”-]\s*(\d{1,2})',
                    r'(\d{4})\.(\d{1,2})\.(\d{1,2})'
                ]
                for pattern in date_patterns:
                    match = re.search(pattern, first_page[:1000])
                    if match:
                        year, month, day = match.groups()
                        metadata['year'] = year
                        metadata['month'] = month.zfill(2)
                        metadata['date'] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                        break

                # ê¸°ì•ˆì ì¶”ì¶œ
                drafter_patterns = [
                    r'ê¸°ì•ˆì[\s:ï¼š]*([ê°€-í£]{2,4})',
                    r'ì‘ì„±ì[\s:ï¼š]*([ê°€-í£]{2,4})'
                ]
                for pattern in drafter_patterns:
                    match = re.search(pattern, first_page[:500])
                    if match:
                        metadata['drafter'] = match.group(1).strip()
                        break

                # ì¹´í…Œê³ ë¦¬ ì¶”ì •
                filename_lower = pdf_path.name.lower()
                if 'êµ¬ë§¤' in filename_lower or 'êµ¬ì…' in filename_lower:
                    metadata['category'] = 'êµ¬ë§¤'
                elif 'ìˆ˜ë¦¬' in filename_lower or 'ë³´ìˆ˜' in filename_lower:
                    metadata['category'] = 'ìˆ˜ë¦¬'
                elif 'íê¸°' in filename_lower:
                    metadata['category'] = 'íê¸°'
                elif 'ê²€í† ' in filename_lower:
                    metadata['category'] = 'ê²€í† '

                return metadata

        except Exception as e:
            logger.error(f"PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜ {pdf_path.name}: {e}")
            return None

    def _parse_pdf_result(self, result: Dict) -> Dict:
        """ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            'text': result.get('text', ''),
            'page_count': result.get('page_count', 0),
            'metadata': result.get('metadata', {}),
            'method': result.get('method', 'parallel')
        }

    def process_pdfs_in_batch(self, pdf_paths: List[Path], batch_size: int = None) -> Dict:
        """ì—¬ëŸ¬ PDFë¥¼ ë°°ì¹˜ë¡œ ë³‘ë ¬ ì²˜ë¦¬ (ì•ˆì „í•˜ê²Œ ê°œì„ )"""

        all_results = {}

        # ë™ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        if batch_size is None:
            batch_size = min(20, max(10, len(pdf_paths) // 30))
            logger.info(f"ë™ì  ë°°ì¹˜ í¬ê¸° ì„¤ì •: {batch_size}")

        # pdf_processorê°€ ì—†ìœ¼ë©´ ìˆœì°¨ ì²˜ë¦¬ë¡œ í´ë°±
        if self.pdf_processor is None:
            if logger:
                logger.info("ë³‘ë ¬ ì²˜ë¦¬ê¸° ë¯¸í™œì„±í™” - ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")

            # ThreadPoolExecutorë¡œ ê°„ë‹¨í•œ ë³‘ë ¬ ì²˜ë¦¬ (CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜ ìµœì í™”)
            optimal_workers = min(os.cpu_count() or 4, 12, max(4, batch_size))
            with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
                for i in range(0, len(pdf_paths), batch_size):
                    batch = pdf_paths[i:i + batch_size]
                    if logger:
                        logger.info(f"ë°°ì¹˜ {i//batch_size + 1}/{(len(pdf_paths)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)")

                    # ê° PDFë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
                    futures = {executor.submit(self._extract_pdf_info, pdf): pdf for pdf in batch}

                    for future in as_completed(futures):
                        pdf_path = futures[future]
                        try:
                            result = future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                            all_results[str(pdf_path)] = result
                        except PDFExtractionException as e:
                            if logger:
                                logger.warning(f"{pdf_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}")
                            all_results[str(pdf_path)] = {'error': str(e)}

                    # ë©”ëª¨ë¦¬ ìµœì í™”
                    if i % (batch_size * 5) == 0:
                        gc.collect()
            for i in range(0, len(pdf_paths), batch_size):
                batch = pdf_paths[i:i + batch_size]

                batch_results = self.pdf_processor.process_multiple_pdfs(batch)
                all_results.update(batch_results)

                # ë©”ëª¨ë¦¬ ê´€ë¦¬
                if len(self.pdf_processor.extraction_cache) > 50:
                    self.pdf_processor.clear_cache()

        return all_results

    def _find_metadata_by_filename(self, filename: str) -> Optional[Dict]:
        """íŒŒì¼ëª…ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì°¾ê¸° (ìƒˆë¡œìš´ ìºì‹œ êµ¬ì¡° ì§€ì›)"""
        if filename in self.metadata_cache:
            return self.metadata_cache[filename]

        for cache_key, metadata in self.metadata_cache.items():
            if metadata.get('filename') == filename:
                return metadata

    def _build_metadata_cache(self):
        """ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ (ìºì‹± ì§€ì›)"""
        cache_file = self.cache_dir / "metadata_cache.pkl"

        # ê¸°ì¡´ ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
        if cache_file.exists():
            try:
                import pickle
                with open(cache_file, 'rb') as f:
                    self.metadata_cache = pickle.load(f)
                if logger:
                    logger.info(f"ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.metadata_cache)}ê°œ ë¬¸ì„œ")
                return  # ìºì‹œê°€ ìˆìœ¼ë©´ ì¬êµ¬ì¶• ë¶ˆí•„ìš”
            except Exception as e:
                if logger:
                    logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¬êµ¬ì¶•: {e}")

        logger.info("ë©”íƒ€ë°ì´í„° ìºì‹œ êµ¬ì¶• ì‹œì‘")
        if logger:
            logger.info("ë¬¸ì„œ ë©”íƒ€ë°ì´í„° êµ¬ì¶• ì¤‘...")

        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • í™•ì¸
        use_parallel = USE_YAML_CONFIG and cfg.get('parallel_processing.enabled', True)

        # ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ process_pdfs_in_batch ì‚¬ìš© (ë‚´ë¶€ì—ì„œ ì²˜ë¦¬)
        if self.pdf_files and len(self.pdf_files) > 10:  # 10ê°œ ì´ìƒì¼ ë•Œë§Œ ë³‘ë ¬ ì²˜ë¦¬
            if logger:
                logger.info(f"{len(self.pdf_files)}ê°œ PDF ì²˜ë¦¬ ì‹œì‘ (ë³‘ë ¬ ëª¨ë“œ)")
            pdf_results = self.process_pdfs_in_batch(self.pdf_files, batch_size=10)

            # ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë©”íƒ€ë°ì´í„° ìºì‹œì— ì €ì¥
            for pdf_path, result in pdf_results.items():
                pdf_path_obj = Path(pdf_path)
                filename = pdf_path_obj.name
                # ìƒëŒ€ ê²½ë¡œë¥¼ í‚¤ë¡œ ì‚¬ìš© (ì¤‘ë³µ íŒŒì¼ëª… ì²˜ë¦¬)
                try:
                    relative_path = pdf_path_obj.relative_to(self.docs_dir)
                    cache_key = str(relative_path)
                except ValueError:
                    cache_key = filename

                if 'error' not in result:
                    self.metadata_cache[cache_key] = {
                        'path': pdf_path_obj,
                        'filename': filename,
                        'text': result.get('text', '')[:1000],  # ìš”ì•½ë§Œ ì €ì¥
                        'page_count': result.get('page_count', 0),
                        'metadata': result.get('metadata', {})
                    }
        
        # PDFì™€ TXT íŒŒì¼ ëª¨ë‘ ì²˜ë¦¬
        for file_path in self.all_files:
            filename = file_path.name

            # ìƒëŒ€ ê²½ë¡œë¥¼ ìºì‹œ í‚¤ë¡œ ì‚¬ìš©
            try:
                relative_path = file_path.relative_to(self.docs_dir)
                cache_key = str(relative_path)
            except ValueError:
                cache_key = filename

            # íŒŒì¼ëª…ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            date_match = re.match(r'(\d{4}-\d{2}-\d{2})', filename)
            date = date_match.group(1) if date_match else ""

            # ì œëª© ì¶”ì¶œ (ë‚ ì§œ ë’¤ ë¶€ë¶„, í™•ì¥ì ì œê±°)
            if filename.endswith('.pdf'):
                title = filename.replace(date + '_', '').replace('.pdf', '') if date else filename.replace('.pdf', '')
            else:
                title = filename.replace(date + '_', '').replace('.txt', '') if date else filename.replace('.txt', '')

            # ì—°ë„ ì¶”ì¶œ
            year = date[:4] if date else ""

            # íŒŒì¼ëª…ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
            keywords = []

            # íŒŒì¼ëª…ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            # í•œê¸€, ì˜ë¬¸, ìˆ«ìë¡œ ë‹¨ì–´ ì¶”ì¶œ
            words = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', filename)

            # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ë‹¨ì–´ë“¤ì„ í‚¤ì›Œë“œë¡œ ì¶”ê°€ (2ê¸€ì ì´ìƒ)
            for word in words:
                if len(word) >= 2 and word not in ['pdf', 'PDF', 'txt', 'TXT', 'ì˜', 'ë°', 'ê±´', 'ê²€í† ì„œ']:
                    keywords.append(word)

            self.metadata_cache[cache_key] = {
                'path': file_path,
                'filename': filename,  # ì‹¤ì œ íŒŒì¼ëª… ì €ì¥
                'date': date,
                'year': year,
                'title': title,
                'keywords': keywords,
                'drafter': None,  # ê¸°ì•ˆì ì •ë³´ëŠ” í•„ìš”ì‹œì—ë§Œ ì¶”ì¶œ
                'full_text': None,  # ë‚˜ì¤‘ì— í•„ìš”ì‹œ ë¡œë“œ
                'is_txt': filename.endswith('.txt'),  # TXT íŒŒì¼ ì—¬ë¶€
                'is_pdf': filename.endswith('.pdf')  # PDF íŒŒì¼ ì—¬ë¶€ ì¶”ê°€
            }

        if logger:
            logger.info(f"{len(self.metadata_cache)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° êµ¬ì¶• ì™„ë£Œ")

        # ìºì‹œ ì €ì¥
        try:
            import pickle
            with open(cache_file, 'wb') as f:
                pickle.dump(self.metadata_cache, f)
            if logger:
                logger.info(f"ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_file}")
        except Exception as e:
            if logger:
                logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _extract_pdf_info_with_retry(self, pdf_path: Path) -> Dict:
        """PDF ì •ë³´ ì¶”ì¶œ (DocumentModule ì‚¬ìš©)"""
        # DocumentModuleì„ ì‚¬ìš©í•˜ì—¬ PDF ì²˜ë¦¬
        if self.document_module:
            try:
                result = self.document_module.extract_pdf_text_with_retry(pdf_path, max_retries=2)
                if result:
                    return result
            except Exception as e:
                if logger:
                    logger.error(f"DocumentModule PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        # DocumentModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ë²• ì‚¬ìš©
        return self._extract_pdf_info(pdf_path)
    
    def _extract_pdf_info(self, pdf_path: Path) -> Dict:
        """PDF ì •ë³´ ì¶”ì¶œ (StatisticsModule ìœ„ì„ ë˜ëŠ” í´ë°±ìš©) - ìºì‹± ì ìš©"""

        cache_key = str(pdf_path)

        if cache_key in self.pdf_text_cache:
            cached_result = self.pdf_text_cache.pop(cache_key)
            self.pdf_text_cache[cache_key] = cached_result
            return cached_result

        text = ""

        def extract_with_pdfplumber():
            nonlocal text
            with pdfplumber.open(pdf_path) as pdf:
                pages_to_read = min(len(pdf.pages), self.max_pdf_pages)
                for page in pdf.pages[:pages_to_read]:
                    try:
                        page_text = page.extract_text() or ""
                    except Exception:
                        page_text = ""
                    if page_text:
                        text += page_text + "\n"
                        if len(text) > self.max_text_length:
                            break
            return text

        text = extract_with_pdfplumber()
        if not text and hasattr(self, '_try_ocr_extraction'):
            text = self._try_ocr_extraction(pdf_path)

        if not text:
            try:
                from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
                ocr = EnhancedOCRProcessor()
                text, _ = ocr.extract_text_with_ocr(str(pdf_path))
            except Exception:
                pass

        if not text:
            return {}

        info = {}

        patterns = [
            r'ê¸°ì•ˆì[\s:ï¼š]*([ê°€-í£]+)',
            r'ì‘ì„±ì[\s:ï¼š]*([ê°€-í£]+)',
            r'ë‹´ë‹¹ì[\s:ï¼š]*([ê°€-í£]+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                info['ê¸°ì•ˆì'] = match.group(1).strip()
                break
            
        date_patterns = [
            r'ê¸°ì•ˆì¼[\s:ï¼š]*(\d{4}[-ë…„]\s*\d{1,2}[-ì›”]\s*\d{1,2})',
            r'ì‹œí–‰ì¼ì[\s:ï¼š]*(\d{4}[-ë…„]\s*\d{1,2}[-ì›”]\s*\d{1,2})',
            r'(\d{4}-\d{2}-\d{2})\s*\d{2}:\d{2}',
            r'ì¼ì[\s:ï¼š]*(\d{4}[-./ë…„]\s*\d{1,2}[-./ì›”]\s*\d{1,2})',
            r'ë‚ ì§œ[\s:ï¼š]*(\d{4}[-./ë…„]\s*\d{1,2}[-./ì›”]\s*\d{1,2})',
            r'(\d{4}[./-]\d{1,2}[./-]\d{1,2})'
        ]

        date_found = False
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                info['ë‚ ì§œ'] = match.group(1).strip()
                date_found = True
                break

        if not date_found:
            filename = pdf_path.name
            filename_date_patterns = [
                r'(20\d{2}[-_.]0?[1-9]|1[0-2][-_.][0-2]?\d|3[01])',
                r'(20\d{2})[-_.]?(\d{1,2})[-_.]?(\d{1,2})',
            ]

            for pattern in filename_date_patterns:
                match = re.search(pattern, filename)
                if match:
                    if len(match.groups()) == 1:
                        date_str = match.group(1)
                        date_str = date_str.replace('_', '-').replace('.', '-')
                        info['ë‚ ì§œ'] = date_str
                        date_found = True
                        break
                    if len(match.groups()) == 3:
                        year, month, day = match.groups()
                        try:
                            normalized_date = f"{year}-{int(month):02d}-{int(day):02d}"
                            info['ë‚ ì§œ'] = normalized_date
                            date_found = True
                            break
                        except ValueError:
                            continue
            
            dept_match = re.search(r'ê¸°ì•ˆë¶€ì„œ[\s:ï¼š]*([^\nì‹œí–‰]+)', text)
            if dept_match:
                dept = dept_match.group(1).strip()
                dept = dept.split('ì‹œí–‰')[0].strip()
                info['ë¶€ì„œ'] = dept
            
            if 'ë¶€ì„œ' not in info:
                team_match = re.search(r'([ê°€-í£]+íŒ€[\-ê°€-í£]+íŒŒíŠ¸)', text)
                if team_match:
                    info['ë¶€ì„œ'] = team_match.group(1)
            
            amounts = re.findall(r'(\d{1,3}(?:,\d{3})*)\s*ì›', text)
            if amounts:
                numeric_amounts = []
                for amt in amounts:
                    try:
                        num = int(amt.replace(',', ''))
                        numeric_amounts.append((num, amt))
                    except (ValueError, AttributeError):
                        pass
                if numeric_amounts:
                    numeric_amounts.sort(reverse=True)
                    info['ê¸ˆì•¡'] = f"{numeric_amounts[0][1]}ì›"
            
            title_match = re.search(r'ì œëª©[\s:ï¼š]*([^\n]+)', text)
            if title_match:
                info['ì œëª©'] = title_match.group(1).strip()
            
            info['text'] = text[:3000]

        MAX_PDF_CACHE_SIZE = 50
        if len(self.pdf_text_cache) >= MAX_PDF_CACHE_SIZE:
            self.pdf_text_cache.popitem(last=False)

        self.pdf_text_cache[cache_key] = info

        return info

    def _search_by_content(self, query: str) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ëª¨ë“ˆë¡œ ìœ„ì„ëœ ê²€ìƒ‰ í•¨ìˆ˜"""
        if self.search_module:
            try:
                results = self.search_module.search_by_content(query, top_k=20)
                if logger:
                    logger.info(f"SearchModule found {len(results)} documents for query: {query}")
                return results
            except Exception: return None

        # SearchModuleì´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if logger:
            logger.warning("SearchModule not available")
        return []

    def _extract_context(self, text: str, keyword: str, window: int = 200) -> str:
        """í‚¤ì›Œë“œ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""

        pos = text_lower.find(keyword_lower)
        if pos == -1:
            return ""

        start = max(0, pos - window)
        end = min(len(text), pos + len(keyword) + window)

        context = text[start:end]
        context = context.replace(keyword, f"**{keyword}**")

        return f"...{context}..."

    def find_best_document(self, query: str) -> Optional[Path]:
        """ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë¬¸ì„œ ì°¾ê¸° - SearchModuleë¡œ ìœ„ì„ ë˜ëŠ” ê°„ë‹¨í•œ í´ë°±"""

        # SearchModuleì´ ìˆìœ¼ë©´ ìœ„ì„
        if self.search_module:
            try:
                result = self.search_module.find_best_document(query)
                if result:
                    return Path(result)
            except Exception as e:
                if logger:
                    logger.error(f"SearchModule ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}, í´ë°± ì‚¬ìš©")

        # í´ë°±: ê°„ë‹¨í•œ ê²€ìƒ‰ ë¡œì§
        content_based_results = self._search_by_content(query)
        if content_based_results and content_based_results[0]['score'] > 20:
            best_result = content_based_results[0]
            return best_result['path']

        # ê¸°ë³¸ íŒŒì¼ëª… ë§¤ì¹­ í´ë°±
        for cache_key, metadata in self.metadata_cache.items():
            filename_lower = metadata.get('filename', cache_key).lower()
            if any(word in filename_lower for word in query.lower().split() if len(word) > 1):
                return Path(metadata.get('path', self.docs_dir / cache_key))
        
    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)
        """
        # ê¸¸ì´ê°€ ë„ˆë¬´ ë‹¤ë¥´ë©´ ë‚®ì€ ìœ ì‚¬ë„
        if abs(len(str1) - len(str2)) > 2:
            return 0.0

        # ê°„ë‹¨í•œ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚°

        def levenshtein_distance(s1: str, s2: str) -> int:
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)

            if len(s2) == 0:
                return len(s1)

            previous_row = range(len(s2) + 1)
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    # ì‚½ì…, ì‚­ì œ, ì¹˜í™˜ ë¹„ìš© ê³„ì‚°
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row

            return previous_row[-1]

        # ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
        distance = levenshtein_distance(str1, str2)
        max_len = max(len(str1), len(str2))
        similarity = 1 - (distance / max_len) if max_len > 0 else 1.0

        # í•œê¸€ íŠ¹ìˆ˜ ì²˜ë¦¬: ììŒ/ëª¨ìŒ í•˜ë‚˜ ì°¨ì´ëŠ” ë†’ì€ ìœ ì‚¬ë„
        # ì˜ˆ: ì¼/ìº , ì½¤/ì»´ ë“±
        if len(str1) == len(str2) and distance == 1:
            # í•œê¸€ì¸ ê²½ìš° ìœ ì‚¬ë„ ë³´ì •
            if all(ord('ê°€') <= ord(c) <= ord('í£') for c in str1 + str2):
                similarity = max(similarity, 0.85)

        return similarity

    def get_document_info(self, file_path: Path, info_type: str = "all") -> str:
        """ë¬¸ì„œì—ì„œ íŠ¹ì • ì •ë³´ ì¶”ì¶œ (PDF/TXT ëª¨ë‘ ì§€ì›)"""
        
        cache_key = f"{file_path.name}_{info_type}"
        if cache_key in self.documents_cache:
            return self.documents_cache[cache_key]
        
        info = self._extract_txt_info(file_path) if file_path.suffix == '.txt' else self._extract_pdf_info(file_path)
        
        if not info:
            return " ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        
        result = ""
        
        if info_type == "all":
            result = f" {file_path.stem}\n"
            result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            for key, value in info.items():
                if key != 'text':
                    result += f"â€¢ {key}: {value}\n"
        if info_type in ["ê¸°ì•ˆì", "ë‚ ì§œ", "ë¶€ì„œ", "ê¸ˆì•¡"]:
            result = f" {info_type}: {info.get(info_type, 'ì •ë³´ ì—†ìŒ')}"
        else:
            result = ""
            if 'ê¸°ì•ˆì' in info:
                result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
            if 'ë¶€ì„œ' in info:
                result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
            if 'ì œëª©' in info:
                result += f"â€¢ ì œëª©: {info['ì œëª©']}\n"
        if info_type == "ìš”ì•½":
            result = f" {file_path.stem} ìš”ì•½\n"
            result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            if 'ê¸°ì•ˆì' in info:
                result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
            if 'ë‚ ì§œ' in info:
                result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
            if 'ë¶€ì„œ' in info:
                result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
            if 'ê¸ˆì•¡' in info:
                result += f"â€¢ ê¸ˆì•¡: {info['ê¸ˆì•¡']}\n"
            if 'ì œëª©' in info:
                result += f"â€¢ ì œëª©: {info['ì œëª©']}\n"
            else:
                result = f" {file_path.stem}\n\n"
            result += f" **ê¸°ë³¸ ì •ë³´**\n"
            if 'ê¸°ì•ˆì' in info:
                result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
            if 'ë‚ ì§œ' in info:
                result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
            if 'ë¶€ì„œ' in info:
                result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
            
            result += f"\n **ì£¼ìš” ë‚´ìš©**\n"
            
            if 'text' in info:
                if self.llm_module:
                    summary = self.llm_module.generate_smart_summary(info['text'], str(file_path.name))
                else:
                    summary = self._generate_smart_summary(info['text'], file_path)
                result += summary
            
            if 'ê¸ˆì•¡' in info and info['ê¸ˆì•¡'] != 'ì •ë³´ ì—†ìŒ':
                result += f"\n **ë¹„ìš© ì •ë³´**\n"
                result += f"â€¢ ê¸ˆì•¡: {info['ê¸ˆì•¡']}\n"
            
            result += f"\n ì¶œì²˜: {file_path.name}"
        
        self.documents_cache[cache_key] = result
        
        return result

    def _remove_duplicate_documents(self, documents: list) -> list:
        """ì¤‘ë³µ ë¬¸ì„œ ì œê±° (íŒŒì¼ëª… ê¸°ì¤€)"""
        seen = set()
        unique_docs = []

        for doc in documents:
            # íŒŒì¼ëª…ì—ì„œ ê²½ë¡œ ë¶€ë¶„ ì œê±°í•˜ì—¬ ë¹„êµ
            filename = Path(doc.get('path', doc.get('filename', ''))).name
            if filename not in seen:
                seen.add(filename)
                unique_docs.append(doc)

        return unique_docs

    def _format_enhanced_response(self, results: list, query: str) -> str:
        """ê°œì„ ëœ ì‘ë‹µ í˜•ì‹"""

        unique_results = self._remove_duplicate_documents(results)

        response = f" **ê²€ìƒ‰ ê²°ê³¼** ({len(unique_results)}ê°œ ë¬¸ì„œ)\n\n"

        for i, doc in enumerate(unique_results[:5], 1):
            title = doc.get('title', 'ì œëª© ì—†ìŒ')
            date = doc.get('date', 'ë‚ ì§œ ë¯¸ìƒ')
            category = doc.get('category', 'ê¸°íƒ€')
            drafter = doc.get('drafter', 'ë¯¸ìƒ')

            if 'extracted_date' in doc:
                date = doc['extracted_date']
            if 'extracted_type' in doc:
                category = doc['extracted_type']
            if 'extracted_dept' in doc:
                drafter = doc['extracted_dept']

            if date and date != 'ë‚ ì§œ ë¯¸ìƒ' and len(date) >= 10:
                display_date = date[:10]
            display_date = date[:4] if date and len(date) >= 4 else "ë‚ ì§œë¯¸ìƒ"

            response += f"**{i}. [{category}] {title}**\n"
            response += f"    {display_date} |  {drafter}"

            if 'extracted_amount' in doc:
                amount = doc['extracted_amount']
                response += f" | ğŸ’° {amount:,}ì›"

            response += "\n"

            if 'path' in doc:
                try:
                    file_path = Path(doc['path'])
                    if file_path.exists():
                        if self.llm_module:
                            summary = self.llm_module.generate_smart_summary("", str(file_path.name))
                        else:
                            summary = self._generate_smart_summary("", file_path)
                        if summary and summary != "â€¢ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ì¤‘...":
                            response += f"   {summary}\n"
                except Exception:
                    pass

            response += "\n"

        if len(unique_results) > 5:
            response += f"... ì™¸ {len(unique_results) - 5}ê°œ ë¬¸ì„œ ë” ìˆìŒ\n\n"

        response += " **íŠ¹ì • ë¬¸ì„œë¥¼ ì„ íƒí•˜ì—¬ ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.**"

        return response

    def answer_from_specific_document(self, query: str, filename: str) -> str:
        """íŠ¹ì • ë¬¸ì„œì— ëŒ€í•´ì„œë§Œ ë‹µë³€ ìƒì„± (ë¬¸ì„œ ì „ìš© ëª¨ë“œ) - ì´ˆìƒì„¸ ë²„ì „
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            filename: íŠ¹ì • ë¬¸ì„œ íŒŒì¼ëª…
        """
        
        doc_metadata = self._find_metadata_by_filename(filename)
        if not doc_metadata:
            return f" ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
        doc_path = doc_metadata['path']
        
        if filename.endswith('.pdf'):
            info = self._extract_pdf_info_with_retry(doc_path)
            if not info.get('text'):
                return f" PDF ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
            
            if self.llm is None:
                print(" LLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
                self._preload_llm()
            
            full_text = info['text'][:15000]
            
            if any(word in query for word in ['ìš”ì•½', 'ì •ë¦¬', 'ê°œìš”', 'ë‚´ìš©']):
                prompt = self._create_detailed_summary_prompt(query, full_text, filename)
            if any(word in query for word in ['ìƒì„¸', 'ìì„¸íˆ', 'êµ¬ì²´ì ', 'ì„¸ì„¸íˆ', 'ì„¸ë¶€']):
                prompt = self._create_ultra_detailed_prompt(query, full_text, filename)
            if any(word in query for word in ['í’ˆëª©', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'í•­ëª©']):
                prompt = self._create_itemized_list_prompt(query, full_text, filename)
            else:
                prompt = self._create_document_specific_prompt(query, full_text, filename)
            
            try:
                context_chunks = [
                    {
                        'content': full_text,
                        'source': filename,
                        'metadata': {
                            'filename': filename,
                            'date': doc_metadata.get('date', ''),
                            'title': doc_metadata.get('title', ''),
                            'is_document_only_mode': True
                        }
                    }
                ]
                
                response = self.llm.generate_response(query, context_chunks)
                answer = response.answer if hasattr(response, 'answer') else str(response)
                
                if len(answer) < 200 and 'ìì„¸íˆ' in query:
                    answer = self._enhance_short_answer(answer, full_text, query)
                    
            except Exception as e:
                print(f"LLM ì˜¤ë¥˜: {e}")
                answer = self._detailed_text_search(info['text'], query, filename)
            
            answer += f"\n\n **ì¶œì²˜**: {filename}"
            return answer

        return None if filename.endswith('.txt') else f" ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {filename}"
    
    def _simple_text_search(self, text: str, query: str, filename: str) -> str:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (LLM ì—†ì´)"""
        lines = text.split('\n')
        relevant_lines = []
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        
        # ê´€ë ¨ ì¤„ ì°¾ê¸°
        for line in lines:
            if any(keyword in line for keyword in keywords if len(keyword) > 1):
                relevant_lines.append(line.strip())
        
        if relevant_lines:
            return f" {filename}ì—ì„œ ì°¾ì€ ê´€ë ¨ ë‚´ìš©:\n\n" + '\n'.join(relevant_lines[:20])
        else:
            return f" {filename}ì—ì„œ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _detailed_text_search(self, text: str, query: str, filename: str) -> str:
        """ìƒì„¸í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (LLM ì—†ì´)"""
        
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        
        for i, line in enumerate(lines):
            if any(keyword in line for keyword in keywords if len(keyword) > 1):
                start = max(0, i-2)
                end = min(len(lines), i+3)
                section = '\n'.join(lines[start:end])
                if section not in relevant_sections:
                    relevant_sections.append(section)
        
        if relevant_sections:
            return f" **{filename}** ìƒì„¸ ë¶„ì„:\n\n" + '\n---\n'.join(relevant_sections[:10])
        else:
            return f" {filename}ì—ì„œ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _enhance_short_answer(self, answer: str, full_text: str, query: str) -> str:
        """ì§§ì€ ë‹µë³€ì„ ë³´ê°•"""
        enhanced = answer + "\n\n **ì¶”ê°€ ìƒì„¸ ì •ë³´**:\n"
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        lines = full_text.split('\n')
        
        additional_info = []
        for line in lines:
            if any(keyword in line for keyword in keywords if len(keyword) > 2):
                if line.strip() and line not in answer:
                    additional_info.append(f"â€¢ {line.strip()}")
        
        if additional_info:
            enhanced += '\n'.join(additional_info[:10])
        
        return enhanced
    
    def _safe_pdf_extract(self, pdf_path, max_retries=3):
        """ì•ˆì „í•œ PDF ì¶”ì¶œ with ì¬ì‹œë„"""

    def _validate_input(self, query):
        """ì…ë ¥ ê²€ì¦"""
        if not query:
            raise ValueError("ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        if len(query) > 1000:
            logger.warning(f"ì¿¼ë¦¬ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {len(query)}ì")
            query = query[:1000]

        # SQL ì¸ì ì…˜ ë°©ì§€
        dangerous_patterns = ['DROP', 'DELETE', 'INSERT', 'UPDATE', '--', ';']
        for pattern in dangerous_patterns:
            if pattern in query.upper():
                raise ValueError(f"í—ˆìš©ë˜ì§€ ì•Šì€ íŒ¨í„´: {pattern}")

    def __del__(self):
        """ì†Œë©¸ì - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""

    def _batch_process_documents(self, documents, process_func, batch_size=10):
        """ë°°ì¹˜ ë¬¸ì„œ ì²˜ë¦¬ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±"""
        total = len(documents)
        processed = 0
        results = []

        for i in range(0, total, batch_size):
            batch = documents[i:i+batch_size]

            with ThreadPoolExecutor(max_workers=min(len(batch), self.MAX_WORKERS)) as executor:
                futures = [executor.submit(process_func, doc) for doc in batch]

                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=30)
                        if result:
                            results.append(result)
                    except Exception as e:
                        logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

            processed += len(batch)
            logger.info(f"ì§„í–‰ë¥ : {processed}/{total} ({100*processed/total:.1f}%)")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if processed % 50 == 0:
                import gc
                gc.collect()

        return results

    def cleanup_executor(self):
        """ë³‘ë ¬ ì²˜ë¦¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""

    def _create_prompt(self, query: str, context: str, filename: str, prompt_type: str = 'default') -> str:
        """í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜"""
        if prompt_type == 'detailed':
            return f"""

ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìƒì„¸íˆ ë‹µë³€í•˜ì„¸ìš”:
{context}

ì¤‘ìš” ì •ë³´ëŠ” **êµµê²Œ** í‘œì‹œí•˜ê³ , ê´€ë ¨ ë‚´ìš©ì„ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”.
"""
        else:  # default
            return f"""

ë¬¸ì„œ ë‚´ìš©:
{context}

ìœ„ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
"""
    
    def _get_enhanced_cache_key(self, query: str, mode: str) -> str:
        """í–¥ìƒëœ ìºì‹œ í‚¤ ìƒì„± - ìœ ì‚¬ ì§ˆë¬¸ë„ ìºì‹œ íˆíŠ¸

        ì˜ˆì‹œ:
        - "2020ë…„ êµ¬ë§¤ ë¬¸ì„œ" â†’ "2020 êµ¬ë§¤ ë¬¸ì„œ"
        - "2020ë…„ì˜ êµ¬ë§¤í•œ ë¬¸ì„œë¥¼" â†’ "2020 êµ¬ë§¤ ë¬¸ì„œ"
        - "êµ¬ë§¤ 2020ë…„ ë¬¸ì„œ" â†’ "2020 êµ¬ë§¤ ë¬¸ì„œ" (ì •ë ¬)
        """

        # 1. ì¿¼ë¦¬ ì •ê·œí™” ë° ì†Œë¬¸ì ë³€í™˜
        normalized = query.strip().lower()

        # 2. í•œê¸€ ì¡°ì‚¬ ì œê±° - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜
        # ë³µí•© ì¡°ì‚¬ë¶€í„° ì œê±° (ì—ì„œëŠ”, ìœ¼ë¡œëŠ” ë“±)
        compound_particles = ['ì—ì„œëŠ”', 'ìœ¼ë¡œëŠ”', 'ì—ê²ŒëŠ”', 'í•œí…ŒëŠ”', 'ì—ì„œë„', 'ìœ¼ë¡œë„',
                            'ì´ë¼ë„', 'ì´ë‚˜ë§ˆ', 'ì´ë“ ì§€', 'ì´ë¼ëŠ”', 'ê¹Œì§€ë„', 'ë¶€í„°ë„']
        simple_particles = ['ì—ì„œ', 'ìœ¼ë¡œ', 'ì´ë‚˜', 'ì´ë“ ', 'ì´ë©´', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜ì„œ',
                          'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ì—',
                          'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ë§ˆë‹¤', 'ë§ˆì €', 'ì¡°ì°¨']

        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ í›„ ì¡°ì‚¬ ì œê±°
        words = normalized.split()
        cleaned_words = []

        for word in words:
            cleaned_word = word

            # ë³µí•© ì¡°ì‚¬ ë¨¼ì € ì œê±°
            for particle in compound_particles:
                if cleaned_word.endswith(particle):
                    cleaned_word = cleaned_word[:-len(particle)]
                    break

            # ë‹¨ìˆœ ì¡°ì‚¬ ì œê±°
            if cleaned_word:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ˆë©´
                for particle in simple_particles:
                    if cleaned_word.endswith(particle):
                        cleaned_word = cleaned_word[:-len(particle)]
                        break

            # 2ê¸€ì ì´ìƒë§Œ í¬í•¨
            if cleaned_word and len(cleaned_word) >= 2:
                cleaned_words.append(cleaned_word)

        # 3. í‚¤ì›Œë“œ ì •ë ¬ (ìˆœì„œ ë¬´ê´€ ìºì‹œ íˆíŠ¸)
        cleaned_words.sort()

        # 4. ìºì‹œ í‚¤ ìƒì„±
        cache_str = f"{mode}:{'_'.join(cleaned_words)}"
        hash_key = hashlib.md5(cache_str.encode('utf-8')).hexdigest()

        # ë””ë²„ê¹… (í•„ìš”ì‹œ í™œì„±í™”)

        return hash_key

    def answer(self, query: str, mode: str = 'auto') -> str:
        """ë‹µë³€ ìƒì„± ë©”ì„œë“œ"""

        if cache_key in self.answer_cache:
            cached_response, cached_time = self.answer_cache[cache_key]
            if time.time() - cached_time < self.cache_ttl:
                self.answer_cache.move_to_end(cache_key)
                return cached_response
            else:
                del self.answer_cache[cache_key]

        response = self._answer_internal(query, mode)

        self.answer_cache[cache_key] = (response, time.time())
        if len(self.answer_cache) > self.max_cache_size:
            self.answer_cache.popitem(last=False)

        return response
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        # CacheModuleì„ ì‚¬ìš©í•˜ì—¬ ìºì‹œ ì´ˆê¸°í™”
        if self.cache_module:
            self.cache_module.clear_all_cache()
            return

        # CacheModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        self.answer_cache.clear()
        self.documents_cache.clear()
        self.metadata_cache.clear()
    
    def get_cache_stats(self) -> Dict:
        """ìºì‹œ ìƒíƒœ ì •ë³´ ë°˜í™˜"""

        response_cache_size = len(self.response_cache) if hasattr(self, 'response_cache') else 0
        total_size = len(self.answer_cache) + len(self.documents_cache) + len(self.metadata_cache) + response_cache_size
        return {
            'size': response_cache_size,
            'hits': self.cache_hits,
            'misses': self.cache_misses,
            'answer_cache_size': len(self.answer_cache),
            'document_cache_size': len(self.documents_cache),
            'metadata_cache_size': len(self.metadata_cache),
            'response_cache_size': response_cache_size,
            'total_cache_size': total_size,
            'max_cache_size': self.max_cache_size,
            'cache_ttl_seconds': self.cache_ttl
        }
    
    def _answer_internal(self, query: str, mode: str = 'auto') -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            mode: ê²€ìƒ‰ ëª¨ë“œ (í•­ìƒ 'document' ì‚¬ìš©)
        """
        
        start_time = time.time()
        error_msg = None
        success = True
        response = ""
        metadata = {}
        
        try:
            
            if mode == 'auto':
                with TimerContext(chat_logger, "classify_intent") if chat_logger else nullcontext():
                    if self.intent_module:
                        mode = self.intent_module.classify_search_intent(query)
                    else:
                        mode = self._classify_search_intent(query)
            
            self.search_mode = mode
            metadata['search_mode'] = mode
            
            query_lower = query.lower()

            if self.search_mode == 'document':
                if any(keyword in query_lower for keyword in ["ë¬¸ì„œ", "ì°¾ì•„", "ê²€ìƒ‰", "ì–´ë–¤", "ë¬´ì—‡", "ë­"]):
                    search_results = self._search_by_content(query)

                    if not search_results:
                        response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                    else:
                        response = f"**{query}** ê²€ìƒ‰ ê²°ê³¼\n\n"

                        seen = set()
                        unique_results = []
                        for r in sorted(search_results, key=lambda x: x.get('score', 0), reverse=True):
                            if r['filename'] not in seen:
                                seen.add(r['filename'])
                                unique_results.append(r)

                        response += f"ì´ {len(unique_results)}ê°œ ë¬¸ì„œ ë°œê²¬\n\n"

                        for i, result in enumerate(unique_results[:10], 1):
                            response += f"**{i}. {result['filename']}**\n"

                            if result.get('source') == 'everything_search':
                                if result.get('date'):
                                    response += f"   ğŸ“… ë‚ ì§œ: {result['date']}\n"
                                if result.get('category') and result['category'] != 'ê¸°íƒ€':
                                    response += f"   ğŸ“ ì¹´í…Œê³ ë¦¬: {result['category']}\n"
                                if result.get('keywords'):
                                    keywords_list = result['keywords'].split()[:5]
                                    if keywords_list:
                                        response += f"   ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(keywords_list)}\n"
                            elif result.get('context'):
                                context = result['context']
                                if '[OCR' in context or 'í˜ì´ì§€' in context:
                                    response += f"   ğŸ“„ ìŠ¤ìº” ë¬¸ì„œ (OCR í•„ìš”)\n"
                                else:
                                    clean_text = context.replace('\n', ' ').strip()[:150]
                                    response += f"   ğŸ“ {clean_text}...\n"
                            response += "\n"

                        if len(unique_results) > 5:
                            response += f"\n... ì™¸ {len(unique_results) - 5}ê°œ ë¬¸ì„œ\n"
                else:
                    if self.everything_search:
                        search_results = self.everything_search.search(query, limit=1)
                        if search_results:
                            top_result = search_results[0]
                            doc_path = Path(top_result['path'])

                            if doc_path.exists():
                                if self.llm_module:
                                    try:
                                        if self.document_module:
                                            doc_info = self.document_module.extract_pdf_text(doc_path)
                                            content = doc_info.get('text', '')
                                        else:
                                            content = self._extract_full_pdf_content(doc_path).get('text', '')

                                        response = self.llm_module.generate_smart_summary(content, str(doc_path.name))
                                    except Exception as e:
                                        logger.error(f"LLMModule ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
                                        response = self._generate_llm_summary(doc_path, query)
                                else:
                                    response = self._generate_llm_summary(doc_path, query)
                            else:
                                response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                        else:
                            response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                    else:
                        doc_path = self.find_best_document(query)

                        if not doc_path:
                            response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                        elif self.llm_module:
                            try:
                                if self.document_module:
                                    doc_info = self.document_module.extract_pdf_text(doc_path)
                                    content = doc_info.get('text', '')
                                else:
                                    content = self._extract_full_pdf_content(doc_path).get('text', '')

                                response = self.llm_module.generate_smart_summary(content, str(doc_path.name))
                            except Exception as e:
                                logger.error(f"LLMModule ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
                                response = self._generate_llm_summary(doc_path, query)
                        else:
                            response = self._generate_llm_summary(doc_path, query)
            else:
                response = "âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            processing_time = time.time() - start_time
            if chat_logger:
                chat_logger.log_query(
                    query=query,
                    response=response,
                    search_mode=self.search_mode,
                    processing_time=processing_time,
                    metadata=metadata
                )

            return response

        except Exception as e:
            error_msg = str(e)
            success = False
            response = f" ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}"
            
            processing_time = time.time() - start_time
            
            if chat_logger:
                chat_logger.log_error(
                    error_type=type(e).__name__,
                    error_msg=error_msg,
                    query=query
                )
            
            return response
    
    def _get_detail_only_prompt(self, query: str, context: str, filename: str) -> str:
        return self._create_prompt(query, context, filename, 'detailed')

    def _placeholder1(self):
        """ê¸°ë³¸ ì •ë³´ ì œì™¸í•œ ìƒì„¸ ë‚´ìš©ë§Œ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸"""
        return f"""

 **êµ¬ë§¤/ìˆ˜ë¦¬ ì‚¬ìœ  ë° í˜„í™©**
â€¢ ì–´ë–¤ ë¬¸ì œê°€ ìˆì—ˆëŠ”ì§€
â€¢ í˜„ì¬ ìƒí™©ì€ ì–´ë–¤ì§€
â€¢ ì œì•ˆí•˜ëŠ” í•´ê²°ì±…ì€ ë¬´ì—‡ì¸ì§€

 **ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­** (ìˆëŠ” ê²½ìš°ë§Œ)
â€¢ ì¥ë¹„ ì‚¬ì–‘ì´ë‚˜ ëª¨ë¸
â€¢ ê²€í† í•œ ëŒ€ì•ˆë“¤
â€¢ ì„ íƒ ê·¼ê±°

 **ë¹„ìš© ê´€ë ¨** (ìˆëŠ” ê²½ìš°ë§Œ)
â€¢ ì˜ˆìƒ ë¹„ìš©
â€¢ ì—…ì²´ ì •ë³´

ë¬¸ì„œ: {filename}
ë‚´ìš©: {context[:5000]}

ì§ˆë¬¸: {query}

ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
"""

    def _is_gian_document(self, text: str) -> bool:
        """ê¸°ì•ˆì„œ ë¬¸ì„œì¸ì§€ í™•ì¸"""
    
    def _try_ocr_extraction(self, pdf_path: Path) -> str:
        """OCRì„ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„"""
        try:
            # OCR í”„ë¡œì„¸ì„œ ì„í¬íŠ¸
            from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
            
            ocr_processor = EnhancedOCRProcessor()
            text, metadata = ocr_processor.extract_text_with_ocr(str(pdf_path))
            
            if metadata.get('ocr_performed'):
                if logger:
                    logger.info(f"OCR ì„±ê³µ: {pdf_path.name} - {metadata.get('ocr_text_length', 0)}ì ì¶”ì¶œ")
                return text
            else:
                if logger:
                    logger.warning(f"OCR ì‹¤íŒ¨: {pdf_path.name}")
                return ""
                
        except ImportError:
            if logger:
                logger.warning("OCR ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€ - pytesseract ë˜ëŠ” Tesseract ë¯¸ì„¤ì¹˜")
            return ""
        except Exception: return None
    
    def _extract_full_pdf_content(self, pdf_path: Path) -> dict:
        """PDF ì „ì²´ ë‚´ìš© ì¶”ì¶œ ë° êµ¬ì¡°í™” - DocumentModule í™œìš©ìœ¼ë¡œ ë‹¨ìˆœí™”"""
        try:
            return self._extract_pdf_info(pdf_path)
        except Exception:
            return None
    
    def _prepare_formatted_data(self, pdf_info: Dict, pdf_path: Path) -> Dict:
        """í¬ë§·í„°ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        formatted_info = {
            'ì œëª©': pdf_info.get('ì œëª©', pdf_path.stem),
            'ê¸°ì•ˆì': pdf_info.get('ê¸°ì•ˆì', ''),
            'ê¸°ì•ˆì¼ì': pdf_info.get('ê¸°ì•ˆì¼ì', ''),
            'ê¸°ì•ˆë¶€ì„œ': pdf_info.get('ê¸°ì•ˆë¶€ì„œ', '')
        }
        
        # í•µì‹¬ ìš”ì•½ ì¶”ì¶œ (3ì¤„)
        if self.formatter and pdf_info.get('ì „ì²´í…ìŠ¤íŠ¸'):
            key_points = self.formatter.extract_key_points(pdf_info['ì „ì²´í…ìŠ¤íŠ¸'])
            formatted_info['í•µì‹¬ìš”ì•½'] = key_points
        
        # ìƒì„¸ ë‚´ìš© êµ¬ì¡°í™”
        detail_content = []
        
        # ê°œìš”ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if pdf_info.get('ê°œìš”'):
            detail_content.append({
                'í•­ëª©': 'ê°œìš”',
                'ë‚´ìš©': pdf_info['ê°œìš”'][:200]
            })
        
        # ì¥ì• /ìˆ˜ë¦¬ ë‚´ìš©
        if any(k in pdf_info for k in ['ì¥ì• ë‚´ìš©', 'ìˆ˜ë¦¬ë‚´ìš©', 'êµ¬ë§¤ì‚¬ìœ ']):
            for key in ['ì¥ì• ë‚´ìš©', 'ìˆ˜ë¦¬ë‚´ìš©', 'êµ¬ë§¤ì‚¬ìœ ']:
                if pdf_info.get(key):
                    detail_content.append({
                        'í•­ëª©': key,
                        'ë‚´ìš©': pdf_info[key][:200]
                    })
        
        formatted_info['ìƒì„¸ë‚´ìš©'] = detail_content
        
        # ë¹„ìš© ì •ë³´
        if pdf_info.get('ë¹„ìš©ë‚´ì—­'):
            formatted_info['ë¹„ìš©ì •ë³´'] = pdf_info['ë¹„ìš©ë‚´ì—­']
        
        # ê²€í†  ì˜ê²¬
        opinions = []
        if pdf_info.get('ê²€í† ê²°ê³¼'):
            opinions.append(pdf_info['ê²€í† ê²°ê³¼'])
        if pdf_info.get('ì¶”ì²œì‚¬í•­'):
            opinions.append(pdf_info['ì¶”ì²œì‚¬í•­'])
        if opinions:
            formatted_info['ê²€í† ì˜ê²¬'] = opinions
        
        # ê´€ë ¨ ì •ë³´
        related = []
        if pdf_info.get('ì—…ì²´'):
            related.append(f"ì—…ì²´: {pdf_info['ì—…ì²´']}")
        if pdf_info.get('ë„ì…ì—°ë„'):
            related.append(f"ë„ì…: {pdf_info['ë„ì…ì—°ë„']}ë…„")
        if related:
            formatted_info['ê´€ë ¨ì •ë³´'] = related
        
        return formatted_info
    
    def _extract_key_sentences(self, content, num_sentences=5):
        """í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ í—¬í¼"""

        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]

        if len(sentences) <= num_sentences:
            return sentences

        important_keywords = ['ê²°ì •', 'ìŠ¹ì¸', 'êµ¬ë§¤', 'ê³„ì•½', 'ì˜ˆì‚°', 'ì§„í–‰', 'ì™„ë£Œ']
        scored_sentences = []

        for sentence in sentences:
            score = sum(1 for keyword in important_keywords if keyword in sentence)
            scored_sentences.append((sentence, score))

        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        return [s[0] for s in scored_sentences[:num_sentences]]

    def _generate_llm_summary(self, pdf_path: Path, query: str) -> str:
        """LLMì„ ì‚¬ìš©í•œ ìƒì„¸ ìš”ì•½ - LLMModuleë¡œ ìœ„ì„ (2025-09-29 ë¦¬íŒ©í† ë§)"""
        if logger:
            logger.info("LLM ìš”ì•½ ìƒì„± ì‹œì‘")

        # LLMModuleì´ ìˆìœ¼ë©´ ìœ„ì„
        if self.llm_module:
            try:
                # PDF ë‚´ìš© ì¶”ì¶œ
                pdf_info = self._extract_full_pdf_content(pdf_path)
                content = pdf_info.get('text', '') if pdf_info and 'error' not in pdf_info else ''

                # LLMModuleì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤ë§ˆíŠ¸ ìš”ì•½ ìƒì„±
                return self.llm_module.generate_smart_summary(content, str(pdf_path.name), query)
            except Exception as e:
                if logger:
                    logger.error(f"LLMModule ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}, í´ë°± ì‚¬ìš©")

        # ê°„ë‹¨í•œ í´ë°± êµ¬í˜„
        pdf_info = self._extract_full_pdf_content(pdf_path)
        if pdf_info and 'error' not in pdf_info:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            info_parts = []
            if 'ì œëª©' in pdf_info:
                info_parts.append(f"ì œëª©: {pdf_info['ì œëª©']}")
            if 'ê¸°ì•ˆì' in pdf_info:
                info_parts.append(f"ê¸°ì•ˆì: {pdf_info['ê¸°ì•ˆì']}")
            if 'ê¸ˆì•¡' in pdf_info:
                info_parts.append(f"ê¸ˆì•¡: {pdf_info['ê¸ˆì•¡']}")

            return "\n".join(info_parts) if info_parts else "ë¬¸ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            return "ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def _collect_statistics_data(self, query: str) -> Dict:
        """í†µê³„ ë°ì´í„° ìˆ˜ì§‘ ë° êµ¬ì¡°í™” - StatisticsModuleë¡œ ìœ„ì„ (2025-09-29 ë¦¬íŒ©í† ë§)"""

        return {
            'title': 'í†µê³„ ë¶„ì„',
            'headers': ['í•­ëª©', 'ê°’'],
            'table_data': [['ì´ ë¬¸ì„œ ìˆ˜', str(len(self.metadata_cache))]],
            'ì´ê³„': f'ì´ {len(self.metadata_cache)}ê°œ ë¬¸ì„œ',
            'ë¶„ì„': {'note': 'StatisticsModule í•„ìš”'},
            'ì¶”ì²œ': ['StatisticsModuleì„ ë¡œë“œí•˜ì—¬ ìƒì„¸ í†µê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”.']
        }
    
    def _format_conditions(self, conditions: dict) -> str:
        """ê²€ìƒ‰ ì¡°ê±´ì„ ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…"""
        formatted = []
        
        if 'location' in conditions:
            formatted.append(f"â€¢ ìœ„ì¹˜: {conditions['location']}")
        
        if 'manufacturer' in conditions:
            formatted.append(f"â€¢ ì œì¡°ì‚¬: {conditions['manufacturer']}")
        
        if 'year' in conditions:
            year_str = f"â€¢ ì—°ë„: {conditions['year']}ë…„"
            if conditions.get('year_range') == 'before':
                year_str += " ì´ì „"
            if conditions.get('year_range') == 'after':
                year_str += " ì´í›„"
            if conditions.get('year_range') == 'between':
                year_str = f"â€¢ ì—°ë„: {conditions.get('year_start')}ë…„ ~ {conditions.get('year_end')}ë…„"
            formatted.append(year_str)
        
        if 'price' in conditions:
            price_str = f"â€¢ ê¸ˆì•¡: {conditions['price']:,.0f}ì›"
            if conditions.get('price_range') == 'above':
                price_str += " ì´ìƒ"
            if conditions.get('price_range') == 'below':
                price_str += " ì´í•˜"
            formatted.append(price_str)
        
        if 'equipment_type' in conditions:
            formatted.append(f"â€¢ ì¥ë¹„ ìœ í˜•: {conditions['equipment_type']}")
        
        if 'model' in conditions:
            formatted.append(f"â€¢ ëª¨ë¸: {conditions['model']}")
        
        return '\n'.join(formatted) if formatted else "ì¡°ê±´ ì—†ìŒ"
    
    def _determine_equipment_category(self, equipment_name: str, item_text: str) -> str:
        """ì¥ë¹„ëª…ê³¼ í…ìŠ¤íŠ¸ë¡œ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
        
        if any(kw in name_lower or kw in text_lower for kw in ['camera', 'ì¹´ë©”ë¼', 'ccu', 'viewfinder', 'ë·°íŒŒì¸ë”']):
            return " ì¹´ë©”ë¼ ì‹œìŠ¤í…œ"
        if any(kw in name_lower or kw in text_lower for kw in ['monitor', 'ëª¨ë‹ˆí„°', 'display']):
            return "ï¸ ëª¨ë‹ˆí„°"
        if any(kw in name_lower or kw in text_lower for kw in ['audio', 'ì˜¤ë””ì˜¤', 'mixer', 'ë¯¹ì„œ', 'mic', 'ë§ˆì´í¬']):
            return "ï¸ ì˜¤ë””ì˜¤ ì¥ë¹„"
        if any(kw in name_lower or kw in text_lower for kw in ['server', 'ì„œë²„', 'storage', 'ìŠ¤í† ë¦¬ì§€']):
            return " ì„œë²„/ìŠ¤í† ë¦¬ì§€"
        if any(kw in name_lower or kw in text_lower for kw in ['switch', 'ìŠ¤ìœ„ì¹˜', 'router', 'ë¼ìš°í„°', 'matrix']):
            return " ìŠ¤ìœ„ì¹­/ë¼ìš°íŒ…"
        if any(kw in name_lower or kw in text_lower for kw in ['cable', 'ì¼€ì´ë¸”', 'connector', 'ì»¤ë„¥í„°']):
            return " ì¼€ì´ë¸”/ì»¤ë„¥í„°"
        if any(kw in name_lower or kw in text_lower for kw in ['tripod', 'íŠ¸ë¼ì´í¬ë“œ', 'pedestal', 'í˜ë°ìŠ¤íƒˆ']):
            return " ì¹´ë©”ë¼ ì§€ì›ì¥ë¹„"
        if any(kw in name_lower or kw in text_lower for kw in ['intercom', 'ì¸í„°ì»´', 'talkback']):
            return " ì¸í„°ì»´"
        if any(kw in name_lower or kw in text_lower for kw in ['converter', 'ì»¨ë²„í„°', 'encoder', 'ì¸ì½”ë”']):
            return " ì»¨ë²„í„°/ì¸ì½”ë”"
        return " ê¸°íƒ€ ì¥ë¹„"

    def _count_by_field(self, content: str, field_name: str, search_value: str) -> dict:
        """íŠ¹ì • í•„ë“œê°’ìœ¼ë¡œ ì¥ë¹„ ìˆ˜ëŸ‰ ê³„ì‚°"""
        lines = content.split('\n')
        count = 0
        items = []
        current_item = []
        is_matching = False
        
        for line in lines:
            # ìƒˆë¡œìš´ ì¥ë¹„ í•­ëª© ì‹œì‘
            if re.match(r'^\[\d{4}\]', line):
                # ì´ì „ í•­ëª©ì´ ë§¤ì¹­ë˜ì—ˆìœ¼ë©´ ì¹´ìš´íŠ¸
                if is_matching and current_item:
                    count += 1
                    if len(items) < 10:  # ìƒ˜í”Œ 10ê°œë§Œ ì €ì¥
                        items.append('\n'.join(current_item))
                
                # ìƒˆ í•­ëª© ì‹œì‘
                current_item = [line]
                is_matching = False
            if current_item:
                current_item.append(line)
                # í•„ë“œë³„ ë§¤ì¹­ í™•ì¸
                if field_name == "ë‹´ë‹¹ì" and "ë‹´ë‹¹ì:" in line:
                    if search_value in line:
                        is_matching = True
                if field_name == "ìœ„ì¹˜" and "ìœ„ì¹˜:" in line:
                    # ì •í™•í•œ ìœ„ì¹˜ ë§¤ì¹­ ë¡œì§ ì ìš©
                    location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', line)
                    if location_match:
                        actual_location = location_match.group(1).strip()
                        
                        # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™
                        if search_value == actual_location:
                            # ì™„ì „ ì¼ì¹˜
                            is_matching = True
                        if search_value == 'ë¶€ì¡°ì •ì‹¤':
                            # 'ë¶€ì¡°ì •ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*ë¶€ì¡°ì •ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                            is_matching = actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
                        if search_value == 'ìŠ¤íŠœë””ì˜¤':
                            # 'ìŠ¤íŠœë””ì˜¤'ë¡œ ê²€ìƒ‰ì‹œ '*ìŠ¤íŠœë””ì˜¤' íŒ¨í„´ë§Œ ë§¤ì¹­ 
                            is_matching = actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
                        if search_value == 'í¸ì§‘ì‹¤':
                            # 'í¸ì§‘ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*í¸ì§‘ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                            is_matching = actual_location.endswith('í¸ì§‘ì‹¤')
                        if search_value in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
                            # ì¤‘ê³„ì°¨ ê²€ìƒ‰ì‹œ Van ê´€ë ¨ ìœ„ì¹˜ ëª¨ë‘ ë§¤ì¹­
                            is_matching = 'Van' in actual_location or 'VAN' in actual_location
                        if len(search_value) > 3:
                            # 3ê¸€ì ì´ìƒì˜ êµ¬ì²´ì ì¸ ìœ„ì¹˜ëª…ì€ ë¶€ë¶„ ë§¤ì¹­ í—ˆìš©
                            is_matching = search_value in actual_location
                if field_name == "ë²¤ë”ì‚¬" and "ë²¤ë”ì‚¬:" in line:
                    if search_value in line:
                        is_matching = True
                if field_name == "ì œì¡°ì‚¬" and "ì œì¡°ì‚¬:" in line:
                    if search_value.upper() in line.upper():
                        is_matching = True
        
        # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
        if is_matching and current_item:
            count += 1
            if len(items) < 10:
                items.append('\n'.join(current_item))
        
        return {
            'count': count,
            'sample_items': items
        }
    
    def _extract_document_metadata(self, file_path):
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í—¬í¼"""

        try:
            filename = file_path.stem if hasattr(file_path, 'stem') else str(file_path)

            date_patterns = [
                r'(\d{4})[.\-_](\d{1,2})[.\-_](\d{1,2})',
                r'(\d{4})(\d{2})(\d{2})',
                r'(\d{2})[.\-_](\d{1,2})[.\-_](\d{1,2})'
            ]
            for pattern in date_patterns:
                match = re.search(pattern, filename)
                if match:
                    metadata['date'] = match.group(0)
                    break

            author_patterns = [
                r'([ê°€-í£]{2,4})([\s_\-])?ê¸°ì•ˆ',
                r'ê¸°ì•ˆì[\s_\-:]*([ê°€-í£]{2,4})',
                r'ì‘ì„±ì[\s_\-:]*([ê°€-í£]{2,4})'
            ]
            for pattern in author_patterns:
                match = re.search(pattern, filename)
                if match:
                    metadata['author'] = match.group(1) if 'ê¸°ì•ˆ' in pattern else match.group(1)
                    break

            return metadata
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {}

    def _score_document_relevance(self, content, keywords):
        """ë¬¸ì„œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° í—¬í¼"""
        if not content or not keywords:
            return 0

        score = 0
        content_lower = content.lower()

        for keyword in keywords:
            keyword_lower = keyword.lower()
            # ì •í™•í•œ ë§¤ì¹­
            exact_matches = content_lower.count(keyword_lower)
            score += exact_matches * 2

            # ë¶€ë¶„ ë§¤ì¹­
            if len(keyword_lower) > 2:
                partial_matches = sum(1 for word in content_lower.split()
                                    if keyword_lower in word)
                score += partial_matches

        # ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™”
        doc_length = len(content)
        if doc_length > 0:
            score = score / (doc_length / 1000)  # 1000ì ë‹¨ìœ„ë¡œ ì •ê·œí™”

        return score

    def _format_search_result(self, file_path, content, metadata):
        """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ… í—¬í¼"""

        filename = file_path.stem if hasattr(file_path, 'stem') else str(file_path)
        result.append(f"ğŸ“„ {filename}")
        result.append("-" * 50)

        if metadata.get('date'):
            result.append(f"ğŸ“… ë‚ ì§œ: {metadata['date']}")
        if metadata.get('author'):
            result.append(f"âœï¸ ê¸°ì•ˆì: {metadata['author']}")

        if content:
            summary = content[:200].replace('\n', ' ')
            result.append(f"\nğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
            result.append(summary + "...")

        return '\n'.join(result)

    def _aggregate_search_results(self, results):
        """ê²€ìƒ‰ ê²°ê³¼ í†µí•© í—¬í¼"""
        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        aggregated = []
        aggregated.append(f"ğŸ” ì´ {len(results)}ê°œ ë¬¸ì„œ ë°œê²¬\n")
        aggregated.append("=" * 60)

        for i, result in enumerate(results, 1):
            aggregated.append(f"\n[{i}] {result}")
            if i < len(results):
                aggregated.append("\n" + "-" * 60)

        return '\n'.join(aggregated)

    def _is_location_match(self, item_lines: list, location: str) -> bool:
        """ìœ„ì¹˜ ë§¤ì¹­ ë¡œì§ ê°œì„  - ì •í™•í•œ ìœ„ì¹˜ ë§¤ì¹­"""
        
        for line in item_lines:
            if 'ìœ„ì¹˜:' in line or 'ìœ„ì¹˜ì •ë³´:' in line:
                location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', line)
                if location_match:
                    actual_location = location_match.group(1).strip()
                    
                    if location == actual_location:
                        return True
                    if location == 'ë¶€ì¡°ì •ì‹¤':
                        return actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
                    if location == 'ìŠ¤íŠœë””ì˜¤':
                        return actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
                    if location == 'í¸ì§‘ì‹¤':
                        return actual_location.endswith('í¸ì§‘ì‹¤')
                    if location in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
                        return 'Van' in actual_location or 'VAN' in actual_location
                    if location == "ê´‘í™”ë¬¸ë¶€ì¡°ì •ì‹¤":
                        return "ê´‘í™”ë¬¸" in actual_location and "ë¶€ì¡°ì •ì‹¤" in actual_location
                    if len(location) > 3:
                        return location in actual_location
        
        return False

    def _check_location_in_item(self, item_text: str, search_location: str) -> bool:
        """í•­ëª©ì—ì„œ ìœ„ì¹˜ ì¡°ê±´ í™•ì¸"""
        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
        location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', item_text)
        if not location_match:
            return False
            
        actual_location = location_match.group(1).strip()
        
        # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™ ì ìš©
        if search_location == actual_location:
            return True
        if search_location == 'ë¶€ì¡°ì •ì‹¤':
            return actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
        if search_location == 'ìŠ¤íŠœë””ì˜¤':
            return actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
        if search_location == 'í¸ì§‘ì‹¤':
            return actual_location.endswith('í¸ì§‘ì‹¤')
        if search_location in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
            return 'Van' in actual_location or 'VAN' in actual_location or 'ì¤‘ê³„ì°¨' in actual_location
        if len(search_location) > 3:
            return search_location in actual_location
        
        return False
