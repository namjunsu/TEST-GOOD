#!/usr/bin/env python3
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import threading
from queue import Queue
from functools import partial

"""
Perfect RAG - ì‹¬í”Œí•˜ì§€ë§Œ ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
ë™ì ìœ¼ë¡œ ëª¨ë“  PDF ë¬¸ì„œì™€ ìì‚° ë°ì´í„°ë¥¼ ì •í™•í•˜ê²Œ ì²˜ë¦¬
"""

import re
import warnings
import logging
import pdfplumber
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import json
import sys
import os
from functools import lru_cache
import hashlib
from contextlib import nullcontext

# ë¡œê¹… ì‹œìŠ¤í…œ ì¶”ê°€
try:
    from log_system import get_logger, TimerContext
    chat_logger = get_logger()
    # chat_loggerë¥¼ loggerë¡œë„ ì‚¬ìš©
    logger = chat_logger
except ImportError:
    chat_logger = None
    logger = None
    TimerContext = None
    
# query_loggerëŠ” log_systemìœ¼ë¡œ í†µí•©ë¨

# ì‘ë‹µ í¬ë§·í„° ì¶”ê°€
try:
    from response_formatter import ResponseFormatter
except ImportError:
    ResponseFormatter = None

# FontBBox ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", message=".*FontBBox.*")
warnings.filterwarnings("ignore", category=UserWarning, module="pdfplumber")

# pdfplumber ë¡œê¹… ë ˆë²¨ ì¡°ì •
logging.getLogger("pdfplumber").setLevel(logging.ERROR)
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ê¸°
current_dir = Path(__file__).parent.absolute()
sys.path.insert(0, str(current_dir))

# ì„¤ì • íŒŒì¼ import
try:
    from config_manager import config_manager as cfg
    USE_YAML_CONFIG = True
except ImportError:
    import config as cfg
    USE_YAML_CONFIG = False

from rag_system.qwen_llm import QwenLLM
from rag_system.llm_singleton import LLMSingleton
from metadata_db import MetadataDB  # Phase 1.2: ë©”íƒ€ë°ì´í„° DB

# Everything-like ì´ˆê³ ì† ê²€ìƒ‰ ì‹œìŠ¤í…œ ì¶”ê°€
try:
    from everything_like_search import EverythingLikeSearch
    EVERYTHING_SEARCH_AVAILABLE = True
except ImportError:
    EVERYTHING_SEARCH_AVAILABLE = False
    if logger:
        logger.warning("EverythingLikeSearch not available, using legacy search")

# ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œìŠ¤í…œ ì¶”ê°€ (2025-09-29)
try:
    from metadata_extractor import MetadataExtractor
    METADATA_EXTRACTOR_AVAILABLE = True
except ImportError:
    METADATA_EXTRACTOR_AVAILABLE = False
    if logger:
        logger.warning("MetadataExtractor not available, metadata extraction disabled")

# ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆë“¤ (2025-09-29)
try:
    from search_module import SearchModule
    SEARCH_MODULE_AVAILABLE = True
except ImportError:
    SEARCH_MODULE_AVAILABLE = False
    if logger:
        logger.warning("SearchModule not available - using embedded search")

try:
    from document_module import DocumentModule
    DOCUMENT_MODULE_AVAILABLE = True
except ImportError:
    DOCUMENT_MODULE_AVAILABLE = False
    if logger:
        logger.warning("DocumentModule not available - using embedded document processing")

try:
    from llm_module import LLMModule
    LLM_MODULE_AVAILABLE = True
except ImportError:
    LLM_MODULE_AVAILABLE = False
    if logger:
        logger.warning("LLMModule not available - using embedded LLM handling")

try:
    from cache_module import CacheModule
    CACHE_MODULE_AVAILABLE = True
except ImportError:
    CACHE_MODULE_AVAILABLE = False
    if logger:
        logger.warning("CacheModule not available - using embedded cache management")

try:
    from statistics_module import StatisticsModule
    STATISTICS_MODULE_AVAILABLE = True
except ImportError:
    STATISTICS_MODULE_AVAILABLE = False
    if logger:
        logger.warning("StatisticsModule not available - using embedded statistics")

try:
    from intent_module import IntentModule
    INTENT_MODULE_AVAILABLE = True
except ImportError:
    INTENT_MODULE_AVAILABLE = False
    if logger:
        logger.warning("IntentModule not available - using embedded intent analysis")

import traceback

# ë¡œê¹… ì„¤ì • - ì´ë¯¸ ìƒë‹¨ì—ì„œ loggerê°€ ì„¤ì •ë¨
# loggerê°€ Noneì¸ ê²½ìš° (log_system import ì‹¤íŒ¨ ì‹œ) í‘œì¤€ ë¡œê¹… ì‚¬ìš©
if logger is None:
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('perfect_rag.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)
class RAGException(Exception):
    """RAG ì‹œìŠ¤í…œ ê¸°ë³¸ ì˜ˆì™¸"""
    pass

class DocumentNotFoundException(RAGException):
    """ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ"""
    pass

class PDFExtractionException(RAGException):
    """PDF ì¶”ì¶œ ì‹¤íŒ¨"""
    pass

class LLMException(RAGException):
    """LLM ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class CacheException(RAGException):
    """ìºì‹œ ê´€ë ¨ ì˜¤ë¥˜"""
    pass
def handle_errors(default_return=None):
    """ì—ëŸ¬ ì²˜ë¦¬ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except RAGPDFExtractionException as e:
                logger.error(f"{func.__name__} - RAG ì˜¤ë¥˜: {str(e)}")
                if default_return is not None:
                    return default_return
                raise
            except PDFExtractionException as e:
                logger.error(f"{func.__name__} - ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}", exc_info=True)
                if default_return is not None:
                    return default_return
                raise RAGPDFExtractionException(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return wrapper
    return decorator

class PerfectRAG:
    """ì •í™•í•˜ê³  ì‹¬í”Œí•œ RAG ì‹œìŠ¤í…œ"""

    def _load_performance_config(self):
        """performance_config.yaml ë¡œë“œ"""
        import yaml
        perf_config_path = Path(__file__).parent / 'performance_config.yaml'
        if perf_config_path.exists():
            try:
                with open(perf_config_path, 'r', encoding='utf-8') as f:
                    self.perf_config = yaml.safe_load(f)
                    if logger:
                        logger.info(f"Performance config loaded from {perf_config_path}")
            except Exception as e:
                if logger:
                    logger.warning(f"Failed to load performance config: {e}")
                self.perf_config = {}
        else:
            self.perf_config = {}

    # í´ë˜ìŠ¤ ë ˆë²¨ ìƒìˆ˜ - config.yamlì—ì„œ ë¡œë“œ
    def _get_manufacturer_pattern(self):
        if USE_YAML_CONFIG:
            manufacturers = cfg.get('patterns.manufacturers', [])
            if manufacturers:
                pattern = '|'.join(manufacturers)
                return rf'\b({pattern})\b'
        return r'\b(SONY|Sony|HARRIS|Harris|HP|TOSHIBA|Toshiba|PANASONIC|Panasonic|CANON|Canon|NIKON|Nikon|DELL|Dell|APPLE|Apple|SAMSUNG|Samsung|LG)\b'

    def _get_model_pattern(self):
        if USE_YAML_CONFIG:
            return cfg.get('patterns.model_regex', r'\b[A-Z]{2,}[-\s]?\d+')
        return r'\b[A-Z]{2,}[-\s]?\d+'
    
    def __init__(self, preload_llm=False):
        # performance_config.yaml ë¡œë“œ ì‹œë„
        self._load_performance_config()

        # ì„¤ì • ë¡œë“œ (YAML ìš°ì„ )
        if USE_YAML_CONFIG:
            self.docs_dir = Path(cfg.get('paths.documents_dir', './docs'))
            self.model_path = cfg.get('models.qwen.path', './models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf')
            self.max_cache_size = self.perf_config.get('cache', {}).get('max_size', 500)
            self.cache_ttl = self.perf_config.get('cache', {}).get('response_ttl', 7200)
            self.max_text_length = self.perf_config.get('memory', {}).get('max_document_length', 8000)
            self.max_pdf_pages = cfg.get('limits.max_pdf_pages', 10)
            self.pdf_workers = self.perf_config.get('parallel', {}).get('pdf_workers', 4)
            self.chunk_size = self.perf_config.get('parallel', {}).get('chunk_size', 5)
        else:
            self.docs_dir = Path(cfg.DOCS_DIR)
            self.model_path = cfg.QWEN_MODEL_PATH
            self.max_cache_size = self.perf_config.get('cache', {}).get('max_size', 500)
            self.cache_ttl = self.perf_config.get('cache', {}).get('response_ttl', 7200)
            self.max_text_length = self.perf_config.get('memory', {}).get('max_document_length', 8000)
            self.max_pdf_pages = getattr(cfg, 'MAX_PDF_PAGES', 10)
            self.pdf_workers = self.perf_config.get('parallel', {}).get('pdf_workers', 4)
            self.chunk_size = self.perf_config.get('parallel', {}).get('chunk_size', 5)

        # PDF ìºì‹œ í¬ê¸° ì„¤ì • (DocumentModule ì´ˆê¸°í™” ì „ì— í•„ìš”)
        self.max_pdf_cache = 50  # ìµœëŒ€ 50ê°œ PDF ìºì‹±

        # ì„¤ì • ë””ë ‰í† ë¦¬ ì„¤ì •
        self.config_dir = Path(__file__).parent / 'config'
        self.config_dir.mkdir(exist_ok=True)

        self.llm = None
        # ìºì‹œ ê´€ë¦¬ ê°œì„  (í¬ê¸° ì œí•œ ë° TTL)
        from collections import OrderedDict
        # ìºì‹œ í¬ê¸° ì œí•œ ì„¤ì •
        self.MAX_CACHE_SIZE = 100  # ì‘ë‹µ ìºì‹œ ìµœëŒ€ í¬ê¸°
        self.MAX_METADATA_CACHE = 500  # ë©”íƒ€ë°ì´í„° ìºì‹œ ìµœëŒ€ í¬ê¸°
        self.MAX_PDF_CACHE = 50  # PDF í…ìŠ¤íŠ¸ ìºì‹œ ìµœëŒ€ í¬ê¸°
        self.CACHE_TTL = 3600  # ìºì‹œ ìœ íš¨ ì‹œê°„ (1ì‹œê°„)

        self.documents_cache = OrderedDict()  # LRU ìºì‹œì²˜ëŸ¼ ë™ì‘
        self.metadata_cache = OrderedDict()  # ë©”íƒ€ë°ì´í„° ìºì‹œ
        self.search_mode = 'document'  # ê²€ìƒ‰ ëª¨ë“œëŠ” í•­ìƒ document
        self.answer_cache = OrderedDict()  # ë‹µë³€ ìºì‹œ (LRU)
        self.pdf_text_cache = OrderedDict()  # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”)

        # PDF ì²˜ë¦¬ - ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        self.pdf_processor = None
        self.error_handler = None
        self.error_recovery = None

        # SearchModule ì´ˆê¸°í™” (2025-09-29 ë¦¬íŒ©í† ë§)
        self.search_module = None
        if SEARCH_MODULE_AVAILABLE:
            try:
                self.search_module = SearchModule(str(self.docs_dir))
                if logger:
                    logger.info("âœ… SearchModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ SearchModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.search_module = None

        # DocumentModule ì´ˆê¸°í™” (2025-09-29 ë¦¬íŒ©í† ë§)
        self.document_module = None
        if DOCUMENT_MODULE_AVAILABLE:
            try:
                doc_config = {
                    'max_pdf_cache': self.max_pdf_cache,
                    'max_text_length': self.max_text_length,
                    'max_pdf_pages': self.max_pdf_pages,
                    'enable_ocr': False  # OCRì€ í•„ìš”ì‹œ í™œì„±í™”
                }
                self.document_module = DocumentModule(doc_config)
                if logger:
                    logger.info("âœ… DocumentModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ DocumentModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.document_module = None

        # LLMModule ì´ˆê¸°í™” (2025-09-29 ë¦¬íŒ©í† ë§)
        self.llm_module = None
        if LLM_MODULE_AVAILABLE:
            try:
                llm_config = {
                    'model_path': self.model_path,
                    'preload_llm': preload_llm  # ìƒì„±ì íŒŒë¼ë¯¸í„° ì „ë‹¬
                }
                self.llm_module = LLMModule(llm_config)
                if logger:
                    logger.info("âœ… LLMModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ LLMModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.llm_module = None

        # CacheModule ì´ˆê¸°í™” (2025-09-29 ë¦¬íŒ©í† ë§)
        self.cache_module = None
        if CACHE_MODULE_AVAILABLE:
            try:
                cache_config = {
                    'max_cache_size': self.MAX_CACHE_SIZE,
                    'max_metadata_cache': self.MAX_METADATA_CACHE,
                    'max_pdf_cache': self.MAX_PDF_CACHE,
                    'cache_ttl': self.CACHE_TTL,
                    'cache_dir': str(self.config_dir / 'cache')
                }
                self.cache_module = CacheModule(cache_config)
                if logger:
                    logger.info("âœ… CacheModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ CacheModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.cache_module = None

        # StatisticsModule ì´ˆê¸°í™” (2025-09-29 ë¦¬íŒ©í† ë§)
        self.statistics_module = None
        if STATISTICS_MODULE_AVAILABLE:
            try:
                stats_config = {
                    'docs_dir': str(self.docs_dir)
                }
                self.statistics_module = StatisticsModule(stats_config)
                if logger:
                    logger.info("âœ… StatisticsModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ StatisticsModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.statistics_module = None

        # IntentModule ì´ˆê¸°í™” (2025-09-30 ë¦¬íŒ©í† ë§)
        self.intent_module = None
        if INTENT_MODULE_AVAILABLE:
            try:
                self.intent_module = IntentModule(llm_module=self.llm_module)
                if logger:
                    logger.info("âœ… IntentModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ IntentModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.intent_module = None

        # Everything-like ì´ˆê³ ì† ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (SearchModuleì´ ì—†ì„ ë•Œë§Œ)
        self.everything_search = None
        if not self.search_module and EVERYTHING_SEARCH_AVAILABLE:
            try:
                self.everything_search = EverythingLikeSearch()
                # ì´ˆê¸° ì¸ë±ì‹± - í•œ ë²ˆë§Œ ì‹¤í–‰
                if not hasattr(self, '_index_initialized'):
                    self.everything_search.index_all_files()
                    self.__class__._index_initialized = True
                if logger:
                    logger.info("Everything-like search initialized successfully")
            except Exception as e:
                if logger:
                    logger.error(f"Failed to initialize Everything-like search: {e}")
                self.everything_search = None

        # ì‘ë‹µ í¬ë§·í„° ì´ˆê¸°í™”
        self.formatter = ResponseFormatter() if ResponseFormatter else None

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° ì´ˆê¸°í™” (2025-09-29 ì¶”ê°€)
        self.metadata_extractor = None
        if METADATA_EXTRACTOR_AVAILABLE:
            try:
                self.metadata_extractor = MetadataExtractor()
                if logger:
                    logger.info("âœ… MetadataExtractor ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ MetadataExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.metadata_extractor = None

        # ë©”íƒ€ë°ì´í„° DB ì´ˆê¸°í™”
        try:
            self.metadata_db = MetadataDB(db_path=str(self.config_dir / "metadata.db"))
            logger.info("âœ… MetadataDB ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.error(f"ï¸ MetadataDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.metadata_db = None

        # ëª¨ë“  PDFì™€ TXT íŒŒì¼ ëª©ë¡ (ìƒˆë¡œìš´ í´ë” êµ¬ì¡° í¬í•¨)
        self.pdf_files = []
        self.txt_files = []

        # ë£¨íŠ¸ í´ë” íŒŒì¼
        self.pdf_files.extend(list(self.docs_dir.glob('*.pdf')))
        self.txt_files.extend(list(self.docs_dir.glob('*.txt')))

        # ì—°ë„ë³„ í´ë” (year_2014 ~ year_2025)
        for year in range(2014, 2026):
            year_folder = self.docs_dir / f"year_{year}"
            if year_folder.exists():
                self.pdf_files.extend(list(year_folder.glob('*.pdf')))
                self.txt_files.extend(list(year_folder.glob('*.txt')))

        # íŠ¹ë³„ í´ë” (ìì‚° ê´€ë ¨ í´ë” ì œê±°)
        special_folders = ['recent', 'archive']
        for folder in special_folders:
            special_folder = self.docs_dir / folder
            if special_folder.exists():
                self.pdf_files.extend(list(special_folder.glob('*.pdf')))
                self.txt_files.extend(list(special_folder.glob('*.txt')))

        # ì¤‘ë³µ ì œê±° (ê°™ì€ íŒŒì¼ì´ ì—¬ëŸ¬ í´ë”ì— ìˆì„ ìˆ˜ ìˆìŒ)
        self.pdf_files = list(set(self.pdf_files))
        self.txt_files = list(set(self.txt_files))
        self.all_files = self.pdf_files + self.txt_files

        if logger:
            logger.info(f"{len(self.pdf_files)}ê°œ PDF, {len(self.txt_files)}ê°œ TXT ë¬¸ì„œ ë°œê²¬")

        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì‚¬ì „ ì¶”ì¶œ (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
        self._build_metadata_cache()

        # ë©”íƒ€ë°ì´í„° DB ì¸ë±ìŠ¤ êµ¬ì¶• (Phase 1.2)
        self._build_metadata_index()

        # LLM ì‚¬ì „ ë¡œë“œ ì˜µì…˜
        if preload_llm:
            self._preload_llm()

    def _manage_cache_size(self, cache_dict, max_size, cache_name="cache"):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬ - LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°"""
        if len(cache_dict) > max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ë“¤ ì œê±° (FIFO)
            items_to_remove = len(cache_dict) - max_size
            for _ in range(items_to_remove):
                removed = cache_dict.popitem(last=False)  # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            if logger:
                logger.debug(f"ìºì‹œ ì •ë¦¬: {cache_name}ì—ì„œ {items_to_remove}ê°œ í•­ëª© ì œê±°")

    def _add_to_cache(self, cache_dict, key, value, max_size):
        """ìºì‹œì— í•­ëª© ì¶”ê°€ with í¬ê¸° ì œí•œ"""
        if key in cache_dict:
            del cache_dict[key]

        cache_dict[key] = {
            'data': value,
            'timestamp': time.time()
        }

        self._manage_cache_size(cache_dict, max_size, str(type(cache_dict)))

    def _preload_llm(self):
        """LLMì„ ë¯¸ë¦¬ ë¡œë“œ"""
        # LLMModuleì„ ì‚¬ìš©í•˜ì—¬ LLM ë¡œë“œ
        if self.llm_module:
            if self.llm_module.load_llm():
                self.llm = self.llm_module.llm
                return

        # LLMModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        if self.llm is None:
            if not LLMSingleton.is_loaded():
                if logger:
                    logger.info("LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
            else:
                if logger:
                    logger.info("LLM ëª¨ë¸ ì¬ì‚¬ìš©")

            try:
                start = time.time()
                self.llm = LLMSingleton.get_instance(model_path=self.model_path)
                elapsed = time.time() - start
                if elapsed > 1.0:  # 1ì´ˆ ì´ìƒ ê±¸ë¦° ê²½ìš°ë§Œ í‘œì‹œ
                    if logger:
                        logger.info(f"LLM ë¡œë“œ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")
            except LLMException as e:
                if logger:
                    logger.error(f"LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _build_metadata_index(self):
        """ëª¨ë“  PDFì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ DBì— ì €ì¥"""
        if not self.metadata_db:
            return

        logger.info("ğŸ“š ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘...")
        indexed = 0

        for pdf_path in self.pdf_files[:30]:  # ì²˜ìŒ 30ê°œë§Œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
            try:
                # DBì— ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
                existing = self.metadata_db.get_document(pdf_path.name)
                if existing and existing.get('title'):
                    continue  # ì´ë¯¸ ì²˜ë¦¬ë¨

                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = self._extract_pdf_metadata(pdf_path)
                if metadata:
                    # DBì— ì €ì¥
                    self.metadata_db.add_document(metadata)
                    indexed += 1

            except Exception as e:
                logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {pdf_path.name}: {e}")

        if indexed > 0:
            logger.info(f"âœ… {indexed}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¸ë±ì‹± ì™„ë£Œ")

    def _extract_pdf_metadata(self, pdf_path: Path) -> Optional[Dict[str, Any]]:
        """PDFì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            with pdfplumber.open(pdf_path) as pdf:
                if not pdf.pages:
                    return None

                # ì²« í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                first_page = pdf.pages[0].extract_text() or ""
                if not first_page:
                    return None

                # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
                metadata = {
                    'path': str(pdf_path),
                    'filename': pdf_path.name,
                    'file_size': pdf_path.stat().st_size,
                    'page_count': len(pdf.pages),
                    'text_preview': first_page[:500]
                }

                # ì œëª© ì¶”ì¶œ
                lines = first_page.split('\n')[:10]
                for line in lines:
                    if len(line) > 10 and len(line) < 100:
                        metadata['title'] = line.strip()
                        break

                # ë‚ ì§œ ì¶”ì¶œ
                date_patterns = [
                    r'(\d{4})[ë…„-]\s*(\d{1,2})[ì›”-]\s*(\d{1,2})',
                    r'(\d{4})\.(\d{1,2})\.(\d{1,2})'
                ]
                for pattern in date_patterns:
                    match = re.search(pattern, first_page[:1000])
                    if match:
                        year, month, day = match.groups()
                        metadata['year'] = year
                        metadata['month'] = month.zfill(2)
                        metadata['date'] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                        break

                # ê¸°ì•ˆì ì¶”ì¶œ
                drafter_patterns = [
                    r'ê¸°ì•ˆì[\s:ï¼š]*([ê°€-í£]{2,4})',
                    r'ì‘ì„±ì[\s:ï¼š]*([ê°€-í£]{2,4})'
                ]
                for pattern in drafter_patterns:
                    match = re.search(pattern, first_page[:500])
                    if match:
                        metadata['drafter'] = match.group(1).strip()
                        break

                # ì¹´í…Œê³ ë¦¬ ì¶”ì •
                filename_lower = pdf_path.name.lower()
                if 'êµ¬ë§¤' in filename_lower or 'êµ¬ì…' in filename_lower:
                    metadata['category'] = 'êµ¬ë§¤'
                elif 'ìˆ˜ë¦¬' in filename_lower or 'ë³´ìˆ˜' in filename_lower:
                    metadata['category'] = 'ìˆ˜ë¦¬'
                elif 'íê¸°' in filename_lower:
                    metadata['category'] = 'íê¸°'
                elif 'ê²€í† ' in filename_lower:
                    metadata['category'] = 'ê²€í† '

                return metadata

        except Exception as e:
            logger.error(f"PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜ {pdf_path.name}: {e}")
            return None

    
    def _parse_pdf_result(self, result: Dict) -> Dict:
        """ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            'text': result.get('text', ''),
            'page_count': result.get('page_count', 0),
            'metadata': result.get('metadata', {}),
            'method': result.get('method', 'parallel')
        }

    def process_pdfs_in_batch(self, pdf_paths: List[Path], batch_size: int = None) -> Dict:
        """ì—¬ëŸ¬ PDFë¥¼ ë°°ì¹˜ë¡œ ë³‘ë ¬ ì²˜ë¦¬ (ì•ˆì „í•˜ê²Œ ê°œì„ )"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import gc

        all_results = {}

        # ë™ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        if batch_size is None:
            batch_size = min(20, max(10, len(pdf_paths) // 30))
            logger.info(f"ë™ì  ë°°ì¹˜ í¬ê¸° ì„¤ì •: {batch_size}")

        # pdf_processorê°€ ì—†ìœ¼ë©´ ìˆœì°¨ ì²˜ë¦¬ë¡œ í´ë°±
        if self.pdf_processor is None:
            if logger:
                logger.info("ë³‘ë ¬ ì²˜ë¦¬ê¸° ë¯¸í™œì„±í™” - ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")

            # ThreadPoolExecutorë¡œ ê°„ë‹¨í•œ ë³‘ë ¬ ì²˜ë¦¬ (CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜ ìµœì í™”)
            optimal_workers = min(os.cpu_count() or 4, 12, max(4, batch_size))
            with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
                for i in range(0, len(pdf_paths), batch_size):
                    batch = pdf_paths[i:i + batch_size]
                    if logger:
                        logger.info(f"ë°°ì¹˜ {i//batch_size + 1}/{(len(pdf_paths)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)")

                    # ê° PDFë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
                    futures = {executor.submit(self._extract_pdf_info, pdf): pdf for pdf in batch}

                    for future in as_completed(futures):
                        pdf_path = futures[future]
                        try:
                            result = future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                            all_results[str(pdf_path)] = result
                        except PDFExtractionException as e:
                            if logger:
                                logger.warning(f"{pdf_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}")
                            all_results[str(pdf_path)] = {'error': str(e)}

                    # ë©”ëª¨ë¦¬ ìµœì í™”
                    if i % (batch_size * 5) == 0:
                        gc.collect()
        else:
            # ê¸°ì¡´ pdf_processor ì‚¬ìš©
            for i in range(0, len(pdf_paths), batch_size):
                batch = pdf_paths[i:i + batch_size]

                batch_results = self.pdf_processor.process_multiple_pdfs(batch)
                all_results.update(batch_results)

                # ë©”ëª¨ë¦¬ ê´€ë¦¬
                if len(self.pdf_processor.extraction_cache) > 50:
                    self.pdf_processor.clear_cache()

        return all_results

    def _find_metadata_by_filename(self, filename: str) -> Optional[Dict]:
        """íŒŒì¼ëª…ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì°¾ê¸° (ìƒˆë¡œìš´ ìºì‹œ êµ¬ì¡° ì§€ì›)"""
        # ë¨¼ì € ì •í™•í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì°¾ê¸°
        if filename in self.metadata_cache:
            return self.metadata_cache[filename]

        # ìƒëŒ€ ê²½ë¡œ í¬í•¨í•œ í‚¤ì—ì„œ ì°¾ê¸°
        for cache_key, metadata in self.metadata_cache.items():
            if metadata.get('filename') == filename:
                return metadata

        return None

    def _build_metadata_cache(self):
        """ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ (ìºì‹± ì§€ì›)"""
        # ìºì‹œ íŒŒì¼ ê²½ë¡œ (Docker volumeì— ì €ì¥)
        cache_dir = Path("/app/cache") if Path("/app/cache").exists() else Path("cache")
        cache_dir.mkdir(exist_ok=True)
        cache_file = cache_dir / "metadata_cache.pkl"

        # ê¸°ì¡´ ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
        if cache_file.exists():
            try:
                import pickle
                with open(cache_file, 'rb') as f:
                    self.metadata_cache = pickle.load(f)
                if logger:
                    logger.info(f"ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.metadata_cache)}ê°œ ë¬¸ì„œ")
                return  # ìºì‹œê°€ ìˆìœ¼ë©´ ì¬êµ¬ì¶• ë¶ˆí•„ìš”
            except Exception as e:
                if logger:
                    logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¬êµ¬ì¶•: {e}")

        logger.info("ë©”íƒ€ë°ì´í„° ìºì‹œ êµ¬ì¶• ì‹œì‘")
        if logger:
            logger.info("ë¬¸ì„œ ë©”íƒ€ë°ì´í„° êµ¬ì¶• ì¤‘...")

        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • í™•ì¸
        use_parallel = USE_YAML_CONFIG and cfg.get('parallel_processing.enabled', True)

        # ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ process_pdfs_in_batch ì‚¬ìš© (ë‚´ë¶€ì—ì„œ ì²˜ë¦¬)
        if self.pdf_files and len(self.pdf_files) > 10:  # 10ê°œ ì´ìƒì¼ ë•Œë§Œ ë³‘ë ¬ ì²˜ë¦¬
            if logger:
                logger.info(f"{len(self.pdf_files)}ê°œ PDF ì²˜ë¦¬ ì‹œì‘ (ë³‘ë ¬ ëª¨ë“œ)")
            pdf_results = self.process_pdfs_in_batch(self.pdf_files, batch_size=10)

            # ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë©”íƒ€ë°ì´í„° ìºì‹œì— ì €ì¥
            for pdf_path, result in pdf_results.items():
                pdf_path_obj = Path(pdf_path)
                filename = pdf_path_obj.name
                # ìƒëŒ€ ê²½ë¡œë¥¼ í‚¤ë¡œ ì‚¬ìš© (ì¤‘ë³µ íŒŒì¼ëª… ì²˜ë¦¬)
                try:
                    relative_path = pdf_path_obj.relative_to(self.docs_dir)
                    cache_key = str(relative_path)
                except ValueError:
                    cache_key = filename

                if 'error' not in result:
                    self.metadata_cache[cache_key] = {
                        'path': pdf_path_obj,
                        'filename': filename,
                        'text': result.get('text', '')[:1000],  # ìš”ì•½ë§Œ ì €ì¥
                        'page_count': result.get('page_count', 0),
                        'metadata': result.get('metadata', {})
                    }
        
        # PDFì™€ TXT íŒŒì¼ ëª¨ë‘ ì²˜ë¦¬
        for file_path in self.all_files:
            filename = file_path.name

            # ìƒëŒ€ ê²½ë¡œë¥¼ ìºì‹œ í‚¤ë¡œ ì‚¬ìš©
            try:
                relative_path = file_path.relative_to(self.docs_dir)
                cache_key = str(relative_path)
            except ValueError:
                cache_key = filename

            # íŒŒì¼ëª…ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            date_match = re.match(r'(\d{4}-\d{2}-\d{2})', filename)
            date = date_match.group(1) if date_match else ""

            # ì œëª© ì¶”ì¶œ (ë‚ ì§œ ë’¤ ë¶€ë¶„, í™•ì¥ì ì œê±°)
            if filename.endswith('.pdf'):
                title = filename.replace(date + '_', '').replace('.pdf', '') if date else filename.replace('.pdf', '')
            else:
                title = filename.replace(date + '_', '').replace('.txt', '') if date else filename.replace('.txt', '')

            # ì—°ë„ ì¶”ì¶œ
            year = date[:4] if date else ""

            # íŒŒì¼ëª…ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
            keywords = []

            # íŒŒì¼ëª…ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            # í•œê¸€, ì˜ë¬¸, ìˆ«ìë¡œ ë‹¨ì–´ ì¶”ì¶œ
            words = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', filename)

            # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ë‹¨ì–´ë“¤ì„ í‚¤ì›Œë“œë¡œ ì¶”ê°€ (2ê¸€ì ì´ìƒ)
            for word in words:
                if len(word) >= 2 and word not in ['pdf', 'PDF', 'txt', 'TXT', 'ì˜', 'ë°', 'ê±´', 'ê²€í† ì„œ']:
                    keywords.append(word)

            self.metadata_cache[cache_key] = {
                'path': file_path,
                'filename': filename,  # ì‹¤ì œ íŒŒì¼ëª… ì €ì¥
                'date': date,
                'year': year,
                'title': title,
                'keywords': keywords,
                'drafter': None,  # ê¸°ì•ˆì ì •ë³´ëŠ” í•„ìš”ì‹œì—ë§Œ ì¶”ì¶œ
                'full_text': None,  # ë‚˜ì¤‘ì— í•„ìš”ì‹œ ë¡œë“œ
                'is_txt': filename.endswith('.txt'),  # TXT íŒŒì¼ ì—¬ë¶€
                'is_pdf': filename.endswith('.pdf')  # PDF íŒŒì¼ ì—¬ë¶€ ì¶”ê°€
            }

        if logger:
            logger.info(f"{len(self.metadata_cache)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° êµ¬ì¶• ì™„ë£Œ")

        # ìºì‹œ ì €ì¥
        try:
            import pickle
            with open(cache_file, 'wb') as f:
                pickle.dump(self.metadata_cache, f)
            if logger:
                logger.info(f"ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_file}")
        except Exception as e:
            if logger:
                logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _extract_txt_info(self, txt_path: Path) -> Dict:
        """TXT íŒŒì¼ì—ì„œ ì •ë³´ ë™ì  ì¶”ì¶œ"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            info = {'text': text[:3000]}  # ì²˜ìŒ 3000ìë§Œ
            
            # ìì‚° íŒŒì¼ ê´€ë ¨ ì½”ë“œ ì œê±°ë¨
            
            return info

        except Exception as e:
            if logger:
                print(f"ï¸ TXT ì½ê¸° ì˜¤ë¥˜ ({txt_path.name}): {e}")
            else:
                print(f"ï¸ TXT ì½ê¸° ì˜¤ë¥˜ ({txt_path.name}): {e}")
            # ì—ëŸ¬ í•¸ë“¤ëŸ¬ë¡œ ì•ˆì „í•˜ê²Œ íŒŒì¼ ì½ê¸° ì‹œë„
            # error_handler ì œê±° - ì§ì ‘ íŒŒì¼ ì½ê¸°
            try:
                with open(txt_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                if content:
                    return {'text': content[:3000]}
            except Exception:
                pass
            return {}

    # ë°ì½”ë ˆì´í„° ì œê±° (error_handler ë°±ì—… í´ë”ë¡œ ì´ë™)
    # @RAGErrorHandler.retry_with_backoff(max_retries=3, backoff_factor=1.5)
    # @RAGErrorHandler.handle_pdf_extraction_error
    def _extract_pdf_info_with_retry(self, pdf_path: Path) -> Dict:
        """PDF ì •ë³´ ì¶”ì¶œ (DocumentModule ì‚¬ìš©)"""
        # DocumentModuleì„ ì‚¬ìš©í•˜ì—¬ PDF ì²˜ë¦¬
        if self.document_module:
            try:
                result = self.document_module.extract_pdf_text_with_retry(pdf_path, max_retries=2)
                if result:
                    return result
            except Exception as e:
                if logger:
                    logger.error(f"DocumentModule PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        # DocumentModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ë²• ì‚¬ìš©
        return self._extract_pdf_info(pdf_path)
    
    def _extract_pdf_info(self, pdf_path: Path) -> Dict:
        """PDF ì •ë³´ ì¶”ì¶œ (StatisticsModule ìœ„ì„ ë˜ëŠ” í´ë°±ìš©) - ìºì‹± ì ìš©"""
        # StatisticsModuleì´ ìˆìœ¼ë©´ ìœ„ì„
        if self.statistics_module:
            try:
                return self.statistics_module._extract_pdf_info(pdf_path)
            except Exception as e:
                if logger:
                    logger.error(f"StatisticsModule PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}, í´ë°± ì‚¬ìš©")

        # StatisticsModuleì´ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ì¡´ ë°©ë²• ì‚¬ìš©
        # ìºì‹œ í‚¤ ìƒì„± (íŒŒì¼ ê²½ë¡œ ê¸°ë°˜)
        cache_key = str(pdf_path)

        # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        if cache_key in self.pdf_text_cache:
            # ìºì‹œ íˆíŠ¸ - LRUë¥¼ ìœ„í•´ ë§¨ ë’¤ë¡œ ì´ë™
            cached_result = self.pdf_text_cache.pop(cache_key)
            self.pdf_text_cache[cache_key] = cached_result
            return cached_result

        text = ""

        # ì—ëŸ¬ í•¸ë“¤ëŸ¬ë¡œ ì•ˆì „í•˜ê²Œ íŒŒì¼ ì½ê¸°
        def extract_with_pdfplumber():
            nonlocal text
            with pdfplumber.open(pdf_path) as pdf:
                # ë¬¸ì„œ ì „ì²´ ë˜ëŠ” ì„¤ì •ëœ ìµœëŒ€ í˜ì´ì§€ ì½ê¸°
                pages_to_read = min(len(pdf.pages), self.max_pdf_pages)
                for page in pdf.pages[:pages_to_read]:
                    # safe_execute ì œê±° (error_handler ë°±ì—… í´ë”ë¡œ ì´ë™)
                    try:
                        page_text = page.extract_text() or ""
                    except Exception:
                        page_text = ""
                    if page_text:
                        text += page_text + "\n"
                        # ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ë©´ ì¤‘ë‹¨ (ë©”ëª¨ë¦¬ ì ˆì•½)
                        if len(text) > self.max_text_length:
                            break
            return text

        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
        # error_recoveryê°€ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ ì‹œë„
        text = extract_with_pdfplumber()
        if not text and hasattr(self, '_try_ocr_extraction'):
            text = self._try_ocr_extraction(pdf_path)

        # pdfplumber ì‹¤íŒ¨ì‹œ OCR ì‹œë„
        if not text:
            try:
                from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
                ocr = EnhancedOCRProcessor()
                text, _ = ocr.extract_text_with_ocr(str(pdf_path))
            except Exception:
                pass  # OCR ì‹¤íŒ¨ì‹œ ë¬´ì‹œ

        if not text:
            return {}

        info = {}

        # ê¸°ì•ˆì ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´)
        patterns = [
            r'ê¸°ì•ˆì[\s:ï¼š]*([ê°€-í£]+)',
            r'ì‘ì„±ì[\s:ï¼š]*([ê°€-í£]+)',
            r'ë‹´ë‹¹ì[\s:ï¼š]*([ê°€-í£]+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                info['ê¸°ì•ˆì'] = match.group(1).strip()
                break
            
        # ë‚ ì§œ ì¶”ì¶œ (ë¬¸ì„œ ë‚´ìš© ìš°ì„ , íŒŒì¼ëª… fallback)
        date_patterns = [
            r'ê¸°ì•ˆì¼[\s:ï¼š]*(\d{4}[-ë…„]\s*\d{1,2}[-ì›”]\s*\d{1,2})',
            r'ì‹œí–‰ì¼ì[\s:ï¼š]*(\d{4}[-ë…„]\s*\d{1,2}[-ì›”]\s*\d{1,2})',
            r'(\d{4}-\d{2}-\d{2})\s*\d{2}:\d{2}',
            r'ì¼ì[\s:ï¼š]*(\d{4}[-./ë…„]\s*\d{1,2}[-./ì›”]\s*\d{1,2})',
            r'ë‚ ì§œ[\s:ï¼š]*(\d{4}[-./ë…„]\s*\d{1,2}[-./ì›”]\s*\d{1,2})',
            r'(\d{4}[./-]\d{1,2}[./-]\d{1,2})'  # ì¼ë°˜ì ì¸ ë‚ ì§œ í˜•ì‹
        ]

        date_found = False
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                info['ë‚ ì§œ'] = match.group(1).strip()
                date_found = True
                break

        # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
        if not date_found:
            filename = pdf_path.name
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (YYYY-MM-DD, YYYY.MM.DD, YYYY_MM_DD ë“±)
            filename_date_patterns = [
                r'(20\d{2}[-_.]0?[1-9]|1[0-2][-_.][0-2]?\d|3[01])',  # YYYY-MM-DD
                r'(20\d{2})[-_.]?(\d{1,2})[-_.]?(\d{1,2})',  # YYYY MM DD (ë¶„ë¦¬ëœ í˜•íƒœ)
            ]

            for pattern in filename_date_patterns:
                match = re.search(pattern, filename)
                if match:
                    if len(match.groups()) == 1:
                        # ì „ì²´ ë§¤ì¹˜ì¸ ê²½ìš°
                        date_str = match.group(1)
                        date_str = date_str.replace('_', '-').replace('.', '-')
                        info['ë‚ ì§œ'] = date_str
                        date_found = True
                        break
                    if len(match.groups()) == 3:
                        # ë¶„ë¦¬ëœ ê·¸ë£¹ì¸ ê²½ìš°
                        year, month, day = match.groups()
                        try:
                            normalized_date = f"{year}-{int(month):02d}-{int(day):02d}"
                            info['ë‚ ì§œ'] = normalized_date
                            date_found = True
                            break
                        except ValueError:
                            continue
            
            # ë¶€ì„œ ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            # íŒ¨í„´ 1: ê¸°ì•ˆë¶€ì„œ ë¼ë²¨
            dept_match = re.search(r'ê¸°ì•ˆë¶€ì„œ[\s:ï¼š]*([^\nì‹œí–‰]+)', text)
            if dept_match:
                dept = dept_match.group(1).strip()
                dept = dept.split('ì‹œí–‰')[0].strip()
                info['ë¶€ì„œ'] = dept
            
            # íŒ¨í„´ 2: íŒ€-íŒŒíŠ¸ í˜•ì‹ (ì±„ë„A ìŠ¤íƒ€ì¼)
            if 'ë¶€ì„œ' not in info:
                team_match = re.search(r'([ê°€-í£]+íŒ€[\-ê°€-í£]+íŒŒíŠ¸)', text)
                if team_match:
                    info['ë¶€ì„œ'] = team_match.group(1)
            
            # ê¸ˆì•¡ ì¶”ì¶œ (ê°€ì¥ í° ê¸ˆì•¡)
            amounts = re.findall(r'(\d{1,3}(?:,\d{3})*)\s*ì›', text)
            if amounts:
                numeric_amounts = []
                for amt in amounts:
                    try:
                        num = int(amt.replace(',', ''))
                        numeric_amounts.append((num, amt))
                    except (ValueError, AttributeError):
                        pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
                if numeric_amounts:
                    numeric_amounts.sort(reverse=True)
                    info['ê¸ˆì•¡'] = f"{numeric_amounts[0][1]}ì›"
            
            # ì œëª© ì¶”ì¶œ
            title_match = re.search(r'ì œëª©[\s:ï¼š]*([^\n]+)', text)
            if title_match:
                info['ì œëª©'] = title_match.group(1).strip()
            
            # í…ìŠ¤íŠ¸ ì €ì¥
            info['text'] = text[:3000]  # ì²˜ìŒ 3000ìë§Œ

        # ìºì‹œì— ì €ì¥ (í¬ê¸° ì œí•œ ì ìš©)
        MAX_PDF_CACHE_SIZE = 50  # ìµœëŒ€ 50ê°œ PDF ìºì‹±
        if len(self.pdf_text_cache) >= MAX_PDF_CACHE_SIZE:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)
            self.pdf_text_cache.popitem(last=False)

        # ìƒˆ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        self.pdf_text_cache[cache_key] = info

        return info

    def _search_by_content(self, query: str) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ëª¨ë“ˆë¡œ ìœ„ì„ëœ ê²€ìƒ‰ í•¨ìˆ˜"""
        if self.search_module:
            try:
                results = self.search_module.search_by_content(query, top_k=20)
                if logger:
                    logger.info(f"SearchModule found {len(results)} documents for query: {query}")
                return results
            except Exception as e:
                if logger:
                    logger.error(f"SearchModule failed: {e}")
                return []

        # SearchModuleì´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if logger:
            logger.warning("SearchModule not available")
        return []

    def _extract_context(self, text: str, keyword: str, window: int = 200) -> str:
        """í‚¤ì›Œë“œ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        keyword_lower = keyword.lower()
        text_lower = text.lower()

        pos = text_lower.find(keyword_lower)
        if pos == -1:
            return ""

        start = max(0, pos - window)
        end = min(len(text), pos + len(keyword) + window)

        context = text[start:end]
        # í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸
        context = context.replace(keyword, f"**{keyword}**")

        return f"...{context}..."

    def find_best_document(self, query: str) -> Optional[Path]:
        """ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë¬¸ì„œ ì°¾ê¸° - SearchModuleë¡œ ìœ„ì„ ë˜ëŠ” ê°„ë‹¨í•œ í´ë°±"""

        # SearchModuleì´ ìˆìœ¼ë©´ ìœ„ì„
        if self.search_module:
            try:
                result = self.search_module.find_best_document(query)
                if result:
                    return Path(result)
            except Exception as e:
                if logger:
                    logger.error(f"SearchModule ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}, í´ë°± ì‚¬ìš©")

        # í´ë°±: ê°„ë‹¨í•œ ê²€ìƒ‰ ë¡œì§
        content_based_results = self._search_by_content(query)
        if content_based_results and content_based_results[0]['score'] > 20:
            best_result = content_based_results[0]
            return best_result['path']

        # ê¸°ë³¸ íŒŒì¼ëª… ë§¤ì¹­ í´ë°±
        for cache_key, metadata in self.metadata_cache.items():
            filename_lower = metadata.get('filename', cache_key).lower()
            if any(word in filename_lower for word in query.lower().split() if len(word) > 1):
                return Path(metadata.get('path', self.docs_dir / cache_key))

        return None
        
    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)
        ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ + í•œê¸€ ìëª¨ ë¶„í•´ ë¹„êµ
        """
        # ê¸¸ì´ê°€ ë„ˆë¬´ ë‹¤ë¥´ë©´ ë‚®ì€ ìœ ì‚¬ë„
        if abs(len(str1) - len(str2)) > 2:
            return 0.0

        # ê°„ë‹¨í•œ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚°
        def levenshtein_distance(s1: str, s2: str) -> int:
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)

            if len(s2) == 0:
                return len(s1)

            previous_row = range(len(s2) + 1)
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    # ì‚½ì…, ì‚­ì œ, ì¹˜í™˜ ë¹„ìš© ê³„ì‚°
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row

            return previous_row[-1]

        # ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
        distance = levenshtein_distance(str1, str2)
        max_len = max(len(str1), len(str2))
        similarity = 1 - (distance / max_len) if max_len > 0 else 1.0

        # í•œê¸€ íŠ¹ìˆ˜ ì²˜ë¦¬: ììŒ/ëª¨ìŒ í•˜ë‚˜ ì°¨ì´ëŠ” ë†’ì€ ìœ ì‚¬ë„
        # ì˜ˆ: ì¼/ìº , ì½¤/ì»´ ë“±
        if len(str1) == len(str2) and distance == 1:
            # í•œê¸€ì¸ ê²½ìš° ìœ ì‚¬ë„ ë³´ì •
            if all(ord('ê°€') <= ord(c) <= ord('í£') for c in str1 + str2):
                similarity = max(similarity, 0.85)

        return similarity

    def get_document_info(self, file_path: Path, info_type: str = "all") -> str:
        """ë¬¸ì„œì—ì„œ íŠ¹ì • ì •ë³´ ì¶”ì¶œ (PDF/TXT ëª¨ë‘ ì§€ì›)"""
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{file_path.name}_{info_type}"
        if cache_key in self.documents_cache:
            return self.documents_cache[cache_key]
        
        # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì •ë³´ ì¶”ì¶œ
        if file_path.suffix == '.txt':
            info = self._extract_txt_info(file_path)
        else:
            info = self._extract_pdf_info(file_path)
        
        if not info:
            return " ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        
        result = ""
        
        # ìì‚° íŒŒì¼ ê´€ë ¨ ì½”ë“œ ì œê±°ë¨

        # ì¼ë°˜ ë¬¸ì„œ ì²˜ë¦¬
        if info_type == "all":
            result = f" {file_path.stem}\n"
            result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            for key, value in info.items():
                if key != 'text':
                    result += f"â€¢ {key}: {value}\n"
        if info_type == "ê¸°ì•ˆì":
            result = f" ê¸°ì•ˆì: {info.get('ê¸°ì•ˆì', 'ì •ë³´ ì—†ìŒ')}"
        if info_type == "ë‚ ì§œ":
            result = f" ë‚ ì§œ: {info.get('ë‚ ì§œ', 'ì •ë³´ ì—†ìŒ')}"
        if info_type == "ë¶€ì„œ":
            result = f" ë¶€ì„œ: {info.get('ë¶€ì„œ', 'ì •ë³´ ì—†ìŒ')}"
        if info_type == "ê¸ˆì•¡":
            amount = info.get('ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')
            result = f" ê¸ˆì•¡: {amount}"
        else:
            # íŠ¹ì • ì •ë³´ ìš”ì²­
            if info_type in info:
                result = f" {info_type}: {info[info_type]}"
            else:
                result = f" {info_type} ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            
            # ì§§ì€ ë‹µë³€ ë³´ì™„ - ì¶”ê°€ ì •ë³´ ì œê³µ
            if amount != 'ì •ë³´ ì—†ìŒ' and len(result) < 50:
                # ë¬¸ì„œ ì •ë³´ ì¶”ê°€
                result += f"\n\n ë¬¸ì„œ ì •ë³´:\n"
                result += f"â€¢ ë¬¸ì„œëª…: {file_path.stem}\n"
                if 'ë‚ ì§œ' in info:
                    result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
                if 'ê¸°ì•ˆì' in info:
                    result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
                if 'ë¶€ì„œ' in info:
                    result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
                if 'ì œëª©' in info:
                    result += f"â€¢ ì œëª©: {info['ì œëª©']}\n"
        if info_type == "ìš”ì•½":
            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
            result = f" {file_path.stem} ìš”ì•½\n"
            result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            if 'ê¸°ì•ˆì' in info:
                result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
            if 'ë‚ ì§œ' in info:
                result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
            if 'ë¶€ì„œ' in info:
                result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
            if 'ê¸ˆì•¡' in info:
                result += f"â€¢ ê¸ˆì•¡: {info['ê¸ˆì•¡']}\n"
            if 'ì œëª©' in info:
                result += f"â€¢ ì œëª©: {info['ì œëª©']}\n"
        else:  # all
            # LLM ì‘ë‹µê³¼ ìœ ì‚¬í•œ í¬ë§·ìœ¼ë¡œ í†µì¼
            result = f" {file_path.stem}\n\n"
            result += f" **ê¸°ë³¸ ì •ë³´**\n"
            if 'ê¸°ì•ˆì' in info:
                result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
            if 'ë‚ ì§œ' in info:
                result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
            if 'ë¶€ì„œ' in info:
                result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
            
            result += f"\n **ì£¼ìš” ë‚´ìš©**\n"
            
            # text í•„ë“œì—ì„œ ì£¼ìš” ë‚´ìš© ì¶”ì¶œ (ê°œì„ ëœ ìš”ì•½ ì‹œìŠ¤í…œ)
            if 'text' in info:
                # LLMModuleì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ìƒì„±
                if self.llm_module:
                    summary = self.llm_module.generate_smart_summary(info['text'], str(file_path.name))
                else:
                    # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                    summary = self._generate_smart_summary(info['text'], file_path)
                result += summary
            
            if 'ê¸ˆì•¡' in info and info['ê¸ˆì•¡'] != 'ì •ë³´ ì—†ìŒ':
                result += f"\n **ë¹„ìš© ì •ë³´**\n"
                result += f"â€¢ ê¸ˆì•¡: {info['ê¸ˆì•¡']}\n"
            
            result += f"\n ì¶œì²˜: {file_path.name}"
        
        # ìºì‹œ ì €ì¥
        self.documents_cache[cache_key] = result
        
        return result
    def _remove_duplicate_documents(self, documents: list) -> list:
        """ì¤‘ë³µ ë¬¸ì„œ ì œê±° (íŒŒì¼ëª… ê¸°ì¤€)"""
        seen = set()
        unique_docs = []

        for doc in documents:
            # íŒŒì¼ëª…ì—ì„œ ê²½ë¡œ ë¶€ë¶„ ì œê±°í•˜ì—¬ ë¹„êµ
            filename = Path(doc.get('path', doc.get('filename', ''))).name
            if filename not in seen:
                seen.add(filename)
                unique_docs.append(doc)

        return unique_docs

    def _format_enhanced_response(self, results: list, query: str) -> str:
        """ê°œì„ ëœ ì‘ë‹µ í˜•ì‹"""
        if not results:
            return " ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ì¤‘ë³µ ì œê±°
        unique_results = self._remove_duplicate_documents(results)

        response = f" **ê²€ìƒ‰ ê²°ê³¼** ({len(unique_results)}ê°œ ë¬¸ì„œ)\n\n"

        for i, doc in enumerate(unique_results[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            title = doc.get('title', 'ì œëª© ì—†ìŒ')
            date = doc.get('date', 'ë‚ ì§œ ë¯¸ìƒ')
            category = doc.get('category', 'ê¸°íƒ€')
            drafter = doc.get('drafter', 'ë¯¸ìƒ')

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì •ë³´ ìš°ì„  ì‚¬ìš© (2025-09-29 ì¶”ê°€)
            if 'extracted_date' in doc:
                date = doc['extracted_date']
            if 'extracted_type' in doc:
                category = doc['extracted_type']
            if 'extracted_dept' in doc:
                drafter = doc['extracted_dept']

            # ë‚ ì§œ í‘œì‹œ ê°œì„ 
            if date and date != 'ë‚ ì§œ ë¯¸ìƒ' and len(date) >= 10:
                display_date = date[:10]  # YYYY-MM-DD
            if date and len(date) >= 4:
                display_date = date[:4]  # ì—°ë„ë§Œ
            else:
                display_date = "ë‚ ì§œë¯¸ìƒ"

            response += f"**{i}. [{category}] {title}**\n"
            response += f"    {display_date} |  {drafter}"

            # ì¶”ì¶œëœ ê¸ˆì•¡ ì •ë³´ ì¶”ê°€ (2025-09-29)
            if 'extracted_amount' in doc:
                amount = doc['extracted_amount']
                response += f" | ğŸ’° {amount:,}ì›"

            response += "\n"

            # ë¬¸ì„œ ìš”ì•½ ì¶”ê°€
            if 'path' in doc:
                try:
                    file_path = Path(doc['path'])
                    if file_path.exists():
                        # LLMModuleì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ìƒì„±
                        if self.llm_module:
                            summary = self.llm_module.generate_smart_summary("", str(file_path.name))
                        else:
                            # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                            summary = self._generate_smart_summary("", file_path)
                        if summary and summary != "â€¢ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ì¤‘...":
                            response += f"   {summary}\n"
                except Exception:
                    pass

            response += "\n"

        if len(unique_results) > 5:
            response += f"... ì™¸ {len(unique_results) - 5}ê°œ ë¬¸ì„œ ë” ìˆìŒ\n\n"

        response += " **íŠ¹ì • ë¬¸ì„œë¥¼ ì„ íƒí•˜ì—¬ ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.**"

        return response
    def answer_from_specific_document(self, query: str, filename: str) -> str:
        """íŠ¹ì • ë¬¸ì„œì— ëŒ€í•´ì„œë§Œ ë‹µë³€ ìƒì„± (ë¬¸ì„œ ì „ìš© ëª¨ë“œ) - ì´ˆìƒì„¸ ë²„ì „
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            filename: íŠ¹ì • ë¬¸ì„œ íŒŒì¼ëª…
        """
        if logger:
            logger.info(f"ë¬¸ì„œ ì „ìš© ëª¨ë“œ: {filename}")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ í•´ë‹¹ ë¬¸ì„œ ì°¾ê¸°
        doc_metadata = self._find_metadata_by_filename(filename)
        if not doc_metadata:
            return f" ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
        doc_path = doc_metadata['path']
        
        # PDFì¸ì§€ TXTì¸ì§€ í™•ì¸
        if filename.endswith('.pdf'):
            # PDF ë¬¸ì„œ ì²˜ë¦¬ - ì „ì²´ ë‚´ìš© ì¶”ì¶œ
            info = self._extract_pdf_info_with_retry(doc_path)
            if not info.get('text'):
                return f" PDF ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
            
            # LLM ì´ˆê¸°í™”
            if self.llm is None:
                print(" LLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
                self._preload_llm()
            
            # ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì‚¬ìš© (15000ìë¡œ í™•ëŒ€)
            full_text = info['text'][:15000]
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
            if any(word in query for word in ['ìš”ì•½', 'ì •ë¦¬', 'ê°œìš”', 'ë‚´ìš©']):
                prompt = self._create_detailed_summary_prompt(query, full_text, filename)
            if any(word in query for word in ['ìƒì„¸', 'ìì„¸íˆ', 'êµ¬ì²´ì ', 'ì„¸ì„¸íˆ', 'ì„¸ë¶€']):
                prompt = self._create_ultra_detailed_prompt(query, full_text, filename)
            if any(word in query for word in ['í’ˆëª©', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'í•­ëª©']):
                prompt = self._create_itemized_list_prompt(query, full_text, filename)
            else:
                prompt = self._create_document_specific_prompt(query, full_text, filename)
            
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ë” ê¸´ ë‹µë³€ í—ˆìš©)
            try:
                # ë¬¸ì„œ ì „ìš© ëª¨ë“œì—ì„œëŠ” ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
                context_chunks = [
                    {
                        'content': full_text,
                        'source': filename,
                        'metadata': {
                            'filename': filename,
                            'date': doc_metadata.get('date', ''),
                            'title': doc_metadata.get('title', ''),
                            'is_document_only_mode': True  # ë¬¸ì„œ ì „ìš© ëª¨ë“œ í‘œì‹œ
                        }
                    }
                ]
                
                response = self.llm.generate_response(query, context_chunks)
                answer = response.answer if hasattr(response, 'answer') else str(response)
                
                # ë‹µë³€ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë³´ê°•
                if len(answer) < 200 and 'ìì„¸íˆ' in query:
                    answer = self._enhance_short_answer(answer, full_text, query)
                    
            except Exception as e:
                print(f"LLM ì˜¤ë¥˜: {e}")
                # í´ë°±: ìƒì„¸í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€
                answer = self._detailed_text_search(info['text'], query, filename)
            
            # ì¶œì²˜ ì¶”ê°€
            answer += f"\n\n **ì¶œì²˜**: {filename}"
            
        if filename.endswith('.txt'):
            # TXT íŒŒì¼ ì²˜ë¦¬ (ìì‚° ë°ì´í„°)
            return None  # Asset ê²€ìƒ‰ ì œê±°
        else:
            return f" ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {filename}"
        
        return answer
    
    def _simple_text_search(self, text: str, query: str, filename: str) -> str:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (LLM ì—†ì´)"""
        lines = text.split('\n')
        relevant_lines = []
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        
        # ê´€ë ¨ ì¤„ ì°¾ê¸°
        for line in lines:
            if any(keyword in line for keyword in keywords if len(keyword) > 1):
                relevant_lines.append(line.strip())
        
        if relevant_lines:
            return f" {filename}ì—ì„œ ì°¾ì€ ê´€ë ¨ ë‚´ìš©:\n\n" + '\n'.join(relevant_lines[:20])
        else:
            return f" {filename}ì—ì„œ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _detailed_text_search(self, text: str, query: str, filename: str) -> str:
        """ìƒì„¸í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (LLM ì—†ì´)"""
        lines = text.split('\n')
        relevant_sections = []
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        
        # ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸° (ì•ë’¤ ë¬¸ë§¥ í¬í•¨)
        for i, line in enumerate(lines):
            if any(keyword in line for keyword in keywords if len(keyword) > 1):
                # ì•ë’¤ 2ì¤„ì”© í¬í•¨
                start = max(0, i-2)
                end = min(len(lines), i+3)
                section = '\n'.join(lines[start:end])
                if section not in relevant_sections:
                    relevant_sections.append(section)
        
        if relevant_sections:
            return f" **{filename}** ìƒì„¸ ë¶„ì„:\n\n" + '\n---\n'.join(relevant_sections[:10])
        else:
            return f" {filename}ì—ì„œ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _enhance_short_answer(self, answer: str, full_text: str, query: str) -> str:
        """ì§§ì€ ë‹µë³€ì„ ë³´ê°•"""
        enhanced = answer + "\n\n **ì¶”ê°€ ìƒì„¸ ì •ë³´**:\n"
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        lines = full_text.split('\n')
        
        additional_info = []
        for line in lines:
            if any(keyword in line for keyword in keywords if len(keyword) > 2):
                if line.strip() and line not in answer:
                    additional_info.append(f"â€¢ {line.strip()}")
        
        if additional_info:
            enhanced += '\n'.join(additional_info[:10])
        
        return enhanced
    

    def _safe_pdf_extract(self, pdf_path, max_retries=3):
        """ì•ˆì „í•œ PDF ì¶”ì¶œ with ì¬ì‹œë„"""
        for attempt in range(max_retries):
            try:
                return self._extract_full_pdf_content(pdf_path)
            except PDFExtractionException as e:
                logger.warning(f"PDF ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    logger.error(f"PDF ì¶”ì¶œ ìµœì¢… ì‹¤íŒ¨: {pdf_path}")
                    return None
                time.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

    def _validate_input(self, query):
        """ì…ë ¥ ê²€ì¦"""
        if not query:
            raise ValueError("ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        if len(query) > 1000:
            logger.warning(f"ì¿¼ë¦¬ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {len(query)}ì")
            query = query[:1000]

        # SQL ì¸ì ì…˜ ë°©ì§€
        dangerous_patterns = ['DROP', 'DELETE', 'INSERT', 'UPDATE', '--', ';']
        for pattern in dangerous_patterns:
            if pattern in query.upper():
                raise ValueError(f"í—ˆìš©ë˜ì§€ ì•Šì€ íŒ¨í„´: {pattern}")
    def __del__(self):
        """ì†Œë©¸ì - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.cleanup_executor()
    def _batch_process_documents(self, documents, process_func, batch_size=10):
        """ë°°ì¹˜ ë¬¸ì„œ ì²˜ë¦¬ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±"""
        total = len(documents)
        processed = 0
        results = []

        for i in range(0, total, batch_size):
            batch = documents[i:i+batch_size]

            with ThreadPoolExecutor(max_workers=min(len(batch), self.MAX_WORKERS)) as executor:
                futures = [executor.submit(process_func, doc) for doc in batch]

                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=30)
                        if result:
                            results.append(result)
                    except Exception as e:
                        logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

            processed += len(batch)
            logger.info(f"ì§„í–‰ë¥ : {processed}/{total} ({100*processed/total:.1f}%)")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if processed % 50 == 0:
                import gc
                gc.collect()

        return results

    def cleanup_executor(self):
        """ë³‘ë ¬ ì²˜ë¦¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
            logger.info("ë³‘ë ¬ ì²˜ë¦¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

    
    
    def _create_prompt(self, query: str, context: str, filename: str, prompt_type: str = 'default') -> str:
        """í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜"""
        if prompt_type == 'detailed':
            return f"""
ë¬¸ì„œ: {filename}
ìš”ì²­: {query}

ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìƒì„¸íˆ ë‹µë³€í•˜ì„¸ìš”:
{context}

ì¤‘ìš” ì •ë³´ëŠ” **êµµê²Œ** í‘œì‹œí•˜ê³ , ê´€ë ¨ ë‚´ìš©ì„ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”.
"""
        else:  # default
            return f"""
ë¬¸ì„œ: {filename}
ì§ˆë¬¸: {query}

ë¬¸ì„œ ë‚´ìš©:
{context}

ìœ„ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
"""
    
    def _get_enhanced_cache_key(self, query: str, mode: str) -> str:
        """í–¥ìƒëœ ìºì‹œ í‚¤ ìƒì„± - ìœ ì‚¬ ì§ˆë¬¸ë„ ìºì‹œ íˆíŠ¸

        ì˜ˆì‹œ:
        - "2020ë…„ êµ¬ë§¤ ë¬¸ì„œ" â†’ "2020 êµ¬ë§¤ ë¬¸ì„œ"
        - "2020ë…„ì˜ êµ¬ë§¤í•œ ë¬¸ì„œë¥¼" â†’ "2020 êµ¬ë§¤ ë¬¸ì„œ"
        - "êµ¬ë§¤ 2020ë…„ ë¬¸ì„œ" â†’ "2020 êµ¬ë§¤ ë¬¸ì„œ" (ì •ë ¬)
        """

        # 1. ì¿¼ë¦¬ ì •ê·œí™” ë° ì†Œë¬¸ì ë³€í™˜
        normalized = query.strip().lower()

        # 2. í•œê¸€ ì¡°ì‚¬ ì œê±° - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜
        # ë³µí•© ì¡°ì‚¬ë¶€í„° ì œê±° (ì—ì„œëŠ”, ìœ¼ë¡œëŠ” ë“±)
        compound_particles = ['ì—ì„œëŠ”', 'ìœ¼ë¡œëŠ”', 'ì—ê²ŒëŠ”', 'í•œí…ŒëŠ”', 'ì—ì„œë„', 'ìœ¼ë¡œë„',
                            'ì´ë¼ë„', 'ì´ë‚˜ë§ˆ', 'ì´ë“ ì§€', 'ì´ë¼ëŠ”', 'ê¹Œì§€ë„', 'ë¶€í„°ë„']
        simple_particles = ['ì—ì„œ', 'ìœ¼ë¡œ', 'ì´ë‚˜', 'ì´ë“ ', 'ì´ë©´', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜ì„œ',
                          'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ì—',
                          'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ë§ˆë‹¤', 'ë§ˆì €', 'ì¡°ì°¨']

        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ í›„ ì¡°ì‚¬ ì œê±°
        words = normalized.split()
        cleaned_words = []

        for word in words:
            cleaned_word = word

            # ë³µí•© ì¡°ì‚¬ ë¨¼ì € ì œê±°
            for particle in compound_particles:
                if cleaned_word.endswith(particle):
                    cleaned_word = cleaned_word[:-len(particle)]
                    break

            # ë‹¨ìˆœ ì¡°ì‚¬ ì œê±°
            if cleaned_word:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ˆë©´
                for particle in simple_particles:
                    if cleaned_word.endswith(particle):
                        cleaned_word = cleaned_word[:-len(particle)]
                        break

            # 2ê¸€ì ì´ìƒë§Œ í¬í•¨
            if cleaned_word and len(cleaned_word) >= 2:
                cleaned_words.append(cleaned_word)

        # 3. í‚¤ì›Œë“œ ì •ë ¬ (ìˆœì„œ ë¬´ê´€ ìºì‹œ íˆíŠ¸)
        cleaned_words.sort()

        # 4. ìºì‹œ í‚¤ ìƒì„±
        cache_str = f"{mode}:{'_'.join(cleaned_words)}"
        hash_key = hashlib.md5(cache_str.encode('utf-8')).hexdigest()

        # ë””ë²„ê¹… (í•„ìš”ì‹œ í™œì„±í™”)

        return hash_key

    
    def answer(self, query: str, mode: str = 'auto') -> str:
        """ë‹µë³€ ìƒì„± ë©”ì„œë“œ"""
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._get_enhanced_cache_key(query, mode)

        # ìºì‹œ í™•ì¸
        if cache_key in self.answer_cache:
            cached_response, cached_time = self.answer_cache[cache_key]
            if time.time() - cached_time < self.cache_ttl:
                self.answer_cache.move_to_end(cache_key)
                return cached_response
            else:
                del self.answer_cache[cache_key]

        # ì‹¤ì œ ë‹µë³€ ìƒì„±
        response = self._answer_internal(query, mode)

        # ìºì‹œ ì €ì¥
        self.answer_cache[cache_key] = (response, time.time())
        if len(self.answer_cache) > self.max_cache_size:
            self.answer_cache.popitem(last=False)

        return response
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        # CacheModuleì„ ì‚¬ìš©í•˜ì—¬ ìºì‹œ ì´ˆê¸°í™”
        if self.cache_module:
            self.cache_module.clear_all_cache()
            return

        # CacheModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        self.answer_cache.clear()
        self.documents_cache.clear()
        self.metadata_cache.clear()
    
    def get_cache_stats(self) -> Dict:
        """ìºì‹œ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        # CacheModuleì„ ì‚¬ìš©í•˜ì—¬ ìºì‹œ í†µê³„ ë°˜í™˜
        if self.cache_module:
            return self.cache_module.get_cache_stats()

        # CacheModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        response_cache_size = len(self.response_cache) if hasattr(self, 'response_cache') else 0
        total_size = len(self.answer_cache) + len(self.documents_cache) + len(self.metadata_cache) + response_cache_size
        return {
            'size': response_cache_size,  # For compatibility with test
            'hits': self.cache_hits,
            'misses': self.cache_misses,
            'answer_cache_size': len(self.answer_cache),
            'document_cache_size': len(self.documents_cache),
            'metadata_cache_size': len(self.metadata_cache),
            'response_cache_size': response_cache_size,
            'total_cache_size': total_size,
            'max_cache_size': self.max_cache_size,
            'cache_ttl_seconds': self.cache_ttl
        }
    
    def _answer_internal(self, query: str, mode: str = 'auto') -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            mode: ê²€ìƒ‰ ëª¨ë“œ (í•­ìƒ 'document' ì‚¬ìš©)
        """
        
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()
        error_msg = None
        success = True
        response = ""
        metadata = {}
        
        try:
            # ë¡œê¹… ì‹œìŠ¤í…œ ì‹œì‘ - log_queryëŠ” ë‹µë³€ ì™„ë£Œ í›„ì— í˜¸ì¶œí•´ì•¼ í•¨
            # if chat_logger:
            #     chat_logger.log_query(query, "started")
            
            # ê²€ìƒ‰ ì˜ë„ ë¶„ë¥˜
            if mode == 'auto':
                with TimerContext(chat_logger, "classify_intent") if chat_logger else nullcontext():
                    if self.intent_module:
                        mode = self.intent_module.classify_search_intent(query)
                    else:
                        mode = self._classify_search_intent(query)
            
            self.search_mode = mode
            metadata['search_mode'] = mode
            
            # ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            query_lower = query.lower()

            # document ëª¨ë“œì¸ ê²½ìš°
            if self.search_mode == 'document':
                # "ë¬¸ì„œ", "ì°¾ì•„", "ê²€ìƒ‰" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì—¬ëŸ¬ ë¬¸ì„œ ëª©ë¡ ë°˜í™˜
                if any(keyword in query_lower for keyword in ["ë¬¸ì„œ", "ì°¾ì•„", "ê²€ìƒ‰", "ì–´ë–¤", "ë¬´ì—‡", "ë­"]):
                    # ì—¬ëŸ¬ ë¬¸ì„œ ê²€ìƒ‰
                    search_results = self._search_by_content(query)

                    if not search_results:
                        response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                    else:
                        # ìƒìœ„ 5ê°œ ë¬¸ì„œ ëª©ë¡ í‘œì‹œ
                        response = f"**{query}** ê²€ìƒ‰ ê²°ê³¼\n\n"

                        # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì •ë ¬ ìˆœì„œ ìœ ì§€
                        seen = set()
                        unique_results = []
                        for r in sorted(search_results, key=lambda x: x.get('score', 0), reverse=True):
                            if r['filename'] not in seen:
                                seen.add(r['filename'])
                                unique_results.append(r)

                        response += f"ì´ {len(unique_results)}ê°œ ë¬¸ì„œ ë°œê²¬\n\n"

                        for i, result in enumerate(unique_results[:10], 1):
                            response += f"**{i}. {result['filename']}**\n"

                            # Everything searchì˜ ê²½ìš° ë©”íƒ€ë°ì´í„°ë§Œ ê¹”ë”í•˜ê²Œ í‘œì‹œ
                            if result.get('source') == 'everything_search':
                                if result.get('date'):
                                    response += f"   ğŸ“… ë‚ ì§œ: {result['date']}\n"
                                if result.get('category') and result['category'] != 'ê¸°íƒ€':
                                    response += f"   ğŸ“ ì¹´í…Œê³ ë¦¬: {result['category']}\n"
                                if result.get('keywords'):
                                    # í‚¤ì›Œë“œë¥¼ ê¹”ë”í•˜ê²Œ í‘œì‹œ
                                    keywords_list = result['keywords'].split()[:5]  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œë§Œ
                                    if keywords_list:
                                        response += f"   ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(keywords_list)}\n"
                            # ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼
                            elif result.get('context'):
                                # OCR í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³  ê¹”ë”í•˜ê²Œ ì²˜ë¦¬
                                context = result['context']
                                if '[OCR' in context or 'í˜ì´ì§€' in context:
                                    # OCR í…ìŠ¤íŠ¸ëŠ” í‘œì‹œí•˜ì§€ ì•Šê³  ë©”íƒ€ë°ì´í„°ë§Œ
                                    response += f"   ğŸ“„ ìŠ¤ìº” ë¬¸ì„œ (OCR í•„ìš”)\n"
                                else:
                                    # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ê¹”ë”í•˜ê²Œ í‘œì‹œ
                                    clean_text = context.replace('\n', ' ').strip()[:150]
                                    response += f"   ğŸ“ {clean_text}...\n"
                            response += "\n"

                        if len(unique_results) > 5:
                            response += f"\n... ì™¸ {len(unique_results) - 5}ê°œ ë¬¸ì„œ\n"
                else:
                    # ë‹¨ì¼ ë¬¸ì„œ ìƒì„¸ ë‹µë³€
                    # Everything searchë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ê´€ë ¨ëœ ë¬¸ì„œ ì°¾ê¸°
                    if self.everything_search:
                        search_results = self.everything_search.search(query, limit=1)
                        if search_results:
                            top_result = search_results[0]
                            doc_path = Path(top_result['path'])

                            if doc_path.exists():
                                # LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ë° ë‹µë³€ ìƒì„±
                                if self.llm_module:
                                    # LLMModuleì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ìš”ì•½ ìƒì„±
                                    try:
                                        # ë¬¸ì„œ ë‚´ìš© ì½ê¸°
                                        if self.document_module:
                                            doc_info = self.document_module.extract_pdf_text(doc_path)
                                            content = doc_info.get('text', '')
                                        else:
                                            content = self._extract_full_pdf_content(doc_path).get('text', '')

                                        response = self.llm_module.generate_smart_summary(content, str(doc_path.name))
                                    except Exception as e:
                                        logger.error(f"LLMModule ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
                                        response = self._generate_llm_summary(doc_path, query)
                                else:
                                    # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                                    response = self._generate_llm_summary(doc_path, query)
                            else:
                                response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                        else:
                            response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                    else:
                        # ê¸°ì¡´ ë°©ì‹ (Everything search ì—†ì„ ë•Œ)
                        doc_path = self.find_best_document(query)

                        if not doc_path:
                            response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                        else:
                            # LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ë° ë‹µë³€ ìƒì„±
                            if self.llm_module:
                                # LLMModuleì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ìš”ì•½ ìƒì„±
                                try:
                                    # ë¬¸ì„œ ë‚´ìš© ì½ê¸°
                                    if self.document_module:
                                        doc_info = self.document_module.extract_pdf_text(doc_path)
                                        content = doc_info.get('text', '')
                                    else:
                                        content = self._extract_full_pdf_content(doc_path).get('text', '')

                                    response = self.llm_module.generate_smart_summary(content, str(doc_path.name))
                                except Exception as e:
                                    logger.error(f"LLMModule ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
                                    response = self._generate_llm_summary(doc_path, query)
                            else:
                                # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                                response = self._generate_llm_summary(doc_path, query)
            else:
                # Document ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš° (ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨)
                response = "âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° ë° ë¡œê¹…
            processing_time = time.time() - start_time
            if chat_logger:
                chat_logger.log_query(
                    query=query,
                    response=response,
                    search_mode=self.search_mode,
                    processing_time=processing_time,
                    metadata=metadata
                )

            return response

        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ
            error_msg = str(e)
            success = False
            response = f" ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}"
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # ë¡œê¹… ì‹œìŠ¤í…œ
            if chat_logger:
                chat_logger.log_error(
                    error_type=type(e).__name__,
                    error_msg=error_msg,
                    query=query
                )
            
            return response
    
    def _get_detail_only_prompt(self, query: str, context: str, filename: str) -> str:
        # _create_promptìœ¼ë¡œ ìœ„ì„
        return self._create_prompt(query, context, filename, 'detailed')

    def _placeholder1(self):
        """ê¸°ë³¸ ì •ë³´ ì œì™¸í•œ ìƒì„¸ ë‚´ìš©ë§Œ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë¬¸ì„œì—ì„œ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì„¸ìš”. 
ï¸ ê¸°ì•ˆì, ë‚ ì§œ, ë¬¸ì„œë²ˆí˜¸ ë“± ê¸°ë³¸ ì •ë³´ëŠ” ì œì™¸í•˜ê³  ì‹¤ì§ˆì ì¸ ë‚´ìš©ë§Œ ì‘ì„±í•˜ì„¸ìš”.

 **êµ¬ë§¤/ìˆ˜ë¦¬ ì‚¬ìœ  ë° í˜„í™©**
â€¢ ì–´ë–¤ ë¬¸ì œê°€ ìˆì—ˆëŠ”ì§€
â€¢ í˜„ì¬ ìƒí™©ì€ ì–´ë–¤ì§€
â€¢ ì œì•ˆí•˜ëŠ” í•´ê²°ì±…ì€ ë¬´ì—‡ì¸ì§€

 **ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­** (ìˆëŠ” ê²½ìš°ë§Œ)
â€¢ ì¥ë¹„ ì‚¬ì–‘ì´ë‚˜ ëª¨ë¸
â€¢ ê²€í† í•œ ëŒ€ì•ˆë“¤
â€¢ ì„ íƒ ê·¼ê±°

 **ë¹„ìš© ê´€ë ¨** (ìˆëŠ” ê²½ìš°ë§Œ)
â€¢ ì˜ˆìƒ ë¹„ìš©
â€¢ ì—…ì²´ ì •ë³´

ë¬¸ì„œ: {filename}
ë‚´ìš©: {context[:5000]}

ì§ˆë¬¸: {query}

ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
"""

    
    def _is_gian_document(self, text: str) -> bool:
        """ê¸°ì•ˆì„œ ë¬¸ì„œì¸ì§€ í™•ì¸"""
        gian_keywords = ['ì¥ë¹„êµ¬ë§¤/ìˆ˜ë¦¬ ê¸°ì•ˆì„œ', 'ê¸°ì•ˆë¶€ì„œ', 'ê¸°ì•ˆì', 'ê¸°ì•ˆì¼ì', 'ê²°ì¬', 'í•©ì˜']
        matches = sum(1 for keyword in gian_keywords if keyword in text[:500])
        return matches >= 3
    
    def _try_ocr_extraction(self, pdf_path: Path) -> str:
        """OCRì„ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„"""
        try:
            # OCR í”„ë¡œì„¸ì„œ ì„í¬íŠ¸
            from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
            
            ocr_processor = EnhancedOCRProcessor()
            text, metadata = ocr_processor.extract_text_with_ocr(str(pdf_path))
            
            if metadata.get('ocr_performed'):
                if logger:
                    logger.info(f"OCR ì„±ê³µ: {pdf_path.name} - {metadata.get('ocr_text_length', 0)}ì ì¶”ì¶œ")
                return text
            else:
                if logger:
                    logger.warning(f"OCR ì‹¤íŒ¨: {pdf_path.name}")
                return ""
                
        except ImportError:
            if logger:
                logger.warning("OCR ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€ - pytesseract ë˜ëŠ” Tesseract ë¯¸ì„¤ì¹˜")
            return ""
        except Exception as e:
            if logger:
                logger.error(f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {pdf_path.name} - {e}")
            return ""
    
    def _extract_full_pdf_content(self, pdf_path: Path) -> dict:
        """PDF ì „ì²´ ë‚´ìš© ì¶”ì¶œ ë° êµ¬ì¡°í™” - DocumentModule í™œìš©ìœ¼ë¡œ ë‹¨ìˆœí™”"""
        try:
            # DocumentModuleì´ ìˆìœ¼ë©´ ì‚¬ìš©
            if self.document_module:
                result = self.document_module.extract_pdf_text_with_retry(pdf_path, max_retries=2)
                if result:
                    return result

            # ê¸°ë³¸ PDF ì¶”ì¶œ ë¡œì§
            return self._extract_pdf_info(pdf_path)

        except Exception as e:
            if logger:
                logger.error(f"PDF ì¶”ì¶œ ì‹¤íŒ¨ ({pdf_path.name}): {e}")
            return {'error': str(e)}
    
    def _prepare_formatted_data(self, pdf_info: Dict, pdf_path: Path) -> Dict:
        """í¬ë§·í„°ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        formatted_info = {
            'ì œëª©': pdf_info.get('ì œëª©', pdf_path.stem),
            'ê¸°ì•ˆì': pdf_info.get('ê¸°ì•ˆì', ''),
            'ê¸°ì•ˆì¼ì': pdf_info.get('ê¸°ì•ˆì¼ì', ''),
            'ê¸°ì•ˆë¶€ì„œ': pdf_info.get('ê¸°ì•ˆë¶€ì„œ', '')
        }
        
        # í•µì‹¬ ìš”ì•½ ì¶”ì¶œ (3ì¤„)
        if self.formatter and pdf_info.get('ì „ì²´í…ìŠ¤íŠ¸'):
            key_points = self.formatter.extract_key_points(pdf_info['ì „ì²´í…ìŠ¤íŠ¸'])
            formatted_info['í•µì‹¬ìš”ì•½'] = key_points
        
        # ìƒì„¸ ë‚´ìš© êµ¬ì¡°í™”
        detail_content = []
        
        # ê°œìš”ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if pdf_info.get('ê°œìš”'):
            detail_content.append({
                'í•­ëª©': 'ê°œìš”',
                'ë‚´ìš©': pdf_info['ê°œìš”'][:200]
            })
        
        # ì¥ì• /ìˆ˜ë¦¬ ë‚´ìš©
        if any(k in pdf_info for k in ['ì¥ì• ë‚´ìš©', 'ìˆ˜ë¦¬ë‚´ìš©', 'êµ¬ë§¤ì‚¬ìœ ']):
            for key in ['ì¥ì• ë‚´ìš©', 'ìˆ˜ë¦¬ë‚´ìš©', 'êµ¬ë§¤ì‚¬ìœ ']:
                if pdf_info.get(key):
                    detail_content.append({
                        'í•­ëª©': key,
                        'ë‚´ìš©': pdf_info[key][:200]
                    })
        
        formatted_info['ìƒì„¸ë‚´ìš©'] = detail_content
        
        # ë¹„ìš© ì •ë³´
        if pdf_info.get('ë¹„ìš©ë‚´ì—­'):
            formatted_info['ë¹„ìš©ì •ë³´'] = pdf_info['ë¹„ìš©ë‚´ì—­']
        
        # ê²€í†  ì˜ê²¬
        opinions = []
        if pdf_info.get('ê²€í† ê²°ê³¼'):
            opinions.append(pdf_info['ê²€í† ê²°ê³¼'])
        if pdf_info.get('ì¶”ì²œì‚¬í•­'):
            opinions.append(pdf_info['ì¶”ì²œì‚¬í•­'])
        if opinions:
            formatted_info['ê²€í† ì˜ê²¬'] = opinions
        
        # ê´€ë ¨ ì •ë³´
        related = []
        if pdf_info.get('ì—…ì²´'):
            related.append(f"ì—…ì²´: {pdf_info['ì—…ì²´']}")
        if pdf_info.get('ë„ì…ì—°ë„'):
            related.append(f"ë„ì…: {pdf_info['ë„ì…ì—°ë„']}ë…„")
        if related:
            formatted_info['ê´€ë ¨ì •ë³´'] = related
        
        return formatted_info
    
    
    
    
    def _extract_key_sentences(self, content, num_sentences=5):
        """í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ í—¬í¼"""
        if not content:
            return []

        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]

        if len(sentences) <= num_sentences:
            return sentences

        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚°
        important_keywords = ['ê²°ì •', 'ìŠ¹ì¸', 'êµ¬ë§¤', 'ê³„ì•½', 'ì˜ˆì‚°', 'ì§„í–‰', 'ì™„ë£Œ']
        scored_sentences = []

        for sentence in sentences:
            score = sum(1 for keyword in important_keywords if keyword in sentence)
            scored_sentences.append((sentence, score))

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        return [s[0] for s in scored_sentences[:num_sentences]]
    def _generate_llm_summary(self, pdf_path: Path, query: str) -> str:
        """LLMì„ ì‚¬ìš©í•œ ìƒì„¸ ìš”ì•½ - LLMModuleë¡œ ìœ„ì„ (2025-09-29 ë¦¬íŒ©í† ë§)"""
        if logger:
            logger.info("LLM ìš”ì•½ ìƒì„± ì‹œì‘")

        # LLMModuleì´ ìˆìœ¼ë©´ ìœ„ì„
        if self.llm_module:
            try:
                # PDF ë‚´ìš© ì¶”ì¶œ
                pdf_info = self._extract_full_pdf_content(pdf_path)
                content = pdf_info.get('text', '') if pdf_info and 'error' not in pdf_info else ''

                # LLMModuleì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤ë§ˆíŠ¸ ìš”ì•½ ìƒì„±
                return self.llm_module.generate_smart_summary(content, str(pdf_path.name), query)
            except Exception as e:
                if logger:
                    logger.error(f"LLMModule ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}, í´ë°± ì‚¬ìš©")

        # ê°„ë‹¨í•œ í´ë°± êµ¬í˜„
        pdf_info = self._extract_full_pdf_content(pdf_path)
        if pdf_info and 'error' not in pdf_info:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            info_parts = []
            if 'ì œëª©' in pdf_info:
                info_parts.append(f"ì œëª©: {pdf_info['ì œëª©']}")
            if 'ê¸°ì•ˆì' in pdf_info:
                info_parts.append(f"ê¸°ì•ˆì: {pdf_info['ê¸°ì•ˆì']}")
            if 'ê¸ˆì•¡' in pdf_info:
                info_parts.append(f"ê¸ˆì•¡: {pdf_info['ê¸ˆì•¡']}")

            return "\n".join(info_parts) if info_parts else "ë¬¸ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            return "ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    def _collect_statistics_data(self, query: str) -> Dict:
        """í†µê³„ ë°ì´í„° ìˆ˜ì§‘ ë° êµ¬ì¡°í™” - StatisticsModuleë¡œ ìœ„ì„ (2025-09-29 ë¦¬íŒ©í† ë§)"""
        if self.statistics_module:
            return self.statistics_module.collect_statistics_data(query, self.metadata_cache)

        # ê°„ë‹¨í•œ í´ë°± êµ¬í˜„
        return {
            'title': 'í†µê³„ ë¶„ì„',
            'headers': ['í•­ëª©', 'ê°’'],
            'table_data': [['ì´ ë¬¸ì„œ ìˆ˜', str(len(self.metadata_cache))]],
            'ì´ê³„': f'ì´ {len(self.metadata_cache)}ê°œ ë¬¸ì„œ',
            'ë¶„ì„': {'note': 'StatisticsModule í•„ìš”'},
            'ì¶”ì²œ': ['StatisticsModuleì„ ë¡œë“œí•˜ì—¬ ìƒì„¸ í†µê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”.']
        }
    
    def _format_conditions(self, conditions: dict) -> str:
        """ê²€ìƒ‰ ì¡°ê±´ì„ ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…"""
        formatted = []
        
        if 'location' in conditions:
            formatted.append(f"â€¢ ìœ„ì¹˜: {conditions['location']}")
        
        if 'manufacturer' in conditions:
            formatted.append(f"â€¢ ì œì¡°ì‚¬: {conditions['manufacturer']}")
        
        if 'year' in conditions:
            year_str = f"â€¢ ì—°ë„: {conditions['year']}ë…„"
            if conditions.get('year_range') == 'before':
                year_str += " ì´ì „"
            if conditions.get('year_range') == 'after':
                year_str += " ì´í›„"
            if conditions.get('year_range') == 'between':
                year_str = f"â€¢ ì—°ë„: {conditions.get('year_start')}ë…„ ~ {conditions.get('year_end')}ë…„"
            formatted.append(year_str)
        
        if 'price' in conditions:
            price_str = f"â€¢ ê¸ˆì•¡: {conditions['price']:,.0f}ì›"
            if conditions.get('price_range') == 'above':
                price_str += " ì´ìƒ"
            if conditions.get('price_range') == 'below':
                price_str += " ì´í•˜"
            formatted.append(price_str)
        
        if 'equipment_type' in conditions:
            formatted.append(f"â€¢ ì¥ë¹„ ìœ í˜•: {conditions['equipment_type']}")
        
        if 'model' in conditions:
            formatted.append(f"â€¢ ëª¨ë¸: {conditions['model']}")
        
        return '\n'.join(formatted) if formatted else "ì¡°ê±´ ì—†ìŒ"
    
    
    def _determine_equipment_category(self, equipment_name: str, item_text: str) -> str:
        """ì¥ë¹„ëª…ê³¼ í…ìŠ¤íŠ¸ë¡œ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
        name_lower = equipment_name.lower()
        text_lower = item_text.lower()
        
        if any(kw in name_lower or kw in text_lower for kw in ['camera', 'ì¹´ë©”ë¼', 'ccu', 'viewfinder', 'ë·°íŒŒì¸ë”']):
            return " ì¹´ë©”ë¼ ì‹œìŠ¤í…œ"
        if any(kw in name_lower or kw in text_lower for kw in ['monitor', 'ëª¨ë‹ˆí„°', 'display']):
            return "ï¸ ëª¨ë‹ˆí„°"
        if any(kw in name_lower or kw in text_lower for kw in ['audio', 'ì˜¤ë””ì˜¤', 'mixer', 'ë¯¹ì„œ', 'mic', 'ë§ˆì´í¬']):
            return "ï¸ ì˜¤ë””ì˜¤ ì¥ë¹„"
        if any(kw in name_lower or kw in text_lower for kw in ['server', 'ì„œë²„', 'storage', 'ìŠ¤í† ë¦¬ì§€']):
            return " ì„œë²„/ìŠ¤í† ë¦¬ì§€"
        if any(kw in name_lower or kw in text_lower for kw in ['switch', 'ìŠ¤ìœ„ì¹˜', 'router', 'ë¼ìš°í„°', 'matrix']):
            return " ìŠ¤ìœ„ì¹­/ë¼ìš°íŒ…"
        if any(kw in name_lower or kw in text_lower for kw in ['cable', 'ì¼€ì´ë¸”', 'connector', 'ì»¤ë„¥í„°']):
            return " ì¼€ì´ë¸”/ì»¤ë„¥í„°"
        if any(kw in name_lower or kw in text_lower for kw in ['tripod', 'íŠ¸ë¼ì´í¬ë“œ', 'pedestal', 'í˜ë°ìŠ¤íƒˆ']):
            return " ì¹´ë©”ë¼ ì§€ì›ì¥ë¹„"
        if any(kw in name_lower or kw in text_lower for kw in ['intercom', 'ì¸í„°ì»´', 'talkback']):
            return " ì¸í„°ì»´"
        if any(kw in name_lower or kw in text_lower for kw in ['converter', 'ì»¨ë²„í„°', 'encoder', 'ì¸ì½”ë”']):
            return " ì»¨ë²„í„°/ì¸ì½”ë”"
        else:
            return " ê¸°íƒ€ ì¥ë¹„"

    def _count_by_field(self, content: str, field_name: str, search_value: str) -> dict:
        """íŠ¹ì • í•„ë“œê°’ìœ¼ë¡œ ì¥ë¹„ ìˆ˜ëŸ‰ ê³„ì‚°"""
        lines = content.split('\n')
        count = 0
        items = []
        current_item = []
        is_matching = False
        
        for line in lines:
            # ìƒˆë¡œìš´ ì¥ë¹„ í•­ëª© ì‹œì‘
            if re.match(r'^\[\d{4}\]', line):
                # ì´ì „ í•­ëª©ì´ ë§¤ì¹­ë˜ì—ˆìœ¼ë©´ ì¹´ìš´íŠ¸
                if is_matching and current_item:
                    count += 1
                    if len(items) < 10:  # ìƒ˜í”Œ 10ê°œë§Œ ì €ì¥
                        items.append('\n'.join(current_item))
                
                # ìƒˆ í•­ëª© ì‹œì‘
                current_item = [line]
                is_matching = False
            if current_item:
                current_item.append(line)
                # í•„ë“œë³„ ë§¤ì¹­ í™•ì¸
                if field_name == "ë‹´ë‹¹ì" and "ë‹´ë‹¹ì:" in line:
                    if search_value in line:
                        is_matching = True
                if field_name == "ìœ„ì¹˜" and "ìœ„ì¹˜:" in line:
                    # ì •í™•í•œ ìœ„ì¹˜ ë§¤ì¹­ ë¡œì§ ì ìš©
                    location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', line)
                    if location_match:
                        actual_location = location_match.group(1).strip()
                        
                        # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™
                        if search_value == actual_location:
                            # ì™„ì „ ì¼ì¹˜
                            is_matching = True
                        if search_value == 'ë¶€ì¡°ì •ì‹¤':
                            # 'ë¶€ì¡°ì •ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*ë¶€ì¡°ì •ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                            is_matching = actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
                        if search_value == 'ìŠ¤íŠœë””ì˜¤':
                            # 'ìŠ¤íŠœë””ì˜¤'ë¡œ ê²€ìƒ‰ì‹œ '*ìŠ¤íŠœë””ì˜¤' íŒ¨í„´ë§Œ ë§¤ì¹­ 
                            is_matching = actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
                        if search_value == 'í¸ì§‘ì‹¤':
                            # 'í¸ì§‘ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*í¸ì§‘ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                            is_matching = actual_location.endswith('í¸ì§‘ì‹¤')
                        if search_value in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
                            # ì¤‘ê³„ì°¨ ê²€ìƒ‰ì‹œ Van ê´€ë ¨ ìœ„ì¹˜ ëª¨ë‘ ë§¤ì¹­
                            is_matching = 'Van' in actual_location or 'VAN' in actual_location
                        if len(search_value) > 3:
                            # 3ê¸€ì ì´ìƒì˜ êµ¬ì²´ì ì¸ ìœ„ì¹˜ëª…ì€ ë¶€ë¶„ ë§¤ì¹­ í—ˆìš©
                            is_matching = search_value in actual_location
                if field_name == "ë²¤ë”ì‚¬" and "ë²¤ë”ì‚¬:" in line:
                    if search_value in line:
                        is_matching = True
                if field_name == "ì œì¡°ì‚¬" and "ì œì¡°ì‚¬:" in line:
                    if search_value.upper() in line.upper():
                        is_matching = True
        
        # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
        if is_matching and current_item:
            count += 1
            if len(items) < 10:
                items.append('\n'.join(current_item))
        
        return {
            'count': count,
            'sample_items': items
        }
    
    

    def _extract_document_metadata(self, file_path):
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í—¬í¼"""
        metadata = {}

        try:
            # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
            filename = file_path.stem if hasattr(file_path, 'stem') else str(file_path)

            # ë‚ ì§œ ì¶”ì¶œ
            date_patterns = [
                r'(\d{4})[.\-_](\d{1,2})[.\-_](\d{1,2})',
                r'(\d{4})(\d{2})(\d{2})',
                r'(\d{2})[.\-_](\d{1,2})[.\-_](\d{1,2})'
            ]
            for pattern in date_patterns:
                match = re.search(pattern, filename)
                if match:
                    metadata['date'] = match.group(0)
                    break

            # ê¸°ì•ˆì ì¶”ì¶œ
            author_patterns = [
                r'([ê°€-í£]{2,4})([\s_\-])?ê¸°ì•ˆ',
                r'ê¸°ì•ˆì[\s_\-:]*([ê°€-í£]{2,4})',
                r'ì‘ì„±ì[\s_\-:]*([ê°€-í£]{2,4})'
            ]
            for pattern in author_patterns:
                match = re.search(pattern, filename)
                if match:
                    metadata['author'] = match.group(1) if 'ê¸°ì•ˆ' in pattern else match.group(1)
                    break

            return metadata
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {}

    def _score_document_relevance(self, content, keywords):
        """ë¬¸ì„œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° í—¬í¼"""
        if not content or not keywords:
            return 0

        score = 0
        content_lower = content.lower()

        for keyword in keywords:
            keyword_lower = keyword.lower()
            # ì •í™•í•œ ë§¤ì¹­
            exact_matches = content_lower.count(keyword_lower)
            score += exact_matches * 2

            # ë¶€ë¶„ ë§¤ì¹­
            if len(keyword_lower) > 2:
                partial_matches = sum(1 for word in content_lower.split()
                                    if keyword_lower in word)
                score += partial_matches

        # ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™”
        doc_length = len(content)
        if doc_length > 0:
            score = score / (doc_length / 1000)  # 1000ì ë‹¨ìœ„ë¡œ ì •ê·œí™”

        return score

    def _format_search_result(self, file_path, content, metadata):
        """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ… í—¬í¼"""
        result = []

        # ì œëª©
        filename = file_path.stem if hasattr(file_path, 'stem') else str(file_path)
        result.append(f"ğŸ“„ {filename}")
        result.append("-" * 50)

        # ë©”íƒ€ë°ì´í„°
        if metadata.get('date'):
            result.append(f"ğŸ“… ë‚ ì§œ: {metadata['date']}")
        if metadata.get('author'):
            result.append(f"âœï¸ ê¸°ì•ˆì: {metadata['author']}")

        # ë‚´ìš© ìš”ì•½ (ì²˜ìŒ 200ì)
        if content:
            summary = content[:200].replace('\n', ' ')
            result.append(f"\nğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
            result.append(summary + "...")

        return '\n'.join(result)

    def _aggregate_search_results(self, results):
        """ê²€ìƒ‰ ê²°ê³¼ í†µí•© í—¬í¼"""
        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        aggregated = []
        aggregated.append(f"ğŸ” ì´ {len(results)}ê°œ ë¬¸ì„œ ë°œê²¬\n")
        aggregated.append("=" * 60)

        for i, result in enumerate(results, 1):
            aggregated.append(f"\n[{i}] {result}")
            if i < len(results):
                aggregated.append("\n" + "-" * 60)

        return '\n'.join(aggregated)
    def _is_location_match(self, item_lines: list, location: str) -> bool:
        """ìœ„ì¹˜ ë§¤ì¹­ ë¡œì§ ê°œì„  - ì •í™•í•œ ìœ„ì¹˜ ë§¤ì¹­"""
        item_text = '\n'.join(item_lines)
        
        # ìœ„ì¹˜ ì •ë³´ê°€ ìˆëŠ” ë¼ì¸ ì°¾ê¸°
        for line in item_lines:
            if 'ìœ„ì¹˜:' in line or 'ìœ„ì¹˜ì •ë³´:' in line:
                # ì‹¤ì œ ìœ„ì¹˜ ì¶”ì¶œ
                location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', line)
                if location_match:
                    actual_location = location_match.group(1).strip()
                    
                    # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™
                    if location == actual_location:
                        # ì™„ì „ ì¼ì¹˜ (ê°€ì¥ ìš°ì„ )
                        return True
                    if location == 'ë¶€ì¡°ì •ì‹¤':
                        # 'ë¶€ì¡°ì •ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*ë¶€ì¡°ì •ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                        return actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
                    if location == 'ìŠ¤íŠœë””ì˜¤':
                        # 'ìŠ¤íŠœë””ì˜¤'ë¡œ ê²€ìƒ‰ì‹œ '*ìŠ¤íŠœë””ì˜¤' íŒ¨í„´ë§Œ ë§¤ì¹­ 
                        return actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
                    if location == 'í¸ì§‘ì‹¤':
                        # 'í¸ì§‘ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*í¸ì§‘ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                        return actual_location.endswith('í¸ì§‘ì‹¤')
                    if location in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
                        # ì¤‘ê³„ì°¨ ê²€ìƒ‰ì‹œ Van ê´€ë ¨ ìœ„ì¹˜ ëª¨ë‘ ë§¤ì¹­
                        return 'Van' in actual_location or 'VAN' in actual_location
                    if location == "ê´‘í™”ë¬¸ë¶€ì¡°ì •ì‹¤":
                        # "ê´‘í™”ë¬¸ ë¶€ì¡°ì •ì‹¤" ê°™ì€ ë³µí•© ìœ„ì¹˜ëª… ì²˜ë¦¬
                        return "ê´‘í™”ë¬¸" in actual_location and "ë¶€ì¡°ì •ì‹¤" in actual_location
                    if len(location) > 3:
                        # 3ê¸€ì ì´ìƒì˜ êµ¬ì²´ì ì¸ ìœ„ì¹˜ëª…ì€ ë¶€ë¶„ ë§¤ì¹­ í—ˆìš©
                        return location in actual_location
        
        return False
    def _check_location_in_item(self, item_text: str, search_location: str) -> bool:
        """í•­ëª©ì—ì„œ ìœ„ì¹˜ ì¡°ê±´ í™•ì¸"""
        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
        location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', item_text)
        if not location_match:
            return False
            
        actual_location = location_match.group(1).strip()
        
        # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™ ì ìš©
        if search_location == actual_location:
            return True
        if search_location == 'ë¶€ì¡°ì •ì‹¤':
            return actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
        if search_location == 'ìŠ¤íŠœë””ì˜¤':
            return actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
        if search_location == 'í¸ì§‘ì‹¤':
            return actual_location.endswith('í¸ì§‘ì‹¤')
        if search_location in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
            return 'Van' in actual_location or 'VAN' in actual_location or 'ì¤‘ê³„ì°¨' in actual_location
        if len(search_location) > 3:
            return search_location in actual_location
        
        return False
