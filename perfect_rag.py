#!/usr/bin/env python3
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import threading
from queue import Queue
from functools import partial

"""
Perfect RAG - ì‹¬í”Œí•˜ì§€ë§Œ ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
ë™ì ìœ¼ë¡œ ëª¨ë“  PDF ë¬¸ì„œì™€ ìì‚° ë°ì´í„°ë¥¼ ì •í™•í•˜ê²Œ ì²˜ë¦¬
"""

import re
import warnings
import logging
import pdfplumber
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import json
import sys
import os
from functools import lru_cache
import hashlib
import time
from contextlib import nullcontext

# ë¡œê¹… ì‹œìŠ¤í…œ ì¶”ê°€
try:
    from log_system import get_logger, TimerContext
    chat_logger = get_logger()
    # chat_loggerë¥¼ loggerë¡œë„ ì‚¬ìš©
    logger = chat_logger
except ImportError:
    chat_logger = None
    logger = None
    TimerContext = None
    
# query_loggerëŠ” log_systemìœ¼ë¡œ í†µí•©ë¨

# ì‘ë‹µ í¬ë§·í„° ì¶”ê°€
try:
    from response_formatter import ResponseFormatter
except ImportError:
    ResponseFormatter = None

# FontBBox ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", message=".*FontBBox.*")
warnings.filterwarnings("ignore", category=UserWarning, module="pdfplumber")

# pdfplumber ë¡œê¹… ë ˆë²¨ ì¡°ì •
logging.getLogger("pdfplumber").setLevel(logging.ERROR)
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ê¸°
current_dir = Path(__file__).parent.absolute()
sys.path.insert(0, str(current_dir))

# ì„¤ì • íŒŒì¼ import
try:
    from config_manager import config_manager as cfg
    USE_YAML_CONFIG = True
except ImportError:
    import config as cfg
    USE_YAML_CONFIG = False

from rag_system.qwen_llm import QwenLLM
from rag_system.llm_singleton import LLMSingleton
from metadata_db import MetadataDB  # Phase 1.2: ë©”íƒ€ë°ì´í„° DB

# Everything-like ì´ˆê³ ì† ê²€ìƒ‰ ì‹œìŠ¤í…œ ì¶”ê°€
try:
    from everything_like_search import EverythingLikeSearch
    EVERYTHING_SEARCH_AVAILABLE = True
except ImportError:
    EVERYTHING_SEARCH_AVAILABLE = False
    if logger:
        logger.warning("EverythingLikeSearch not available, using legacy search")

# ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œìŠ¤í…œ ì¶”ê°€ (2025-09-29)
try:
    from metadata_extractor import MetadataExtractor
    METADATA_EXTRACTOR_AVAILABLE = True
except ImportError:
    METADATA_EXTRACTOR_AVAILABLE = False
    if logger:
        logger.warning("MetadataExtractor not available, metadata extraction disabled")

# ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆë“¤ (2025-09-29)
try:
    from search_module import SearchModule
    SEARCH_MODULE_AVAILABLE = True
except ImportError:
    SEARCH_MODULE_AVAILABLE = False
    if logger:
        logger.warning("SearchModule not available - using embedded search")

try:
    from document_module import DocumentModule
    DOCUMENT_MODULE_AVAILABLE = True
except ImportError:
    DOCUMENT_MODULE_AVAILABLE = False
    if logger:
        logger.warning("DocumentModule not available - using embedded document processing")

try:
    from llm_module import LLMModule
    LLM_MODULE_AVAILABLE = True
except ImportError:
    LLM_MODULE_AVAILABLE = False
    if logger:
        logger.warning("LLMModule not available - using embedded LLM handling")

try:
    from cache_module import CacheModule
    CACHE_MODULE_AVAILABLE = True
except ImportError:
    CACHE_MODULE_AVAILABLE = False
    if logger:
        logger.warning("CacheModule not available - using embedded cache management")

# ìƒˆë¡œìš´ ëª¨ë“ˆ import (ì œê±°ë¨ - ë°±ì—… í´ë”ë¡œ ì´ë™)
# from pdf_parallel_processor import PDFParallelProcessor
# from error_handler import RAGErrorHandler, ErrorRecovery, DetailedError, safe_execute


import logging
from typing import Optional, Dict, Any, List, Tuple
import traceback

# ë¡œê¹… ì„¤ì • - ì´ë¯¸ ìƒë‹¨ì—ì„œ loggerê°€ ì„¤ì •ë¨
# loggerê°€ Noneì¸ ê²½ìš° (log_system import ì‹¤íŒ¨ ì‹œ) í‘œì¤€ ë¡œê¹… ì‚¬ìš©
if logger is None:
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('perfect_rag.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)


class RAGException(Exception):
    """RAG ì‹œìŠ¤í…œ ê¸°ë³¸ ì˜ˆì™¸"""
    pass

class DocumentNotFoundException(RAGException):
    """ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ"""
    pass

class PDFExtractionException(RAGException):
    """PDF ì¶”ì¶œ ì‹¤íŒ¨"""
    pass

class LLMException(RAGException):
    """LLM ê´€ë ¨ ì˜¤ë¥˜"""
    pass

class CacheException(RAGException):
    """ìºì‹œ ê´€ë ¨ ì˜¤ë¥˜"""
    pass


def handle_errors(default_return=None):
    """ì—ëŸ¬ ì²˜ë¦¬ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except RAGPDFExtractionException as e:
                logger.error(f"{func.__name__} - RAG ì˜¤ë¥˜: {str(e)}")
                if default_return is not None:
                    return default_return
                raise
            except PDFExtractionException as e:
                logger.error(f"{func.__name__} - ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}", exc_info=True)
                if default_return is not None:
                    return default_return
                raise RAGPDFExtractionException(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return wrapper
    return decorator

class PerfectRAG:
    """ì •í™•í•˜ê³  ì‹¬í”Œí•œ RAG ì‹œìŠ¤í…œ"""

    def _load_performance_config(self):
        """performance_config.yaml ë¡œë“œ"""
        import yaml
        perf_config_path = Path(__file__).parent / 'performance_config.yaml'
        if perf_config_path.exists():
            try:
                with open(perf_config_path, 'r', encoding='utf-8') as f:
                    self.perf_config = yaml.safe_load(f)
                    if logger:
                        logger.info(f"Performance config loaded from {perf_config_path}")
            except Exception as e:
                if logger:
                    logger.warning(f"Failed to load performance config: {e}")
                self.perf_config = {}
        else:
            self.perf_config = {}

    # í´ë˜ìŠ¤ ë ˆë²¨ ìƒìˆ˜ - config.yamlì—ì„œ ë¡œë“œ
    def _get_manufacturer_pattern(self):
        if USE_YAML_CONFIG:
            manufacturers = cfg.get('patterns.manufacturers', [])
            if manufacturers:
                pattern = '|'.join(manufacturers)
                return rf'\b({pattern})\b'
        return r'\b(SONY|Sony|HARRIS|Harris|HP|TOSHIBA|Toshiba|PANASONIC|Panasonic|CANON|Canon|NIKON|Nikon|DELL|Dell|APPLE|Apple|SAMSUNG|Samsung|LG)\b'

    def _get_model_pattern(self):
        if USE_YAML_CONFIG:
            return cfg.get('patterns.model_regex', r'\b[A-Z]{2,}[-\s]?\d+')
        return r'\b[A-Z]{2,}[-\s]?\d+'
    
    def __init__(self, preload_llm=False):
        # performance_config.yaml ë¡œë“œ ì‹œë„
        self._load_performance_config()

        # ì„¤ì • ë¡œë“œ (YAML ìš°ì„ )
        if USE_YAML_CONFIG:
            self.docs_dir = Path(cfg.get('paths.documents_dir', './docs'))
            self.model_path = cfg.get('models.qwen.path', './models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf')
            self.max_cache_size = self.perf_config.get('cache', {}).get('max_size', 500)
            self.cache_ttl = self.perf_config.get('cache', {}).get('response_ttl', 7200)
            self.max_text_length = self.perf_config.get('memory', {}).get('max_document_length', 8000)
            self.max_pdf_pages = cfg.get('limits.max_pdf_pages', 10)
            self.pdf_workers = self.perf_config.get('parallel', {}).get('pdf_workers', 4)
            self.chunk_size = self.perf_config.get('parallel', {}).get('chunk_size', 5)
        else:
            self.docs_dir = Path(cfg.DOCS_DIR)
            self.model_path = cfg.QWEN_MODEL_PATH
            self.max_cache_size = self.perf_config.get('cache', {}).get('max_size', 500)
            self.cache_ttl = self.perf_config.get('cache', {}).get('response_ttl', 7200)
            self.max_text_length = self.perf_config.get('memory', {}).get('max_document_length', 8000)
            self.max_pdf_pages = getattr(cfg, 'MAX_PDF_PAGES', 10)
            self.pdf_workers = self.perf_config.get('parallel', {}).get('pdf_workers', 4)
            self.chunk_size = self.perf_config.get('parallel', {}).get('chunk_size', 5)

        # PDF ìºì‹œ í¬ê¸° ì„¤ì • (DocumentModule ì´ˆê¸°í™” ì „ì— í•„ìš”)
        self.max_pdf_cache = 50  # ìµœëŒ€ 50ê°œ PDF ìºì‹±

        # ì„¤ì • ë””ë ‰í† ë¦¬ ì„¤ì •
        self.config_dir = Path(__file__).parent / 'config'
        self.config_dir.mkdir(exist_ok=True)

        self.llm = None
        # ìºì‹œ ê´€ë¦¬ ê°œì„  (í¬ê¸° ì œí•œ ë° TTL)
        from collections import OrderedDict
        # ìºì‹œ í¬ê¸° ì œí•œ ì„¤ì •
        self.MAX_CACHE_SIZE = 100  # ì‘ë‹µ ìºì‹œ ìµœëŒ€ í¬ê¸°
        self.MAX_METADATA_CACHE = 500  # ë©”íƒ€ë°ì´í„° ìºì‹œ ìµœëŒ€ í¬ê¸°
        self.MAX_PDF_CACHE = 50  # PDF í…ìŠ¤íŠ¸ ìºì‹œ ìµœëŒ€ í¬ê¸°
        self.CACHE_TTL = 3600  # ìºì‹œ ìœ íš¨ ì‹œê°„ (1ì‹œê°„)

        self.documents_cache = OrderedDict()  # LRU ìºì‹œì²˜ëŸ¼ ë™ì‘
        self.metadata_cache = OrderedDict()  # ë©”íƒ€ë°ì´í„° ìºì‹œ
        self.search_mode = 'document'  # ê²€ìƒ‰ ëª¨ë“œëŠ” í•­ìƒ document
        self.answer_cache = OrderedDict()  # ë‹µë³€ ìºì‹œ (LRU)
        self.pdf_text_cache = OrderedDict()  # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”)

        # PDF ë³‘ë ¬ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (ì œê±°ë¨ - ë°±ì—… í´ë”ë¡œ ì´ë™)
        # self.pdf_processor = PDFParallelProcessor(config_manager=cfg if USE_YAML_CONFIG else None)
        # self.error_handler = RAGErrorHandler()
        # self.error_recovery = ErrorRecovery()
        self.pdf_processor = None
        self.error_handler = None
        self.error_recovery = None

        # SearchModule ì´ˆê¸°í™” (2025-09-29 ë¦¬íŒ©í† ë§)
        self.search_module = None
        if SEARCH_MODULE_AVAILABLE:
            try:
                self.search_module = SearchModule(str(self.docs_dir))
                if logger:
                    logger.info("âœ… SearchModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ SearchModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.search_module = None

        # DocumentModule ì´ˆê¸°í™” (2025-09-29 ë¦¬íŒ©í† ë§)
        self.document_module = None
        if DOCUMENT_MODULE_AVAILABLE:
            try:
                doc_config = {
                    'max_pdf_cache': self.max_pdf_cache,
                    'max_text_length': self.max_text_length,
                    'max_pdf_pages': self.max_pdf_pages,
                    'enable_ocr': False  # OCRì€ í•„ìš”ì‹œ í™œì„±í™”
                }
                self.document_module = DocumentModule(doc_config)
                if logger:
                    logger.info("âœ… DocumentModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ DocumentModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.document_module = None

        # LLMModule ì´ˆê¸°í™” (2025-09-29 ë¦¬íŒ©í† ë§)
        self.llm_module = None
        if LLM_MODULE_AVAILABLE:
            try:
                llm_config = {
                    'model_path': self.model_path,
                    'preload_llm': preload_llm  # ìƒì„±ì íŒŒë¼ë¯¸í„° ì „ë‹¬
                }
                self.llm_module = LLMModule(llm_config)
                if logger:
                    logger.info("âœ… LLMModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ LLMModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.llm_module = None

        # CacheModule ì´ˆê¸°í™” (2025-09-29 ë¦¬íŒ©í† ë§)
        self.cache_module = None
        if CACHE_MODULE_AVAILABLE:
            try:
                cache_config = {
                    'max_cache_size': self.MAX_CACHE_SIZE,
                    'max_metadata_cache': self.MAX_METADATA_CACHE,
                    'max_pdf_cache': self.MAX_PDF_CACHE,
                    'cache_ttl': self.CACHE_TTL,
                    'cache_dir': str(self.config_dir / 'cache')
                }
                self.cache_module = CacheModule(cache_config)
                if logger:
                    logger.info("âœ… CacheModule ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ CacheModule ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.cache_module = None

        # Everything-like ì´ˆê³ ì† ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (SearchModuleì´ ì—†ì„ ë•Œë§Œ)
        self.everything_search = None
        if not self.search_module and EVERYTHING_SEARCH_AVAILABLE:
            try:
                self.everything_search = EverythingLikeSearch()
                # ì´ˆê¸° ì¸ë±ì‹± - í•œ ë²ˆë§Œ ì‹¤í–‰
                if not hasattr(self, '_index_initialized'):
                    self.everything_search.index_all_files()
                    self.__class__._index_initialized = True
                if logger:
                    logger.info("Everything-like search initialized successfully")
            except Exception as e:
                if logger:
                    logger.error(f"Failed to initialize Everything-like search: {e}")
                self.everything_search = None
        
        # ì‘ë‹µ í¬ë§·í„° ì´ˆê¸°í™”
        self.formatter = ResponseFormatter() if ResponseFormatter else None

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° ì´ˆê¸°í™” (2025-09-29 ì¶”ê°€)
        self.metadata_extractor = None
        if METADATA_EXTRACTOR_AVAILABLE:
            try:
                self.metadata_extractor = MetadataExtractor()
                if logger:
                    logger.info("âœ… MetadataExtractor ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                if logger:
                    logger.error(f"âŒ MetadataExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.metadata_extractor = None

        # LLM ê°œì„  ëª¨ë“ˆ ì´ˆê¸°í™”

        # ìì‚° ë°ì´í„° ì œê±° (ê¸°ì•ˆì„œ ì¤‘ì‹¬ ì‹œìŠ¤í…œìœ¼ë¡œ ì „í™˜)

        # ë©”íƒ€ë°ì´í„° DB ì´ˆê¸°í™”
        try:
            self.metadata_db = MetadataDB(db_path=str(self.config_dir / "metadata.db"))
            logger.info("âœ… MetadataDB ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.error(f"ï¸ MetadataDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.metadata_db = None
        
        # ëª¨ë“  PDFì™€ TXT íŒŒì¼ ëª©ë¡ (ìƒˆë¡œìš´ í´ë” êµ¬ì¡° í¬í•¨)
        self.pdf_files = []
        self.txt_files = []

        # ë£¨íŠ¸ í´ë” íŒŒì¼
        self.pdf_files.extend(list(self.docs_dir.glob('*.pdf')))
        self.txt_files.extend(list(self.docs_dir.glob('*.txt')))

        # ì—°ë„ë³„ í´ë” (year_2014 ~ year_2025)
        for year in range(2014, 2026):
            year_folder = self.docs_dir / f"year_{year}"
            if year_folder.exists():
                self.pdf_files.extend(list(year_folder.glob('*.pdf')))
                self.txt_files.extend(list(year_folder.glob('*.txt')))

        # ì¹´í…Œê³ ë¦¬ë³„ í´ë”ëŠ” ì‹¬ë³¼ë¦­ ë§í¬ë§Œ ìˆìœ¼ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
        # ì‹¤ì œ íŒŒì¼ì€ ëª¨ë‘ year_* í´ë”ì— ìˆìŒ
        # category_folders = ['category_purchase', 'category_repair', 'category_review',
        #                   'category_disposal', 'category_consumables']
        # for folder in category_folders:
        #     cat_folder = self.docs_dir / folder
        #     if cat_folder.exists():
        #         self.pdf_files.extend(list(cat_folder.glob('*.pdf')))
        #         self.txt_files.extend(list(cat_folder.glob('*.txt')))

        # íŠ¹ë³„ í´ë” (ìì‚° ê´€ë ¨ í´ë” ì œê±°)
        special_folders = ['recent', 'archive']
        for folder in special_folders:
            special_folder = self.docs_dir / folder
            if special_folder.exists():
                self.pdf_files.extend(list(special_folder.glob('*.pdf')))
                self.txt_files.extend(list(special_folder.glob('*.txt')))

        # ì¤‘ë³µ ì œê±° (ê°™ì€ íŒŒì¼ì´ ì—¬ëŸ¬ í´ë”ì— ìˆì„ ìˆ˜ ìˆìŒ)
        self.pdf_files = list(set(self.pdf_files))
        self.txt_files = list(set(self.txt_files))
        self.all_files = self.pdf_files + self.txt_files

        print(f" {len(self.pdf_files)}ê°œ PDF, {len(self.txt_files)}ê°œ TXT ë¬¸ì„œ ë°œê²¬")

        # ìì‚° ë°ì´í„° ë¡œë“œ (metadata_db ì´ˆê¸°í™” í¬í•¨)

        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì‚¬ì „ ì¶”ì¶œ (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
        self._build_metadata_cache()

        # ë©”íƒ€ë°ì´í„° DB ì¸ë±ìŠ¤ êµ¬ì¶• (Phase 1.2)
        self._build_metadata_index()

        # LLM ì‚¬ì „ ë¡œë“œ ì˜µì…˜
        if preload_llm:
            self._preload_llm()

    def _preload_llm(self):
        """LLMì„ ë¯¸ë¦¬ ë¡œë“œ"""
        # LLMModuleì„ ì‚¬ìš©í•˜ì—¬ LLM ë¡œë“œ
        if self.llm_module:
            if self.llm_module.load_llm():
                self.llm = self.llm_module.llm
                return

        # LLMModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        if self.llm is None:
            if not LLMSingleton.is_loaded():
                print(" LLM ëª¨ë¸ ìµœì´ˆ ë¡œë”© ì¤‘...")
            else:
                print("ï¸ LLM ëª¨ë¸ ì¬ì‚¬ìš©")

            try:
                start = time.time()
                self.llm = LLMSingleton.get_instance(model_path=self.model_path)
                elapsed = time.time() - start
                if elapsed > 1.0:  # 1ì´ˆ ì´ìƒ ê±¸ë¦° ê²½ìš°ë§Œ í‘œì‹œ
                    print(f" LLM ë¡œë“œ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")
            except LLMException as e:
                print(f"ï¸ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _build_metadata_index(self):
        """ëª¨ë“  PDFì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ DBì— ì €ì¥"""
        if not self.metadata_db:
            return

        logger.info("ğŸ“š ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘...")
        indexed = 0

        for pdf_path in self.pdf_files[:30]:  # ì²˜ìŒ 30ê°œë§Œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
            try:
                # DBì— ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
                existing = self.metadata_db.get_document(pdf_path.name)
                if existing and existing.get('title'):
                    continue  # ì´ë¯¸ ì²˜ë¦¬ë¨

                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = self._extract_pdf_metadata(pdf_path)
                if metadata:
                    # DBì— ì €ì¥
                    self.metadata_db.add_document(metadata)
                    indexed += 1

            except Exception as e:
                logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {pdf_path.name}: {e}")

        if indexed > 0:
            logger.info(f"âœ… {indexed}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¸ë±ì‹± ì™„ë£Œ")

    def _extract_pdf_metadata(self, pdf_path: Path) -> Optional[Dict[str, Any]]:
        """PDFì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            with pdfplumber.open(pdf_path) as pdf:
                if not pdf.pages:
                    return None

                # ì²« í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                first_page = pdf.pages[0].extract_text() or ""
                if not first_page:
                    return None

                # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
                metadata = {
                    'path': str(pdf_path),
                    'filename': pdf_path.name,
                    'file_size': pdf_path.stat().st_size,
                    'page_count': len(pdf.pages),
                    'text_preview': first_page[:500]
                }

                # ì œëª© ì¶”ì¶œ
                lines = first_page.split('\n')[:10]
                for line in lines:
                    if len(line) > 10 and len(line) < 100:
                        metadata['title'] = line.strip()
                        break

                # ë‚ ì§œ ì¶”ì¶œ
                date_patterns = [
                    r'(\d{4})[ë…„-]\s*(\d{1,2})[ì›”-]\s*(\d{1,2})',
                    r'(\d{4})\.(\d{1,2})\.(\d{1,2})'
                ]
                for pattern in date_patterns:
                    match = re.search(pattern, first_page[:1000])
                    if match:
                        year, month, day = match.groups()
                        metadata['year'] = year
                        metadata['month'] = month.zfill(2)
                        metadata['date'] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                        break

                # ê¸°ì•ˆì ì¶”ì¶œ
                drafter_patterns = [
                    r'ê¸°ì•ˆì[\s:ï¼š]*([ê°€-í£]{2,4})',
                    r'ì‘ì„±ì[\s:ï¼š]*([ê°€-í£]{2,4})'
                ]
                for pattern in drafter_patterns:
                    match = re.search(pattern, first_page[:500])
                    if match:
                        metadata['drafter'] = match.group(1).strip()
                        break

                # ì¹´í…Œê³ ë¦¬ ì¶”ì •
                filename_lower = pdf_path.name.lower()
                if 'êµ¬ë§¤' in filename_lower or 'êµ¬ì…' in filename_lower:
                    metadata['category'] = 'êµ¬ë§¤'
                elif 'ìˆ˜ë¦¬' in filename_lower or 'ë³´ìˆ˜' in filename_lower:
                    metadata['category'] = 'ìˆ˜ë¦¬'
                elif 'íê¸°' in filename_lower:
                    metadata['category'] = 'íê¸°'
                elif 'ê²€í† ' in filename_lower:
                    metadata['category'] = 'ê²€í† '

                return metadata

        except Exception as e:
            logger.error(f"PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜ {pdf_path.name}: {e}")
            return None

    def _manage_cache(self, cache_dict, key, value):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬ - LRU ë°©ì‹"""
        # CacheModuleì„ ì‚¬ìš©í•˜ì—¬ ìºì‹œ ê´€ë¦¬
        if self.cache_module:
            self.cache_module.manage_cache(cache_dict, key, value)
            return

        # CacheModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        if key in cache_dict:
            # ê¸°ì¡´ í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™ (ê°€ì¥ ìµœê·¼ ì‚¬ìš©)
            cache_dict.move_to_end(key)
        else:
            # ìƒˆ í•­ëª© ì¶”ê°€
            if len(cache_dict) >= self.max_cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                cache_dict.popitem(last=False)
            cache_dict[key] = (value, time.time())  # ê°’ê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
    
    def _get_from_cache(self, cache_dict, key):
        """ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸° (TTL ì²´í¬ ë° íƒ€ì„ìŠ¤íƒ¬í”„ ê°±ì‹ )"""
        # CacheModuleì„ ì‚¬ìš©í•˜ì—¬ ìºì‹œ ì¡°íšŒ
        if self.cache_module:
            return self.cache_module.get_from_cache(cache_dict, key)

        # CacheModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        if key in cache_dict:
            cache_value = cache_dict[key]
            current_time = time.time()

            # íŠ€í”Œ í˜•ì‹ (value, timestamp) ì²´í¬
            if isinstance(cache_value, tuple) and len(cache_value) == 2:
                value, timestamp = cache_value

                if current_time - timestamp < self.cache_ttl:
                    # LRU: ì‚¬ìš©í•œ í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™
                    cache_dict.move_to_end(key)
                    # íƒ€ì„ìŠ¤íƒ¬í”„ ê°±ì‹  (ì‚¬ìš© ì‹œê°„ ì—°ì¥)
                    cache_dict[key] = (value, current_time)
                    return value
                else:
                    # TTL ë§Œë£Œ - ì‚­ì œ
                    del cache_dict[key]
                    return None
            else:
                # ì´ì „ í˜•ì‹ í˜¸í™˜ (íŠ€í”Œ ì•„ë‹Œ ê²½ìš°)
                cache_dict.move_to_end(key)
                return cache_value

        return None
    
    def _parse_pdf_result(self, result: Dict) -> Dict:
        """ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            'text': result.get('text', ''),
            'page_count': result.get('page_count', 0),
            'metadata': result.get('metadata', {}),
            'method': result.get('method', 'parallel')
        }

    def process_pdfs_in_batch(self, pdf_paths: List[Path], batch_size: int = None) -> Dict:
        """ì—¬ëŸ¬ PDFë¥¼ ë°°ì¹˜ë¡œ ë³‘ë ¬ ì²˜ë¦¬ (ì•ˆì „í•˜ê²Œ ê°œì„ )"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import gc

        all_results = {}

        # ë™ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        if batch_size is None:
            batch_size = min(20, max(10, len(pdf_paths) // 30))
            logger.info(f"ë™ì  ë°°ì¹˜ í¬ê¸° ì„¤ì •: {batch_size}")

        # pdf_processorê°€ ì—†ìœ¼ë©´ ìˆœì°¨ ì²˜ë¦¬ë¡œ í´ë°±
        if self.pdf_processor is None:
            print("ï¸ ë³‘ë ¬ ì²˜ë¦¬ê¸° ë¯¸í™œì„±í™” - ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")

            # ThreadPoolExecutorë¡œ ê°„ë‹¨í•œ ë³‘ë ¬ ì²˜ë¦¬ (CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜ ìµœì í™”)
            optimal_workers = min(os.cpu_count() or 4, 12, max(4, batch_size))
            with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
                for i in range(0, len(pdf_paths), batch_size):
                    batch = pdf_paths[i:i + batch_size]
                    print(f" ë°°ì¹˜ {i//batch_size + 1}/{(len(pdf_paths)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)")

                    # ê° PDFë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
                    futures = {executor.submit(self._extract_pdf_info, pdf): pdf for pdf in batch}

                    for future in as_completed(futures):
                        pdf_path = futures[future]
                        try:
                            result = future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                            all_results[str(pdf_path)] = result
                        except PDFExtractionException as e:
                            print(f"   {pdf_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}")
                            all_results[str(pdf_path)] = {'error': str(e)}

                    # ë©”ëª¨ë¦¬ ìµœì í™”
                    if i % (batch_size * 5) == 0:
                        gc.collect()
        else:
            # ê¸°ì¡´ pdf_processor ì‚¬ìš©
            for i in range(0, len(pdf_paths), batch_size):
                batch = pdf_paths[i:i + batch_size]
                print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)")

                batch_results = self.pdf_processor.process_multiple_pdfs(batch)
                all_results.update(batch_results)

                # ë©”ëª¨ë¦¬ ê´€ë¦¬
                if len(self.pdf_processor.extraction_cache) > 50:
                    self.pdf_processor.clear_cache()

        return all_results

    def _find_metadata_by_filename(self, filename: str) -> Optional[Dict]:
        """íŒŒì¼ëª…ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì°¾ê¸° (ìƒˆë¡œìš´ ìºì‹œ êµ¬ì¡° ì§€ì›)"""
        # ë¨¼ì € ì •í™•í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì°¾ê¸°
        if filename in self.metadata_cache:
            return self.metadata_cache[filename]

        # ìƒëŒ€ ê²½ë¡œ í¬í•¨í•œ í‚¤ì—ì„œ ì°¾ê¸°
        for cache_key, metadata in self.metadata_cache.items():
            if metadata.get('filename') == filename:
                return metadata

        return None

    def _build_metadata_cache(self):
        """ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ (ìºì‹± ì§€ì›)"""
        # ìºì‹œ íŒŒì¼ ê²½ë¡œ (Docker volumeì— ì €ì¥)
        cache_dir = Path("/app/cache") if Path("/app/cache").exists() else Path("cache")
        cache_dir.mkdir(exist_ok=True)
        cache_file = cache_dir / "metadata_cache.pkl"

        # ê¸°ì¡´ ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
        if cache_file.exists():
            try:
                import pickle
                with open(cache_file, 'rb') as f:
                    self.metadata_cache = pickle.load(f)
                print(f"âœ… ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.metadata_cache)}ê°œ ë¬¸ì„œ")
                return  # ìºì‹œê°€ ìˆìœ¼ë©´ ì¬êµ¬ì¶• ë¶ˆí•„ìš”
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¬êµ¬ì¶•: {e}")

        logger.info("ë©”íƒ€ë°ì´í„° ìºì‹œ êµ¬ì¶• ì‹œì‘")
        print(" ë¬¸ì„œ ë©”íƒ€ë°ì´í„° êµ¬ì¶• ì¤‘...")

        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • í™•ì¸
        use_parallel = USE_YAML_CONFIG and cfg.get('parallel_processing.enabled', True)

        # ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ process_pdfs_in_batch ì‚¬ìš© (ë‚´ë¶€ì—ì„œ ì²˜ë¦¬)
        if self.pdf_files and len(self.pdf_files) > 10:  # 10ê°œ ì´ìƒì¼ ë•Œë§Œ ë³‘ë ¬ ì²˜ë¦¬
            print(f" {len(self.pdf_files)}ê°œ PDF ì²˜ë¦¬ ì‹œì‘ (ë³‘ë ¬ ëª¨ë“œ)...")
            pdf_results = self.process_pdfs_in_batch(self.pdf_files, batch_size=10)

            # ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë©”íƒ€ë°ì´í„° ìºì‹œì— ì €ì¥
            for pdf_path, result in pdf_results.items():
                pdf_path_obj = Path(pdf_path)
                filename = pdf_path_obj.name
                # ìƒëŒ€ ê²½ë¡œë¥¼ í‚¤ë¡œ ì‚¬ìš© (ì¤‘ë³µ íŒŒì¼ëª… ì²˜ë¦¬)
                try:
                    relative_path = pdf_path_obj.relative_to(self.docs_dir)
                    cache_key = str(relative_path)
                except ValueError:
                    cache_key = filename

                if 'error' not in result:
                    self.metadata_cache[cache_key] = {
                        'path': pdf_path_obj,
                        'filename': filename,
                        'text': result.get('text', '')[:1000],  # ìš”ì•½ë§Œ ì €ì¥
                        'page_count': result.get('page_count', 0),
                        'metadata': result.get('metadata', {})
                    }
        
        # PDFì™€ TXT íŒŒì¼ ëª¨ë‘ ì²˜ë¦¬
        for file_path in self.all_files:
            filename = file_path.name

            # ìƒëŒ€ ê²½ë¡œë¥¼ ìºì‹œ í‚¤ë¡œ ì‚¬ìš©
            try:
                relative_path = file_path.relative_to(self.docs_dir)
                cache_key = str(relative_path)
            except ValueError:
                cache_key = filename

            # íŒŒì¼ëª…ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            date_match = re.match(r'(\d{4}-\d{2}-\d{2})', filename)
            date = date_match.group(1) if date_match else ""

            # ì œëª© ì¶”ì¶œ (ë‚ ì§œ ë’¤ ë¶€ë¶„, í™•ì¥ì ì œê±°)
            if filename.endswith('.pdf'):
                title = filename.replace(date + '_', '').replace('.pdf', '') if date else filename.replace('.pdf', '')
            else:
                title = filename.replace(date + '_', '').replace('.txt', '') if date else filename.replace('.txt', '')

            # ì—°ë„ ì¶”ì¶œ
            year = date[:4] if date else ""

            # íŒŒì¼ëª…ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
            keywords = []

            # íŒŒì¼ëª…ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            # í•œê¸€, ì˜ë¬¸, ìˆ«ìë¡œ ë‹¨ì–´ ì¶”ì¶œ
            words = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', filename)

            # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ë‹¨ì–´ë“¤ì„ í‚¤ì›Œë“œë¡œ ì¶”ê°€ (2ê¸€ì ì´ìƒ)
            for word in words:
                if len(word) >= 2 and word not in ['pdf', 'PDF', 'txt', 'TXT', 'ì˜', 'ë°', 'ê±´', 'ê²€í† ì„œ']:
                    keywords.append(word)

            self.metadata_cache[cache_key] = {
                'path': file_path,
                'filename': filename,  # ì‹¤ì œ íŒŒì¼ëª… ì €ì¥
                'date': date,
                'year': year,
                'title': title,
                'keywords': keywords,
                'drafter': None,  # ê¸°ì•ˆì ì •ë³´ëŠ” í•„ìš”ì‹œì—ë§Œ ì¶”ì¶œ
                'full_text': None,  # ë‚˜ì¤‘ì— í•„ìš”ì‹œ ë¡œë“œ
                'is_txt': filename.endswith('.txt'),  # TXT íŒŒì¼ ì—¬ë¶€
                'is_pdf': filename.endswith('.pdf')  # PDF íŒŒì¼ ì—¬ë¶€ ì¶”ê°€
            }

        print(f" {len(self.metadata_cache)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° êµ¬ì¶• ì™„ë£Œ")

        # ìºì‹œ ì €ì¥
        try:
            import pickle
            with open(cache_file, 'wb') as f:
                pickle.dump(self.metadata_cache, f)
            print(f"âœ… ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_file}")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _extract_txt_info(self, txt_path: Path) -> Dict:
        """TXT íŒŒì¼ì—ì„œ ì •ë³´ ë™ì  ì¶”ì¶œ"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            info = {'text': text[:3000]}  # ì²˜ìŒ 3000ìë§Œ
            
            # ìì‚° íŒŒì¼ ê´€ë ¨ ì½”ë“œ ì œê±°ë¨
            
            return info

        except Exception as e:
            if logger:
                print(f"ï¸ TXT ì½ê¸° ì˜¤ë¥˜ ({txt_path.name}): {e}")
            else:
                print(f"ï¸ TXT ì½ê¸° ì˜¤ë¥˜ ({txt_path.name}): {e}")
            # ì—ëŸ¬ í•¸ë“¤ëŸ¬ë¡œ ì•ˆì „í•˜ê²Œ íŒŒì¼ ì½ê¸° ì‹œë„
            # error_handler ì œê±° - ì§ì ‘ íŒŒì¼ ì½ê¸°
            try:
                with open(txt_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                if content:
                    return {'text': content[:3000]}
            except Exception:
                pass
            return {}

    # ë°ì½”ë ˆì´í„° ì œê±° (error_handler ë°±ì—… í´ë”ë¡œ ì´ë™)
    # @RAGErrorHandler.retry_with_backoff(max_retries=3, backoff_factor=1.5)
    # @RAGErrorHandler.handle_pdf_extraction_error

    def _optimize_context(self, text: str, query: str, max_length: int = 3000) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìµœì í™” - ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
        # DocumentModuleì„ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìµœì í™”
        if self.document_module:
            return self.document_module.optimize_context(text, query, max_length)

        # DocumentModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ë²• ì‚¬ìš©
        if not text or len(text) <= max_length:
            return text

        # ì¿¼ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query.lower())
        keywords = [k for k in keywords if len(k) >= 2]

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = re.split(r'[.!?\n]+', text)

        # ê° ë¬¸ì¥ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        scored_sentences = []
        for i, sentence in enumerate(sentences):
            if not sentence.strip():
                continue

            score = 0
            sentence_lower = sentence.lower()

            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            for keyword in keywords:
                if keyword in sentence_lower:
                    score += 10

            # ì¤‘ìš” íŒ¨í„´ ë³´ë„ˆìŠ¤
            if re.search(r'\d+[,\d]*\s*ì›', sentence):  # ê¸ˆì•¡
                score += 5
            if re.search(r'\d{4}[-ë…„]', sentence):  # ì—°ë„
                score += 3
            if re.search(r'ì´|í•©ê³„|ì „ì²´', sentence):  # ìš”ì•½ ì •ë³´
                score += 3

            # ìœ„ì¹˜ ì ìˆ˜ (ë¬¸ì„œ ì•ë¶€ë¶„ ì„ í˜¸)
            position_score = max(0, 5 - i * 0.1)
            score += position_score

            scored_sentences.append((sentence, score))

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        # ìƒìœ„ ë¬¸ì¥ë“¤ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        result = []
        current_length = 0

        for sentence, score in scored_sentences:
            if current_length + len(sentence) > max_length:
                break
            result.append(sentence)
            current_length += len(sentence)

        # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬
        result_text = '. '.join(result)
        return result_text if result_text else text[:max_length]

    def _extract_pdf_info_with_retry(self, pdf_path: Path) -> Dict:
        """PDF ì •ë³´ ì¶”ì¶œ (DocumentModule ì‚¬ìš©)"""
        # DocumentModuleì„ ì‚¬ìš©í•˜ì—¬ PDF ì²˜ë¦¬
        if self.document_module:
            try:
                result = self.document_module.extract_pdf_text_with_retry(pdf_path, max_retries=2)
                if result:
                    return result
            except Exception as e:
                if logger:
                    logger.error(f"DocumentModule PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        # DocumentModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ë²• ì‚¬ìš©
        return self._extract_pdf_info(pdf_path)
    
    def _extract_pdf_info(self, pdf_path: Path) -> Dict:
        """ê¸°ì¡´ PDF ì¶”ì¶œ ë°©ì‹ (í´ë°±ìš©) - ìºì‹± ì ìš©"""
        # ìºì‹œ í‚¤ ìƒì„± (íŒŒì¼ ê²½ë¡œ ê¸°ë°˜)
        cache_key = str(pdf_path)

        # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        if cache_key in self.pdf_text_cache:
            # ìºì‹œ íˆíŠ¸ - LRUë¥¼ ìœ„í•´ ë§¨ ë’¤ë¡œ ì´ë™
            cached_result = self.pdf_text_cache.pop(cache_key)
            self.pdf_text_cache[cache_key] = cached_result
            return cached_result

        text = ""

        # ì—ëŸ¬ í•¸ë“¤ëŸ¬ë¡œ ì•ˆì „í•˜ê²Œ íŒŒì¼ ì½ê¸°
        def extract_with_pdfplumber():
            nonlocal text
            with pdfplumber.open(pdf_path) as pdf:
                # ë¬¸ì„œ ì „ì²´ ë˜ëŠ” ì„¤ì •ëœ ìµœëŒ€ í˜ì´ì§€ ì½ê¸°
                pages_to_read = min(len(pdf.pages), self.max_pdf_pages)
                for page in pdf.pages[:pages_to_read]:
                    # safe_execute ì œê±° (error_handler ë°±ì—… í´ë”ë¡œ ì´ë™)
                    try:
                        page_text = page.extract_text() or ""
                    except Exception:
                        page_text = ""
                    if page_text:
                        text += page_text + "\n"
                        # ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ë©´ ì¤‘ë‹¨ (ë©”ëª¨ë¦¬ ì ˆì•½)
                        if len(text) > self.max_text_length:
                            break
            return text

        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
        # error_recoveryê°€ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ ì‹œë„
        text = extract_with_pdfplumber()
        if not text and hasattr(self, '_try_ocr_extraction'):
            text = self._try_ocr_extraction(pdf_path)

        # pdfplumber ì‹¤íŒ¨ì‹œ OCR ì‹œë„
        if not text:
            try:
                from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
                ocr = EnhancedOCRProcessor()
                text, _ = ocr.extract_text_with_ocr(str(pdf_path))
            except Exception:
                pass  # OCR ì‹¤íŒ¨ì‹œ ë¬´ì‹œ

        if not text:
            return {}

        info = {}

        # ê¸°ì•ˆì ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´)
        patterns = [
            r'ê¸°ì•ˆì[\s:ï¼š]*([ê°€-í£]+)',
            r'ì‘ì„±ì[\s:ï¼š]*([ê°€-í£]+)',
            r'ë‹´ë‹¹ì[\s:ï¼š]*([ê°€-í£]+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                info['ê¸°ì•ˆì'] = match.group(1).strip()
                break
            
        # ë‚ ì§œ ì¶”ì¶œ (ë¬¸ì„œ ë‚´ìš© ìš°ì„ , íŒŒì¼ëª… fallback)
        date_patterns = [
            r'ê¸°ì•ˆì¼[\s:ï¼š]*(\d{4}[-ë…„]\s*\d{1,2}[-ì›”]\s*\d{1,2})',
            r'ì‹œí–‰ì¼ì[\s:ï¼š]*(\d{4}[-ë…„]\s*\d{1,2}[-ì›”]\s*\d{1,2})',
            r'(\d{4}-\d{2}-\d{2})\s*\d{2}:\d{2}',
            r'ì¼ì[\s:ï¼š]*(\d{4}[-./ë…„]\s*\d{1,2}[-./ì›”]\s*\d{1,2})',
            r'ë‚ ì§œ[\s:ï¼š]*(\d{4}[-./ë…„]\s*\d{1,2}[-./ì›”]\s*\d{1,2})',
            r'(\d{4}[./-]\d{1,2}[./-]\d{1,2})'  # ì¼ë°˜ì ì¸ ë‚ ì§œ í˜•ì‹
        ]

        date_found = False
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                info['ë‚ ì§œ'] = match.group(1).strip()
                date_found = True
                break

        # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
        if not date_found:
            filename = pdf_path.name
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (YYYY-MM-DD, YYYY.MM.DD, YYYY_MM_DD ë“±)
            filename_date_patterns = [
                r'(20\d{2}[-_.]0?[1-9]|1[0-2][-_.][0-2]?\d|3[01])',  # YYYY-MM-DD
                r'(20\d{2})[-_.]?(\d{1,2})[-_.]?(\d{1,2})',  # YYYY MM DD (ë¶„ë¦¬ëœ í˜•íƒœ)
            ]

            for pattern in filename_date_patterns:
                match = re.search(pattern, filename)
                if match:
                    if len(match.groups()) == 1:
                        # ì „ì²´ ë§¤ì¹˜ì¸ ê²½ìš°
                        date_str = match.group(1)
                        date_str = date_str.replace('_', '-').replace('.', '-')
                        info['ë‚ ì§œ'] = date_str
                        date_found = True
                        break
                    if len(match.groups()) == 3:
                        # ë¶„ë¦¬ëœ ê·¸ë£¹ì¸ ê²½ìš°
                        year, month, day = match.groups()
                        try:
                            normalized_date = f"{year}-{int(month):02d}-{int(day):02d}"
                            info['ë‚ ì§œ'] = normalized_date
                            date_found = True
                            break
                        except ValueError:
                            continue
            
            # ë¶€ì„œ ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            # íŒ¨í„´ 1: ê¸°ì•ˆë¶€ì„œ ë¼ë²¨
            dept_match = re.search(r'ê¸°ì•ˆë¶€ì„œ[\s:ï¼š]*([^\nì‹œí–‰]+)', text)
            if dept_match:
                dept = dept_match.group(1).strip()
                dept = dept.split('ì‹œí–‰')[0].strip()
                info['ë¶€ì„œ'] = dept
            
            # íŒ¨í„´ 2: íŒ€-íŒŒíŠ¸ í˜•ì‹ (ì±„ë„A ìŠ¤íƒ€ì¼)
            if 'ë¶€ì„œ' not in info:
                team_match = re.search(r'([ê°€-í£]+íŒ€[\-ê°€-í£]+íŒŒíŠ¸)', text)
                if team_match:
                    info['ë¶€ì„œ'] = team_match.group(1)
            
            # ê¸ˆì•¡ ì¶”ì¶œ (ê°€ì¥ í° ê¸ˆì•¡)
            amounts = re.findall(r'(\d{1,3}(?:,\d{3})*)\s*ì›', text)
            if amounts:
                numeric_amounts = []
                for amt in amounts:
                    try:
                        num = int(amt.replace(',', ''))
                        numeric_amounts.append((num, amt))
                    except (ValueError, AttributeError):
                        pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
                if numeric_amounts:
                    numeric_amounts.sort(reverse=True)
                    info['ê¸ˆì•¡'] = f"{numeric_amounts[0][1]}ì›"
            
            # ì œëª© ì¶”ì¶œ
            title_match = re.search(r'ì œëª©[\s:ï¼š]*([^\n]+)', text)
            if title_match:
                info['ì œëª©'] = title_match.group(1).strip()
            
            # í…ìŠ¤íŠ¸ ì €ì¥
            info['text'] = text[:3000]  # ì²˜ìŒ 3000ìë§Œ

        # ìºì‹œì— ì €ì¥ (í¬ê¸° ì œí•œ ì ìš©)
        MAX_PDF_CACHE_SIZE = 50  # ìµœëŒ€ 50ê°œ PDF ìºì‹±
        if len(self.pdf_text_cache) >= MAX_PDF_CACHE_SIZE:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)
            self.pdf_text_cache.popitem(last=False)

        # ìƒˆ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        self.pdf_text_cache[cache_key] = info

        return info

    def _search_by_content(self, query: str) -> List[Dict[str, Any]]:
        """ğŸ”¥ NEW: Everything-like ì´ˆê³ ì† íŒŒì¼ ê²€ìƒ‰"""

        # SearchModule ì‚¬ìš© (2025-09-29 ë¦¬íŒ©í† ë§)
        if self.search_module:
            try:
                results = self.search_module.search_by_content(query, top_k=20)
                if logger:
                    logger.info(f"SearchModule found {len(results)} documents for query: {query}")
                return results
            except Exception as e:
                if logger:
                    logger.error(f"SearchModule failed: {e}, falling back to embedded search")

        # Everything-like ê²€ìƒ‰ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° (í´ë°±)
        if self.everything_search:
            try:
                # ì´ˆê³ ì† SQLite ê²€ìƒ‰
                search_results = self.everything_search.search(query, limit=20)

                results = []
                for doc in search_results:
                    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    result = {
                        'filename': doc['filename'],
                        'path': doc['path'],
                        'date': doc.get('date', ''),
                        'year': doc.get('year', ''),
                        'category': doc.get('category', 'ê¸°íƒ€'),
                        'keywords': doc.get('keywords', ''),
                        'score': 1.0,  # Everything ê²€ìƒ‰ì€ ê´€ë ¨ë„ ì ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’
                        'source': 'everything_search'
                    }

                    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (2025-09-29 ì¶”ê°€)
                    if self.metadata_extractor:
                        try:
                            # PDF íŒŒì¼ì¸ ê²½ìš° ì²« í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            if doc['path'].endswith('.pdf'):
                                with pdfplumber.open(doc['path']) as pdf:
                                    text = pdf.pages[0].extract_text() if pdf.pages else ""

                                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                                metadata = self.metadata_extractor.extract_all(
                                    text[:1000] if text else "",  # ì²« 1000ìë§Œ
                                    doc['filename']
                                )

                                # ìš”ì•½ ì •ë³´ë¥¼ ê²°ê³¼ì— ì¶”ê°€
                                result['metadata_info'] = metadata.get('summary', {})

                                # ì£¼ìš” í•„ë“œ ì§ì ‘ ì¶”ê°€ (í˜¸í™˜ì„± ìœ ì§€)
                                if metadata['summary'].get('date'):
                                    result['extracted_date'] = metadata['summary']['date']
                                if metadata['summary'].get('amount'):
                                    result['extracted_amount'] = metadata['summary']['amount']
                                if metadata['summary'].get('department'):
                                    result['extracted_dept'] = metadata['summary']['department']
                                if metadata['summary'].get('doc_type'):
                                    result['extracted_type'] = metadata['summary']['doc_type']

                        except Exception as e:
                            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨í•´ë„ ê²€ìƒ‰ì€ ê³„ì†
                            if logger:
                                logger.debug(f"Metadata extraction failed for {doc['filename']}: {e}")

                    results.append(result)

                if logger:
                    logger.info(f"Everything search found {len(results)} documents for query: {query}")

                return results

            except Exception as e:
                if logger:
                    logger.error(f"Everything search failed: {e}, falling back to legacy search")
                # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±

        # ê¸°ì¡´ ê²€ìƒ‰ ë¡œì§ (Everything ì‚¬ìš© ë¶ˆê°€ ì‹œ)
        file_scores = {}  # filename -> result dict
        query_lower = query.lower()
        keywords = [kw.lower() for kw in query.split() if len(kw) > 1]

        # ëª¨ë“  PDF íŒŒì¼ ê²€ìƒ‰
        pdf_files = list(self.docs_dir.rglob("*.pdf"))

        if logger:
            logger.info(f"ë‚´ìš© ê²€ìƒ‰ ì‹œì‘: {len(pdf_files)}ê°œ PDF, í‚¤ì›Œë“œ: {keywords}")

        # ë¨¼ì € íŒŒì¼ëª… ê²€ìƒ‰ (ë¹ ë¦„)
        for pdf_path in pdf_files:
            filename_lower = pdf_path.name.lower()
            score = 0
            matched_keywords = []

            # íŒŒì¼ëª…ì— ì „ì²´ ì¿¼ë¦¬ê°€ í¬í•¨ë˜ë©´ ìµœê³  ì ìˆ˜
            if query_lower in filename_lower:
                score = 100
                matched_keywords = [query]
            # íŒŒì¼ëª…ì— ê° í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ì ìˆ˜ ë¶€ì—¬
            else:
                # í•µì‹¬ í‚¤ì›Œë“œ ì°¾ê¸° (ê´€ë ¨, ë¬¸ì„œ ì œì™¸)
                important_keywords = [kw for kw in keywords if kw not in ['ê´€ë ¨', 'ë¬¸ì„œ', 'ì°¾ì•„', 'ë‚´ìš©']]

                # ëª¨ë“  ì¤‘ìš” í‚¤ì›Œë“œê°€ íŒŒì¼ëª…ì— ìˆëŠ”ì§€ í™•ì¸
                if important_keywords:
                    all_important_match = all(kw in filename_lower for kw in important_keywords)

                    for kw in keywords:
                        if kw in filename_lower:
                            # ì¤‘ìš” í‚¤ì›Œë“œëŠ” ë†’ì€ ì ìˆ˜
                            if kw in important_keywords:
                                score += 50 if all_important_match else 30
                            else:
                                score += 10  # ê´€ë ¨, ë¬¸ì„œ ë“±ì€ ë‚®ì€ ì ìˆ˜
                            matched_keywords.append(kw)

            if score > 0:
                file_scores[pdf_path.name] = {
                    'path': pdf_path,
                    'filename': pdf_path.name,
                    'score': score,
                    'matched_keywords': matched_keywords,
                    'context': f"íŒŒì¼ëª… ë§¤ì¹­: {pdf_path.name}"
                }

        # ë‚´ìš© ê²€ìƒ‰ (ëŠë¦¼, ì¼ë¶€ë§Œ)
        for pdf_path in pdf_files[:50]:  # ìƒìœ„ 50ê°œë§Œ
            try:
                # ìºì‹œ í™•ì¸
                cache_key = str(pdf_path)
                if cache_key in self.pdf_text_cache:
                    text = self.pdf_text_cache[cache_key].get('text', '')
                else:
                    # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    with pdfplumber.open(pdf_path) as pdf:
                        text = ""
                        for page_num, page in enumerate(pdf.pages[:5]):  # ì²˜ìŒ 5í˜ì´ì§€ë§Œ
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"

                        # OCR í•„ìš” ì‹œ
                        if not text.strip():
                            text = self._try_ocr_extraction(pdf_path)
                            if not text:
                                continue

                    # ìºì‹œ ì €ì¥
                    self.pdf_text_cache[cache_key] = {'text': text, 'timestamp': time.time()}

                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                text_lower = text.lower()
                matched_keywords = [kw for kw in keywords if kw in text_lower]

                if matched_keywords:
                    # ì ìˆ˜ ê³„ì‚°: ë§¤ì¹­ëœ í‚¤ì›Œë“œ ìˆ˜ + ê·¼ì ‘ë„
                    content_score = len(matched_keywords) * 10

                    # í‚¤ì›Œë“œê°€ ê°€ê¹Œì´ ìˆìœ¼ë©´ ë³´ë„ˆìŠ¤
                    for i in range(len(matched_keywords) - 1):
                        if abs(text_lower.find(matched_keywords[i]) - text_lower.find(matched_keywords[i+1])) < 200:
                            content_score += 5

                    # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (í‚¤ì›Œë“œ ì£¼ë³€ 200ì)
                    context = self._extract_context(text, matched_keywords[0], 200)

                    # ê¸°ì¡´ íŒŒì¼ëª… ì ìˆ˜ì™€ ë‚´ìš© ì ìˆ˜ ì¤‘ ë†’ì€ ê²ƒ ì‚¬ìš©
                    if pdf_path.name in file_scores:
                        # ì´ë¯¸ íŒŒì¼ëª…ìœ¼ë¡œ ë§¤ì¹­ëœ ê²½ìš°, ë” ë†’ì€ ì ìˆ˜ ìœ ì§€
                        if content_score > file_scores[pdf_path.name]['score']:
                            file_scores[pdf_path.name]['score'] = content_score
                            file_scores[pdf_path.name]['context'] = context
                            file_scores[pdf_path.name]['matched_keywords'].extend(matched_keywords)
                    else:
                        # ìƒˆë¡œìš´ íŒŒì¼ ì¶”ê°€
                        file_scores[pdf_path.name] = {
                            'path': pdf_path,
                            'filename': pdf_path.name,
                            'score': content_score,
                            'matched_keywords': matched_keywords,
                            'context': context
                        }

            except Exception as e:
                if logger:
                    logger.warning(f"PDF ë‚´ìš© ê²€ìƒ‰ ì‹¤íŒ¨ {pdf_path.name}: {e}")
                continue

        # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        results = list(file_scores.values())
        results.sort(key=lambda x: x['score'], reverse=True)

        if logger and results:
            logger.info(f"ë‚´ìš© ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì„œ ì°¾ìŒ")
            for r in results[:3]:
                logger.info(f"  - {r['filename']}: ì ìˆ˜ {r['score']}, ë§¤ì¹­ {r['matched_keywords']}")

        return results[:10]  # ìƒìœ„ 10ê°œ ë°˜í™˜

    def _extract_context(self, text: str, keyword: str, window: int = 200) -> str:
        """í‚¤ì›Œë“œ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        keyword_lower = keyword.lower()
        text_lower = text.lower()

        pos = text_lower.find(keyword_lower)
        if pos == -1:
            return ""

        start = max(0, pos - window)
        end = min(len(text), pos + len(keyword) + window)

        context = text[start:end]
        # í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸
        context = context.replace(keyword, f"**{keyword}**")

        return f"...{context}..."

    def find_best_document(self, query: str) -> Optional[Path]:
        """ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë¬¸ì„œ ì°¾ê¸° - ë™ì  ë§¤ì¹­ + ë‚´ìš© ê²€ìƒ‰"""

        # ë‚´ìš© ê²€ìƒ‰ ê¸°ëŠ¥ í†µí•©
        if not hasattr(self, 'content_searcher'):
            from content_search import ContentSearcher
            self.content_searcher = ContentSearcher(self.docs_dir)

        # ğŸ”¥ NEW: ì‹¤ì œ PDF ë‚´ìš© ê¸°ë°˜ ê²€ìƒ‰ ì¶”ê°€
        content_based_results = self._search_by_content(query)

        # ë‚´ìš© ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if content_based_results and content_based_results[0]['score'] > 20:
            best_result = content_based_results[0]
            if logger:
                logger.info(f"âœ… ë‚´ìš© ê²€ìƒ‰ìœ¼ë¡œ ë¬¸ì„œ ì°¾ìŒ: {best_result['filename']} (ì ìˆ˜: {best_result['score']})")
                logger.info(f"   ë§¤ì¹­ í‚¤ì›Œë“œ: {best_result['matched_keywords']}")
            return best_result['path']

        query_lower = query.lower()

        # PDF ë¬¸ì„œ ìš°ì„  ì²˜ë¦¬ í‚¤ì›Œë“œ í™•ì¥
        pdf_priority_keywords = [
            'ìˆ˜ë¦¬', 'ë³´ìˆ˜', 'ë‚´ìš©', 'ìš”ì•½', 'ê²€í† ì„œ', 'ê¸°ìˆ ê²€í† ',
            'êµì²´', 'êµ¬ë§¤', 'íê¸°', 'ì†Œëª¨í’ˆ', 'ê¸°ì•ˆ', 'ê²€í† ',
            'ì–´ë–¤', 'ë¬´ì—‡', 'ë­', 'ë­˜ë¡œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€'
        ]

        # ë¬¸ì„œ ê²€ìƒ‰ í›„ë³´ ì´ˆê¸°í™” - ë”•ì…”ë„ˆë¦¬ë¡œ ë³€ê²½í•˜ì—¬ ì¤‘ë³µ ê´€ë¦¬
        candidates = {}  # path -> (score, filename)
        
        # ê¸°ì¡´ ë¡œì§ ê³„ì†
        # ì§ˆë¬¸ ì •ê·œí™” ë° í† í°í™”
        query_tokens = set(query_lower.split())
        
        # ì—°ë„ì™€ ì›” ì¶”ì¶œ
        year_match = re.search(r'(20\d{2})', query)
        query_year = year_match.group(1) if year_match else None
        
        # ì›” ì¶”ì¶œ (1ì›”, 01ì›”, 1-ì›” ë“± ë‹¤ì–‘í•œ í˜•ì‹)
        month_match = re.search(r'(\d{1,2})\s*ì›”', query)
        query_month = None
        if month_match:
            query_month = int(month_match.group(1))
        
        for cache_key, metadata in self.metadata_cache.items():
            score = 0
            filename = metadata.get('filename', cache_key)
            filename_lower = filename.lower()
            
            # 1. ì—°ë„ì™€ ì›” ë§¤ì¹­ (ì—°ë„ê°€ ì§€ì •ëœ ê²½ìš°ì—ë§Œ)
            if query_year:
                if metadata['year'] == query_year:
                    score += 20

                    # ì›”ë„ ì§€ì •ëœ ê²½ìš° ì›”ê¹Œì§€ ì²´í¬
                    if query_month:
                        # íŒŒì¼ëª…ì—ì„œ ì›” ì¶”ì¶œ (YYYY-MM-DD í˜•ì‹)
                        file_month_match = re.search(r'\d{4}-(\d{2})-\d{2}', filename)
                        if file_month_match:
                            file_month = int(file_month_match.group(1))
                            if file_month == query_month:
                                score += 30  # ì›”ê¹Œì§€ ì¼ì¹˜í•˜ë©´ ë†’ì€ ì ìˆ˜
                            else:
                                continue  # ì›”ì´ ë‹¤ë¥´ë©´ ì œì™¸
                else:
                    continue  # ì—°ë„ê°€ ë‹¤ë¥´ë©´ ì œì™¸
            # ì—°ë„ê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°ëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ëŒ€ìƒìœ¼ë¡œ ê³„ì† ì§„í–‰
            
            # 2. íŠ¹ì • ì¥ë¹„/ì¥ì†Œëª… ì •í™• ë§¤ì¹­ (ë§¤ìš° ë†’ì€ ê°€ì¤‘ì¹˜)
            # ë™ì  í‚¤ì›Œë“œ ë§¤ì¹­ - í•˜ë“œì½”ë”© ì—†ì´ ìë™ìœ¼ë¡œ
            # ì§ˆë¬¸ì˜ ë‹¨ì–´ë“¤ì„ ì¶”ì¶œ
            query_words = re.findall(r'[ê°€-í£]+|[A-Za-z]+|[0-9]+', query_lower)
            filename_words = re.findall(r'[ê°€-í£]+|[A-Za-z]+|[0-9]+', filename_lower)
            
            # ê³µí†µ ë‹¨ì–´ ì°¾ê¸° (2ê¸€ì ì´ìƒ)
            for q_word in query_words:
                if len(q_word) >= 2:
                    for f_word in filename_words:
                        if len(f_word) >= 2:
                            # ì™„ì „ ì¼ì¹˜ (ê°€ì¥ ë†’ì€ ì ìˆ˜)
                            if q_word == f_word:
                                # ë‹¨ì–´ ê¸¸ì´ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë¶€ì—¬
                                weight = len(q_word) * 5  # 2 -> 5ë¡œ ì¦ê°€
                                score += weight
                            # ìœ ì‚¬ë„ ê²€ì‚¬ (ì˜¤íƒ€ ì²˜ë¦¬) - ì™„ì „ ì¼ì¹˜ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                            elif self._calculate_similarity(q_word, f_word) >= 0.85:  # 0.8 -> 0.85ë¡œ ìƒí–¥
                                # 85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ë§¤ì¹­ìœ¼ë¡œ ê°„ì£¼
                                weight = len(q_word) * 1.5
                                score += weight
                            # ë¶€ë¶„ ì¼ì¹˜ (ê¸´ ë‹¨ì–´ì¼ ê²½ìš°)
                            if len(q_word) >= 3 and len(f_word) >= 3:
                                if q_word in f_word or f_word in q_word:
                                    weight = min(len(q_word), len(f_word))
                                    score += weight
            
            # 3. í‚¤ì›Œë“œ ë§¤ì¹­ (ë©”íƒ€ë°ì´í„°)
            for keyword in metadata['keywords']:
                if keyword.lower() in query_lower:
                    score += 5
            
            # 4. í† í° ê¸°ë°˜ ìœ ì‚¬ë„ (ë‹¨ì–´ ê²¹ì¹¨)
            filename_tokens = set(filename_lower.replace('_', ' ').replace('-', ' ').split())
            common_tokens = query_tokens & filename_tokens
            score += len(common_tokens) * 2
            
            # 5. ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
            # ì§ˆë¬¸ì˜ ì£¼ìš” ë‹¨ì–´ê°€ íŒŒì¼ëª…ì— í¬í•¨ë˜ëŠ”ì§€
            important_words = [w for w in query_lower.split() if len(w) > 2]
            for word in important_words:
                if word in filename_lower:
                    score += 3
            
            # 6. ë¬¸ì„œ íƒ€ì…ë³„ íŠ¹ë³„ ì²˜ë¦¬
            # "ê²€í† ì„œ", "ìš”ì²­ì˜ ê±´" ë“± ë¬¸ì„œ íƒ€ì… ë§¤ì¹­
            doc_types = ['ê²€í† ì„œ', 'ìš”ì²­ì˜ ê±´', 'ê¸°ìˆ ê²€í† ì„œ', 'êµ¬ë§¤ê²€í† ì˜ ê±´', 'ë³´ìˆ˜ê±´']
            for doc_type in doc_types:
                if doc_type in query and doc_type in filename:
                    score += 5

            if score > 0:
                candidates[metadata['path']] = (score, filename)

        # ë‚´ìš© ê²€ìƒ‰ ì¶”ê°€ (íŒŒì¼ëª… ë§¤ì¹­ê³¼ í•¨ê»˜)
        try:
            # PDF íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
            pdf_files = [p for p in self.pdf_files if p.suffix.lower() == '.pdf']

            if pdf_files:
                logger.info(f"ë‚´ìš© ê²€ìƒ‰ ì‹œì‘: {len(pdf_files)}ê°œ PDF ëŒ€ìƒ")
                # ì„±ëŠ¥ì„ ìœ„í•´ ìµœëŒ€ 30ê°œ íŒŒì¼ë§Œ ë‚´ìš© ê²€ìƒ‰ (ë” ì •í™•í•œ íŒŒì¼ëª… ë§¤ì¹­ì´ ìš°ì„ )
                content_results = self.content_searcher.search_by_content(query, pdf_files, top_k=10, max_files=30)

                # ë‚´ìš© ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì ìˆ˜ì— ë°˜ì˜
                for result in content_results:
                    pdf_path = result['path']
                    content_score = result['score']

                    if pdf_path in candidates:
                        # ì´ë¯¸ íŒŒì¼ëª… ë§¤ì¹­ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš°, ê°€ì¤‘ í‰ê· 
                        old_score, filename = candidates[pdf_path]
                        # íŒŒì¼ëª… ì ìˆ˜ 60%, ë‚´ìš© ì ìˆ˜ 40%
                        new_score = old_score * 0.6 + content_score * 0.4
                        candidates[pdf_path] = (new_score, filename)
                    else:
                        # íŒŒì¼ëª… ë§¤ì¹­ë˜ì§€ ì•Šì€ ê²½ìš°, ë‚´ìš© ì ìˆ˜ë§Œ ì‚¬ìš© (0.8 ê°€ì¤‘ì¹˜)
                        candidates[pdf_path] = (content_score * 0.8, pdf_path.name)

                logger.info(f"ë‚´ìš© ê²€ìƒ‰ ì™„ë£Œ: {len(content_results)}ê°œ ë§¤ì¹­")
        except Exception as e:
            logger.warning(f"ë‚´ìš© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")

        # Phase 1.2: ë©”íƒ€ë°ì´í„° DB ê²€ìƒ‰ ì¶”ê°€
        if self.metadata_db:
            try:
                # ì—°ë„ë¡œ ê²€ìƒ‰
                if query_year:
                    db_results = self.metadata_db.search_by_year(query_year)
                    for doc in db_results[:10]:  # ìµœëŒ€ 10ê°œ
                        doc_path = Path(doc['path'])
                        if doc_path.exists():
                            if doc_path in candidates:
                                # ì´ë¯¸ ìˆìœ¼ë©´ ì ìˆ˜ ë³´ì •
                                old_score, filename = candidates[doc_path]
                                candidates[doc_path] = (old_score + 5, filename)
                            else:
                                # ìƒˆë¡œ ì¶”ê°€ (DB ê²€ìƒ‰ ê¸°ë°˜)
                                candidates[doc_path] = (10, doc['filename'])

                # ê¸°ì•ˆìë¡œ ê²€ìƒ‰
                drafter_patterns = [r'([ê°€-í£]{2,4})(?:ê°€|ì˜|ì—ì„œ|ì´)\s*(?:ì‘ì„±|ê¸°ì•ˆ)']
                for pattern in drafter_patterns:
                    match = re.search(pattern, query)
                    if match:
                        drafter_name = match.group(1)
                        db_results = self.metadata_db.search_by_text(drafter_name)
                        for doc in db_results[:5]:
                            doc_path = Path(doc['path'])
                            if doc_path.exists():
                                if doc_path in candidates:
                                    old_score, filename = candidates[doc_path]
                                    candidates[doc_path] = (old_score + 10, filename)
                                else:
                                    candidates[doc_path] = (15, doc['filename'])

                logger.info("âœ… ë©”íƒ€ë°ì´í„° DB ê²€ìƒ‰ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ë©”íƒ€ë°ì´í„° DB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")

        # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì ìˆ˜ìˆœ ì •ë ¬
        sorted_candidates = sorted(
            [(score, path, filename) for path, (score, filename) in candidates.items()],
            reverse=True
        )

        # ë””ë²„ê¹… ì¶œë ¥ (ìƒìœ„ 3ê°œ)
        if sorted_candidates:
            logger.info("ìƒìœ„ 3ê°œ í›„ë³´ ë¬¸ì„œ:")
            for score, path, filename in sorted_candidates[:3]:
                logger.info(f"  - {filename}: {score:.2f}ì ")

            top_score = sorted_candidates[0][0]
            # ë™ì ìê°€ ìˆëŠ”ì§€ í™•ì¸
            same_score = [c for c in sorted_candidates if c[0] == top_score]
            if len(same_score) > 1:
                # ë™ì ì¼ ë•ŒëŠ” íŒŒì¼ëª… ê¸¸ì´ê°€ ì§§ì€ ê²ƒ ìš°ì„ 
                same_score.sort(key=lambda x: len(x[2]))
                return same_score[0][1]
            return sorted_candidates[0][1]

        return None

    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)
        ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ + í•œê¸€ ìëª¨ ë¶„í•´ ë¹„êµ
        """
        # ê¸¸ì´ê°€ ë„ˆë¬´ ë‹¤ë¥´ë©´ ë‚®ì€ ìœ ì‚¬ë„
        if abs(len(str1) - len(str2)) > 2:
            return 0.0

        # ê°„ë‹¨í•œ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚°
        def levenshtein_distance(s1: str, s2: str) -> int:
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)

            if len(s2) == 0:
                return len(s1)

            previous_row = range(len(s2) + 1)
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    # ì‚½ì…, ì‚­ì œ, ì¹˜í™˜ ë¹„ìš© ê³„ì‚°
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row

            return previous_row[-1]

        # ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
        distance = levenshtein_distance(str1, str2)
        max_len = max(len(str1), len(str2))
        similarity = 1 - (distance / max_len) if max_len > 0 else 1.0

        # í•œê¸€ íŠ¹ìˆ˜ ì²˜ë¦¬: ììŒ/ëª¨ìŒ í•˜ë‚˜ ì°¨ì´ëŠ” ë†’ì€ ìœ ì‚¬ë„
        # ì˜ˆ: ì¼/ìº , ì½¤/ì»´ ë“±
        if len(str1) == len(str2) and distance == 1:
            # í•œê¸€ì¸ ê²½ìš° ìœ ì‚¬ë„ ë³´ì •
            if all(ord('ê°€') <= ord(c) <= ord('í£') for c in str1 + str2):
                similarity = max(similarity, 0.85)

        return similarity

    def get_document_info(self, file_path: Path, info_type: str = "all") -> str:
        """ë¬¸ì„œì—ì„œ íŠ¹ì • ì •ë³´ ì¶”ì¶œ (PDF/TXT ëª¨ë‘ ì§€ì›)"""
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{file_path.name}_{info_type}"
        if cache_key in self.documents_cache:
            return self.documents_cache[cache_key]
        
        # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì •ë³´ ì¶”ì¶œ
        if file_path.suffix == '.txt':
            info = self._extract_txt_info(file_path)
        else:
            info = self._extract_pdf_info(file_path)
        
        if not info:
            return " ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        
        result = ""
        
        # ìì‚° íŒŒì¼ ê´€ë ¨ ì½”ë“œ ì œê±°ë¨

        # ì¼ë°˜ ë¬¸ì„œ ì²˜ë¦¬
        if info_type == "all":
            result = f" {file_path.stem}\n"
            result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            for key, value in info.items():
                if key != 'text':
                    result += f"â€¢ {key}: {value}\n"
        if info_type == "ê¸°ì•ˆì":
            result = f" ê¸°ì•ˆì: {info.get('ê¸°ì•ˆì', 'ì •ë³´ ì—†ìŒ')}"
        if info_type == "ë‚ ì§œ":
            result = f" ë‚ ì§œ: {info.get('ë‚ ì§œ', 'ì •ë³´ ì—†ìŒ')}"
        if info_type == "ë¶€ì„œ":
            result = f" ë¶€ì„œ: {info.get('ë¶€ì„œ', 'ì •ë³´ ì—†ìŒ')}"
        if info_type == "ê¸ˆì•¡":
            amount = info.get('ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')
            result = f" ê¸ˆì•¡: {amount}"
        else:
            # íŠ¹ì • ì •ë³´ ìš”ì²­
            if info_type in info:
                result = f" {info_type}: {info[info_type]}"
            else:
                result = f" {info_type} ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            
            # ì§§ì€ ë‹µë³€ ë³´ì™„ - ì¶”ê°€ ì •ë³´ ì œê³µ
            if amount != 'ì •ë³´ ì—†ìŒ' and len(result) < 50:
                # ë¬¸ì„œ ì •ë³´ ì¶”ê°€
                result += f"\n\n ë¬¸ì„œ ì •ë³´:\n"
                result += f"â€¢ ë¬¸ì„œëª…: {file_path.stem}\n"
                if 'ë‚ ì§œ' in info:
                    result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
                if 'ê¸°ì•ˆì' in info:
                    result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
                if 'ë¶€ì„œ' in info:
                    result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
                if 'ì œëª©' in info:
                    result += f"â€¢ ì œëª©: {info['ì œëª©']}\n"
        if info_type == "ìš”ì•½":
            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
            result = f" {file_path.stem} ìš”ì•½\n"
            result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            if 'ê¸°ì•ˆì' in info:
                result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
            if 'ë‚ ì§œ' in info:
                result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
            if 'ë¶€ì„œ' in info:
                result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
            if 'ê¸ˆì•¡' in info:
                result += f"â€¢ ê¸ˆì•¡: {info['ê¸ˆì•¡']}\n"
            if 'ì œëª©' in info:
                result += f"â€¢ ì œëª©: {info['ì œëª©']}\n"
        else:  # all
            # LLM ì‘ë‹µê³¼ ìœ ì‚¬í•œ í¬ë§·ìœ¼ë¡œ í†µì¼
            result = f" {file_path.stem}\n\n"
            result += f" **ê¸°ë³¸ ì •ë³´**\n"
            if 'ê¸°ì•ˆì' in info:
                result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
            if 'ë‚ ì§œ' in info:
                result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
            if 'ë¶€ì„œ' in info:
                result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
            
            result += f"\n **ì£¼ìš” ë‚´ìš©**\n"
            
            # text í•„ë“œì—ì„œ ì£¼ìš” ë‚´ìš© ì¶”ì¶œ (ê°œì„ ëœ ìš”ì•½ ì‹œìŠ¤í…œ)
            if 'text' in info:
                summary = self._generate_smart_summary(info['text'], file_path)
                result += summary
            
            if 'ê¸ˆì•¡' in info and info['ê¸ˆì•¡'] != 'ì •ë³´ ì—†ìŒ':
                result += f"\n **ë¹„ìš© ì •ë³´**\n"
                result += f"â€¢ ê¸ˆì•¡: {info['ê¸ˆì•¡']}\n"
            
            result += f"\n ì¶œì²˜: {file_path.name}"
        
        # ìºì‹œ ì €ì¥
        self.documents_cache[cache_key] = result
        
        return result

    def _generate_smart_summary(self, text: str, file_path: Path) -> str:
        """ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì˜ë¯¸ ìˆëŠ” ìš”ì•½ ìƒì„±"""

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = text[:3000]  # ì²˜ìŒ 3000ìë§Œ ì‚¬ìš©
        lines = [line.strip() for line in text.split('\n') if line.strip()]

        summary_parts = []

        # 1. ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ì¥ë¹„ëª…, ê¸ˆì•¡, ì—…ì²´ ë“±)
        equipment_keywords = []
        financial_keywords = []
        company_keywords = []

        # ì¥ë¹„ëª… íŒ¨í„´ (ì˜ë¬¸ ëŒ€ë¬¸ì + ìˆ«ì ì¡°í•©)
        equipment_pattern = r'\b[A-Z][A-Za-z0-9\-]{2,20}\b'
        equipment_matches = re.findall(equipment_pattern, text)
        for match in equipment_matches:
            if len(match) >= 3 and not match.isdigit():
                equipment_keywords.append(match)

        # ê¸ˆì•¡ ì •ë³´
        amount_pattern = r'(\d{1,3}(?:,\d{3})*)\s*(?:ì›|ë§Œì›|ì–µ)'
        amount_matches = re.findall(amount_pattern, text)
        if amount_matches:
            financial_keywords.extend([f"{amt}ì›" for amt in amount_matches[:3]])

        # ì—…ì²´ëª… (ì£¼ì‹íšŒì‚¬, (ì£¼), ë²•ì¸ëª… ë“±)
        company_pattern = r'(?:ì£¼ì‹íšŒì‚¬\s*|ãˆœ\s*|\(ì£¼\)\s*)?([ê°€-í£A-Za-z]{2,20})(?:\s*ì£¼ì‹íšŒì‚¬|\s*ãˆœ|\s*\(ì£¼\))?'
        company_matches = re.findall(company_pattern, text)
        for match in company_matches:
            if len(match) >= 2 and match not in ['ê¸°ì•ˆì', 'ë¶€ì„œ', 'ë‚ ì§œ', 'ì‹œí–‰']:
                company_keywords.append(match)

        # 2. ë¬¸ì„œ ìœ í˜•ë³„ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        file_name = file_path.name.lower()

        if 'êµ¬ë§¤' in file_name or 'êµ¬ì…' in file_name:
            # êµ¬ë§¤ ë¬¸ì„œ
            purchase_info = []
            if equipment_keywords:
                purchase_info.append(f"êµ¬ë§¤ ì¥ë¹„: {', '.join(set(equipment_keywords[:3]))}")
            if financial_keywords:
                purchase_info.append(f"ì˜ˆìƒ ë¹„ìš©: {', '.join(financial_keywords[:2])}")
            if company_keywords:
                purchase_info.append(f"ê´€ë ¨ ì—…ì²´: {', '.join(set(company_keywords[:2]))}")

            if purchase_info:
                summary_parts.append(" " + " | ".join(purchase_info))

        if 'ìˆ˜ë¦¬' in file_name or 'ë³´ìˆ˜' in file_name:
            # ìˆ˜ë¦¬ ë¬¸ì„œ
            repair_info = []
            if equipment_keywords:
                repair_info.append(f"ìˆ˜ë¦¬ ëŒ€ìƒ: {', '.join(set(equipment_keywords[:3]))}")
            if financial_keywords:
                repair_info.append(f"ìˆ˜ë¦¬ ë¹„ìš©: {', '.join(financial_keywords[:2])}")

            if repair_info:
                summary_parts.append(" " + " | ".join(repair_info))

        if 'íê¸°' in file_name:
            # íê¸° ë¬¸ì„œ
            disposal_info = []
            if equipment_keywords:
                disposal_info.append(f"íê¸° ì¥ë¹„: {', '.join(set(equipment_keywords[:3]))}")

            if disposal_info:
                summary_parts.append("ï¸ " + " | ".join(disposal_info))

        # 3. ì¼ë°˜ì ì¸ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
        important_lines = []
        priority_keywords = [
            'ëª©ì ', 'ê°œìš”', 'ì£¼ìš”ë‚´ìš©', 'ê²€í† ê²°ê³¼', 'ê²°ë¡ ', 'ì˜ê²¬',
            'ìŠ¹ì¸', 'ë°˜ë ¤', 'ë³´ì™„', 'ì¶”ì§„', 'ê³„íš', 'ì¼ì •',
            '1.', '2.', '3.', 'â‘ ', 'â‘¡', 'â‘¢', 'â—¦', 'â–¶'
        ]

        for line in lines[:30]:  # ì²˜ìŒ 30ì¤„ ê²€í† 
            if len(line) > 15:  # ì˜ë¯¸ ìˆëŠ” ê¸¸ì´ì˜ ë¬¸ì¥ë§Œ
                # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¼ì¸
                for keyword in priority_keywords:
                    if keyword in line:
                        cleaned_line = line.replace(keyword, '').strip()
                        if len(cleaned_line) > 10:
                            important_lines.append(f"â€¢ {cleaned_line[:80]}{'...' if len(cleaned_line) > 80 else ''}")
                        break

        # ì¤‘ìš”í•œ ë¼ì¸ì´ ì—†ìœ¼ë©´ ì²˜ìŒ ëª‡ ë¼ì¸ ì‚¬ìš©
        if not important_lines:
            for line in lines[:5]:
                if len(line) > 20 and not line.isdigit():
                    important_lines.append(f"â€¢ {line[:80]}{'...' if len(line) > 80 else ''}")
                    if len(important_lines) >= 3:
                        break

        # ìµœì¢… ìš”ì•½ ì¡°í•©
        result = ""
        if summary_parts:
            result += "\n".join(summary_parts) + "\n\n"

        if important_lines:
            result += "\n".join(important_lines[:4])  # ìµœëŒ€ 4ì¤„

        return result if result else "â€¢ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ì¤‘..."

    def _remove_duplicate_documents(self, documents: list) -> list:
        """ì¤‘ë³µ ë¬¸ì„œ ì œê±° (íŒŒì¼ëª… ê¸°ì¤€)"""
        seen = set()
        unique_docs = []

        for doc in documents:
            # íŒŒì¼ëª…ì—ì„œ ê²½ë¡œ ë¶€ë¶„ ì œê±°í•˜ì—¬ ë¹„êµ
            filename = Path(doc.get('path', doc.get('filename', ''))).name
            if filename not in seen:
                seen.add(filename)
                unique_docs.append(doc)

        return unique_docs

    def _format_enhanced_response(self, results: list, query: str) -> str:
        """ê°œì„ ëœ ì‘ë‹µ í˜•ì‹"""
        if not results:
            return " ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ì¤‘ë³µ ì œê±°
        unique_results = self._remove_duplicate_documents(results)

        response = f" **ê²€ìƒ‰ ê²°ê³¼** ({len(unique_results)}ê°œ ë¬¸ì„œ)\n\n"

        for i, doc in enumerate(unique_results[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            title = doc.get('title', 'ì œëª© ì—†ìŒ')
            date = doc.get('date', 'ë‚ ì§œ ë¯¸ìƒ')
            category = doc.get('category', 'ê¸°íƒ€')
            drafter = doc.get('drafter', 'ë¯¸ìƒ')

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì •ë³´ ìš°ì„  ì‚¬ìš© (2025-09-29 ì¶”ê°€)
            if 'extracted_date' in doc:
                date = doc['extracted_date']
            if 'extracted_type' in doc:
                category = doc['extracted_type']
            if 'extracted_dept' in doc:
                drafter = doc['extracted_dept']

            # ë‚ ì§œ í‘œì‹œ ê°œì„ 
            if date and date != 'ë‚ ì§œ ë¯¸ìƒ' and len(date) >= 10:
                display_date = date[:10]  # YYYY-MM-DD
            if date and len(date) >= 4:
                display_date = date[:4]  # ì—°ë„ë§Œ
            else:
                display_date = "ë‚ ì§œë¯¸ìƒ"

            response += f"**{i}. [{category}] {title}**\n"
            response += f"    {display_date} |  {drafter}"

            # ì¶”ì¶œëœ ê¸ˆì•¡ ì •ë³´ ì¶”ê°€ (2025-09-29)
            if 'extracted_amount' in doc:
                amount = doc['extracted_amount']
                response += f" | ğŸ’° {amount:,}ì›"

            response += "\n"

            # ë¬¸ì„œ ìš”ì•½ ì¶”ê°€
            if 'path' in doc:
                try:
                    file_path = Path(doc['path'])
                    if file_path.exists():
                        summary = self._generate_smart_summary("", file_path)
                        if summary and summary != "â€¢ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ì¤‘...":
                            response += f"   {summary}\n"
                except Exception:
                    pass

            response += "\n"

        if len(unique_results) > 5:
            response += f"... ì™¸ {len(unique_results) - 5}ê°œ ë¬¸ì„œ ë” ìˆìŒ\n\n"

        response += " **íŠ¹ì • ë¬¸ì„œë¥¼ ì„ íƒí•˜ì—¬ ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.**"

        return response

    def _classify_search_intent(self, query: str) -> str:
        """ê²€ìƒ‰ ì˜ë„ ë¶„ë¥˜ - í•­ìƒ document ëª¨ë“œ ë°˜í™˜"""
        return 'document'  # Asset ëª¨ë“œ ì œê±°, í•­ìƒ ë¬¸ì„œ ê²€ìƒ‰

    def answer_from_specific_document(self, query: str, filename: str) -> str:
        """íŠ¹ì • ë¬¸ì„œì— ëŒ€í•´ì„œë§Œ ë‹µë³€ ìƒì„± (ë¬¸ì„œ ì „ìš© ëª¨ë“œ) - ì´ˆìƒì„¸ ë²„ì „
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            filename: íŠ¹ì • ë¬¸ì„œ íŒŒì¼ëª…
        """
        print(f" ë¬¸ì„œ ì „ìš© ëª¨ë“œ: {filename}")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ í•´ë‹¹ ë¬¸ì„œ ì°¾ê¸°
        doc_metadata = self._find_metadata_by_filename(filename)
        if not doc_metadata:
            return f" ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
        doc_path = doc_metadata['path']
        
        # PDFì¸ì§€ TXTì¸ì§€ í™•ì¸
        if filename.endswith('.pdf'):
            # PDF ë¬¸ì„œ ì²˜ë¦¬ - ì „ì²´ ë‚´ìš© ì¶”ì¶œ
            info = self._extract_pdf_info_with_retry(doc_path)
            if not info.get('text'):
                return f" PDF ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
            
            # LLM ì´ˆê¸°í™”
            if self.llm is None:
                print(" LLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
                self._preload_llm()
            
            # ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì‚¬ìš© (15000ìë¡œ í™•ëŒ€)
            full_text = info['text'][:15000]
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
            if any(word in query for word in ['ìš”ì•½', 'ì •ë¦¬', 'ê°œìš”', 'ë‚´ìš©']):
                prompt = self._create_detailed_summary_prompt(query, full_text, filename)
            if any(word in query for word in ['ìƒì„¸', 'ìì„¸íˆ', 'êµ¬ì²´ì ', 'ì„¸ì„¸íˆ', 'ì„¸ë¶€']):
                prompt = self._create_ultra_detailed_prompt(query, full_text, filename)
            if any(word in query for word in ['í’ˆëª©', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'í•­ëª©']):
                prompt = self._create_itemized_list_prompt(query, full_text, filename)
            else:
                prompt = self._create_document_specific_prompt(query, full_text, filename)
            
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ë” ê¸´ ë‹µë³€ í—ˆìš©)
            try:
                # ë¬¸ì„œ ì „ìš© ëª¨ë“œì—ì„œëŠ” ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
                context_chunks = [
                    {
                        'content': full_text,
                        'source': filename,
                        'metadata': {
                            'filename': filename,
                            'date': doc_metadata.get('date', ''),
                            'title': doc_metadata.get('title', ''),
                            'is_document_only_mode': True  # ë¬¸ì„œ ì „ìš© ëª¨ë“œ í‘œì‹œ
                        }
                    }
                ]
                
                response = self.llm.generate_response(query, context_chunks)
                answer = response.answer if hasattr(response, 'answer') else str(response)
                
                # ë‹µë³€ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë³´ê°•
                if len(answer) < 200 and 'ìì„¸íˆ' in query:
                    answer = self._enhance_short_answer(answer, full_text, query)
                    
            except Exception as e:
                print(f"LLM ì˜¤ë¥˜: {e}")
                # í´ë°±: ìƒì„¸í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€
                answer = self._detailed_text_search(info['text'], query, filename)
            
            # ì¶œì²˜ ì¶”ê°€
            answer += f"\n\n **ì¶œì²˜**: {filename}"
            
        if filename.endswith('.txt'):
            # TXT íŒŒì¼ ì²˜ë¦¬ (ìì‚° ë°ì´í„°)
            return None  # Asset ê²€ìƒ‰ ì œê±°
        else:
            return f" ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {filename}"
        
        return answer
    
    def _simple_text_search(self, text: str, query: str, filename: str) -> str:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (LLM ì—†ì´)"""
        lines = text.split('\n')
        relevant_lines = []
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        
        # ê´€ë ¨ ì¤„ ì°¾ê¸°
        for line in lines:
            if any(keyword in line for keyword in keywords if len(keyword) > 1):
                relevant_lines.append(line.strip())
        
        if relevant_lines:
            return f" {filename}ì—ì„œ ì°¾ì€ ê´€ë ¨ ë‚´ìš©:\n\n" + '\n'.join(relevant_lines[:20])
        else:
            return f" {filename}ì—ì„œ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _detailed_text_search(self, text: str, query: str, filename: str) -> str:
        """ìƒì„¸í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (LLM ì—†ì´)"""
        lines = text.split('\n')
        relevant_sections = []
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        
        # ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸° (ì•ë’¤ ë¬¸ë§¥ í¬í•¨)
        for i, line in enumerate(lines):
            if any(keyword in line for keyword in keywords if len(keyword) > 1):
                # ì•ë’¤ 2ì¤„ì”© í¬í•¨
                start = max(0, i-2)
                end = min(len(lines), i+3)
                section = '\n'.join(lines[start:end])
                if section not in relevant_sections:
                    relevant_sections.append(section)
        
        if relevant_sections:
            return f" **{filename}** ìƒì„¸ ë¶„ì„:\n\n" + '\n---\n'.join(relevant_sections[:10])
        else:
            return f" {filename}ì—ì„œ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _enhance_short_answer(self, answer: str, full_text: str, query: str) -> str:
        """ì§§ì€ ë‹µë³€ì„ ë³´ê°•"""
        enhanced = answer + "\n\n **ì¶”ê°€ ìƒì„¸ ì •ë³´**:\n"
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        lines = full_text.split('\n')
        
        additional_info = []
        for line in lines:
            if any(keyword in line for keyword in keywords if len(keyword) > 2):
                if line.strip() and line not in answer:
                    additional_info.append(f"â€¢ {line.strip()}")
        
        if additional_info:
            enhanced += '\n'.join(additional_info[:10])
        
        return enhanced
    
    def _create_detailed_summary_prompt(self, query: str, context: str, filename: str) -> str:
        """ìƒì„¸ ìš”ì•½ ì „ìš© í”„ë¡¬í”„íŠ¸ - í†µì¼ëœ í¬ë§·"""

    def _safe_pdf_extract(self, pdf_path, max_retries=3):
        """ì•ˆì „í•œ PDF ì¶”ì¶œ with ì¬ì‹œë„"""
        for attempt in range(max_retries):
            try:
                return self._extract_full_pdf_content(pdf_path)
            except PDFExtractionException as e:
                logger.warning(f"PDF ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    logger.error(f"PDF ì¶”ì¶œ ìµœì¢… ì‹¤íŒ¨: {pdf_path}")
                    return None
                time.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

    def _validate_input(self, query):
        """ì…ë ¥ ê²€ì¦"""
        if not query:
            raise ValueError("ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        if len(query) > 1000:
            logger.warning(f"ì¿¼ë¦¬ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {len(query)}ì")
            query = query[:1000]

        # SQL ì¸ì ì…˜ ë°©ì§€
        dangerous_patterns = ['DROP', 'DELETE', 'INSERT', 'UPDATE', '--', ';']
        for pattern in dangerous_patterns:
            if pattern in query.upper():
                raise ValueError(f"í—ˆìš©ë˜ì§€ ì•Šì€ íŒ¨í„´: {pattern}")


    def __del__(self):
        """ì†Œë©¸ì - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.cleanup_executor()

    def _parallel_search_pdfs(self, pdf_files, query, top_k=5):
        """ë³‘ë ¬ PDF ê²€ìƒ‰ - ì„±ëŠ¥ ìµœì í™”"""
        logger.info(f"ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘: {len(pdf_files)}ê°œ PDF, {self.MAX_WORKERS}ê°œ ì›Œì»¤")

        results = []
        futures = []

        # ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
        def search_single_pdf(pdf_path):
            try:
                # ìºì‹œ í™•ì¸
                cache_key = f"{pdf_path}:{query}"
                if cache_key in self.documents_cache:
                    return self.documents_cache[cache_key]['data']

                # PDF ë‚´ìš© ì¶”ì¶œ
                content = self._safe_pdf_extract(pdf_path, max_retries=1)
                if not content:
                    return None

                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                keywords = query.split()
                score = self._score_document_relevance(content, keywords)

                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = self._extract_document_metadata(pdf_path)

                result = {
                    'path': pdf_path,
                    'score': score,
                    'content': content[:500],  # ë¯¸ë¦¬ë³´ê¸°ìš©
                    'metadata': metadata
                }

                # ìºì‹œì— ì €ì¥
                self._add_to_cache(self.documents_cache, cache_key, result, self.MAX_CACHE_SIZE)

                return result

            except Exception as e:
                logger.error(f"PDF ê²€ìƒ‰ ì˜¤ë¥˜ {pdf_path}: {e}")
                return None

        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            # ëª¨ë“  PDFì— ëŒ€í•´ ë¹„ë™ê¸° ì‘ì—… ì œì¶œ
            future_to_pdf = {
                executor.submit(search_single_pdf, pdf): pdf
                for pdf in pdf_files
            }

            # ì™„ë£Œëœ ì‘ì—…ë¶€í„° ì²˜ë¦¬
            for future in as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    result = future.result(timeout=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                    if result and result['score'] > 0:
                        results.append(result)
                        logger.debug(f"ê²€ìƒ‰ ì™„ë£Œ: {pdf.name}, ì ìˆ˜: {result['score']:.2f}")
                except Exception as e:
                    logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨ {pdf}: {e}")

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x['score'], reverse=True)

        logger.info(f"ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results[:top_k]

    def _parallel_extract_metadata(self, files):
        """ë³‘ë ¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        logger.info(f"ë³‘ë ¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ: {len(files)}ê°œ íŒŒì¼")

        def extract_single(file_path):
            try:
                return self._extract_document_metadata(file_path)
            except Exception as e:
                logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
                return {}

        with ThreadPoolExecutor(max_workers=self.MAX_WORKERS) as executor:
            futures = [executor.submit(extract_single, f) for f in files]
            results = []

            for future in as_completed(futures):
                try:
                    metadata = future.result(timeout=5)
                    if metadata:
                        results.append(metadata)
                except Exception as e:
                    logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        return results

    def _batch_process_documents(self, documents, process_func, batch_size=10):
        """ë°°ì¹˜ ë¬¸ì„œ ì²˜ë¦¬ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±"""
        total = len(documents)
        processed = 0
        results = []

        for i in range(0, total, batch_size):
            batch = documents[i:i+batch_size]

            with ThreadPoolExecutor(max_workers=min(len(batch), self.MAX_WORKERS)) as executor:
                futures = [executor.submit(process_func, doc) for doc in batch]

                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=30)
                        if result:
                            results.append(result)
                    except Exception as e:
                        logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

            processed += len(batch)
            logger.info(f"ì§„í–‰ë¥ : {processed}/{total} ({100*processed/total:.1f}%)")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if processed % 50 == 0:
                import gc
                gc.collect()

        return results

    def cleanup_executor(self):
        """ë³‘ë ¬ ì²˜ë¦¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
            logger.info("ë³‘ë ¬ ì²˜ë¦¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

    def _create_ultra_detailed_prompt(self, query: str, context: str, filename: str) -> str:
        """ì´ˆìƒì„¸ ë‹µë³€ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
[ì´ˆìƒì„¸ ë¶„ì„ ëª¨ë“œ] - ëª¨ë“  ì„¸ë¶€ì‚¬í•­ í¬í•¨

ë¬¸ì„œ: {filename}
ìš”ì²­: {query}

 **ìµœëŒ€í•œ ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”**:

1. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ì°¾ì•„ì„œ ì œê³µ
2. ë¬¸ì„œì˜ ì•ë’¤ ë¬¸ë§¥ê¹Œì§€ í¬í•¨í•˜ì—¬ ì„¤ëª…
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë‚ ì§œ, ì´ë¦„, ëª¨ë¸ëª… ë“± ëª¨ë‘ ëª…ì‹œ
4. ê´€ë ¨ ë°°ê²½ ì •ë³´ë„ í•¨ê»˜ ì œê³µ
5. ë¬¸ì„œì— ì•”ì‹œëœ ë‚´ìš©ë„ í•´ì„í•˜ì—¬ ì„¤ëª…

ë¬¸ì„œ ì „ì²´ ë‚´ìš©:
{context}

ë‹µë³€ ê·œì¹™:
 ìµœì†Œ 500ì ì´ìƒ ìƒì„¸ ë‹µë³€
 ëª¨ë“  ê´€ë ¨ ì •ë³´ ë‚˜ì—´
 í‘œë‚˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬
 ì¤‘ìš” ì •ë³´ëŠ” **êµµê²Œ** í‘œì‹œ
 ë¬¸ì„œì˜ ëª¨ë“  ê´€ë ¨ ë¶€ë¶„ ì¸ìš©
"""
    
    def _create_itemized_list_prompt(self, query: str, context: str, filename: str) -> str:
        """í’ˆëª© ë¦¬ìŠ¤íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
[í’ˆëª©/í•­ëª© ìƒì„¸ ë¶„ì„ ëª¨ë“œ]

ë¬¸ì„œ: {filename}

ë¬¸ì„œì— ìˆëŠ” ëª¨ë“  í’ˆëª©/í•­ëª©ì„ ì™„ì „í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”:

 **ì¶”ì¶œ í˜•ì‹**:
1. í’ˆëª©ëª…/ëª¨ë¸ëª…
   - ì œì¡°ì‚¬: 
   - ëª¨ë¸ë²ˆí˜¸:
   - ìˆ˜ëŸ‰:
   - ë‹¨ê°€:
   - ê¸ˆì•¡:
   - ìš©ë„:
   - íŠ¹ì§•:
   - ê¸°íƒ€ì‚¬í•­:

2. (ë‹¤ìŒ í’ˆëª©...)

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

ì¤‘ìš”: 
- ë¬¸ì„œì— ë‚˜ì˜¨ ëª¨ë“  í’ˆëª©ì„ ë¹ ì§ì—†ì´
- ê° í’ˆëª©ì˜ ëª¨ë“  ì •ë³´ë¥¼ ìƒì„¸íˆ
- ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ì„œ ì •ë¦¬
"""
    
    def _create_document_specific_prompt(self, query: str, context: str, filename: str) -> str:
        """íŠ¹ì • ë¬¸ì„œ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± - í†µì¼ëœ í¬ë§·"""
        return f"""
[ë¬¸ì„œ ì „ìš© ì •ë°€ ë¶„ì„ ëª¨ë“œ] 

 ë¶„ì„ ëŒ€ìƒ ë¬¸ì„œ: {filename}

ì´ ë¬¸ì„œë§Œì„ ë¶„ì„í•˜ì—¬ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.


 [ë¬¸ì„œ ì œëª©]

 **ê¸°ë³¸ ì •ë³´**
â€¢ ê¸°ì•ˆì: [ë¬¸ì„œì—ì„œ ì°¾ì€ ê¸°ì•ˆìëª…]
â€¢ ë‚ ì§œ: [ë¬¸ì„œ ë‚ ì§œ]
â€¢ ë¬¸ì„œ ì¢…ë¥˜: [ê¸°ì•ˆì„œ/ê²€í† ì„œ/ë³´ê³ ì„œ ë“±]

 **ì£¼ìš” ë‚´ìš©**
[ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ êµ¬ì¡°í™”í•˜ì—¬ í‘œì‹œ]
â€¢ [ì£¼ìš” ì‚¬í•­ 1]
â€¢ [ì£¼ìš” ì‚¬í•­ 2]
â€¢ [ì„¸ë¶€ ë‚´ìš©ë“¤...]

 **ë¹„ìš© ì •ë³´** (ë¹„ìš© ê´€ë ¨ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°)
â€¢ ì´ì•¡: [ê¸ˆì•¡]
â€¢ ì„¸ë¶€ ë‚´ì—­:
  - [í’ˆëª©1]: [ê¸ˆì•¡]
  - [í’ˆëª©2]: [ê¸ˆì•¡]

 **ê²€í†  ì˜ê²¬** (ê²€í†  ì˜ê²¬ì´ ìˆëŠ” ê²½ìš°)
â€¢ [ê²€í† ì‚¬í•­ 1]
â€¢ [ê²€í† ì‚¬í•­ 2]
â€¢ ê²°ë¡ : [ìµœì¢… ì˜ê²¬]

 ì¶œì²˜: {filename}

 **ë¬¸ì„œ ì „ì²´ ë‚´ìš©**:
{context}

 **ì‚¬ìš©ì ì§ˆë¬¸**: {query}

ï¸ ì£¼ì˜ì‚¬í•­:
- ìœ„ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ
- ì´ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
- ë¬¸ì„œì˜ ì •í™•í•œ í‘œí˜„ ì‚¬ìš©
- ê°€ëŠ¥í•œ í•œ ë§ì€ ì„¸ë¶€ì‚¬í•­ í¬í•¨
"""
    
    def _get_enhanced_cache_key(self, query: str, mode: str) -> str:
        """í–¥ìƒëœ ìºì‹œ í‚¤ ìƒì„± - ìœ ì‚¬ ì§ˆë¬¸ë„ ìºì‹œ íˆíŠ¸

        ì˜ˆì‹œ:
        - "2020ë…„ êµ¬ë§¤ ë¬¸ì„œ" â†’ "2020 êµ¬ë§¤ ë¬¸ì„œ"
        - "2020ë…„ì˜ êµ¬ë§¤í•œ ë¬¸ì„œë¥¼" â†’ "2020 êµ¬ë§¤ ë¬¸ì„œ"
        - "êµ¬ë§¤ 2020ë…„ ë¬¸ì„œ" â†’ "2020 êµ¬ë§¤ ë¬¸ì„œ" (ì •ë ¬)
        """

        # 1. ì¿¼ë¦¬ ì •ê·œí™” ë° ì†Œë¬¸ì ë³€í™˜
        normalized = query.strip().lower()

        # 2. í•œê¸€ ì¡°ì‚¬ ì œê±° - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜
        # ë³µí•© ì¡°ì‚¬ë¶€í„° ì œê±° (ì—ì„œëŠ”, ìœ¼ë¡œëŠ” ë“±)
        compound_particles = ['ì—ì„œëŠ”', 'ìœ¼ë¡œëŠ”', 'ì—ê²ŒëŠ”', 'í•œí…ŒëŠ”', 'ì—ì„œë„', 'ìœ¼ë¡œë„',
                            'ì´ë¼ë„', 'ì´ë‚˜ë§ˆ', 'ì´ë“ ì§€', 'ì´ë¼ëŠ”', 'ê¹Œì§€ë„', 'ë¶€í„°ë„']
        simple_particles = ['ì—ì„œ', 'ìœ¼ë¡œ', 'ì´ë‚˜', 'ì´ë“ ', 'ì´ë©´', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜ì„œ',
                          'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ì—',
                          'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ë§ˆë‹¤', 'ë§ˆì €', 'ì¡°ì°¨']

        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ í›„ ì¡°ì‚¬ ì œê±°
        words = normalized.split()
        cleaned_words = []

        for word in words:
            cleaned_word = word

            # ë³µí•© ì¡°ì‚¬ ë¨¼ì € ì œê±°
            for particle in compound_particles:
                if cleaned_word.endswith(particle):
                    cleaned_word = cleaned_word[:-len(particle)]
                    break

            # ë‹¨ìˆœ ì¡°ì‚¬ ì œê±°
            if cleaned_word:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ˆë©´
                for particle in simple_particles:
                    if cleaned_word.endswith(particle):
                        cleaned_word = cleaned_word[:-len(particle)]
                        break

            # 2ê¸€ì ì´ìƒë§Œ í¬í•¨
            if cleaned_word and len(cleaned_word) >= 2:
                cleaned_words.append(cleaned_word)

        # 3. í‚¤ì›Œë“œ ì •ë ¬ (ìˆœì„œ ë¬´ê´€ ìºì‹œ íˆíŠ¸)
        cleaned_words.sort()

        # 4. ìºì‹œ í‚¤ ìƒì„±
        cache_str = f"{mode}:{'_'.join(cleaned_words)}"
        hash_key = hashlib.md5(cache_str.encode('utf-8')).hexdigest()

        # ë””ë²„ê¹… (í•„ìš”ì‹œ í™œì„±í™”)
        # print(f"Cache: '{query}' â†’ '{' '.join(cleaned_words)}' â†’ {hash_key[:8]}...")

        return hash_key

    def answer_with_logging(self, query: str, mode: str = 'auto') -> str:
        """ë¡œê¹…ì´ í†µí•©ëœ answer ë©”ì„œë“œ (ìºì‹± í¬í•¨)"""
        # í–¥ìƒëœ ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._get_enhanced_cache_key(query, mode)
        
        # ìºì‹œ í™•ì¸
        if cache_key in self.answer_cache:
            cached_response, cached_time = self.answer_cache[cache_key]
            # TTL í™•ì¸ (ê¸°ë³¸ 1ì‹œê°„)
            if time.time() - cached_time < self.cache_ttl:
                print(f" ìºì‹œ íˆíŠ¸! (í‚¤: {cache_key[:8]}...)")
                # LRU ì—…ë°ì´íŠ¸ (ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ ì´ë™)
                self.answer_cache.move_to_end(cache_key)
                return cached_response
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del self.answer_cache[cache_key]
        
        # ì‹¤ì œ ë‹µë³€ ìƒì„± (_answer_internalì—ì„œ ëª¨ë“  ë¡œê¹… ì²˜ë¦¬)
        start_time = time.time()
        response = self._answer_internal(query, mode)
        generation_time = time.time() - start_time
        
        # ìºì‹œ ì €ì¥
        self.answer_cache[cache_key] = (response, time.time())
        print(f"â±ï¸ ë‹µë³€ ìƒì„±: {generation_time:.1f}ì´ˆ (ìºì‹œ ì €ì¥ë¨)")
        
        # ìºì‹œ í¬ê¸° ì œí•œ (LRU ë°©ì‹)
        if len(self.answer_cache) > self.max_cache_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            self.answer_cache.popitem(last=False)
        
        return response
    
    def answer(self, query: str, mode: str = 'auto') -> str:
        """ë¡œê¹…ì´ í†µí•©ëœ ë‹µë³€ ìƒì„±"""
        return self.answer_with_logging(query, mode)
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        # CacheModuleì„ ì‚¬ìš©í•˜ì—¬ ìºì‹œ ì´ˆê¸°í™”
        if self.cache_module:
            self.cache_module.clear_all_cache()
            return

        # CacheModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        self.answer_cache.clear()
        self.documents_cache.clear()
        self.metadata_cache.clear()
        print("ï¸ ëª¨ë“  ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_cache_stats(self) -> Dict:
        """ìºì‹œ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        # CacheModuleì„ ì‚¬ìš©í•˜ì—¬ ìºì‹œ í†µê³„ ë°˜í™˜
        if self.cache_module:
            return self.cache_module.get_cache_stats()

        # CacheModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        response_cache_size = len(self.response_cache) if hasattr(self, 'response_cache') else 0
        total_size = len(self.answer_cache) + len(self.documents_cache) + len(self.metadata_cache) + response_cache_size
        return {
            'size': response_cache_size,  # For compatibility with test
            'hits': self.cache_hits,
            'misses': self.cache_misses,
            'answer_cache_size': len(self.answer_cache),
            'document_cache_size': len(self.documents_cache),
            'metadata_cache_size': len(self.metadata_cache),
            'response_cache_size': response_cache_size,
            'total_cache_size': total_size,
            'max_cache_size': self.max_cache_size,
            'cache_ttl_seconds': self.cache_ttl
        }
    
    def _answer_internal(self, query: str, mode: str = 'auto') -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            mode: ê²€ìƒ‰ ëª¨ë“œ (í•­ìƒ 'document' ì‚¬ìš©)
        """
        
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()
        error_msg = None
        success = True
        response = ""
        metadata = {}
        
        try:
            # ë¡œê¹… ì‹œìŠ¤í…œ ì‹œì‘ - log_queryëŠ” ë‹µë³€ ì™„ë£Œ í›„ì— í˜¸ì¶œí•´ì•¼ í•¨
            # if chat_logger:
            #     chat_logger.log_query(query, "started")
            
            # ê²€ìƒ‰ ì˜ë„ ë¶„ë¥˜
            if mode == 'auto':
                with TimerContext(chat_logger, "classify_intent") if chat_logger else nullcontext():
                    mode = self._classify_search_intent(query)
                print(f" ê²€ìƒ‰ ëª¨ë“œ: {mode}")
            
            self.search_mode = mode
            metadata['search_mode'] = mode
            
            # ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            query_lower = query.lower()

            # document ëª¨ë“œì¸ ê²½ìš°
            if self.search_mode == 'document':
                # "ë¬¸ì„œ", "ì°¾ì•„", "ê²€ìƒ‰" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì—¬ëŸ¬ ë¬¸ì„œ ëª©ë¡ ë°˜í™˜
                if any(keyword in query_lower for keyword in ["ë¬¸ì„œ", "ì°¾ì•„", "ê²€ìƒ‰", "ì–´ë–¤", "ë¬´ì—‡", "ë­"]):
                    # ì—¬ëŸ¬ ë¬¸ì„œ ê²€ìƒ‰
                    search_results = self._search_by_content(query)

                    if not search_results:
                        response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                    else:
                        # ìƒìœ„ 5ê°œ ë¬¸ì„œ ëª©ë¡ í‘œì‹œ
                        response = f"**{query}** ê²€ìƒ‰ ê²°ê³¼\n\n"

                        # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì •ë ¬ ìˆœì„œ ìœ ì§€
                        seen = set()
                        unique_results = []
                        for r in sorted(search_results, key=lambda x: x.get('score', 0), reverse=True):
                            if r['filename'] not in seen:
                                seen.add(r['filename'])
                                unique_results.append(r)

                        response += f"ì´ {len(unique_results)}ê°œ ë¬¸ì„œ ë°œê²¬\n\n"

                        for i, result in enumerate(unique_results[:10], 1):
                            response += f"**{i}. {result['filename']}**\n"

                            # Everything searchì˜ ê²½ìš° ë©”íƒ€ë°ì´í„°ë§Œ ê¹”ë”í•˜ê²Œ í‘œì‹œ
                            if result.get('source') == 'everything_search':
                                if result.get('date'):
                                    response += f"   ğŸ“… ë‚ ì§œ: {result['date']}\n"
                                if result.get('category') and result['category'] != 'ê¸°íƒ€':
                                    response += f"   ğŸ“ ì¹´í…Œê³ ë¦¬: {result['category']}\n"
                                if result.get('keywords'):
                                    # í‚¤ì›Œë“œë¥¼ ê¹”ë”í•˜ê²Œ í‘œì‹œ
                                    keywords_list = result['keywords'].split()[:5]  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œë§Œ
                                    if keywords_list:
                                        response += f"   ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(keywords_list)}\n"
                            # ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼
                            elif result.get('context'):
                                # OCR í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³  ê¹”ë”í•˜ê²Œ ì²˜ë¦¬
                                context = result['context']
                                if '[OCR' in context or 'í˜ì´ì§€' in context:
                                    # OCR í…ìŠ¤íŠ¸ëŠ” í‘œì‹œí•˜ì§€ ì•Šê³  ë©”íƒ€ë°ì´í„°ë§Œ
                                    response += f"   ğŸ“„ ìŠ¤ìº” ë¬¸ì„œ (OCR í•„ìš”)\n"
                                else:
                                    # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ê¹”ë”í•˜ê²Œ í‘œì‹œ
                                    clean_text = context.replace('\n', ' ').strip()[:150]
                                    response += f"   ğŸ“ {clean_text}...\n"
                            response += "\n"

                        if len(unique_results) > 5:
                            response += f"\n... ì™¸ {len(unique_results) - 5}ê°œ ë¬¸ì„œ\n"
                else:
                    # ë‹¨ì¼ ë¬¸ì„œ ìƒì„¸ ë‹µë³€
                    # Everything searchë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ê´€ë ¨ëœ ë¬¸ì„œ ì°¾ê¸°
                    if self.everything_search:
                        search_results = self.everything_search.search(query, limit=1)
                        if search_results:
                            top_result = search_results[0]
                            doc_path = Path(top_result['path'])

                            if doc_path.exists():
                                print(f"ğŸ“„ ì„ íƒëœ ë¬¸ì„œ: {doc_path.name}")
                                # LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ë° ë‹µë³€ ìƒì„±
                                response = self._generate_llm_summary(doc_path, query)
                            else:
                                response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                        else:
                            response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                    else:
                        # ê¸°ì¡´ ë°©ì‹ (Everything search ì—†ì„ ë•Œ)
                        doc_path = self.find_best_document(query)

                        if not doc_path:
                            response = "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                        else:
                            print(f"ğŸ“„ ì„ íƒëœ ë¬¸ì„œ: {doc_path.name}")
                            # LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ë° ë‹µë³€ ìƒì„±
                            response = self._generate_llm_summary(doc_path, query)
            else:
                # Document ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš° (ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨)
                response = "âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° ë° ë¡œê¹…
            processing_time = time.time() - start_time
            if chat_logger:
                chat_logger.log_query(
                    query=query,
                    response=response,
                    search_mode=self.search_mode,
                    processing_time=processing_time,
                    metadata=metadata
                )
                print(f"âœ… Query completed in {processing_time:.2f}s")

            return response

        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ
            error_msg = str(e)
            success = False
            response = f" ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}"
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # ë¡œê¹… ì‹œìŠ¤í…œ
            if chat_logger:
                chat_logger.log_error(
                    error_type=type(e).__name__,
                    error_msg=error_msg,
                    query=query
                )
            
            return response
    
    def _get_detail_only_prompt(self, query: str, context: str, filename: str) -> str:
        """ê¸°ë³¸ ì •ë³´ ì œì™¸í•œ ìƒì„¸ ë‚´ìš©ë§Œ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë¬¸ì„œì—ì„œ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì„¸ìš”. 
ï¸ ê¸°ì•ˆì, ë‚ ì§œ, ë¬¸ì„œë²ˆí˜¸ ë“± ê¸°ë³¸ ì •ë³´ëŠ” ì œì™¸í•˜ê³  ì‹¤ì§ˆì ì¸ ë‚´ìš©ë§Œ ì‘ì„±í•˜ì„¸ìš”.

 **êµ¬ë§¤/ìˆ˜ë¦¬ ì‚¬ìœ  ë° í˜„í™©**
â€¢ ì–´ë–¤ ë¬¸ì œê°€ ìˆì—ˆëŠ”ì§€
â€¢ í˜„ì¬ ìƒí™©ì€ ì–´ë–¤ì§€
â€¢ ì œì•ˆí•˜ëŠ” í•´ê²°ì±…ì€ ë¬´ì—‡ì¸ì§€

 **ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­** (ìˆëŠ” ê²½ìš°ë§Œ)
â€¢ ì¥ë¹„ ì‚¬ì–‘ì´ë‚˜ ëª¨ë¸
â€¢ ê²€í† í•œ ëŒ€ì•ˆë“¤
â€¢ ì„ íƒ ê·¼ê±°

 **ë¹„ìš© ê´€ë ¨** (ìˆëŠ” ê²½ìš°ë§Œ)
â€¢ ì˜ˆìƒ ë¹„ìš©
â€¢ ì—…ì²´ ì •ë³´

ë¬¸ì„œ: {filename}
ë‚´ìš©: {context[:5000]}

ì§ˆë¬¸: {query}

ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
"""

    def _get_optimized_prompt(self, query: str, context: str, filename: str) -> str:
        """ê¸°ìˆ ê´€ë¦¬íŒ€ ì‹¤ë¬´ ìµœì í™” í”„ë¡¬í”„íŠ¸"""
        
        # ê¸´ê¸‰ ìƒí™© í‚¤ì›Œë“œ
        if any(word in query for word in ["ê¸´ê¸‰", "ìˆ˜ë¦¬", "ê³ ì¥", "ì—…ì²´", "ì—°ë½ì²˜"]):
            return f"""
[ê¸´ê¸‰ ì¥ë¹„ ì¡°íšŒ] ë°©ì†¡ ì¥ë¹„ ê´€ë¦¬ ì‹œìŠ¤í…œ

ìƒí™©: ê¸´ê¸‰ ëŒ€ì‘ í•„ìš”
ë¬¸ì„œ: {filename}

ë‹¤ìŒ ì •ë³´ë¥¼ ì¦‰ì‹œ ì œê³µí•˜ì„¸ìš”:
â€¢ ìˆ˜ë¦¬ ì—…ì²´ëª…:
â€¢ ë‹´ë‹¹ì ì—°ë½ì²˜:
â€¢ ì´ì „ ì²˜ë¦¬ ë¹„ìš©:
â€¢ ì²˜ë¦¬ ê¸°ê°„:

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

ï¸ 30ì´ˆ ë‚´ ë‹µë³€ í•„ìš”
ï¸ ì—†ëŠ” ì •ë³´ëŠ” "í™•ì¸ í•„ìš”"ë¡œ í‘œì‹œ
"""
        
        # ë³´ê³ ì„œ/ë‚´ì—­ ì •ë¦¬/ê¸°ìˆ ê²€í† ì„œ
        if any(word in query for word in ["ë‚´ì—­", "ì •ë¦¬", "ì´", "ëª©ë¡", "êµ¬ë§¤", "í’ˆëª©", "ê²€í† ì„œ", "ë‚´ìš©", "ìš”ì•½", "ì•Œë ¤"]):
            # ê¸°ë³¸ ì •ë³´ê°€ ì´ë¯¸ ì¶”ì¶œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            has_basic_info = 'basic_summary' in locals() if 'locals' in dir() else False
            
            if has_basic_info:
                # ê¸°ë³¸ ì •ë³´ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ìƒì„¸ ë‚´ìš©ë§Œ ìš”ì²­
                return f"""
[ë¬¸ì„œ ìƒì„¸ ë¶„ì„] {filename}

ï¸ ê¸°ë³¸ ì •ë³´(ê¸°ì•ˆì, ë‚ ì§œ ë“±)ëŠ” ì´ë¯¸ ì¶”ì¶œë¨. ì•„ë˜ ë‚´ìš©ë§Œ ì‘ì„±í•˜ì„¸ìš”:

 **í•µì‹¬ ë‚´ìš©**
â€¢ êµ¬ë§¤/ìˆ˜ë¦¬ ì‚¬ìœ : [íŒŒì† ìƒíƒœ, ë¬¸ì œì  ë“± êµ¬ì²´ì ìœ¼ë¡œ]
â€¢ í˜„ì¬ ìƒí™©: [í˜„í™© ì„¤ëª…]  
â€¢ í•´ê²° ë°©ì•ˆ: [ì œì•ˆ ë‚´ìš©]

 **ê¸°ìˆ  ê²€í†  ë‚´ìš©** (í•´ë‹¹ì‹œ)
â€¢ ê¸°ì¡´ ì¥ë¹„ ë¬¸ì œì : [êµ¬ì²´ì  ë¬¸ì œ ì„¤ëª…]
â€¢ ëŒ€ì²´ ì¥ë¹„: [ëª¨ë¸ëª…, ì œì¡°ì‚¬]
â€¢ ì£¼ìš” ì‚¬ì–‘: [í•µì‹¬ ìŠ¤í™]
â€¢ ì„ ì • ì´ìœ : [ì„ íƒ ê·¼ê±°]

 **ë¹„ìš© ì •ë³´**
â€¢ ì´ì•¡: [ê¸ˆì•¡] (ë¶€ê°€ì„¸ í¬í•¨/ë³„ë„)
â€¢ ì„¸ë¶€ ë‚´ì—­:
  - [í’ˆëª©1]: [ëª¨ë¸ëª…] - [ìˆ˜ëŸ‰] x [ë‹¨ê°€] = [ê¸ˆì•¡]
  - [í’ˆëª©2]: [ëª¨ë¸ëª…] - [ìˆ˜ëŸ‰] x [ë‹¨ê°€] = [ê¸ˆì•¡]
â€¢ ë‚©í’ˆì—…ì²´: [ì—…ì²´ëª…]

 **ê²€í†  ì˜ê²¬**
â€¢ [ê²€í† ì‚¬í•­ 1]
â€¢ [ê²€í† ì‚¬í•­ 2]
â€¢ ê²°ë¡ : [ìµœì¢… ì˜ê²¬/ìŠ¹ì¸ì‚¬í•­]

 ì¶œì²˜: {filename}

ë¬¸ì„œ ë‚´ìš©:
{context}

ìš”ì²­: {query}

ï¸ ì£¼ì˜ì‚¬í•­:
- ìœ„ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ
- ëª¨ë“  í’ˆëª©/ì¥ë¹„ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨
- ê¸ˆì•¡ì€ ì²œë‹¨ìœ„ ì½¤ë§ˆ í¬í•¨ (ì˜ˆ: 820,000ì›)
- ëª¨ë¸ëª…, ì—…ì²´ëª… ë“± ê³ ìœ ëª…ì‚¬ëŠ” ì •í™•íˆ í‘œê¸°
"""
        
        # ê°ì‚¬/ì ˆì°¨ í™•ì¸
        if any(word in query for word in ["ê°ì‚¬", "ì ˆì°¨", "ìŠ¹ì¸", "íê¸°"]):
            return f"""
[ê°ì‚¬ ëŒ€ì‘ ìë£Œ] ê¸°ìˆ ê´€ë¦¬íŒ€ ë¬¸ì„œ ì‹œìŠ¤í…œ

ë¬¸ì„œ: {filename}
ìš”ì²­: {query}

í•„ìˆ˜ í™•ì¸ ì‚¬í•­:
â–¡ ìš”ì²­ ì¼ì:
â–¡ í’ˆì˜ì„œ ë²ˆí˜¸:
â–¡ ëŒ€ìƒ ì¥ë¹„:
â–¡ ì²˜ë¦¬ ì‚¬ìœ :
â–¡ ìŠ¹ì¸ ë¼ì¸:
  - 1ì°¨:
  - 2ì°¨:
  - ìµœì¢…:
â–¡ ì²˜ë¦¬ ì™„ë£Œì¼:

ë¬¸ì„œ ë‚´ìš©:
{context}

ï¸ ê°ì‚¬ ì§€ì  ë°©ì§€ë¥¼ ìœ„í•´ ì •í™•íˆ í™•ì¸
"""
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (í†µì¼ëœ í¬ë§·)
        else:
            return f"""
[ê¸°ìˆ ê´€ë¦¬íŒ€ ë¬¸ì„œ ë¶„ì„]


 {filename.replace('.pdf', '')}

 **ê¸°ë³¸ ì •ë³´**
â€¢ ê¸°ì•ˆì: [ë¬¸ì„œì—ì„œ ì°¾ì€ ê¸°ì•ˆìëª…]
â€¢ ë‚ ì§œ: [ë¬¸ì„œ ë‚ ì§œ]
â€¢ ë¬¸ì„œ ì¢…ë¥˜: [ê¸°ì•ˆì„œ/ê²€í† ì„œ/ë³´ê³ ì„œ ë“±]

 **ì£¼ìš” ë‚´ìš©**
[ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ êµ¬ì¡°í™”í•˜ì—¬ í‘œì‹œ]
â€¢ [ì£¼ìš” ì‚¬í•­ 1]
â€¢ [ì£¼ìš” ì‚¬í•­ 2]
â€¢ [ì„¸ë¶€ ë‚´ìš©ë“¤...]

 **ë¹„ìš© ì •ë³´** (ë¹„ìš© ê´€ë ¨ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°)
â€¢ ì´ì•¡: [ê¸ˆì•¡]
â€¢ ì„¸ë¶€ ë‚´ì—­:
  - [í’ˆëª©1]: [ê¸ˆì•¡]
  - [í’ˆëª©2]: [ê¸ˆì•¡]

 **ê²€í†  ì˜ê²¬** (ê²€í†  ì˜ê²¬ì´ ìˆëŠ” ê²½ìš°)
â€¢ [ê²€í† ì‚¬í•­ 1]
â€¢ [ê²€í† ì‚¬í•­ 2]
â€¢ ê²°ë¡ : [ìµœì¢… ì˜ê²¬]

 ì¶œì²˜: {filename}

ë¬¸ì„œ ë‚´ìš©:
{context}

ìš”ì²­: {query}

ï¸ ì£¼ì˜ì‚¬í•­:
- ìœ„ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ
- ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ìƒì„¸íˆ í¬í•¨
- ì‹¤ë¬´ìê°€ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€
- ëª¨ë“  ê³ ìœ ëª…ì‚¬(ëª¨ë¸ëª…, ì—…ì²´ëª…, ë‹´ë‹¹ìëª…)ëŠ” ì •í™•íˆ í‘œê¸°
- ê¸ˆì•¡ì€ ì²œë‹¨ìœ„ ì½¤ë§ˆ í¬í•¨ (ì˜ˆ: 1,234,000ì›)
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
"""
    
    def _is_gian_document(self, text: str) -> bool:
        """ê¸°ì•ˆì„œ ë¬¸ì„œì¸ì§€ í™•ì¸"""
        gian_keywords = ['ì¥ë¹„êµ¬ë§¤/ìˆ˜ë¦¬ ê¸°ì•ˆì„œ', 'ê¸°ì•ˆë¶€ì„œ', 'ê¸°ì•ˆì', 'ê¸°ì•ˆì¼ì', 'ê²°ì¬', 'í•©ì˜']
        matches = sum(1 for keyword in gian_keywords if keyword in text[:500])
        return matches >= 3
    
    def _try_ocr_extraction(self, pdf_path: Path) -> str:
        """OCRì„ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„"""
        try:
            # OCR í”„ë¡œì„¸ì„œ ì„í¬íŠ¸
            from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
            
            ocr_processor = EnhancedOCRProcessor()
            text, metadata = ocr_processor.extract_text_with_ocr(str(pdf_path))
            
            if metadata.get('ocr_performed'):
                if logger:
                    logger.info(f"OCR ì„±ê³µ: {pdf_path.name} - {metadata.get('ocr_text_length', 0)}ì ì¶”ì¶œ")
                return text
            else:
                if logger:
                    logger.warning(f"OCR ì‹¤íŒ¨: {pdf_path.name}")
                return ""
                
        except ImportError:
            if logger:
                logger.warning("OCR ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€ - pytesseract ë˜ëŠ” Tesseract ë¯¸ì„¤ì¹˜")
            return ""
        except Exception as e:
            if logger:
                logger.error(f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {pdf_path.name} - {e}")
            return ""
    
    def _extract_full_pdf_content(self, pdf_path: Path) -> dict:
        """PDF ì „ì²´ ë‚´ìš© ì¶”ì¶œ ë° êµ¬ì¡°í™”"""
        try:
            import PyPDF2
            
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                
                # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)
                full_text = ""
                max_pages = min(len(reader.pages), 50)  # ìµœëŒ€ 50í˜ì´ì§€ë¡œ ì¦ê°€
                for page_num in range(max_pages):
                    page = reader.pages[page_num]
                    page_text = page.extract_text()
                    
                    # ê·¸ë£¹ì›¨ì–´ URL ì œê±°
                    page_text = re.sub(r'gw\.channela[^\n]+', '', page_text)
                    page_text = re.sub(r'\d+\.\s*\d+\.\s*\d+\.\s*ì˜¤[ì „í›„]\s*\d+:\d+\s*ì¥ë¹„êµ¬ë§¤.*?ê¸°ì•ˆì„œ', '', page_text)
                    
                    full_text += f"\n[í˜ì´ì§€ {page_num+1}]\n{page_text}\n"
                    
                    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì¤‘ë‹¨
                    if len(full_text) > 100000:  # 100K ë¬¸ìë¡œ ì¦ê°€
                        break
                
                # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ OCR ì‹œë„
                if not full_text.strip():
                    logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨, OCR ì‹œë„: {pdf_path.name}")
                    full_text = self._try_ocr_extraction(pdf_path)
                    if not full_text:
                        return None
                
                # êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
                info = {}
                
                # ê¸°ì•ˆì„œ ë¬¸ì„œì¸ì§€ í™•ì¸
                is_gian = self._is_gian_document(full_text)
                
                if is_gian:
                    # ê¸°ì•ˆì„œ ì „ìš© íŒŒì‹±
                    patterns = {
                        'ê¸°ì•ˆì': r'ê¸°ì•ˆì\s+([ê°€-í£]+)',
                        'ì œëª©': r'ì œëª©\s+(.+?)(?:\n|$)',
                        'ê¸°ì•ˆì¼ì': r'ê¸°ì•ˆì¼ì\s+(\d{4}-\d{2}-\d{2})',
                        'ê¸°ì•ˆë¶€ì„œ': r'ê¸°ì•ˆë¶€ì„œ\s+([^\s]+)',
                        'ë³´ì¡´ê¸°ê°„': r'ë³´ì¡´ê¸°ê°„\s+([^\s\n]+)',
                        'ì‹œí–‰ì¼ì': r'ì‹œí–‰ì¼ì\s+(\d{4}-\d{2}-\d{2})',
                        'ë¬¸ì„œë²ˆí˜¸': r'ë¬¸ì„œë²ˆí˜¸\s+([^\s]+)',
                    }
                else:
                    # ì¼ë°˜ ë¬¸ì„œ íŒŒì‹±
                    patterns = {
                        'ê¸°ì•ˆì': r'ê¸°ì•ˆì\s+([^\s\n]+)',
                        'ì œëª©': r'ì œëª©\s+(.+?)(?:\n|$)',
                        'ê¸°ì•ˆì¼ì': r'ê¸°ì•ˆì¼ì\s+(\d{4}-\d{2}-\d{2})',
                        'ê¸°ì•ˆë¶€ì„œ': r'ê¸°ì•ˆë¶€ì„œ\s+([^\s\n]+)',
                        'ë³´ì¡´ê¸°ê°„': r'ë³´ì¡´ê¸°ê°„\s+([^\s\n]+)'
                    }
                
                for key, pattern in patterns.items():
                    match = re.search(pattern, full_text)
                    if match:
                        info[key] = match.group(1).strip()
                
                # ê°œìš” ì¶”ì¶œ (ê¸°ì•ˆì„œ í˜•ì‹ì— ë§ì¶¤)
                if is_gian:
                    # ê¸°ì•ˆì„œëŠ” 1. ê°œìš” í˜•ì‹
                    match = re.search(r'1\.\s*ê°œìš”\s*\n(.+?)(?:\n2\.|$)', full_text, re.DOTALL)
                    if match:
                        overview = match.group(1).strip()
                        # ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì •ë¦¬
                        overview = re.sub(r'\n(?![-â€¢Â·])', ' ', overview)
                        info['ê°œìš”'] = overview[:800]  # 800ì ì œí•œ
                    
                    # 2. ë‚´ìš© ì¶”ì¶œ
                    match = re.search(r'2\.\s*ë‚´ìš©\s*\n(.+?)(?:\n3\.|$)', full_text, re.DOTALL)
                    if match:
                        content = match.group(1).strip()
                        content = re.sub(r'\n(?![-â€¢Â·\d\)])', ' ', content)
                        info['ë‚´ìš©'] = content[:1000]  # 1000ì ì œí•œ
                    
                    # 3. ê²€í†  ì˜ê²¬ ì¶”ì¶œ
                    match = re.search(r'3\.\s*ê²€í† \s*ì˜ê²¬\s*\n(.+?)(?:\n4\.|$)', full_text, re.DOTALL)
                    if match:
                        review = match.group(1).strip()
                        review = re.sub(r'\n(?![-â€¢Â·])', ' ', review)
                        info['ê²€í† ì˜ê²¬'] = review[:2500]  # 800 -> 2500
                else:
                    # ì¼ë°˜ ë¬¸ì„œëŠ” ê¸°ì¡´ ë°©ì‹
                    if 'ê°œìš”' in full_text:
                        match = re.search(r'ê°œìš”\s*\n(.+?)(?:\n\d+\.|$)', full_text, re.DOTALL)
                        if match:
                            info['ê°œìš”'] = match.group(1).strip()
                
                # ê¸ˆì•¡ ì •ë³´ (íŒ¨í„´ ê°œì„  - ë” ì •í™•í•œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì¶œ)
                # ì‹¤ì œ ê¸ˆì•¡ì´ ë‚˜ì˜¤ëŠ” ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ íŒ¨í„´
                amount_patterns = [
                    # ì´ì•¡, í•©ê³„ ë“± ëª…ì‹œì  ê¸ˆì•¡ (ì› ì—†ì´ë„ ë§¤ì¹­)
                    r'(?:ì´ì•¡|í•©ê³„|ì´\s*ì•¡|ì´\s*ë¹„ìš©|ê²€í† \s*ë¹„ìš©|ê²€í† \s*ê¸ˆì•¡)[:\s]*(\d{1,3}(?:,\d{3})*)\s*(?:ì›)?',
                    r'(?:ê¸ˆì•¡|ë¹„ìš©|ê°€ê²©)[:\s]*(\d{1,3}(?:,\d{3})*)\s*(?:ì›)?',
                    # VAT ê´€ë ¨ ê¸ˆì•¡
                    r'(\d{1,3}(?:,\d{3})*)\s*ì›\s*\(?(?:VAT|ë¶€ê°€ì„¸)',
                    r'(\d{1,3}(?:,\d{3})*)\s*\(?(?:VAT|ë¶€ê°€ì„¸)',  # ì› ì—†ì´
                    # ë°œìƒ ë¹„ìš© íŒ¨í„´
                    r'ë°œìƒ\s*ë¹„ìš©\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                    # ë°±ë§Œì›, ì²œë§Œì› ë‹¨ìœ„ í‘œê¸°
                    r'(\d{1,3}(?:,\d{3})*)\s*(?:ë°±ë§Œ|ì²œë§Œ|ì–µ)\s*ì›',
                    # ì¼ë°˜ì ì¸ ê¸ˆì•¡ íŒ¨í„´ (ì²œë§Œì› ì´ìƒë§Œ)
                    r'(\d{1,3}(?:,\d{3})*)\s*ì›',
                ]
                
                amounts = []
                amount_contexts = []  # ê¸ˆì•¡ê³¼ ì»¨í…ìŠ¤íŠ¸ í•¨ê»˜ ì €ì¥
                
                for pattern in amount_patterns:
                    matches = re.finditer(pattern, full_text, re.IGNORECASE)
                    for match in matches:
                        amount = match.group(1)
                        # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸ˆì•¡ ì£¼ë³€ í…ìŠ¤íŠ¸)
                        start = max(0, match.start() - 50)
                        end = min(len(full_text), match.end() + 50)
                        context = full_text[start:end].strip()
                        
                        # ê¸ˆì•¡ì´ ìœ ì˜ë¯¸í•œì§€ ê²€ì¦ (10ë§Œì› ì´ìƒ)
                        try:
                            amount_int = int(amount.replace(',', ''))
                            if amount_int >= 100000:  # 10ë§Œì› ì´ìƒë§Œ
                                amounts.append(amount)
                                amount_contexts.append({
                                    'amount': amount,
                                    'context': context
                                })
                        except Exception as e:
                            pass
                
                # ê°€ì¥ í° ê¸ˆì•¡ì„ ì£¼ìš” ê¸ˆì•¡ìœ¼ë¡œ íŒë‹¨
                if amounts:
                    # ê¸ˆì•¡ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬
                    sorted_amounts = sorted(amounts, 
                                          key=lambda x: int(x.replace(',', '')), 
                                          reverse=True)
                    # ìƒìœ„ 3ê°œë§Œ ì €ì¥
                    info['ê¸ˆì•¡ì •ë³´'] = sorted_amounts[:3]
                    info['ê¸ˆì•¡ì»¨í…ìŠ¤íŠ¸'] = amount_contexts[:3]
                
                # ì—…ì²´ ì •ë³´
                if 'ì—…ì²´' in full_text or 'ë²¤ë”' in full_text:
                    vendor_match = re.search(r'(?:ì—…ì²´|ë²¤ë”)[:\s]*([^\n]+)', full_text)
                    if vendor_match:
                        info['ì—…ì²´'] = vendor_match.group(1).strip()
                
                # ê²€í†  ì˜ê²¬ ì¶”ì¶œ (ìƒˆë¡œ ì¶”ê°€)
                if 'ê²€í†  ì˜ê²¬' in full_text:
                    match = re.search(r'ê²€í†  ì˜ê²¬(.+?)(?:3\.|$)', full_text, re.DOTALL)
                    if match:
                        opinion = match.group(1).strip()
                        opinion = re.sub(r'\n+', ' ', opinion)
                        opinion = re.sub(r'\s+', ' ', opinion)
                        info['ê²€í† ì˜ê²¬'] = opinion[:2500]  # 500 -> 2500ìë¡œ ì¦ê°€
                
                # ì„¸ë¶€ í•­ëª© ì¶”ì¶œ (í…Œì´ë¸” ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°ë¥¼ ìœ„í•´)
                info['ì„¸ë¶€í•­ëª©'] = []
                
                # ì¤‘ê³„ì°¨ ë³´ìˆ˜ í•­ëª© ì°¾ê¸°
                if 'ì¤‘ê³„ì°¨' in full_text and 'ë³´ìˆ˜' in full_text:
                    # ë„ì–´, ë°œì „ê¸°, ë°°í„°ë¦¬ ë“± í•­ëª© ì°¾ê¸°
                    repair_items = []
                    if 'ë„ì–´' in full_text:
                        repair_items.append({'í•­ëª©': 'ë„ì–´', 'ë‚´ìš©': 'ë¶€ì‹ ë° ì‘ë™ ë¶ˆëŸ‰'})
                    if 'ë°œì „ê¸°' in full_text:
                        repair_items.append({'í•­ëª©': 'ë°œì „ê¸°', 'ë‚´ìš©': 'ëˆ„ìˆ˜ ë° ì ê²€ í•„ìš”'})
                    if 'ë ˆë²¨ì­' in full_text:
                        repair_items.append({'í•­ëª©': 'ë ˆë²¨ì­', 'ë‚´ìš©': 'ì‘ë™ ë¶ˆëŸ‰'})
                    if 'ë°°í„°ë¦¬' in full_text:
                        repair_items.append({'í•­ëª©': 'ë°°í„°ë¦¬', 'ë‚´ìš©': 'êµì²´ í•„ìš”'})
                    if repair_items:
                        info['ì„¸ë¶€í•­ëª©'] = repair_items
                
                # ì§€ë¯¸ì§‘ Control Box ìˆ˜ë¦¬ í•­ëª© ì°¾ê¸°
                if 'Control Box' in full_text or 'ì§€ë¯¸ì§‘' in full_text:
                    repair_items = []
                    if 'Tilt ìŠ¤í”¼ë“œ' in full_text:
                        repair_items.append({'í•­ëª©': 'Tilt ìŠ¤í”¼ë“œë‹¨', 'ë‚´ìš©': 'ë¶€í’ˆ êµì²´'})
                    if repair_items:
                        info['ì„¸ë¶€í•­ëª©'] = repair_items
                    
                    # ì¥ì•  ë‚´ìš© ì¶”ì¶œ
                    if 'ì¥ì•  ë‚´ìš©' in full_text:
                        match = re.search(r'ì¥ì•  ë‚´ìš©(.+?)(?:\d+\)|$)', full_text, re.DOTALL)
                        if match:
                            info['ì¥ì• ë‚´ìš©'] = match.group(1).strip()[:300]
                
                # ë¹„ìš© ë‚´ì—­ ì¶”ì¶œ (ê°œì„  - DVR í¬í•¨)
                info['ë¹„ìš©ë‚´ì—­'] = {}
                
                # DVR ê´€ë ¨ ë¹„ìš© ì²´í¬
                if 'DVR' in full_text or '2,446,000' in full_text:
                    # DVR ë¹„ìš© í‘œ ì°¾ê¸°
                    cost_match = re.search(r'ê²€í†  ë¹„ìš©.*?ì´ì•¡\s*([\d,]+)', full_text, re.DOTALL)
                    if cost_match:
                        info['ë¹„ìš©ë‚´ì—­']['ì´ì•¡'] = cost_match.group(1) + 'ì›'
                    
                    # ì„¸ë¶€ í•­ëª© ì¶”ì¶œ
                    if '666,000' in full_text:
                        info['ë¹„ìš©ë‚´ì—­']['DVR'] = '666,000ì› (2EA)'
                    if '1,520,000' in full_text:
                        info['ë¹„ìš©ë‚´ì—­']['HDD'] = '1,520,000ì› (10TB x 4EA)'
                    if '260,000' in full_text:
                        info['ë¹„ìš©ë‚´ì—­']['ì»¨ë²„í„°'] = '260,000ì› (2EA)'
                    if '2,446,000' in full_text:
                        info['ë¹„ìš©ë‚´ì—­']['ì´ì•¡'] = '2,446,000ì›'
                
                # ê¸°ì¡´ ê¸ˆì•¡ ì²˜ë¦¬
                for amt in info.get('ê¸ˆì•¡ì •ë³´', []):
                    try:
                        amt_int = int(amt.replace(',', ''))
                        if amt_int >= 100000:  # 10ë§Œì› ì´ìƒìœ¼ë¡œ ë‚®ì¶¤
                            if '26,660,000' in amt:
                                info['ë¹„ìš©ë‚´ì—­']['ë‚´ì™¸ê´€ë³´ìˆ˜'] = amt + 'ì›'
                            if '7,680,000' in amt:
                                info['ë¹„ìš©ë‚´ì—­']['ë°©ì†¡ì‹œìŠ¤í…œ'] = amt + 'ì›'
                            if '34,340,000' in amt:
                                info['ë¹„ìš©ë‚´ì—­']['ì´í•©ê³„'] = amt + 'ì› (VATë³„ë„)'
                            if '200,000' in amt:
                                info['ë¹„ìš©ë‚´ì—­']['ìˆ˜ë¦¬ë¹„ìš©'] = amt + 'ì› (VATë³„ë„)'
                            if not info['ë¹„ìš©ë‚´ì—­']:  # ì²« ë²ˆì§¸ ê¸ˆì•¡
                                info['ë¹„ìš©ë‚´ì—­']['ê¸ˆì•¡'] = amt + 'ì›'
                    except (ValueError, AttributeError):
                        pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
                
                info['ì „ì²´í…ìŠ¤íŠ¸'] = full_text[:8000]  # LLM ì»¨í…ìŠ¤íŠ¸ ì œí•œ
                
                return info
                
        except Exception as e:
            return {'error': str(e)}
    
    def _prepare_formatted_data(self, pdf_info: Dict, pdf_path: Path) -> Dict:
        """í¬ë§·í„°ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        formatted_info = {
            'ì œëª©': pdf_info.get('ì œëª©', pdf_path.stem),
            'ê¸°ì•ˆì': pdf_info.get('ê¸°ì•ˆì', ''),
            'ê¸°ì•ˆì¼ì': pdf_info.get('ê¸°ì•ˆì¼ì', ''),
            'ê¸°ì•ˆë¶€ì„œ': pdf_info.get('ê¸°ì•ˆë¶€ì„œ', '')
        }
        
        # í•µì‹¬ ìš”ì•½ ì¶”ì¶œ (3ì¤„)
        if self.formatter and pdf_info.get('ì „ì²´í…ìŠ¤íŠ¸'):
            key_points = self.formatter.extract_key_points(pdf_info['ì „ì²´í…ìŠ¤íŠ¸'])
            formatted_info['í•µì‹¬ìš”ì•½'] = key_points
        
        # ìƒì„¸ ë‚´ìš© êµ¬ì¡°í™”
        detail_content = []
        
        # ê°œìš”ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if pdf_info.get('ê°œìš”'):
            detail_content.append({
                'í•­ëª©': 'ê°œìš”',
                'ë‚´ìš©': pdf_info['ê°œìš”'][:200]
            })
        
        # ì¥ì• /ìˆ˜ë¦¬ ë‚´ìš©
        if any(k in pdf_info for k in ['ì¥ì• ë‚´ìš©', 'ìˆ˜ë¦¬ë‚´ìš©', 'êµ¬ë§¤ì‚¬ìœ ']):
            for key in ['ì¥ì• ë‚´ìš©', 'ìˆ˜ë¦¬ë‚´ìš©', 'êµ¬ë§¤ì‚¬ìœ ']:
                if pdf_info.get(key):
                    detail_content.append({
                        'í•­ëª©': key,
                        'ë‚´ìš©': pdf_info[key][:200]
                    })
        
        formatted_info['ìƒì„¸ë‚´ìš©'] = detail_content
        
        # ë¹„ìš© ì •ë³´
        if pdf_info.get('ë¹„ìš©ë‚´ì—­'):
            formatted_info['ë¹„ìš©ì •ë³´'] = pdf_info['ë¹„ìš©ë‚´ì—­']
        
        # ê²€í†  ì˜ê²¬
        opinions = []
        if pdf_info.get('ê²€í† ê²°ê³¼'):
            opinions.append(pdf_info['ê²€í† ê²°ê³¼'])
        if pdf_info.get('ì¶”ì²œì‚¬í•­'):
            opinions.append(pdf_info['ì¶”ì²œì‚¬í•­'])
        if opinions:
            formatted_info['ê²€í† ì˜ê²¬'] = opinions
        
        # ê´€ë ¨ ì •ë³´
        related = []
        if pdf_info.get('ì—…ì²´'):
            related.append(f"ì—…ì²´: {pdf_info['ì—…ì²´']}")
        if pdf_info.get('ë„ì…ì—°ë„'):
            related.append(f"ë„ì…: {pdf_info['ë„ì…ì—°ë„']}ë…„")
        if related:
            formatted_info['ê´€ë ¨ì •ë³´'] = related
        
        return formatted_info
    
    def _analyze_user_intent(self, query: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„ì„"""
        query_lower = query.lower()
        
        intent = {
            'type': 'general',
            'needs_detail': False,
            'wants_comparison': False,
            'wants_recommendation': False,
            'is_urgent': False,
            'tone': 'informative',
            'context_keywords': [],
            'response_style': 'conversational'
        }
        
        # ì˜ë„ íŒŒì•…
        if any(word in query_lower for word in ['ìš”ì•½', 'ì •ë¦¬', 'ì•Œë ¤', 'ì„¤ëª…']):
            intent['type'] = 'summary'
            intent['needs_detail'] = True
        if any(word in query_lower for word in ['ë¹„êµ', 'ì°¨ì´', 'ì–´ë–¤ê²Œ ë‚˜ì€', 'ë­ê°€ ì¢‹']):
            intent['type'] = 'comparison'
            intent['wants_comparison'] = True
            intent['wants_recommendation'] = True
        if any(word in query_lower for word in ['ì¶”ì²œ', 'ê¶Œì¥', 'ì–´ë–»ê²Œ', 'ë°©ë²•']):
            intent['type'] = 'recommendation'
            intent['wants_recommendation'] = True
        if any(word in query_lower for word in ['ê¸´ê¸‰', 'ë¹¨ë¦¬', 'ê¸‰í•´', 'ë°”ë¡œ']):
            intent['type'] = 'urgent'
            intent['is_urgent'] = True
            intent['tone'] = 'direct'
        if any(word in query_lower for word in ['ì–¼ë§ˆ', 'ë¹„ìš©', 'ê°€ê²©', 'ê¸ˆì•¡']):
            intent['type'] = 'cost'
            intent['needs_detail'] = True
        if any(word in query_lower for word in ['ë¬¸ì œ', 'ê³ ì¥', 'ìˆ˜ë¦¬', 'ì¥ì• ']):
            intent['type'] = 'problem'
            intent['wants_recommendation'] = True
        
        # ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        important_words = ['DVR', 'ì¤‘ê³„ì°¨', 'ì¹´ë©”ë¼', 'ì‚¼ê°ëŒ€', 'ë°©ì†¡', 'ì¥ë¹„', 'êµ¬ë§¤', 'ìˆ˜ë¦¬', 'êµì²´', 'ì—…ê·¸ë ˆì´ë“œ']
        intent['context_keywords'] = [word for word in important_words if word.lower() in query_lower]
        
        # ì‘ë‹µ ìŠ¤íƒ€ì¼ ê²°ì •
        if '?' in query:
            intent['response_style'] = 'explanatory'
        if any(word in query_lower for word in ['í•´ì¤˜', 'ë¶€íƒ', 'ì¢€']):
            intent['response_style'] = 'helpful'
        
        return intent
    
    def _generate_conversational_response(self, context: str, query: str, intent: Dict[str, Any],
                                         pdf_info: Dict[str, Any] = None) -> str:
        """ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”í˜• ì‘ë‹µ ìƒì„±"""

        # LLMModuleì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        if self.llm_module:
            try:
                response = self.llm_module.generate_conversational_response(
                    context=context[:3000],  # ì»¨í…ìŠ¤íŠ¸ ì œí•œ
                    query=query,
                    intent=intent
                )

                # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë‚˜ ì¶”ì²œ ì‚¬í•­ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€
                if intent.get('wants_recommendation') and 'ì¶”ì²œ' not in response:
                    response += "\n\nì°¸ê³ ë¡œ, ì´ì™€ ê´€ë ¨í•´ì„œ ì¶”ê°€ë¡œ ê²€í† í•˜ì‹œë©´ ì¢‹ì„ ì‚¬í•­ë“¤ë„ ìˆìŠµë‹ˆë‹¤. í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."

                return response
            except Exception as e:
                logger.error(f"LLMModule ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
                return self._generate_fallback_response(context, query, intent)

        # LLMModuleì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        # LLMì—ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”í˜• ì‘ë‹µì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ì¤‘ìš” ì›ì¹™:
1. ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”í•˜ë“¯ ë‹µë³€í•˜ì„¸ìš”
2. í…œí”Œë¦¿ì´ë‚˜ ì •í˜•í™”ëœ í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ í•„ìš”ë¡œ í•˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
4. ì¶”ê°€ë¡œ ë„ì›€ì´ ë  ë§Œí•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì œì•ˆí•˜ì„¸ìš”
5. ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”"""
        
        # ì˜ë„ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
        if intent['type'] == 'summary':
            user_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ê°„ë‹¨íˆ ì„¤ëª…
- ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ì„¤ëª…
- ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì„ ì •ë³´ ì œì•ˆ
- ë”±ë”±í•œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹Œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì—°ê²°"""
        
        if intent['type'] == 'comparison':
            user_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹„êµ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- ë¹„êµ ëŒ€ìƒë“¤ì˜ ì£¼ìš” ì°¨ì´ì ì„ ë¨¼ì € ì„¤ëª…
- ê°ê°ì˜ ì¥ë‹¨ì ì„ ì‹¤ìš©ì  ê´€ì ì—ì„œ ì„¤ëª…
- ìƒí™©ì— ë”°ë¥¸ ì¶”ì²œ ì œê³µ
- "ì´ëŸ° ê²½ìš°ì—” Aê°€ ì¢‹ê³ , ì €ëŸ° ê²½ìš°ì—” Bê°€ ë‚«ë‹¤"ëŠ” ì‹ìœ¼ë¡œ ì„¤ëª…"""
        
        if intent['type'] == 'recommendation':
            user_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ìš©ì ì¸ ì¶”ì²œì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- ì¶”ì²œ ì‚¬í•­ì„ ëª…í™•í•˜ê²Œ ì œì‹œ
- ì¶”ì²œ ì´ìœ ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…
- ê³ ë ¤ì‚¬í•­ì´ë‚˜ ì£¼ì˜ì ë„ í•¨ê»˜ ì–¸ê¸‰
- ëŒ€ì•ˆì´ ìˆë‹¤ë©´ ê°„ë‹¨íˆ ì†Œê°œ"""
        
        if intent['type'] == 'cost':
            user_prompt = f"""ë‹¤ìŒ ì •ë³´ì—ì„œ ë¹„ìš© ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- êµ¬ì²´ì ì¸ ê¸ˆì•¡ì„ ë¨¼ì € ì œì‹œ
- ë¹„ìš© êµ¬ì„±ì´ë‚˜ ë‚´ì—­ ì„¤ëª…
- ë¹„ìš© ëŒ€ë¹„ ê°€ì¹˜ë‚˜ íš¨ê³¼ ì–¸ê¸‰
- ì˜ˆì‚° ê´€ë ¨ ì¡°ì–¸ì´ ìˆë‹¤ë©´ ì¶”ê°€"""
        
        if intent['is_urgent']:
            user_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹ ë¥´ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- í•µì‹¬ ì •ë³´ë¥¼ ë°”ë¡œ ì œì‹œ
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜ ì‚¬í•­ ì œê³µ
- ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­ ëª…ì‹œ"""
        
        else:
            user_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€
- ê´€ë ¨ëœ ì¶”ê°€ ì •ë³´ ì œê³µ
- ê¶ê¸ˆí•  ë§Œí•œ ë‹¤ë¥¸ ì‚¬í•­ ì–¸ê¸‰
- ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í†¤ ìœ ì§€"""
        
        # LLM í˜¸ì¶œ
        if self.llm:
            try:
                # ëŒ€í™”í˜• ì‘ë‹µ ìƒì„±
                context_chunks = [{
                    'content': context,
                    'source': pdf_info.get('ì œëª©', 'document') if pdf_info else 'document',
                    'score': 1.0
                }]
                
                # ëŒ€í™”í˜• ì‘ë‹µ ë©”ì„œë“œ í˜¸ì¶œ
                response = self.llm.generate_conversational_response(query, context_chunks)
                
                if response and hasattr(response, 'answer'):
                    answer = response.answer
                else:
                    answer = str(response)
                
                # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë‚˜ ì¶”ì²œ ì‚¬í•­ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€
                if intent['wants_recommendation'] and 'ì¶”ì²œ' not in answer:
                    answer += "\n\nì°¸ê³ ë¡œ, ì´ì™€ ê´€ë ¨í•´ì„œ ì¶”ê°€ë¡œ ê²€í† í•˜ì‹œë©´ ì¢‹ì„ ì‚¬í•­ë“¤ë„ ìˆìŠµë‹ˆë‹¤. í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
                
                return answer
                
            except Exception as e:
                print(f"LLM ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
                # í´ë°±: ê¸°ë³¸ ì‘ë‹µ ìƒì„±
                return self._generate_fallback_response(context, query, intent)
        
        return self._generate_fallback_response(context, query, intent)
    
    def _generate_fallback_response(self, context: str, query: str, intent: Dict[str, Any]) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ í´ë°± ì‘ë‹µ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        lines = context.split('\n')
        key_info = []
        
        for line in lines:
            if any(keyword in line for keyword in ['ê¸ˆì•¡', 'ë¹„ìš©', 'ì›', 'ì œëª©', 'ê¸°ì•ˆ', 'ë‚ ì§œ']):
                key_info.append(line.strip())
        
        response = f"ë¬¸ì„œë¥¼ í™•ì¸í•œ ê²°ê³¼, "
        
        if intent['type'] == 'summary':
            response += f"ìš”ì²­í•˜ì‹  ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. "
            if key_info:
                response += ' '.join(key_info[:3])
        if intent['type'] == 'cost':
            cost_info = [line for line in key_info if 'ì›' in line or 'ê¸ˆì•¡' in line]
            if cost_info:
                response += f"ë¹„ìš© ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤. {cost_info[0]}"
        else:
            if key_info:
                response += f"ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. {key_info[0]}"
        
        return response
    

    def _prepare_llm_context(self, content, max_length=2000):
        """LLM ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ í—¬í¼"""
        if not content:
            return ""

        # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½
        if len(content) > max_length:
            # ì²˜ìŒê³¼ ë ë¶€ë¶„ ì¶”ì¶œ
            start = content[:max_length//2]
            end = content[-(max_length//2):]
            content = f"{start}\n\n... [ì¤‘ëµ] ...\n\n{end}"

        return content

    def _extract_key_sentences(self, content, num_sentences=5):
        """í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ í—¬í¼"""
        if not content:
            return []

        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]

        if len(sentences) <= num_sentences:
            return sentences

        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚°
        important_keywords = ['ê²°ì •', 'ìŠ¹ì¸', 'êµ¬ë§¤', 'ê³„ì•½', 'ì˜ˆì‚°', 'ì§„í–‰', 'ì™„ë£Œ']
        scored_sentences = []

        for sentence in sentences:
            score = sum(1 for keyword in important_keywords if keyword in sentence)
            scored_sentences.append((sentence, score))

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        return [s[0] for s in scored_sentences[:num_sentences]]

    def _format_llm_response(self, raw_response):
        """LLM ì‘ë‹µ í¬ë§·íŒ… í—¬í¼"""
        if not raw_response:
            return "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"

        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        formatted = re.sub(r'\n{3,}', '\n\n', raw_response)
        formatted = formatted.strip()

        # ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ ê°œì„ 
        formatted = re.sub(r'^#', '##', formatted, flags=re.MULTILINE)

        return formatted

    def _generate_llm_summary(self, pdf_path: Path, query: str) -> str:
        """LLMì„ ì‚¬ìš©í•œ ìƒì„¸ ìš”ì•½ - ëŒ€í™”í˜• ìŠ¤íƒ€ì¼"""
        logger.info("LLM ìš”ì•½ ìƒì„± ì‹œì‘")
        # ì‚¬ìš©ì ì˜ë„ ë¶„ì„
        intent = self._analyze_user_intent(query)

        # ë³€ìˆ˜ ì´ˆê¸°í™” (ìŠ¤ì½”í”„ ë¬¸ì œ ë°©ì§€)
        basic_summary = ""
        summary = []

        # PDF íŒŒì¼ì¸ ê²½ìš° ë¨¼ì € êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ ì‹œë„
        if pdf_path.suffix.lower() == '.pdf':
            pdf_info = self._extract_full_pdf_content(pdf_path)
            
            # ëŒ€í™”í˜• ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            context_parts = []

            # ìš”ì•½ì´ë‚˜ ë‚´ìš© ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°
            if pdf_info and 'error' not in pdf_info:
                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ
                if 'ì œëª©' in pdf_info:
                    context_parts.append(f"ì œëª©: {pdf_info['ì œëª©']}")
                if 'ê¸°ì•ˆì' in pdf_info:
                    context_parts.append(f"ê¸°ì•ˆì: {pdf_info['ê¸°ì•ˆì']}")
                if 'ê¸°ì•ˆì¼ì' in pdf_info:
                    context_parts.append(f"ì‘ì„±ì¼: {pdf_info['ê¸°ì•ˆì¼ì']}")
                if 'ê¸°ì•ˆë¶€ì„œ' in pdf_info:
                    context_parts.append(f"ë‹´ë‹¹ë¶€ì„œ: {pdf_info['ê¸°ì•ˆë¶€ì„œ']}")
                
                # ê°œìš”
                if 'ê°œìš”' in pdf_info:
                    overview = pdf_info['ê°œìš”'].replace('\n', ' ').strip()
                    if len(overview) > 300:
                        overview = overview[:300] + "..."
                    context_parts.append(f"\nê°œìš”: {overview}")
                
                # ì¥ì•  ë‚´ìš©
                if 'ì¥ì• ë‚´ìš©' in pdf_info and pdf_info['ì¥ì• ë‚´ìš©']:
                    ì¥ì• _text = pdf_info['ì¥ì• ë‚´ìš©'].replace('\n', ' ').strip()
                    if len(ì¥ì• _text) > 300:
                        ì¥ì• _text = ì¥ì• _text[:300] + "..."
                    context_parts.append(f"\nì¥ì•  ë‚´ìš©: {ì¥ì• _text}")
                
                # ì„¸ë¶€ í•­ëª© (ìƒˆë¡œ ì¶”ê°€)
                if 'ì„¸ë¶€í•­ëª©' in pdf_info and pdf_info['ì„¸ë¶€í•­ëª©']:
                    summary.append(f"\n **ì„¸ë¶€ ì¥ì• /ìˆ˜ë¦¬ ë‚´ì—­**")
                    
                    # ì¤‘ê³„ì°¨ ë‚´ì™¸ê´€
                    ì¤‘ê³„ì°¨_items = [item for item in pdf_info['ì„¸ë¶€í•­ëª©'] if 'ë„ì–´' in item.get('í•­ëª©', '') or 'ë°œì „ê¸°' in item.get('í•­ëª©', '')]
                    if ì¤‘ê³„ì°¨_items:
                        summary.append("\n**[ì¤‘ê³„ì°¨ ë‚´ì™¸ê´€]**")
                        for item in ì¤‘ê³„ì°¨_items:
                            summary.append(f"â€¢ {item['í•­ëª©']}: {item['ë‚´ìš©']}")
                    
                    # ë°©ì†¡ ì‹œìŠ¤í…œ
                    ë°©ì†¡_items = [item for item in pdf_info['ì„¸ë¶€í•­ëª©'] if 'ë¹„ë””ì˜¤' in item.get('í•­ëª©', '') or 'ì˜¤ë””ì˜¤' in item.get('í•­ëª©', '')]
                    if ë°©ì†¡_items:
                        summary.append("\n**[ë°©ì†¡ ì‹œìŠ¤í…œ]**")
                        for item in ë°©ì†¡_items:
                            summary.append(f"â€¢ {item['í•­ëª©']}: {item['ë‚´ìš©']}")
                    
                    # ì§€ë¯¸ì§‘ ë“± ê¸°íƒ€ í•­ëª©
                    ê¸°íƒ€_items = [item for item in pdf_info['ì„¸ë¶€í•­ëª©'] if 'Tilt' in item.get('í•­ëª©', '') or 'Control' in item.get('í•­ëª©', '')]
                    if ê¸°íƒ€_items:
                        for item in ê¸°íƒ€_items:
                            summary.append(f"â€¢ {item['í•­ëª©']}: {item['ë‚´ìš©']}")
                
                # ë¹„ìš© ë‚´ì—­ (ê°œì„ )
                if 'ë¹„ìš©ë‚´ì—­' in pdf_info and pdf_info['ë¹„ìš©ë‚´ì—­']:
                    summary.append(f"\n **ë¹„ìš© ë‚´ì—­**")
                    if 'ë‚´ì™¸ê´€ë³´ìˆ˜' in pdf_info['ë¹„ìš©ë‚´ì—­']:
                        summary.append(f"â€¢ ì¤‘ê³„ì°¨ ë‚´ì™¸ê´€ ë³´ìˆ˜: {pdf_info['ë¹„ìš©ë‚´ì—­']['ë‚´ì™¸ê´€ë³´ìˆ˜']}")
                    if 'ë°©ì†¡ì‹œìŠ¤í…œ' in pdf_info['ë¹„ìš©ë‚´ì—­']:
                        summary.append(f"â€¢ ë°©ì†¡ ì‹œìŠ¤í…œ ë³´ìˆ˜: {pdf_info['ë¹„ìš©ë‚´ì—­']['ë°©ì†¡ì‹œìŠ¤í…œ']}")
                    if 'ì´í•©ê³„' in pdf_info['ë¹„ìš©ë‚´ì—­']:
                        summary.append(f"â€¢ **ì´ ë¹„ìš©: {pdf_info['ë¹„ìš©ë‚´ì—­']['ì´í•©ê³„']}**")
                # ê¸ˆì•¡ ì •ë³´ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
                if 'ê¸ˆì•¡ì •ë³´' in pdf_info and pdf_info['ê¸ˆì•¡ì •ë³´']:
                    summary.append(f"\n **ì£¼ìš” ê¸ˆì•¡**")
                    # ê¸ˆì•¡ ì •ë ¬ ë° ìƒìœ„ í‘œì‹œ
                    amounts = []
                    for amt in pdf_info['ê¸ˆì•¡ì •ë³´']:
                        try:
                            amt_int = int(amt.replace(',', ''))
                            if amt_int > 1000000:  # 100ë§Œì› ì´ìƒë§Œ
                                amounts.append((amt, amt_int))
                        except (ValueError, AttributeError):
                            pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
                    amounts.sort(key=lambda x: x[1], reverse=True)
                    for amt, _ in amounts[:3]:
                        summary.append(f"â€¢ {amt}ì›")
                
                # ê²€í†  ì˜ê²¬ (ê°œì„ ëœ ì •ë¦¬)
                if 'ê²€í† ì˜ê²¬' in pdf_info and pdf_info['ê²€í† ì˜ê²¬']:
                    summary.append(f"\n **ê²€í†  ì˜ê²¬**")
                    opinion = pdf_info['ê²€í† ì˜ê²¬']
                    
                    # DVR ê´€ë ¨ ê²€í† ì¸ ê²½ìš°
                    if 'DVR' in opinion or ('1ì•ˆ' in opinion and '2ì•ˆ' in opinion):
                        # 1ì•ˆ ì¶”ì¶œ ë° ì •ë¦¬
                        if '1ì•ˆ' in opinion:
                            ì•ˆ1_text = re.search(r'1ì•ˆ[^2]*(?=2ì•ˆ|$)', opinion, re.DOTALL)
                            if ì•ˆ1_text:
                                ì•ˆ1_clean = re.sub(r'[\d]+\.\s*[\d]+\.\s*[\d]+.*?(?=\n)', '', ì•ˆ1_text.group(0))
                                ì•ˆ1_clean = re.sub(r'\[í˜ì´ì§€ \d+\]', '', ì•ˆ1_clean)
                                ì•ˆ1_clean = ' '.join(ì•ˆ1_clean.split())
                                # HD-SDI í™•ì¸ ë˜ëŠ” 1ì•ˆ ê´€ë ¨ ë‚´ìš©ì´ ìˆìœ¼ë©´ í‘œì‹œ
                                if 'HD-SDI' in ì•ˆ1_clean or 'HDê¸‰' in ì•ˆ1_clean or 'í™”ì§ˆ í–¥ìƒ' in ì•ˆ1_clean or '1ì•ˆ' in ì•ˆ1_clean:
                                    summary.append("\n** 1ì•ˆ: HD-SDI ì…ë ¥ ëª¨ë¸**")
                                    summary.append("â€¢ í™”ì§ˆ í–¥ìƒìœ¼ë¡œ ì˜ìƒ ê²€ìˆ˜ ìš©ì´")
                                    summary.append("â€¢ HDê¸‰ ë…¹í™”, ë‹¤ì–‘í•œ ì…ë ¥ ì§€ì›")
                                    summary.append("â€¢ ì¶”ê°€ ë¹„ìš© ë°œìƒ (ì»¨ë²„í„° ë“±)")
                        
                        # 2ì•ˆ ì¶”ì¶œ ë° ì •ë¦¬
                        if '2ì•ˆ' in opinion:
                            ì•ˆ2_text = re.search(r'2ì•ˆ[^ì¢…í•©]*(?=ì¢…í•©|$)', opinion, re.DOTALL)
                            if ì•ˆ2_text:
                                ì•ˆ2_clean = re.sub(r'[\d]+\.\s*[\d]+\.\s*[\d]+.*?(?=\n)', '', ì•ˆ2_text.group(0))
                                ì•ˆ2_clean = re.sub(r'\[í˜ì´ì§€ \d+\]', '', ì•ˆ2_clean)
                                if 'CVBS' in ì•ˆ2_clean or 'ê¸°ì¡´' in ì•ˆ2_clean:
                                    summary.append("\n** 2ì•ˆ: ê¸°ì¡´ ë™ì¼ ëª¨ë¸**")
                                    summary.append("â€¢ í˜„ì¬ ì‹œìŠ¤í…œê³¼ í˜¸í™˜ì„± ë†’ìŒ")
                                    summary.append("â€¢ ë‚®ì€ ë¹„ìš©, ì„¤ì¹˜ ìš©ì´")
                                    summary.append("â€¢ SDê¸‰ í™”ì§ˆë¡œ ê°œì„  íš¨ê³¼ ì—†ìŒ")
                        
                        # ì¢…í•© ì˜ê²¬
                        if 'ì¢…í•©' in opinion or 'ê²°ë¡ ' in opinion:
                            summary.append("\n** ìµœì¢… ì¶”ì²œ**")
                            if '1ì•ˆ' in opinion and ('ìœ ë¦¬' in opinion or 'ì ì ˆ' in opinion or 'ì¶”ì²œ' in opinion):
                                summary.append("â€¢ **1ì•ˆ ì±„íƒ ê¶Œì¥** - ì¥ê¸°ì  ìš´ì˜ ë° í™”ì§ˆ ê°œì„  í•„ìš”")
                            if '2ì•ˆ' in opinion and ('ìœ ë¦¬' in opinion or 'ì ì ˆ' in opinion):
                                summary.append("â€¢ **2ì•ˆ ì±„íƒ ê¶Œì¥** - ë¹„ìš© ì ˆê° ìš°ì„ ")
                    
                    # ì¤‘ê³„ì°¨ ê´€ë ¨ì¸ ê²½ìš°
                    if 'ì¤‘ê³„ì°¨ ì„ëŒ€' in opinion:
                        summary.append("â€¢ ì¤‘ê³„ì°¨ ì„ëŒ€: ê¸‰ì‘ìŠ¤ëŸ° íŠ¹ë³´ ìƒí™© ì‹œ ëŒ€ì‘ ì–´ë ¤ì›€")
                        if 'ì¤‘ê³„ì°¨ ì œì‘' in opinion:
                            summary.append("â€¢ ì‹ ê·œ ì œì‘: 25-30ì–µì› ê³¼ë„í•œ ë¹„ìš©, 4K ì†¡ì¶œ ì¼ì • ë¶ˆí™•ì‹¤")
                        if 'ë³´ìˆ˜í•˜ì—¬' in opinion:
                            summary.append("â€¢ **ê²°ë¡ : í˜„ ì¤‘ê³„ì°¨ ë³´ìˆ˜ë¡œ ì‹œìŠ¤í…œ ì•ˆì •ì„± í™•ë³´ê°€ ì ì ˆ**")
                    else:
                        # ì¼ë°˜ ê²€í†  ì˜ê²¬ (ê¸°ì¡´ ë°©ì‹)
                        if len(opinion) > 500:
                            opinion = opinion[:500] + "..."
                        summary.append(opinion)
                
                # ì£¼ìš” ë‚´ìš© (ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ) - ê¸°ì¡´ ì½”ë“œ ìœ ì§€
                if 'ì „ì²´í…ìŠ¤íŠ¸' in pdf_info:
                    full_text = pdf_info['ì „ì²´í…ìŠ¤íŠ¸']
                    
                    # ë„ì… ì—°ë„ ì°¾ê¸°
                    if 'ë„ì…' in query or 'ì–¸ì œ' in query:
                        ë„ì…_match = re.search(r'ë„ì…\s*ë…„ë„\s*[:ï¼š]?\s*(\d{4})', full_text)
                        if ë„ì…_match:
                            summary.append(f"\n **ë„ì… ì—°ë„**: {ë„ì…_match.group(1)}ë…„")
                
                # ì—…ì²´ ì •ë³´
                if 'ì—…ì²´' in pdf_info:
                    summary.append(f"\n **ê´€ë ¨ ì—…ì²´**: {pdf_info['ì—…ì²´']}")
            
            # ê¸°ë³¸ ì •ë³´ë¥¼ ë³´ê´€ (if ë¸”ë¡ ë°–ìœ¼ë¡œ ì´ë™)
            basic_summary = '\n'.join(summary) if summary else ""
            
            # ëŒ€í™”í˜• ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš° ë°”ë¡œ ì²˜ë¦¬
            if 'ìš”ì•½' in query or 'ë‚´ìš©' in query or 'ì„¤ëª…' in query:
                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                context_text = ""
                if pdf_info and 'error' not in pdf_info:
                    # ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                    context_parts = []
                    if 'ì œëª©' in pdf_info:
                        context_parts.append(f"ë¬¸ì„œ ì œëª©: {pdf_info['ì œëª©']}")
                    if 'ê¸°ì•ˆì' in pdf_info:
                        context_parts.append(f"ì‘ì„±ì: {pdf_info['ê¸°ì•ˆì']}")
                    if 'ê¸°ì•ˆì¼ì' in pdf_info:
                        context_parts.append(f"ì‘ì„±ì¼: {pdf_info['ê¸°ì•ˆì¼ì']}")
                    if 'ê°œìš”' in pdf_info:
                        context_parts.append(f"\nê°œìš”: {pdf_info['ê°œìš”']}")
                    if 'ê¸ˆì•¡ì •ë³´' in pdf_info:
                        amounts = pdf_info['ê¸ˆì•¡ì •ë³´']
                        if amounts:
                            # ê°€ì¥ í° ê¸ˆì•¡ë§Œ ì£¼ìš” ê¸ˆì•¡ìœ¼ë¡œ í‘œì‹œ
                            main_amount = amounts[0] if amounts else None
                            if main_amount:
                                # ê¸ˆì•¡ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í•¨ê»˜ í‘œì‹œ
                                if 'ê¸ˆì•¡ì»¨í…ìŠ¤íŠ¸' in pdf_info and pdf_info['ê¸ˆì•¡ì»¨í…ìŠ¤íŠ¸']:
                                    context_info = pdf_info['ê¸ˆì•¡ì»¨í…ìŠ¤íŠ¸'][0].get('context', '')
                                    # ì´ì•¡, í•©ê³„ ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ëª…ì‹œ
                                    if 'ì´ì•¡' in context_info or 'í•©ê³„' in context_info:
                                        context_parts.append(f"\nì´ ê¸ˆì•¡: {main_amount}ì›")
                                    else:
                                        context_parts.append(f"\nê¸ˆì•¡: {main_amount}ì›")
                                else:
                                    context_parts.append(f"\nê¸ˆì•¡: {main_amount}ì›")
                    if 'ê²€í† ì˜ê²¬' in pdf_info:
                        context_parts.append(f"\nê²€í†  ì˜ê²¬: {pdf_info['ê²€í† ì˜ê²¬'][:500]}")
                    
                    context_text = "\n".join(context_parts)
                
                # LLM ë¡œë“œ
                if self.llm is None:
                    if not LLMSingleton.is_loaded():
                        print(" LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
                    self.llm = LLMSingleton.get_instance(model_path=self.model_path)
                
                # ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± - ì „ì²´ í…ìŠ¤íŠ¸ í¬í•¨
                if self.llm and context_text:
                    # ì „ì²´ í…ìŠ¤íŠ¸ë„ í¬í•¨ (ì¤‘ìš” ì •ë³´ ëˆ„ë½ ë°©ì§€)
                    if 'ì „ì²´í…ìŠ¤íŠ¸' in pdf_info and pdf_info['ì „ì²´í…ìŠ¤íŠ¸']:
                        full_context = f"{context_text}\n\n[ì „ì²´ ë¬¸ì„œ ë‚´ìš©]\n{pdf_info['ì „ì²´í…ìŠ¤íŠ¸'][:3000]}"
                    else:
                        full_context = context_text
                    
                    context_chunks = [{
                        'content': full_context,
                        'source': pdf_path.name,
                        'score': 1.0
                    }]
                    
                    response = self.llm.generate_conversational_response(query, context_chunks)
                    
                    if response and hasattr(response, 'answer'):
                        answer = response.answer
                    else:
                        answer = str(response)
                    
                    # ì¶œì²˜ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€
                    if pdf_path.name not in answer:
                        answer += f"\n\n(ì°¸ê³ : {pdf_path.name})"
                    
                    return answer
            
            # ëŒ€í™”í˜•ì´ ì•„ë‹Œ ê²½ìš° ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        
        # LLM ë¡œë“œ (í•„ìš”ì‹œ) - ì‹±ê¸€í†¤ ì‚¬ìš©
        if self.llm is None:
            if not LLMSingleton.is_loaded():
                print(" LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.llm = LLMSingleton.get_instance(model_path=self.model_path)
        
        # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì½ê¸°
        try:
            text = ""
            
            # TXT íŒŒì¼ì¸ ê²½ìš°
            if pdf_path.suffix.lower() == '.txt':
                with open(pdf_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            # PDF íŒŒì¼ì¸ ê²½ìš°
            else:
                with pdfplumber.open(pdf_path) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                
                # pdfplumber ì‹¤íŒ¨ì‹œ OCR ì‹œë„
                if not text:
                    try:
                        from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
                        ocr = EnhancedOCRProcessor()
                        text, _ = ocr.extract_text_with_ocr(str(pdf_path))
                    except PDFExtractionException as e:
                        pass
            
            if not text:
                return " ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            
            # ê°„ë‹¨í•˜ê³  ëª…í™•í•œ í”„ë¡¬í”„íŠ¸
            # ê¸°ìˆ ê´€ë¦¬íŒ€ ì‹¤ë¬´ ìµœì í™” í”„ë¡¬í”„íŠ¸
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ì¦ê°€ (5000 -> 15000)
            max_text_length = 15000  # 15,000ìë¡œ ì¦ê°€
            
            # ê¸°ë³¸ ì •ë³´ê°€ ì´ë¯¸ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ì¡°ì •
            if 'basic_summary' in locals():
                # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸
                prompt = self._get_detail_only_prompt(query, text[:max_text_length], pdf_path.name)
            else:
                prompt = self._get_optimized_prompt(query, text[:max_text_length], pdf_path.name)
            
            # LLM í˜¸ì¶œ - ëŒ€í™”í˜• ì‘ë‹µ ìš°ì„ 
            context_chunks = [{'content': text[:max_text_length], 'metadata': {'source': pdf_path.name}, 'score': 1.0}]
            
            # ìš”ì•½/ë‚´ìš© ìš”ì²­ì¸ ê²½ìš° ëŒ€í™”í˜• ì‘ë‹µ
            if 'ìš”ì•½' in query or 'ë‚´ìš©' in query or 'ì„¤ëª…' in query:
                response = self.llm.generate_conversational_response(query, context_chunks)
            else:
                # ê¸°ì¡´ ë°©ì‹
                response = self.llm.generate_response(prompt, context_chunks)
            
            answer = response.answer if hasattr(response, 'answer') else str(response)
            
            # ëŒ€í™”í˜• ì‘ë‹µì¸ ê²½ìš° ì¶œì²˜ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€
            if 'ìš”ì•½' in query or 'ë‚´ìš©' in query or 'ì„¤ëª…' in query:
                # ì¶œì²˜ê°€ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì¶”ê°€
                if pdf_path.name not in answer:
                    answer += f"\n\n(ì°¸ê³  ë¬¸ì„œ: {pdf_path.name})"
                return answer
            else:
                # ê¸°ì¡´ ë°©ì‹ (í…œí”Œë¦¿ í˜•ì‹)
                if 'basic_summary' in locals() and basic_summary:
                    combined_answer = f"{basic_summary}\n\n **ìƒì„¸ ë‚´ìš©**\n{answer}"
                    return f"{combined_answer}\n\n ì¶œì²˜: {pdf_path.name}"
                else:
                    return f"{answer}\n\n ì¶œì²˜: {pdf_path.name}"
            
        except Exception as e:
            return f" ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _collect_statistics_data(self, query: str) -> Dict:
        """í†µê³„ ë°ì´í„° ìˆ˜ì§‘ ë° êµ¬ì¡°í™”"""
        stats_data = {
            'title': '',
            'headers': [],
            'table_data': [],
            'ì´ê³„': '',
            'ë¶„ì„': {},
            'ì¶”ì²œ': []
        }
        
        # ì—°ë„ ì¶”ì¶œ
        year_match = re.search(r'(20\d{2})', query)
        target_year = year_match.group(1) if year_match else None
        
        if "ì—°ë„ë³„" in query and "êµ¬ë§¤" in query:
            stats_data['title'] = "ì—°ë„ë³„ êµ¬ë§¤ í˜„í™©"
            stats_data['headers'] = ['ì—°ë„', 'ê±´ìˆ˜', 'ì´ ê¸ˆì•¡', 'ì£¼ìš” í’ˆëª©']
            
            yearly_data = {}
            for filename, metadata in self.metadata_cache.items():
                if 'êµ¬ë§¤' in filename or 'êµ¬ì…' in filename:
                    year = metadata['year']
                    if year not in yearly_data:
                        yearly_data[year] = {'count': 0, 'total': 0, 'items': []}
                    
                    yearly_data[year]['count'] += 1
                    # ê¸ˆì•¡ ì¶”ì¶œ ë¡œì§
                    pdf_path = self.docs_dir / filename
                    info = self._extract_pdf_info(pdf_path)
                    if info.get('ê¸ˆì•¡'):
                        amount = self._parse_amount(info['ê¸ˆì•¡'])
                        yearly_data[year]['total'] += amount
                    if info.get('í’ˆëª©'):
                        yearly_data[year]['items'].append(info['í’ˆëª©'])
            
            # í…Œì´ë¸” ë°ì´í„° ìƒì„±
            total_amount = 0
            for year in sorted(yearly_data.keys()):
                data = yearly_data[year]
                total_amount += data['total']
                items_str = ', '.join(data['items'][:2])  # ìƒìœ„ 2ê°œë§Œ
                if len(data['items']) > 2:
                    items_str += f" ì™¸ {len(data['items'])-2}ê±´"
                
                stats_data['table_data'].append([
                    year,
                    f"{data['count']}ê±´",
                    f"{data['total']:,}ì›",
                    items_str
                ])
            
            stats_data['ì´ê³„'] = f"{total_amount:,}ì›"
            stats_data['ë¶„ì„']['í‰ê·  ì—°ê°„ êµ¬ë§¤ì•¡'] = f"{total_amount // len(yearly_data):,}ì›"
            stats_data['ì¶”ì²œ'].append("êµ¬ë§¤ ì§‘ì¤‘ ì‹œê¸°ë¥¼ íŒŒì•…í•˜ì—¬ ì˜ˆì‚° ê³„íš ìˆ˜ë¦½")
            
        if target_year:
            # íŠ¹ì • ì—°ë„ ì „ì²´ í†µê³„
            stats_data['title'] = f"{target_year}ë…„ ì „ì²´ í˜„í™©"
            stats_data['headers'] = ['êµ¬ë¶„', 'ê±´ìˆ˜', 'ì´ ê¸ˆì•¡', 'ë¹„ìœ¨']
            
            categories = {'êµ¬ë§¤': 0, 'ìˆ˜ë¦¬': 0, 'íê¸°': 0, 'ê¸°íƒ€': 0}
            amounts = {'êµ¬ë§¤': 0, 'ìˆ˜ë¦¬': 0, 'íê¸°': 0, 'ê¸°íƒ€': 0}
            
            for filename, metadata in self.metadata_cache.items():
                if metadata['year'] == target_year:
                    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                    if 'êµ¬ë§¤' in filename or 'êµ¬ì…' in filename:
                        cat = 'êµ¬ë§¤'
                    if 'ìˆ˜ë¦¬' in filename or 'ë³´ìˆ˜' in filename:
                        cat = 'ìˆ˜ë¦¬'
                    if 'íê¸°' in filename:
                        cat = 'íê¸°'
                    else:
                        cat = 'ê¸°íƒ€'
                    
                    categories[cat] += 1
                    
                    # ê¸ˆì•¡ ì¶”ì¶œ
                    pdf_path = self.docs_dir / filename
                    info = self._extract_pdf_info(pdf_path)
                    if info.get('ê¸ˆì•¡'):
                        amounts[cat] += self._parse_amount(info['ê¸ˆì•¡'])
            
            # ì´ê³„ ê³„ì‚°
            total_docs = sum(categories.values())
            total_amount = sum(amounts.values())
            
            # í…Œì´ë¸” ë°ì´í„° ìƒì„±
            for cat in ['êµ¬ë§¤', 'ìˆ˜ë¦¬', 'íê¸°', 'ê¸°íƒ€']:
                if categories[cat] > 0:
                    ratio = (categories[cat] / total_docs * 100) if total_docs > 0 else 0
                    stats_data['table_data'].append([
                        cat,
                        f"{categories[cat]}ê±´",
                        f"{amounts[cat]:,}ì›",
                        f"{ratio:.1f}%"
                    ])
            
            stats_data['ì´ê³„'] = f"ë¬¸ì„œ {total_docs}ê±´, ê¸ˆì•¡ {total_amount:,}ì›"
            stats_data['ë¶„ì„']['êµ¬ë§¤ ë¹„ì¤‘'] = f"{amounts['êµ¬ë§¤']/total_amount*100:.1f}%" if total_amount > 0 else "0%"
            stats_data['ë¶„ì„']['ìˆ˜ë¦¬ ë¹„ì¤‘'] = f"{amounts['ìˆ˜ë¦¬']/total_amount*100:.1f}%" if total_amount > 0 else "0%"
        
        return stats_data
    
    def _generate_statistics_report(self, query: str) -> str:
        """ì „ì²´ ë¬¸ì„œì— ëŒ€í•œ í†µê³„ ë³´ê³ ì„œ ìƒì„± - êµ¬ì¡°í™”ëœ í¬ë§·"""
        try:
            # formatter ì‚¬ìš© ê°€ëŠ¥ ì‹œ êµ¬ì¡°í™”ëœ í¬ë§· ì ìš©
            if self.formatter:
                stats_data = self._collect_statistics_data(query)
                return self.formatter.format_statistics_response(stats_data, query)
            
            # ê¸°ì¡´ ë°©ì‹ (formatter ì—†ì„ ë•Œ)
            # í†µê³„ íƒ€ì… íŒŒì•…
            if "ì—°ë„ë³„" in query and "êµ¬ë§¤" in query:
                return self._generate_yearly_purchase_report(query)
            if "ê¸°ì•ˆìë³„" in query:
                return self._generate_drafter_report(query)
            if "ì›”ë³„" in query and "ìˆ˜ë¦¬" in query:
                return self._generate_monthly_repair_report(query)
            
            # ê¸°ë³¸: íŠ¹ì • ì—°ë„ ì „ì²´ í†µê³„
            year_match = re.search(r'(20\d{2})', query)
            target_year = year_match.group(1) if year_match else None
            
            # í†µê³„ ë°ì´í„° ìˆ˜ì§‘
            stats = {
                'êµ¬ë§¤': [],
                'ìˆ˜ë¦¬': [],
                'íê¸°': [],
                'ì†Œëª¨í’ˆ': [],
                'ê¸°íƒ€': []
            }
            
            drafters = {}  # ê¸°ì•ˆìë³„ í†µê³„
            monthly = {}  # ì›”ë³„ í†µê³„
            total_amount = 0
            doc_count = 0
            
            for filename, metadata in self.metadata_cache.items():
                # ì—°ë„ í•„í„°ë§
                if target_year and metadata['year'] != target_year:
                    continue
                
                doc_count += 1
                pdf_path = self.docs_dir / filename
                info = self._extract_pdf_info(pdf_path)
                
                # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                category = 'ê¸°íƒ€'
                if 'êµ¬ë§¤' in filename:
                    category = 'êµ¬ë§¤'
                if 'ìˆ˜ë¦¬' in filename or 'ë³´ìˆ˜' in filename:
                    category = 'ìˆ˜ë¦¬'
                if 'íê¸°' in filename:
                    category = 'íê¸°'
                if 'ì†Œëª¨í’ˆ' in filename:
                    category = 'ì†Œëª¨í’ˆ'
                
                # í†µê³„ì— ì¶”ê°€
                doc_info = {
                    'filename': filename,
                    'date': info.get('ë‚ ì§œ', ''),
                    'drafter': info.get('ê¸°ì•ˆì', 'ë¯¸ìƒ'),
                    'amount': info.get('ê¸ˆì•¡', ''),
                    'title': info.get('ì œëª©', filename.replace('.pdf', ''))
                }
                
                stats[category].append(doc_info)
                
                # ê¸°ì•ˆìë³„ í†µê³„
                drafter = doc_info['drafter']
                if drafter not in drafters:
                    drafters[drafter] = 0
                drafters[drafter] += 1
                
                # ì›”ë³„ í†µê³„ (ë‚ ì§œê°€ ìˆëŠ” ê²½ìš°)
                if doc_info['date']:
                    month_match = re.search(r'-(\d{2})-', doc_info['date'])
                    if month_match:
                        month = month_match.group(1)
                        if month not in monthly:
                            monthly[month] = 0
                        monthly[month] += 1
                
                # ê¸ˆì•¡ í•©ê³„
                if doc_info['amount']:
                    amount_num = re.search(r'(\d+(?:,\d+)*)', doc_info['amount'])
                    if amount_num:
                        try:
                            total_amount += int(amount_num.group(1).replace(',', ''))
                        except (ValueError, AttributeError):
                            pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
            
            # ë³´ê³ ì„œ ìƒì„±
            report = []
            
            if target_year:
                report.append(f" {target_year}ë…„ ê¸°ìˆ ê´€ë¦¬íŒ€ ë¬¸ì„œ í†µê³„ ë³´ê³ ì„œ")
            else:
                report.append(" ì „ì²´ ê¸°ê°„ ê¸°ìˆ ê´€ë¦¬íŒ€ ë¬¸ì„œ í†µê³„ ë³´ê³ ì„œ")
            
            report.append("=" * 50)
            report.append("")
            
            # ì „ì²´ ìš”ì•½
            report.append("###  ì „ì²´ ìš”ì•½")
            report.append(f"â€¢ ì´ ë¬¸ì„œ ìˆ˜: {doc_count}ê°œ")
            if total_amount > 0:
                report.append(f"â€¢ ì´ ê¸ˆì•¡: {total_amount:,}ì›")
            report.append("")
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            report.append("###  ì¹´í…Œê³ ë¦¬ë³„ í˜„í™©")
            report.append("")
            
            for category, docs in stats.items():
                if docs:
                    count = len(docs)
                    ratio = (count / doc_count * 100) if doc_count > 0 else 0
                    report.append(f"â€¢ **{category}**: {count}ê±´ ({ratio:.1f}%)")
            report.append("")
            
            # ê¸°ì•ˆìë³„ í†µê³„
            if drafters:
                report.append("###  ê¸°ì•ˆìë³„ í˜„í™©")
                report.append("")
                
                for drafter, count in sorted(drafters.items(), key=lambda x: x[1], reverse=True):
                    if drafter and drafter != 'ë¯¸ìƒ':
                        report.append(f"â€¢ **{drafter}**: {count}ê±´")
                
                report.append("")
            
            # ì›”ë³„ í†µê³„ (ì—°ë„ ì§€ì •ì‹œ)
            if target_year and monthly:
                report.append("###  ì›”ë³„ í˜„í™©")
                report.append("")
                
                for month in sorted(monthly.keys()):
                    count = monthly[month]
                    report.append(f"â€¢ **{int(month)}ì›”**: {count}ê±´")
                
                report.append("")
            
            # ì£¼ìš” ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            report.append("###  ì£¼ìš” ë¬¸ì„œ ëª©ë¡")
            for category, docs in stats.items():
                if docs:
                    report.append(f"\nâ–¶ {category} ({len(docs)}ê±´)")
                    for doc in docs[:3]:  # ê° ì¹´í…Œê³ ë¦¬ë³„ ìµœëŒ€ 3ê°œ
                        date = doc['date'][:10] if doc['date'] else 'ë‚ ì§œì—†ìŒ'
                        drafter = doc['drafter'] if doc['drafter'] != 'ë¯¸ìƒ' else ''
                        amount = f" - {doc['amount']}" if doc['amount'] else ""
                        
                        title = doc['title'][:30] + "..." if len(doc['title']) > 30 else doc['title']
                        report.append(f"  â€¢ [{date}] {title}")
                        if drafter or amount:
                            report.append(f"    {drafter}{amount}")
            
            return "\n".join(report)
            
        except Exception as e:
            return f" í†µê³„ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _generate_yearly_purchase_report(self, query: str) -> str:
        """ì—°ë„ë³„ êµ¬ë§¤ í˜„í™© ë³´ê³ ì„œ"""
        try:
            yearly_stats = {}
            
            for filename, metadata in self.metadata_cache.items():
                if 'êµ¬ë§¤' not in filename:
                    continue
                    
                year = metadata['year']
                if year not in yearly_stats:
                    yearly_stats[year] = {'count': 0, 'amount': 0, 'items': []}
                
                pdf_path = self.docs_dir / filename
                info = self._extract_pdf_info(pdf_path)
                
                yearly_stats[year]['count'] += 1
                yearly_stats[year]['items'].append({
                    'filename': filename,
                    'date': info.get('ë‚ ì§œ', ''),
                    'drafter': info.get('ê¸°ì•ˆì', ''),
                    'amount': info.get('ê¸ˆì•¡', ''),
                    'title': info.get('ì œëª©', filename.replace('.pdf', ''))
                })
                
                # ê¸ˆì•¡ í•©ê³„
                if info.get('ê¸ˆì•¡'):
                    amount_num = re.search(r'(\d+(?:,\d+)*)', info['ê¸ˆì•¡'])
                    if amount_num:
                        try:
                            yearly_stats[year]['amount'] += int(amount_num.group(1).replace(',', ''))
                        except (ValueError, AttributeError):
                            pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
            
            # ë³´ê³ ì„œ ìƒì„±
            report = []
            report.append(" ì—°ë„ë³„ êµ¬ë§¤ í˜„í™© ë³´ê³ ì„œ (2021-2025)")
            report.append("=" * 50)
            report.append("")
            
            report.append("###  ì—°ë„ë³„ êµ¬ë§¤ í†µê³„")
            report.append("")
            
            total_count = 0
            total_amount = 0
            
            for year in sorted(yearly_stats.keys()):
                stats = yearly_stats[year]
                total_count += stats['count']
                total_amount += stats['amount']
                
                amount_str = f"{stats['amount']:,}ì›" if stats['amount'] > 0 else "ê¸ˆì•¡ë¯¸ìƒ"
                report.append(f"â€¢ **{year}ë…„**: {stats['count']}ê±´ - {amount_str}")
            
            report.append("")
            report.append(f"** ì´ê³„: {total_count}ê±´ - {total_amount:,}ì›**")
            report.append("")
            
            # ì—°ë„ë³„ ìƒì„¸ ë‚´ì—­
            report.append("###  ì—°ë„ë³„ ìƒì„¸ ë‚´ì—­")
            for year in sorted(yearly_stats.keys()):
                stats = yearly_stats[year]
                if stats['items']:
                    report.append(f"\nâ–¶ {year}ë…„ ({stats['count']}ê±´)")
                    for item in stats['items']:
                        date = item['date'][:10] if item['date'] else 'ë‚ ì§œì—†ìŒ'
                        title = item['title'][:35] + "..." if len(item['title']) > 35 else item['title']
                        amount = f" - {item['amount']}" if item['amount'] else ""
                        report.append(f"  â€¢ [{date}] {title}{amount}")
            
            return "\n".join(report)
            
        except Exception as e:
            return f" ì—°ë„ë³„ êµ¬ë§¤ í˜„í™© ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _generate_drafter_report(self, query: str) -> str:
        """ê¸°ì•ˆìë³„ ë¬¸ì„œ í˜„í™© ë³´ê³ ì„œ"""
        try:
            drafter_stats = {}
            
            for filename, metadata in self.metadata_cache.items():
                pdf_path = self.docs_dir / filename
                info = self._extract_pdf_info(pdf_path)
                
                drafter = info.get('ê¸°ì•ˆì', 'ë¯¸ìƒ')
                if drafter not in drafter_stats:
                    drafter_stats[drafter] = {
                        'count': 0,
                        'categories': {'êµ¬ë§¤': 0, 'ìˆ˜ë¦¬': 0, 'íê¸°': 0, 'ê¸°íƒ€': 0},
                        'years': {},
                        'items': []
                    }
                
                # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                category = 'ê¸°íƒ€'
                if 'êµ¬ë§¤' in filename:
                    category = 'êµ¬ë§¤'
                if 'ìˆ˜ë¦¬' in filename or 'ë³´ìˆ˜' in filename:
                    category = 'ìˆ˜ë¦¬'
                if 'íê¸°' in filename:
                    category = 'íê¸°'
                
                drafter_stats[drafter]['count'] += 1
                drafter_stats[drafter]['categories'][category] += 1
                
                # ì—°ë„ë³„ ì§‘ê³„
                year = metadata['year']
                if year not in drafter_stats[drafter]['years']:
                    drafter_stats[drafter]['years'][year] = 0
                drafter_stats[drafter]['years'][year] += 1
                
                drafter_stats[drafter]['items'].append({
                    'filename': filename,
                    'date': info.get('ë‚ ì§œ', ''),
                    'category': category,
                    'title': info.get('ì œëª©', filename.replace('.pdf', ''))
                })
            
            # ë³´ê³ ì„œ ìƒì„±
            report = []
            report.append(" ê¸°ì•ˆìë³„ ë¬¸ì„œ ì‘ì„± í˜„í™©")
            report.append("=" * 50)
            report.append("")
            
            report.append("###  ê¸°ì•ˆìë³„ ì „ì²´ í†µê³„")
            report.append("")
            
            for drafter in sorted(drafter_stats.keys()):
                if drafter and drafter != 'ë¯¸ìƒ':
                    stats = drafter_stats[drafter]
                    cat_str = f"êµ¬ë§¤ {stats['categories']['êµ¬ë§¤']}ê±´, ìˆ˜ë¦¬ {stats['categories']['ìˆ˜ë¦¬']}ê±´"
                    if stats['categories']['íê¸°'] > 0:
                        cat_str += f", íê¸° {stats['categories']['íê¸°']}ê±´"
                    if stats['categories']['ê¸°íƒ€'] > 0:
                        cat_str += f", ê¸°íƒ€ {stats['categories']['ê¸°íƒ€']}ê±´"
                    report.append(f"â€¢ **{drafter}**: ì´ {stats['count']}ê±´ ({cat_str})")
            
            report.append("")
            
            # ê¸°ì•ˆìë³„ ì—°ë„ ë¶„í¬
            report.append("###  ê¸°ì•ˆìë³„ ì—°ë„ ë¶„í¬")
            for drafter in sorted(drafter_stats.keys()):
                if drafter and drafter != 'ë¯¸ìƒ':
                    stats = drafter_stats[drafter]
                    year_str = ", ".join([f"{year}ë…„({count}ê±´)" for year, count in sorted(stats['years'].items())])
                    report.append(f"â€¢ {drafter}: {year_str}")
            report.append("")
            
            # ê¸°ì•ˆìë³„ ëª¨ë“  ë¬¸ì„œ
            report.append("###  ê¸°ì•ˆìë³„ ë‹´ë‹¹ ë¬¸ì„œ (ì „ì²´)")
            report.append("* ì‹¤ë¬´ ë‹´ë‹¹ìì—ê²Œ ì§ì ‘ ë¬¸ì˜ ê°€ëŠ¥*")
            report.append("")
            
            for drafter in sorted(drafter_stats.keys()):
                if drafter and drafter != 'ë¯¸ìƒ':
                    stats = drafter_stats[drafter]
                    report.append(f"####  **{drafter}** ({stats['count']}ê±´)")
                    
                    # ì—°ë„ë³„ë¡œ ê·¸ë£¹í™”
                    docs_by_year = {}
                    for item in sorted(stats['items'], key=lambda x: x['date'], reverse=True):
                        year = item['date'][:4] if item['date'] else 'ì—°ë„ì—†ìŒ'
                        if year not in docs_by_year:
                            docs_by_year[year] = []
                        docs_by_year[year].append(item)
                    
                    # ì—°ë„ë³„ë¡œ í‘œì‹œ
                    for year in sorted(docs_by_year.keys(), reverse=True):
                        report.append(f"\n**{year}ë…„:**")
                        for item in docs_by_year[year]:
                            date = item['date'][5:10] if item['date'] and len(item['date']) >= 10 else 'ë‚ ì§œì—†ìŒ'
                            cat_emoji = {
                                'êµ¬ë§¤': '',
                                'ìˆ˜ë¦¬': '', 
                                'íê¸°': 'ï¸',
                                'ê¸°íƒ€': ''
                            }.get(item['category'], '')
                            
                            # ì „ì²´ ì œëª© í‘œì‹œ (ì¶•ì•½ ì—†ì´)
                            title = item['title']
                            report.append(f"  â€¢ [{date}] {cat_emoji} {title}")
                    report.append("")
            
            return "\n".join(report)
            
        except Exception as e:
            return f" ê¸°ì•ˆìë³„ í˜„í™© ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _generate_monthly_repair_report(self, query: str) -> str:
        """ì›”ë³„ ìˆ˜ë¦¬ ë‚´ì—­ ë³´ê³ ì„œ"""
        try:
            monthly_stats = {}
            total_amount = 0
            
            for filename, metadata in self.metadata_cache.items():
                if 'ìˆ˜ë¦¬' not in filename and 'ë³´ìˆ˜' not in filename:
                    continue
                
                pdf_path = self.docs_dir / filename
                info = self._extract_pdf_info(pdf_path)
                
                # ì›” ì¶”ì¶œ
                date = info.get('ë‚ ì§œ', '')
                if date:
                    month_match = re.search(r'-(\d{2})-', date)
                    if month_match:
                        month = int(month_match.group(1))
                        year = metadata['year']
                        month_key = f"{year}-{month:02d}"
                        
                        if month_key not in monthly_stats:
                            monthly_stats[month_key] = {'count': 0, 'amount': 0, 'items': []}
                        
                        monthly_stats[month_key]['count'] += 1
                        monthly_stats[month_key]['items'].append({
                            'filename': filename,
                            'date': date,
                            'drafter': info.get('ê¸°ì•ˆì', ''),
                            'amount': info.get('ê¸ˆì•¡', ''),
                            'title': info.get('ì œëª©', filename.replace('.pdf', ''))
                        })
                        
                        # ê¸ˆì•¡ í•©ê³„
                        if info.get('ê¸ˆì•¡'):
                            amount_num = re.search(r'(\d+(?:,\d+)*)', info['ê¸ˆì•¡'])
                            if amount_num:
                                try:
                                    amount = int(amount_num.group(1).replace(',', ''))
                                    monthly_stats[month_key]['amount'] += amount
                                    total_amount += amount
                                except (ValueError, KeyError):
                                    pass  # ê¸ˆì•¡ ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
            
            # ë³´ê³ ì„œ ìƒì„±
            report = []
            report.append(" ì›”ë³„ ìˆ˜ë¦¬ ë‚´ì—­ ë° ë¹„ìš© ë¶„ì„")
            report.append("=" * 50)
            report.append("")
            
            report.append("###  ì „ì²´ ìš”ì•½")
            total_count = sum(stats['count'] for stats in monthly_stats.values())
            report.append(f"â€¢ ì´ ìˆ˜ë¦¬ ê±´ìˆ˜: {total_count}ê±´")
            if total_amount > 0:
                report.append(f"â€¢ ì´ ìˆ˜ë¦¬ ë¹„ìš©: {total_amount:,}ì›")
                report.append(f"â€¢ í‰ê·  ìˆ˜ë¦¬ ë¹„ìš©: {total_amount // total_count:,}ì›")
            report.append("")
            
            report.append("###  ì›”ë³„ ìˆ˜ë¦¬ í˜„í™©")
            report.append("")
            
            for month_key in sorted(monthly_stats.keys()):
                stats = monthly_stats[month_key]
                year, month = month_key.split('-')
                amount_str = f"{stats['amount']:,}ì›" if stats['amount'] > 0 else "ê¸ˆì•¡ë¯¸ìƒ"
                report.append(f"â€¢ **{year}ë…„ {int(month)}ì›”**: {stats['count']}ê±´ - {amount_str}")
            
            report.append("")
            
            # ì›”ë³„ ìƒì„¸ ë‚´ì—­
            report.append("###  ì›”ë³„ ìƒì„¸ ìˆ˜ë¦¬ ë‚´ì—­")
            for month_key in sorted(monthly_stats.keys()):
                stats = monthly_stats[month_key]
                year, month = month_key.split('-')
                report.append(f"\nâ–¶ {year}ë…„ {int(month)}ì›” ({stats['count']}ê±´)")
                
                for item in stats['items']:
                    date = item['date'][:10] if item['date'] else 'ë‚ ì§œì—†ìŒ'
                    title = item['title'][:35] + "..." if len(item['title']) > 35 else item['title']
                    amount = f" - {item['amount']}" if item['amount'] else ""
                    report.append(f"  â€¢ [{date}] {title}{amount}")
            
            return "\n".join(report)
            
        except Exception as e:
            return f" ì›”ë³„ ìˆ˜ë¦¬ ë‚´ì—­ ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _format_conditions(self, conditions: dict) -> str:
        """ê²€ìƒ‰ ì¡°ê±´ì„ ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…"""
        formatted = []
        
        if 'location' in conditions:
            formatted.append(f"â€¢ ìœ„ì¹˜: {conditions['location']}")
        
        if 'manufacturer' in conditions:
            formatted.append(f"â€¢ ì œì¡°ì‚¬: {conditions['manufacturer']}")
        
        if 'year' in conditions:
            year_str = f"â€¢ ì—°ë„: {conditions['year']}ë…„"
            if conditions.get('year_range') == 'before':
                year_str += " ì´ì „"
            if conditions.get('year_range') == 'after':
                year_str += " ì´í›„"
            if conditions.get('year_range') == 'between':
                year_str = f"â€¢ ì—°ë„: {conditions.get('year_start')}ë…„ ~ {conditions.get('year_end')}ë…„"
            formatted.append(year_str)
        
        if 'price' in conditions:
            price_str = f"â€¢ ê¸ˆì•¡: {conditions['price']:,.0f}ì›"
            if conditions.get('price_range') == 'above':
                price_str += " ì´ìƒ"
            if conditions.get('price_range') == 'below':
                price_str += " ì´í•˜"
            formatted.append(price_str)
        
        if 'equipment_type' in conditions:
            formatted.append(f"â€¢ ì¥ë¹„ ìœ í˜•: {conditions['equipment_type']}")
        
        if 'model' in conditions:
            formatted.append(f"â€¢ ëª¨ë¸: {conditions['model']}")
        
        return '\n'.join(formatted) if formatted else "ì¡°ê±´ ì—†ìŒ"
    
    def _search_location_summary(self, txt_path: Path, location: str) -> str:
        """íŠ¹ì • ìœ„ì¹˜ì˜ ì¥ë¹„ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì •ë¦¬"""
        # Asset ëª¨ë“œ ì œê±°ë¨
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            equipment_by_category = {}  # {ì¹´í…Œê³ ë¦¬: [ì¥ë¹„ ì •ë³´]}
            current_item = []
            total_count = 0
            total_amount = 0  # ì´ ê¸ˆì•¡
            
            for line in lines:
                if re.match(r'^\[\d{4}\]', line.strip()):
                    if current_item:
                        item_text = '\n'.join(current_item)
                        # ìœ„ì¹˜ í™•ì¸
                        if self._check_location_in_item(item_text, location):
                            total_count += 1
                            # ì¥ë¹„ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                            equipment_name = current_item[0].split(']')[1].strip() if ']' in current_item[0] else current_item[0]
                            
                            # ì¹´í…Œê³ ë¦¬ ê²°ì •
                            category = self._determine_equipment_category(equipment_name, item_text)
                            
                            if category not in equipment_by_category:
                                equipment_by_category[category] = []
                            
                            # ì •ë³´ ì¶”ì¶œ
                            info = {
                                'name': equipment_name,
                                'model': "N/A",
                                'price': 0,
                                'quantity': 1,
                                'date': "N/A"
                            }
                            
                            for item_line in current_item:
                                # ëª¨ë¸ëª…
                                if "ëª¨ë¸:" in item_line:
                                    model_match = re.search(r'ëª¨ë¸:\s*([^|]+)', item_line)
                                    if model_match:
                                        info['model'] = model_match.group(1).strip()
                                
                                # ê¸ˆì•¡ ì •ë³´
                                if "ê¸ˆì•¡:" in item_line:
                                    amount_match = re.search(r'ê¸ˆì•¡:\s*([\d,]+)ì›', item_line)
                                    if amount_match:
                                        amount_str = amount_match.group(1).replace(',', '')
                                        try:
                                            info['price'] = int(amount_str)
                                            total_amount += info['price']
                                        except Exception as e:
                                            pass
                                
                                # ìˆ˜ëŸ‰
                                if "ìˆ˜ëŸ‰:" in item_line:
                                    qty_match = re.search(r'ìˆ˜ëŸ‰:\s*(\d+)', item_line)
                                    if qty_match:
                                        info['quantity'] = int(qty_match.group(1))
                                
                                # êµ¬ì…ì¼
                                if "êµ¬ì…ì¼:" in item_line:
                                    date_match = re.search(r'êµ¬ì…ì¼:\s*([^\s|]+)', item_line)
                                    if date_match:
                                        info['date'] = date_match.group(1).strip()
                            
                            equipment_by_category[category].append(info)
                    
                    current_item = [line]
                else:
                    if current_item:
                        current_item.append(line)
            
            # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
            if current_item:
                item_text = '\n'.join(current_item)
                if self._check_location_in_item(item_text, location):
                    total_count += 1
                    equipment_name = current_item[0].split(']')[1].strip() if ']' in current_item[0] else current_item[0]
                    category = self._determine_equipment_category(equipment_name, item_text)
                    if category not in equipment_by_category:
                        equipment_by_category[category] = []
                    
                    info = {
                        'name': equipment_name,
                        'model': "N/A",
                        'price': 0,
                        'quantity': 1,
                        'date': "N/A"
                    }
                    
                    for item_line in current_item:
                        if "ëª¨ë¸:" in item_line:
                            model_match = re.search(r'ëª¨ë¸:\s*([^|]+)', item_line)
                            if model_match:
                                info['model'] = model_match.group(1).strip()
                        if "ê¸ˆì•¡:" in item_line:
                            amount_match = re.search(r'ê¸ˆì•¡:\s*([\d,]+)ì›', item_line)
                            if amount_match:
                                amount_str = amount_match.group(1).replace(',', '')
                                try:
                                    info['price'] = int(amount_str)
                                    total_amount += info['price']
                                except Exception as e:
                                    pass
                        if "ìˆ˜ëŸ‰:" in item_line:
                            qty_match = re.search(r'ìˆ˜ëŸ‰:\s*(\d+)', item_line)
                            if qty_match:
                                info['quantity'] = int(qty_match.group(1))
                        if "êµ¬ì…ì¼:" in item_line:
                            date_match = re.search(r'êµ¬ì…ì¼:\s*([^\s|]+)', item_line)
                            if date_match:
                                info['date'] = date_match.group(1).strip()
                    
                    equipment_by_category[category].append(info)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            if equipment_by_category:
                response = f" **{location} ì¥ë¹„ í˜„í™©**\n"
                response += "=" * 70 + "\n"
                response += f" ì´ **{total_count}ê°œ** ì¥ë¹„\n"
                if total_amount > 0:
                    # ê¸ˆì•¡ í¬ë§·íŒ… (ì–µ/ì²œë§Œì› ë‹¨ìœ„)
                    if total_amount >= 100000000:  # 1ì–µ ì´ìƒ
                        amount_str = f"{total_amount/100000000:.1f}ì–µì›"
                    if total_amount >= 10000000:  # 1ì²œë§Œì› ì´ìƒ
                        amount_str = f"{total_amount/10000000:.0f}ì²œë§Œì›"
                    else:
                        amount_str = f"{total_amount:,}ì›"
                    response += f" ì´ ìì‚°ê°€ì¹˜: **{amount_str}**\n\n"
                else:
                    response += "\n"
                
                # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
                response += "###  ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ í˜„í™©\n"
                response += "-" * 50 + "\n"
                
                # ì¹´í…Œê³ ë¦¬ ì •ë ¬ (ì¥ë¹„ ìˆ˜ ë§ì€ ìˆœ)
                sorted_categories = sorted(equipment_by_category.items(), key=lambda x: len(x[1]), reverse=True)
                
                for category, items in sorted_categories:
                    # ì¹´í…Œê³ ë¦¬ë³„ ì´ì•¡ ê³„ì‚°
                    category_amount = sum(item['price'] for item in items)
                    
                    response += f"\n**{category}** ({len(items)}ê°œ"
                    if category_amount > 0:
                        if category_amount >= 100000000:
                            response += f", {category_amount/100000000:.1f}ì–µì›"
                        if category_amount >= 10000000:
                            response += f", {category_amount/10000000:.0f}ì²œë§Œì›"
                        else:
                            response += f", {category_amount:,}ì›"
                    response += ")\n"
                    
                    # ê°™ì€ ì¥ë¹„ëª…ë¼ë¦¬ ê·¸ë£¹í™”
                    equipment_summary = {}
                    for item in items:
                        key = f"{item['name']} ({item['model']})" if item['model'] != "N/A" else item['name']
                        if key not in equipment_summary:
                            equipment_summary[key] = {
                                'count': 0,
                                'total_price': 0,
                                'unit_price': item['price'] // item['quantity'] if item['quantity'] > 0 else item['price']
                            }
                        equipment_summary[key]['count'] += item['quantity']
                        equipment_summary[key]['total_price'] += item['price']
                    
                    # ê¸ˆì•¡ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                    sorted_equipment = sorted(equipment_summary.items(), 
                                           key=lambda x: x[1]['total_price'], 
                                           reverse=True)
                    
                    # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                    for i, (equip_name, equip_info) in enumerate(sorted_equipment[:5], 1):
                        line = f"  {i}. {equip_name}"
                        if equip_info['count'] > 1:
                            line += f" - {equip_info['count']}ê°œ"
                        if equip_info['total_price'] > 0:
                            line += f" ({equip_info['total_price']:,}ì›)"
                        response += line + "\n"
                    
                    if len(sorted_equipment) > 5:
                        response += f"  ... ì™¸ {len(sorted_equipment)-5}ì¢…\n"
                
                response += f"\n ì¶œì²˜: {txt_path.name}"
                # Asset ëª¨ë“œ ì œê±°ë¨ - ì§ì ‘ ì‘ë‹µ ë°˜í™˜
                return response
            else:
                return f" {location}ì—ì„œ ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            return f" ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _determine_equipment_category(self, equipment_name: str, item_text: str) -> str:
        """ì¥ë¹„ëª…ê³¼ í…ìŠ¤íŠ¸ë¡œ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
        name_lower = equipment_name.lower()
        text_lower = item_text.lower()
        
        if any(kw in name_lower or kw in text_lower for kw in ['camera', 'ì¹´ë©”ë¼', 'ccu', 'viewfinder', 'ë·°íŒŒì¸ë”']):
            return " ì¹´ë©”ë¼ ì‹œìŠ¤í…œ"
        if any(kw in name_lower or kw in text_lower for kw in ['monitor', 'ëª¨ë‹ˆí„°', 'display']):
            return "ï¸ ëª¨ë‹ˆí„°"
        if any(kw in name_lower or kw in text_lower for kw in ['audio', 'ì˜¤ë””ì˜¤', 'mixer', 'ë¯¹ì„œ', 'mic', 'ë§ˆì´í¬']):
            return "ï¸ ì˜¤ë””ì˜¤ ì¥ë¹„"
        if any(kw in name_lower or kw in text_lower for kw in ['server', 'ì„œë²„', 'storage', 'ìŠ¤í† ë¦¬ì§€']):
            return " ì„œë²„/ìŠ¤í† ë¦¬ì§€"
        if any(kw in name_lower or kw in text_lower for kw in ['switch', 'ìŠ¤ìœ„ì¹˜', 'router', 'ë¼ìš°í„°', 'matrix']):
            return " ìŠ¤ìœ„ì¹­/ë¼ìš°íŒ…"
        if any(kw in name_lower or kw in text_lower for kw in ['cable', 'ì¼€ì´ë¸”', 'connector', 'ì»¤ë„¥í„°']):
            return " ì¼€ì´ë¸”/ì»¤ë„¥í„°"
        if any(kw in name_lower or kw in text_lower for kw in ['tripod', 'íŠ¸ë¼ì´í¬ë“œ', 'pedestal', 'í˜ë°ìŠ¤íƒˆ']):
            return " ì¹´ë©”ë¼ ì§€ì›ì¥ë¹„"
        if any(kw in name_lower or kw in text_lower for kw in ['intercom', 'ì¸í„°ì»´', 'talkback']):
            return " ì¸í„°ì»´"
        if any(kw in name_lower or kw in text_lower for kw in ['converter', 'ì»¨ë²„í„°', 'encoder', 'ì¸ì½”ë”']):
            return " ì»¨ë²„í„°/ì¸ì½”ë”"
        else:
            return " ê¸°íƒ€ ì¥ë¹„"

    def _count_by_field(self, content: str, field_name: str, search_value: str) -> dict:
        """íŠ¹ì • í•„ë“œê°’ìœ¼ë¡œ ì¥ë¹„ ìˆ˜ëŸ‰ ê³„ì‚°"""
        lines = content.split('\n')
        count = 0
        items = []
        current_item = []
        is_matching = False
        
        for line in lines:
            # ìƒˆë¡œìš´ ì¥ë¹„ í•­ëª© ì‹œì‘
            if re.match(r'^\[\d{4}\]', line):
                # ì´ì „ í•­ëª©ì´ ë§¤ì¹­ë˜ì—ˆìœ¼ë©´ ì¹´ìš´íŠ¸
                if is_matching and current_item:
                    count += 1
                    if len(items) < 10:  # ìƒ˜í”Œ 10ê°œë§Œ ì €ì¥
                        items.append('\n'.join(current_item))
                
                # ìƒˆ í•­ëª© ì‹œì‘
                current_item = [line]
                is_matching = False
            if current_item:
                current_item.append(line)
                # í•„ë“œë³„ ë§¤ì¹­ í™•ì¸
                if field_name == "ë‹´ë‹¹ì" and "ë‹´ë‹¹ì:" in line:
                    if search_value in line:
                        is_matching = True
                if field_name == "ìœ„ì¹˜" and "ìœ„ì¹˜:" in line:
                    # ì •í™•í•œ ìœ„ì¹˜ ë§¤ì¹­ ë¡œì§ ì ìš©
                    location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', line)
                    if location_match:
                        actual_location = location_match.group(1).strip()
                        
                        # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™
                        if search_value == actual_location:
                            # ì™„ì „ ì¼ì¹˜
                            is_matching = True
                        if search_value == 'ë¶€ì¡°ì •ì‹¤':
                            # 'ë¶€ì¡°ì •ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*ë¶€ì¡°ì •ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                            is_matching = actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
                        if search_value == 'ìŠ¤íŠœë””ì˜¤':
                            # 'ìŠ¤íŠœë””ì˜¤'ë¡œ ê²€ìƒ‰ì‹œ '*ìŠ¤íŠœë””ì˜¤' íŒ¨í„´ë§Œ ë§¤ì¹­ 
                            is_matching = actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
                        if search_value == 'í¸ì§‘ì‹¤':
                            # 'í¸ì§‘ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*í¸ì§‘ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                            is_matching = actual_location.endswith('í¸ì§‘ì‹¤')
                        if search_value in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
                            # ì¤‘ê³„ì°¨ ê²€ìƒ‰ì‹œ Van ê´€ë ¨ ìœ„ì¹˜ ëª¨ë‘ ë§¤ì¹­
                            is_matching = 'Van' in actual_location or 'VAN' in actual_location
                        if len(search_value) > 3:
                            # 3ê¸€ì ì´ìƒì˜ êµ¬ì²´ì ì¸ ìœ„ì¹˜ëª…ì€ ë¶€ë¶„ ë§¤ì¹­ í—ˆìš©
                            is_matching = search_value in actual_location
                if field_name == "ë²¤ë”ì‚¬" and "ë²¤ë”ì‚¬:" in line:
                    if search_value in line:
                        is_matching = True
                if field_name == "ì œì¡°ì‚¬" and "ì œì¡°ì‚¬:" in line:
                    if search_value.upper() in line.upper():
                        is_matching = True
        
        # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
        if is_matching and current_item:
            count += 1
            if len(items) < 10:
                items.append('\n'.join(current_item))
        
        return {
            'count': count,
            'sample_items': items
        }
    
    

    def _extract_document_metadata(self, file_path):
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í—¬í¼"""
        metadata = {}

        try:
            # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
            filename = file_path.stem if hasattr(file_path, 'stem') else str(file_path)

            # ë‚ ì§œ ì¶”ì¶œ
            date_patterns = [
                r'(\d{4})[.\-_](\d{1,2})[.\-_](\d{1,2})',
                r'(\d{4})(\d{2})(\d{2})',
                r'(\d{2})[.\-_](\d{1,2})[.\-_](\d{1,2})'
            ]
            for pattern in date_patterns:
                match = re.search(pattern, filename)
                if match:
                    metadata['date'] = match.group(0)
                    break

            # ê¸°ì•ˆì ì¶”ì¶œ
            author_patterns = [
                r'([ê°€-í£]{2,4})([\s_\-])?ê¸°ì•ˆ',
                r'ê¸°ì•ˆì[\s_\-:]*([ê°€-í£]{2,4})',
                r'ì‘ì„±ì[\s_\-:]*([ê°€-í£]{2,4})'
            ]
            for pattern in author_patterns:
                match = re.search(pattern, filename)
                if match:
                    metadata['author'] = match.group(1) if 'ê¸°ì•ˆ' in pattern else match.group(1)
                    break

            return metadata
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {}

    def _score_document_relevance(self, content, keywords):
        """ë¬¸ì„œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° í—¬í¼"""
        if not content or not keywords:
            return 0

        score = 0
        content_lower = content.lower()

        for keyword in keywords:
            keyword_lower = keyword.lower()
            # ì •í™•í•œ ë§¤ì¹­
            exact_matches = content_lower.count(keyword_lower)
            score += exact_matches * 2

            # ë¶€ë¶„ ë§¤ì¹­
            if len(keyword_lower) > 2:
                partial_matches = sum(1 for word in content_lower.split()
                                    if keyword_lower in word)
                score += partial_matches

        # ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™”
        doc_length = len(content)
        if doc_length > 0:
            score = score / (doc_length / 1000)  # 1000ì ë‹¨ìœ„ë¡œ ì •ê·œí™”

        return score

    def _format_search_result(self, file_path, content, metadata):
        """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ… í—¬í¼"""
        result = []

        # ì œëª©
        filename = file_path.stem if hasattr(file_path, 'stem') else str(file_path)
        result.append(f"ğŸ“„ {filename}")
        result.append("-" * 50)

        # ë©”íƒ€ë°ì´í„°
        if metadata.get('date'):
            result.append(f"ğŸ“… ë‚ ì§œ: {metadata['date']}")
        if metadata.get('author'):
            result.append(f"âœï¸ ê¸°ì•ˆì: {metadata['author']}")

        # ë‚´ìš© ìš”ì•½ (ì²˜ìŒ 200ì)
        if content:
            summary = content[:200].replace('\n', ' ')
            result.append(f"\nğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
            result.append(summary + "...")

        return '\n'.join(result)

    def _aggregate_search_results(self, results):
        """ê²€ìƒ‰ ê²°ê³¼ í†µí•© í—¬í¼"""
        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        aggregated = []
        aggregated.append(f"ğŸ” ì´ {len(results)}ê°œ ë¬¸ì„œ ë°œê²¬\n")
        aggregated.append("=" * 60)

        for i, result in enumerate(results, 1):
            aggregated.append(f"\n[{i}] {result}")
            if i < len(results):
                aggregated.append("\n" + "-" * 60)

        return '\n'.join(aggregated)

    def _search_multiple_documents(self, query: str) -> str:
        """ì—¬ëŸ¬ ë¬¸ì„œ ê²€ìƒ‰ ë° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        try:
            # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            query_lower = query.lower()
            
            # ë§¤ì¹­ëœ ë¬¸ì„œë“¤ ìˆ˜ì§‘
            matched_docs = []
            
            for cache_key, metadata in self.metadata_cache.items():
                # TXT íŒŒì¼ì€ ì œì™¸ (PDFë§Œ ì²˜ë¦¬)
                if metadata.get('is_txt', False):
                    continue

                filename = metadata.get('filename', cache_key)
                score = 0
                filename_lower = filename.lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                keywords_in_query = []

                # ë™ì  í‚¤ì›Œë“œ ì¶”ì¶œ (í•˜ë“œì½”ë”© ì œê±°)
                query_words = re.findall(r'[ê°€-í£]+|[A-Za-z]+', query_lower)
                file_words = re.findall(r'[ê°€-í£]+|[A-Za-z]+', filename_lower)

                # ë¶ˆìš©ì–´ ì œì™¸
                stopwords = ['ì˜', 'ë°', 'ê±´', 'ê²€í† ì„œ', 'ê´€ë ¨', 'ë¬¸ì„œ', 'ì°¾ì•„', 'ì¤˜', 'ìˆì–´', 'ì–´ë–¤', 'ê¸°ì•ˆì„œ']

                # ê¸°ì•ˆì ê²€ìƒ‰ ì²˜ë¦¬ (ë©”íƒ€ë°ì´í„° DB í™œìš©)
                if 'ê¸°ì•ˆì' in query_lower:
                    # "ìµœìƒˆë¦„ ê¸°ì•ˆì" ë˜ëŠ” "ê¸°ì•ˆì ìµœìƒˆë¦„" í˜•íƒœ ì¶”ì¶œ
                    drafter_match = re.search(r'([ê°€-í£]{2,4})\s*ê¸°ì•ˆì|ê¸°ì•ˆì\s*([ê°€-í£]{2,4})', query)
                    if drafter_match:
                        search_drafter = drafter_match.group(1) or drafter_match.group(2)
                        if search_drafter and metadata.get('is_pdf'):
                            found_drafter = False

                            # 1. ë©”íƒ€ë°ì´í„° DBì—ì„œ ë¨¼ì € í™•ì¸
                            if self.metadata_db:
                                db_info = self.metadata_db.get_document(filename)
                                if db_info and db_info.get('drafter'):
                                    if search_drafter in db_info['drafter']:
                                        score += 50
                                        found_drafter = True

                            # 2. DBì— ì—†ìœ¼ë©´ PDFì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
                            if not found_drafter and metadata.get('drafter') is None:
                                try:
                                    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œë§Œ ì‹œë„ (ë¹ ë¥¸ ì²˜ë¦¬)
                                    import pdfplumber
                                    with pdfplumber.open(metadata['path']) as pdf:
                                        if pdf.pages:
                                            # ì²« í˜ì´ì§€ë§Œ ë¹ ë¥´ê²Œ í™•ì¸
                                            text = pdf.pages[0].extract_text() or ""
                                            if len(text) > 50:  # í…ìŠ¤íŠ¸ PDFì¸ ê²½ìš°
                                                # ê¸°ì•ˆì íŒ¨í„´ ê²€ìƒ‰
                                                patterns = [
                                                    r'ê¸°ì•ˆì[\s:ï¼š]*([ê°€-í£]{2,4})',
                                                    r'ì‘ì„±ì[\s:ï¼š]*([ê°€-í£]{2,4})',
                                                ]
                                                for pattern in patterns:
                                                    match = re.search(pattern, text)
                                                    if match:
                                                        drafter = match.group(1).strip()
                                                        # DBì— ì €ì¥
                                                        if self.metadata_db:
                                                            self.metadata_db.update_document(filename, drafter=drafter)
                                                        if search_drafter in drafter:
                                                            score += 50
                                                            found_drafter = True
                                                        break
                                except Exception as e:
                                    pass

                        # ê¸°ì•ˆì ê²€ìƒ‰ì¸ë° ë§¤ì¹­ ì•ˆë˜ë©´ ê±´ë„ˆëœ€
                        if score < 50:
                            continue

                # ì¥ë¹„ëª… íŠ¹ë³„ ê°€ì¤‘ì¹˜ (DVR, CCU ë“±) - ì •í™•í•œ ë§¤ì¹­ë§Œ
                equipment_names = ['dvr', 'ccu', 'ì¹´ë©”ë¼', 'ë Œì¦ˆ', 'ëª¨ë‹ˆí„°', 'ìŠ¤ìœ„ì²˜', 'ë§ˆì´í¬', 'ë¯¹ì„œ', 'ì‚¼ê°ëŒ€', 'ì¤‘ê³„ì°¨']
                for equipment in equipment_names:
                    if equipment in query_lower:
                        # DVR ê²€ìƒ‰ì‹œ DVRë§Œ ì°¾ê¸° (ë‹¨ì–´ ê²½ê³„ ì²´í¬)
                        if equipment == 'dvr':
                            # DVRì´ ì •í™•íˆ ìˆëŠ”ì§€ í™•ì¸ (D-tap, VR ë“± ì œì™¸)
                            if re.search(r'\bDVR\b', filename, re.IGNORECASE):
                                score += 20  # DVR ì™„ì „ ë§¤ì¹­
                            if 'dvr' in filename_lower and 'd-tap' not in filename_lower and 'vr' not in filename_lower:
                                score += 15
                        else:
                            # ë‹¤ë¥¸ ì¥ë¹„ëª…ì€ ê¸°ì¡´ ë°©ì‹
                            if equipment in filename_lower:
                                score += 15

                        # í‚¤ì›Œë“œ ì²´í¬
                        if any(equipment == kw.lower() for kw in metadata.get('keywords', [])):
                            score += 8
                
                for word in query_words:
                    if len(word) >= 2 and word not in stopwords:
                        keywords_in_query.append(word)
                        # íŒŒì¼ëª…ì— í•´ë‹¹ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì ìˆ˜ ë¶€ì—¬
                        if word in file_words:
                            # ë‹¨ì–´ ê¸¸ì´ì— ë¹„ë¡€í•œ ì ìˆ˜
                            score += len(word) * 2
                        # ë¶€ë¶„ ë§¤ì¹­ - DVR ê°™ì€ ì§§ì€ ë‹¨ì–´ëŠ” ì œì™¸
                        if len(word) >= 4:  # 4ê¸€ì ì´ìƒë§Œ ë¶€ë¶„ ë§¤ì¹­
                            for f_word in file_words:
                                # ì „ì²´ í¬í•¨ì´ ì•„ë‹Œ ë¶€ë¶„ ì¼ì¹˜ë§Œ
                                if len(f_word) >= 4 and (word in f_word or f_word in word):
                                    score += len(word) // 2  # ë¶€ë¶„ ë§¤ì¹­ì€ ì ìˆ˜ ì ˆë°˜
                
                # ë©”íƒ€ë°ì´í„° í‚¤ì›Œë“œ ë§¤ì¹­
                for keyword in metadata['keywords']:
                    if keyword.lower() in query_lower:
                        score += 3
                
                # ì—°ë„ì™€ ì›” ë§¤ì¹­
                year_match = re.search(r'(20\d{2})', query)
                month_match = re.search(r'(\d{1,2})\s*ì›”', query)
                
                if year_match:
                    query_year = year_match.group(1)
                    if metadata['year'] == query_year:
                        score += 5
                        
                        # ì›”ë„ ì§€ì •ëœ ê²½ìš°
                        if month_match:
                            query_month = int(month_match.group(1))
                            file_month_match = re.search(r'\d{4}-(\d{2})-\d{2}', filename)
                            if file_month_match:
                                file_month = int(file_month_match.group(1))
                                if file_month == query_month:
                                    score += 10  # ì›”ê¹Œì§€ ì¼ì¹˜í•˜ë©´ ë†’ì€ ì ìˆ˜
                                else:
                                    score = 0  # ì›”ì´ ë‹¤ë¥´ë©´ ì œì™¸
                                    continue
                    else:
                        # ì—°ë„ê°€ ë‹¤ë¥´ë©´ ì œì™¸
                        continue  # ì—°ë„ê°€ ë‹¤ë¥´ë©´ ë¬´ì¡°ê±´ ì œì™¸
                
                # ìµœì†Œ ì ìˆ˜ ê¸°ì¤€ ì„¤ì • (ë„ˆë¬´ ë§ì€ ë¬¸ì„œ ë°©ì§€)
                # ê¸°ì•ˆì ê²€ìƒ‰ì‹œ ì ìˆ˜ê°€ ìˆìœ¼ë©´ í¬í•¨
                has_equipment = False  # ë³€ìˆ˜ë¥¼ ë¨¼ì € ì´ˆê¸°í™”

                if 'ê¸°ì•ˆì' in query_lower:
                    MIN_SCORE = 1 if score > 0 else 999  # ê¸°ì•ˆì ê²€ìƒ‰ì€ ì ìˆ˜ ìˆìœ¼ë©´ í¬í•¨
                elif re.search(r'20\d{2}ë…„', query):
                    MIN_SCORE = 2  # ë…„ë„ ê²€ìƒ‰ì€ ë‚®ì€ ê¸°ì¤€
                else:
                    MIN_SCORE = 3  # ê¸°ë³¸ ìµœì†Œ 3ì  ì´ìƒë§Œ í¬í•¨

                # ì¥ë¹„ëª… ê²€ìƒ‰ì‹œ ì ì ˆí•œ ê¸°ì¤€ ì ìš©
                for equipment in equipment_names:
                    if equipment in query_lower:
                        has_equipment = True
                        # DVRì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                        if equipment == 'dvr':
                            if 'DVR' in filename or ('dvr' in filename_lower and 'd-tap' not in filename_lower and 'vr' not in filename_lower):
                                MIN_SCORE = 0  # DVRì´ ì •í™•íˆ ìˆìœ¼ë©´ í¬í•¨
                            else:
                                MIN_SCORE = 10  # DVR ê²€ìƒ‰ì¸ë° DVRì´ ì—†ìœ¼ë©´ ë†’ì€ ê¸°ì¤€
                        else:
                            # ë‹¤ë¥¸ ì¥ë¹„ëª…
                            if equipment in filename_lower:
                                MIN_SCORE = 0
                            else:
                                MIN_SCORE = max(3, MIN_SCORE)
                        break
                
                if score >= MIN_SCORE:
                    # metadataì˜ path ì‚¬ìš©
                    pdf_path = metadata.get('path')
                    if isinstance(pdf_path, str):
                        pdf_path = Path(pdf_path)
                    if not pdf_path:
                        pdf_path = self.docs_dir / filename

                    info = self._extract_pdf_info(pdf_path)
                    matched_docs.append({
                        'filename': filename,
                        'score': score,
                        'info': info,
                        'year': metadata['year'],
                        'cache_key': cache_key  # íŒŒì¼ ê²½ë¡œ ì •ë³´ ì¶”ê°€
                    })
            
            # ì¤‘ë³µ ì œê±° ë° ìµœì í™”
            unique_docs = {}
            for doc in matched_docs:
                filename = doc['filename']
                if filename not in unique_docs:
                    unique_docs[filename] = doc
                else:
                    # ë” ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ë¬¸ì„œë¥¼ ìœ ì§€
                    if doc['score'] > unique_docs[filename]['score']:
                        unique_docs[filename] = doc
                    # ê°™ì€ ì ìˆ˜ë©´ year_ í´ë” ìš°ì„ 
                    if doc['score'] == unique_docs[filename]['score'] and 'year_' in doc.get('cache_key', ''):
                        unique_docs[filename] = doc

            matched_docs = list(unique_docs.values())

            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            matched_docs.sort(key=lambda x: x['score'], reverse=True)

            # ê²°ê³¼ ì œí•œ (ì„±ëŠ¥ ìµœì í™”)
            # ì¥ë¹„ ê²€ìƒ‰ì€ 20ê°œê¹Œì§€, ì¼ë°˜ ê²€ìƒ‰ì€ 15ê°œê¹Œì§€
            max_results = 20 if has_equipment else 15
            if len(matched_docs) > max_results:
                matched_docs = matched_docs[:max_results]
            
            if not matched_docs:
                return " ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê²°ê³¼ í¬ë§·íŒ… (í†µí•©í˜• UI)
            report = []
            report.append(f"##  '{query}' ê²€ìƒ‰ ê²°ê³¼")
            report.append(f"**ì´ {len(matched_docs)}ê°œ ë¬¸ì„œ ë°œê²¬**\n")
            report.append("---\n")
            
            # ì—°ë„ë³„ë¡œ ê·¸ë£¹í™” (ì¤‘ë³µ ì œê±°ëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì™„ë£Œ)
            docs_by_year = {}

            for doc in matched_docs:
                year = doc['year']
                if year not in docs_by_year:
                    docs_by_year[year] = []
                docs_by_year[year].append(doc)
            
            # ì—°ë„ë³„ë¡œ í‘œì‹œ
            for year in sorted(docs_by_year.keys(), reverse=True):
                report.append(f"###  {year}ë…„ ({len(docs_by_year[year])}ê°œ)\n")
                
                for doc in docs_by_year[year]:
                    info = doc['info']
                    filename = doc['filename']
                    relative_path = doc.get('cache_key', filename)  # ìºì‹œ í‚¤ê°€ ìƒëŒ€ ê²½ë¡œ
                    
                    # ì¹´í…Œê³ ë¦¬ íŒë‹¨ ë° ì´ëª¨ì§€
                    if 'êµ¬ë§¤' in filename:
                        category = "êµ¬ë§¤ìš”ì²­"
                        emoji = ""
                    if 'ìˆ˜ë¦¬' in filename or 'ë³´ìˆ˜' in filename:
                        category = "ìˆ˜ë¦¬/ë³´ìˆ˜"
                        emoji = ""
                    if 'íê¸°' in filename:
                        category = "íê¸°ì²˜ë¦¬"
                        emoji = "ï¸"
                    if 'ê²€í† ' in filename:
                        category = "ê²€í† ë³´ê³ ì„œ"
                        emoji = ""
                    else:
                        category = "ê¸°íƒ€"
                        emoji = ""
                    
                    # ì œëª© ì¶”ì¶œ (ë‚ ì§œ ì œì™¸)
                    title_parts = filename.replace('.pdf', '').split('_', 1)
                    title = title_parts[1] if len(title_parts) > 1 else title_parts[0]
                    
                    # ê¸°ë³¸ ì •ë³´
                    drafter = info.get('ê¸°ì•ˆì', '')
                    date = info.get('ë‚ ì§œ', '')
                    amount = info.get('ê¸ˆì•¡', '')
                    
                    # ê°œìš” ìƒì„± - metadata_cacheì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ í™œìš©
                    summary = ""

                    # metadataì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                    cached_metadata = None
                    for ck, md in self.metadata_cache.items():
                        if md.get('filename') == filename:
                            cached_metadata = md
                            break

                    if cached_metadata and cached_metadata.get('text'):
                        # ìºì‹œëœ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
                        text = cached_metadata['text'][:500]  # ì²˜ìŒ 500ì

                        # ì£¼ìš” ì •ë³´ ì¶”ì¶œ
                        if 'ëª©ì ' in text:
                            purpose_match = re.search(r'ëª©ì [:\s]+([^\n]+)', text)
                            if purpose_match:
                                summary = purpose_match.group(1).strip()
                        if 'ë‚´ìš©' in text:
                            content_match = re.search(r'ë‚´ìš©[:\s]+([^\n]+)', text)
                            if content_match:
                                summary = content_match.group(1).strip()
                        if 'ì‚¬ìœ ' in text:
                            reason_match = re.search(r'ì‚¬ìœ [:\s]+([^\n]+)', text)
                            if reason_match:
                                summary = reason_match.group(1).strip()

                    # í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ì¶”ì¶œ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
                    if not summary:
                        if 'êµ¬ë§¤' in filename:
                            summary = "ì¥ë¹„ êµ¬ë§¤ ìš”ì²­"
                        if 'ìˆ˜ë¦¬' in filename or 'ë³´ìˆ˜' in filename:
                            summary = "ì¥ë¹„ ìˆ˜ë¦¬/ë³´ìˆ˜ ê±´"
                        if 'íê¸°' in filename:
                            summary = "ë…¸í›„ ì¥ë¹„ íê¸° ì²˜ë¦¬"
                        if 'êµì²´' in filename:
                            summary = "ë…¸í›„ ì¥ë¹„ êµì²´ ê²€í† "
                        if 'ê²€í† ' in filename:
                            summary = "ê¸°ìˆ  ê²€í†  ë³´ê³ ì„œ"
                        else:
                            summary = "ê¸°ìˆ ê´€ë¦¬íŒ€ ì—…ë¬´ ë¬¸ì„œ"
                    
                    # ì¹´ë“œ UI í˜•íƒœë¡œ ì¶œë ¥
                    report.append(f"#### {emoji} **{title}**")
                    report.append(f"**[{category}]** | {date if date else 'ë‚ ì§œ ë¯¸ìƒ'}")
                    
                    # ìƒì„¸ ì •ë³´
                    if drafter:
                        report.append(f"- **ê¸°ì•ˆì**: {drafter}")
                    if amount:
                        report.append(f"- **ê¸ˆì•¡**: {amount}")
                    if summary:
                        report.append(f"- **ê°œìš”**: {summary}")
                    
                    # íŒŒì¼ ê²½ë¡œ ì •ë³´ í¬í•¨ (web_interfaceì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡)
                    # íŠ¹ë³„í•œ ë§ˆì»¤ ì‚¬ìš©: @@PDF_PREVIEW@@
                    file_path_str = str(relative_path) if relative_path else filename
                    report.append(f"- **íŒŒì¼**: [{file_path_str}] @@PDF_PREVIEW@@{file_path_str}@@")
                    report.append("")  # ê°„ê²©
                
                report.append("---\n")
            
            return "\n".join(report)
            
        except Exception as e:
            return f" ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"

    def _search_and_analyze_by_content(self, query: str) -> str:
        """íŠ¹ì • ë‚´ìš©ì´ ì–¸ê¸‰ëœ ê²½ìš° ê´€ë ¨ ë¬¸ì„œë“¤ì„ ëª¨ë‘ ì°¾ì•„ì„œ ë¶„ì„

        ì˜ˆì‹œ:
        - "DVR êµì²´ ê²€í†  ë‚´ìš©" â†’ DVR ê´€ë ¨ ëª¨ë“  ë¬¸ì„œ ì°¾ê³  êµì²´ ê²€í†  ë‚´ìš© ì •ë¦¬
        - "ì‚¼ê°ëŒ€ êµ¬ë§¤ ê±´" â†’ ì‚¼ê°ëŒ€ ê´€ë ¨ ëª¨ë“  êµ¬ë§¤ ë¬¸ì„œ ì°¾ê¸°
        """
        try:
            # 1. í•µì‹¬ í‚¤ì›Œë“œì™€ ì‘ì—… íƒ€ì… ë¶„ë¦¬
            query_lower = query.lower()

            # ì¥ë¹„/ì‹œìŠ¤í…œ í‚¤ì›Œë“œ
            equipment_keywords = []
            equipment_terms = ['DVR', 'ì¤‘ê³„ì°¨', 'ì¹´ë©”ë¼', 'ì‚¼ê°ëŒ€', 'ëª¨ë‹ˆí„°', 'CCU', 'ì˜¤ë””ì˜¤', 'ì„œë²„', 'ë§ˆì´í¬', 'ìŠ¤ìœ„ì¹˜']
            for term in equipment_terms:
                if term.lower() in query_lower:
                    equipment_keywords.append(term)

            # ì‘ì—… íƒ€ì… í‚¤ì›Œë“œ
            action_keywords = []
            action_terms = ['êµì²´', 'ê²€í† ', 'êµ¬ë§¤', 'ìˆ˜ë¦¬', 'ë³´ìˆ˜', 'íê¸°', 'ë„ì…', 'ì—…ê·¸ë ˆì´ë“œ', 'ì„¤ì¹˜']
            for term in action_terms:
                if term in query_lower:
                    action_keywords.append(term)

            if not equipment_keywords:
                # ë¬¸ì¥ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ
                nouns = re.findall(r'[\uac00-\ud7a3]{2,}', query)
                equipment_keywords = [n for n in nouns if n not in ['ê´€ë ¨', 'ë¬¸ì„œ', 'ë‚´ìš©', 'ì •ë¦¬', 'ë¶„ì„']]

            print(f" ì¥ë¹„ í‚¤ì›Œë“œ: {equipment_keywords}, ì‘ì—… í‚¤ì›Œë“œ: {action_keywords}")

            # 2. ë‹¨ê³„ë³„ ë¬¸ì„œ ê²€ìƒ‰
            # ë‹¨ê³„ 1: íŒŒì¼ëª…ì— í‚¤ì›Œë“œê°€ ìˆëŠ” ë¬¸ì„œ
            primary_files = []
            # ë‹¨ê³„ 2: ì‘ì—… íƒ€ì…ë§Œ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ
            secondary_files = []
            # ë‹¨ê³„ 3: ë‚´ìš©ì— í‚¤ì›Œë“œê°€ ìˆëŠ” ë¬¸ì„œ (ëŠë¦¼, í•„ìš”ì‹œë§Œ)
            content_match_files = []

            for cache_key, metadata in self.metadata_cache.items():
                if metadata.get('is_pdf'):
                    # ì‹¤ì œ íŒŒì¼ëª… ì‚¬ìš© (cache_keyê°€ ì•„ë‹Œ metadata['filename'])
                    filename_lower = metadata.get('filename', cache_key).lower()
                    path = metadata['path']

                    # íŒŒì¼ëª…ì— ì¥ë¹„ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                    has_equipment_keyword = any(kw.lower() in filename_lower for kw in equipment_keywords)
                    has_action_keyword = any(kw.lower() in filename_lower for kw in action_keywords)

                    if has_equipment_keyword:
                        primary_files.append(path)
                    if has_action_keyword:
                        secondary_files.append(path)

            # 3. ê²°ê³¼ ë³‘í•© (ìµœëŒ€ 15ê°œ)
            relevant_files = primary_files[:10] + secondary_files[:5]

            if not relevant_files:
                # í‚¤ì›Œë“œê°€ ë„ˆë¬´ ì—†ìœ¼ë©´ ë‚´ìš© ê²€ìƒ‰ ì‹œë„ (ì‹œê°„ ì†Œìš”)
                if len(equipment_keywords) > 0:
                    print(" íŒŒì¼ëª…ì—ì„œ ì°¾ì§€ ëª»í•¨, ë‚´ìš© ê²€ìƒ‰ ì‹œì‘...")
                    for file_path, metadata in list(self.metadata_cache.items())[:30]:  # ìµœëŒ€ 30ê°œë§Œ
                        if metadata.get('is_pdf'):
                            try:
                                info = self._extract_pdf_info(metadata['path'])
                                if info and 'text' in info:
                                    content = info['text'][:2000]  # ì²˜ìŒ 2000ìë§Œ
                                    if any(kw.lower() in content.lower() for kw in equipment_keywords):
                                        content_match_files.append(metadata['path'])
                                        if len(content_match_files) >= 5:  # ìµœëŒ€ 5ê°œ
                                            break
                            except Exception as e:
                                continue
                    relevant_files.extend(content_match_files)

            if not relevant_files:
                return f" '{', '.join(equipment_keywords + action_keywords)}' ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            print(f" {len(relevant_files)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")

            # ì„±ëŠ¥ ìµœì í™”: ìƒìœ„ 5ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬
            max_docs_to_process = 5
            files_to_process = relevant_files[:max_docs_to_process]
            if len(relevant_files) > max_docs_to_process:
                print(f" ì„±ëŠ¥ ìµœì í™”: ìƒìœ„ {max_docs_to_process}ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬ (ì „ì²´ {len(relevant_files)}ê°œ ì¤‘)")

            # 4. ê° ë¬¸ì„œì˜ ë‚´ìš© ì¶”ì¶œ ë° ë¶„ì„
            document_analyses = []
            all_contents = []

            for file_path in files_to_process:
                try:
                    info = self._extract_pdf_info(file_path)
                    if info:
                        # ì‘ì—… í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë¶€ë¶„ ì¶”ì¶œ
                        relevant_content = []
                        if 'text' in info:
                            lines = info['text'].split('\n')
                            for i, line in enumerate(lines):
                                line_lower = line.lower()
                                # ì‘ì—… í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ê³¼ ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ
                                if any(kw in line_lower for kw in action_keywords + equipment_keywords):
                                    # ì „í›„ 2ì¤„ì”© í¬í•¨
                                    start = max(0, i-2)
                                    end = min(len(lines), i+3)
                                    context = ' '.join(lines[start:end])
                                    relevant_content.append(context)

                        doc_analysis = {
                            'filename': file_path.name,
                            'title': info.get('ì œëª©', file_path.stem),
                            'date': info.get('ë‚ ì§œ', ''),
                            'drafter': info.get('ê¸°ì•ˆì', ''),
                            'amount': info.get('ê¸ˆì•¡', ''),
                            'relevant_content': relevant_content[:3],  # ìµœëŒ€ 3ê°œ ê´€ë ¨ ë¶€ë¶„
                            'full_text': info.get('text', '')[:2000]  # ì „ì²´ í…ìŠ¤íŠ¸ ì¼ë¶€
                        }
                        document_analyses.append(doc_analysis)
                        all_contents.append(f"[{file_path.name}]\n" + '\n'.join(relevant_content[:3]))
                except Exception as e:
                    print(f"ï¸ {file_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue

            if not document_analyses:
                return " ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # 5. LLMì„ ì‚¬ìš©í•˜ì—¬ ì¢…í•© ë¶„ì„
            if self.llm is None:
                if not LLMSingleton.is_loaded():
                    print(" LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.llm = LLMSingleton.get_instance(model_path=self.model_path)

            combined_text = '\n\n'.join(all_contents)

            prompt = f"""ë‹¤ìŒì€ '{', '.join(equipment_keywords)}' ê´€ë ¨ '{', '.join(action_keywords)}' ë¬¸ì„œë“¤ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë¬¸ì„œ ë‚´ìš©:
{combined_text[:6000]}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.
í¬í•¨í•´ì•¼ í•  ë‚´ìš©:
1. ê° ë¬¸ì„œì—ì„œ ì°¾ì€ í•µì‹¬ ì •ë³´
2. ì—°ë„ë³„/ì‹œê¸°ë³„ ë³€í™” (ìˆë‹¤ë©´)
3. ê¸°ìˆ ì  ì‚¬ì–‘ì´ë‚˜ ëª¨ë¸ ì •ë³´
4. ë¹„ìš© ì •ë³´
5. ê²°ë¡  ë° ì¶”ì²œì‚¬í•­

ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""

            context_chunks = [{
                'content': combined_text[:6000],
                'metadata': {'source': 'multiple_documents'},
                'score': 1.0
            }]

            response_obj = self.llm.generate_response(prompt, context_chunks)
            llm_response = response_obj.answer if hasattr(response_obj, 'answer') else str(response_obj)

            # 6. ê²°ê³¼ êµ¬ì„±
            result = []
            result.append(f" **'{', '.join(equipment_keywords)}' ê´€ë ¨ {len(document_analyses)}ê°œ ë¬¸ì„œ ë¶„ì„**\n")
            result.append("="*50 + "\n\n")

            # LLM ë¶„ì„ ê²°ê³¼
            result.append(llm_response)
            result.append("\n" + "="*50 + "\n")

            # ë¶„ì„ëœ ë¬¸ì„œ ëª©ë¡
            result.append("\n **ë¶„ì„ëœ ë¬¸ì„œ:**\n")
            for doc in document_analyses:
                result.append(f"\nâ€¢ **{doc['title']}**")
                if doc['date']:
                    result.append(f"  - ë‚ ì§œ: {doc['date']}")
                if doc['drafter']:
                    result.append(f"  - ê¸°ì•ˆì: {doc['drafter']}")
                if doc['amount']:
                    result.append(f"  - ê¸ˆì•¡: {doc['amount']}")
                if doc['relevant_content']:
                    result.append(f"  - í•µì‹¬ ë‚´ìš©: {len(doc['relevant_content'])}ê°œ ë¶€ë¶„ ë°œê²¬")

            return '\n'.join(result)

        except Exception as e:
            return f" ë‚´ìš© ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨: {e}"

    def _read_and_summarize_documents(self, query: str) -> str:
        """ê´€ë ¨ ë¬¸ì„œë“¤ì„ ì‹¤ì œë¡œ ì½ê³  ì¢…í•© ì •ë¦¬í•˜ëŠ” ë©”ì„œë“œ

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸ (ì˜ˆ: "DVRê´€ë ¨ ë¬¸ì„œ ë‹¤ ì½ê³  ì •ë¦¬í•´ì¤˜")

        Returns:
            ì¢…í•© ì •ë¦¬ëœ ë‚´ìš©
        """
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            query_lower = query.lower()
            keywords = []

            # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            important_keywords = ['DVR', 'ì¤‘ê³„ì°¨', 'ì¹´ë©”ë¼', 'ì‚¼ê°ëŒ€', 'ëª¨ë‹ˆí„°', 'ì˜¤ë””ì˜¤', 'ì„œë²„', 'ìŠ¤ìœ„ì¹˜']
            for kw in important_keywords:
                if kw.lower() in query_lower:
                    keywords.append(kw)

            # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê¸°ì¤€ ì‚¬ìš©
            if not keywords:
                # ë¬¸ì¥ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ
                nouns = re.findall(r'[\uac00-\ud7a3]{2,}', query)
                keywords = [n for n in nouns if n not in ['ê´€ë ¨', 'ë¬¸ì„œ', 'ì½ê³ ', 'ì •ë¦¬', 'ë‚´ìš©', 'ëª¨ë‘', 'ì „ë¶€']]

            if not keywords:
                return " ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”."

            print(f" í‚¤ì›Œë“œë¡œ ë¬¸ì„œ ê²€ìƒ‰: {keywords}")

            # ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
            relevant_files = []
            for file_path, metadata in self.metadata_cache.items():
                if metadata.get('is_pdf'):
                    # íŒŒì¼ëª…ì´ë‚˜ ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    filename_lower = file_path.lower()
                    for kw in keywords:
                        if kw.lower() in filename_lower:
                            relevant_files.append(metadata['path'])
                            break

            if not relevant_files:
                return f" '{', '.join(keywords)}' ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            print(f" {len(relevant_files)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")

            # ê° ë¬¸ì„œì˜ ë‚´ìš© ì¶”ì¶œ
            all_contents = []
            document_summaries = []

            for file_path in relevant_files[:10]:  # ìµœëŒ€ 10ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬
                try:
                    # PDF ë‚´ìš© ì¶”ì¶œ
                    info = self._extract_pdf_info(file_path)
                    if info:
                        doc_summary = {
                            'filename': file_path.name,
                            'title': info.get('ì œëª©', file_path.stem),
                            'date': info.get('ë‚ ì§œ', 'ë‚ ì§œ ë¯¸ìƒ'),
                            'drafter': info.get('ê¸°ì•ˆì', 'ë¯¸ìƒ'),
                            'amount': info.get('ê¸ˆì•¡', ''),
                            'content': info.get('text', '')[:3000]  # ì²˜ìŒ 3000ì
                        }

                        # ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
                        if 'text' in info:
                            # ì¤‘ìš”í•œ ë¬¸ì¥ ì¶”ì¶œ
                            important_sentences = []
                            lines = info['text'].split('\n')
                            for line in lines:
                                line = line.strip()
                                if line and len(line) > 20:
                                    # ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥
                                    if any(kw in line for kw in ['ê°œìš”', 'ëª©ì ', 'ë‚´ìš©', 'ê²°ê³¼', 'ê²°ë¡ ', 'ì¶”ì²œ', 'í•„ìš”', 'ì˜ˆì‚°', 'ê¸ˆì•¡']):
                                        important_sentences.append(line[:200])

                            doc_summary['key_points'] = important_sentences[:5]

                        document_summaries.append(doc_summary)
                        all_contents.append(f"\n[{file_path.name}]\n{info.get('text', '')[:3000]}")

                except Exception as e:
                    print(f"ï¸ {file_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue

            if not document_summaries:
                return " ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # LLMì„ ì‚¬ìš©í•˜ì—¬ ì¢…í•© ì •ë¦¬
            if self.llm is None:
                if not LLMSingleton.is_loaded():
                    print(" LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.llm = LLMSingleton.get_instance(model_path=self.model_path)

            # ì¢…í•© ì •ë¦¬ í”„ë¡¬í”„íŠ¸
            combined_text = '\n\n'.join(all_contents)

            prompt = f"""ë‹¤ìŒì€ '{', '.join(keywords)}' ê´€ë ¨ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.

ì´ ë¬¸ì„œë“¤ì„ ì½ê³  ì¢…í•©ì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

í¬í•¨í•´ì•¼ í•  ë‚´ìš©:
1. ì£¼ìš” ë‚´ìš© ìš”ì•½
2. ì—°ë„ë³„/ì‹œê¸°ë³„ ì£¼ìš” ì‚¬í•­
3. ê¸°ìˆ ì  ì‚¬ì–‘ì´ë‚˜ ëª¨ë¸ ì •ë³´ (ìˆë‹¤ë©´)
4. ë¹„ìš© ì •ë³´ (ìˆë‹¤ë©´)
5. ê²€í†  ê²°ê³¼ë‚˜ ì¶”ì²œì‚¬í•­
6. ê³µí†µì ê³¼ ì°¨ì´ì 

ë¬¸ì„œ ë‚´ìš©:
{combined_text[:8000]}

ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”. í…Œí”Œë¦¿ í˜•ì‹ì´ ì•„ë‹Œ ëŒ€í™”í˜• í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

            # LLM í˜¸ì¶œ
            context_chunks = [{
                'content': combined_text[:8000],
                'metadata': {'source': 'multiple_documents'},
                'score': 1.0
            }]

            response_obj = self.llm.generate_response(prompt, context_chunks)
            llm_response = response_obj.answer if hasattr(response_obj, 'answer') else str(response_obj)

            # ê²°ê³¼ êµ¬ì„±
            result = []
            result.append(f" **{len(document_summaries)}ê°œ {', '.join(keywords)} ê´€ë ¨ ë¬¸ì„œ ì¢…í•© ë¶„ì„**\n")
            result.append("="*50 + "\n")

            # LLM ì‘ë‹µ ì¶”ê°€
            result.append(llm_response)
            result.append("\n" + "="*50 + "\n")

            # ê° ë¬¸ì„œ ê°„ë‹¨ ì •ë³´
            result.append("\n **ë¶„ì„ëœ ë¬¸ì„œ ëª©ë¡:**\n")
            for doc in document_summaries:
                result.append(f"\nâ€¢ **{doc['title']}**")
                result.append(f"  - ë‚ ì§œ: {doc['date']}")
                result.append(f"  - ê¸°ì•ˆì: {doc['drafter']}")
                if doc['amount']:
                    result.append(f"  - ê¸ˆì•¡: {doc['amount']}")

            return '\n'.join(result)

        except Exception as e:
            return f" ë¬¸ì„œ ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {e}"

    def _is_location_match(self, item_lines: list, location: str) -> bool:
        """ìœ„ì¹˜ ë§¤ì¹­ ë¡œì§ ê°œì„  - ì •í™•í•œ ìœ„ì¹˜ ë§¤ì¹­"""
        item_text = '\n'.join(item_lines)
        
        # ìœ„ì¹˜ ì •ë³´ê°€ ìˆëŠ” ë¼ì¸ ì°¾ê¸°
        for line in item_lines:
            if 'ìœ„ì¹˜:' in line or 'ìœ„ì¹˜ì •ë³´:' in line:
                # ì‹¤ì œ ìœ„ì¹˜ ì¶”ì¶œ
                location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', line)
                if location_match:
                    actual_location = location_match.group(1).strip()
                    
                    # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™
                    if location == actual_location:
                        # ì™„ì „ ì¼ì¹˜ (ê°€ì¥ ìš°ì„ )
                        return True
                    if location == 'ë¶€ì¡°ì •ì‹¤':
                        # 'ë¶€ì¡°ì •ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*ë¶€ì¡°ì •ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                        return actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
                    if location == 'ìŠ¤íŠœë””ì˜¤':
                        # 'ìŠ¤íŠœë””ì˜¤'ë¡œ ê²€ìƒ‰ì‹œ '*ìŠ¤íŠœë””ì˜¤' íŒ¨í„´ë§Œ ë§¤ì¹­ 
                        return actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
                    if location == 'í¸ì§‘ì‹¤':
                        # 'í¸ì§‘ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*í¸ì§‘ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                        return actual_location.endswith('í¸ì§‘ì‹¤')
                    if location in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
                        # ì¤‘ê³„ì°¨ ê²€ìƒ‰ì‹œ Van ê´€ë ¨ ìœ„ì¹˜ ëª¨ë‘ ë§¤ì¹­
                        return 'Van' in actual_location or 'VAN' in actual_location
                    if location == "ê´‘í™”ë¬¸ë¶€ì¡°ì •ì‹¤":
                        # "ê´‘í™”ë¬¸ ë¶€ì¡°ì •ì‹¤" ê°™ì€ ë³µí•© ìœ„ì¹˜ëª… ì²˜ë¦¬
                        return "ê´‘í™”ë¬¸" in actual_location and "ë¶€ì¡°ì •ì‹¤" in actual_location
                    if len(location) > 3:
                        # 3ê¸€ì ì´ìƒì˜ êµ¬ì²´ì ì¸ ìœ„ì¹˜ëª…ì€ ë¶€ë¶„ ë§¤ì¹­ í—ˆìš©
                        return location in actual_location
        
        return False

    def _search_equipment_all_locations(self, txt_path: Path, equipment: str) -> str:
        """ëª¨ë“  ìœ„ì¹˜ë³„ ì¥ë¹„ í˜„í™© ì •ë¦¬"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # í•­ëª©ë³„ë¡œ ê²€ìƒ‰
            lines = content.split('\n')
            location_equipment = {}  # {ìœ„ì¹˜: [ì¥ë¹„ ëª©ë¡]}
            current_item = []
            
            for line in lines:
                # [NNNN] í˜•ì‹ì˜ ì‹œì‘ ë¼ì¸ì„ ì°¾ê¸°
                if re.match(r'^\[\d{4}\]', line.strip()):
                    # ì´ì „ í•­ëª© ê²€ì‚¬
                    if current_item:
                        item_text = '\n'.join(current_item)
                        # ì¥ë¹„ëª… ì¡°ê±´ í™•ì¸
                        if equipment.upper() == "CCU":
                            equipment_match = "CCU" in item_text.upper() or "Camera Control Unit" in item_text
                        else:
                            equipment_match = equipment.upper() in item_text.upper()
                        
                        if equipment_match:
                            # ìœ„ì¹˜ ì¶”ì¶œ
                            location_info = None
                            for item_line in current_item:
                                if "ìœ„ì¹˜:" in item_line:
                                    # ìœ„ì¹˜: ë’¤ì˜ ê°’ ì¶”ì¶œ
                                    match = re.search(r'ìœ„ì¹˜:\s*([^|]+)', item_line)
                                    if match:
                                        location_info = match.group(1).strip()
                                        break
                            
                            if location_info:
                                if location_info not in location_equipment:
                                    location_equipment[location_info] = []
                                # ì¥ë¹„ ì •ë³´ ì¶”ì¶œ (ì²« ì¤„ë§Œ)
                                location_equipment[location_info].append(current_item[0])
                    
                    current_item = [line]
                else:
                    if current_item:
                        current_item.append(line)
            
            # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
            if current_item:
                item_text = '\n'.join(current_item)
                if equipment.upper() == "CCU":
                    equipment_match = "CCU" in item_text.upper() or "Camera Control Unit" in item_text
                else:
                    equipment_match = equipment.upper() in item_text.upper()
                
                if equipment_match:
                    location_info = None
                    for item_line in current_item:
                        if "ìœ„ì¹˜:" in item_line:
                            match = re.search(r'ìœ„ì¹˜:\s*([^|]+)', item_line)
                            if match:
                                location_info = match.group(1).strip()
                                break
                    
                    if location_info:
                        if location_info not in location_equipment:
                            location_equipment[location_info] = []
                        location_equipment[location_info].append(current_item[0])
            
            # ê²°ê³¼ í¬ë§·íŒ…
            if location_equipment:
                total_count = sum(len(items) for items in location_equipment.values())
                response = f" **{equipment.upper()} ìœ„ì¹˜ë³„ í˜„í™©**\n"
                response += "=" * 70 + "\n"
                response += f" ì´ {total_count}ê°œ ì¥ë¹„ê°€ {len(location_equipment)}ê°œ ìœ„ì¹˜ì— ë¶„í¬\n\n"
                
                # ìœ„ì¹˜ë³„ ì •ë ¬ (ë§ì€ ìˆœ)
                sorted_locations = sorted(location_equipment.items(), key=lambda x: len(x[1]), reverse=True)
                
                for location, items in sorted_locations:
                    response += f" **{location}**: {len(items)}ê°œ\n"
                    # ìƒ˜í”Œ 3ê°œë§Œ í‘œì‹œ
                    for i, item in enumerate(items[:3], 1):
                        response += f"   {i}. {item}\n"
                    if len(items) > 3:
                        response += f"   ... ì™¸ {len(items)-3}ê°œ\n"
                    response += "\n"
                
                response += f" ì¶œì²˜: {txt_path.name}"
                return response
            else:
                return f" {equipment.upper()} ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            return f" ê²€ìƒ‰ ì‹¤íŒ¨: {e}"

    def _check_location_in_item(self, item_text: str, search_location: str) -> bool:
        """í•­ëª©ì—ì„œ ìœ„ì¹˜ ì¡°ê±´ í™•ì¸"""
        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
        location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', item_text)
        if not location_match:
            return False
            
        actual_location = location_match.group(1).strip()
        
        # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™ ì ìš©
        if search_location == actual_location:
            return True
        if search_location == 'ë¶€ì¡°ì •ì‹¤':
            return actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
        if search_location == 'ìŠ¤íŠœë””ì˜¤':
            return actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
        if search_location == 'í¸ì§‘ì‹¤':
            return actual_location.endswith('í¸ì§‘ì‹¤')
        if search_location in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
            return 'Van' in actual_location or 'VAN' in actual_location or 'ì¤‘ê³„ì°¨' in actual_location
        if len(search_location) > 3:
            return search_location in actual_location
        
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print(" Perfect RAG - ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = PerfectRAG()
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_queries = [
        "2024 ì±„ë„ì—ì´ ì¤‘ê³„ì°¨ ë…¸í›„ ë³´ìˆ˜ê±´ ê¸°ì•ˆì ëˆ„êµ¬?",
        "ë·°íŒŒì¸ë” ì†Œëª¨í’ˆ ì¼€ì´ë¸” êµ¬ë§¤ ë‚ ì§œ ì–¸ì œ?",
        "í‹°ë¹„ë¡œì§ ì›”ëª¨ë‹ˆí„° ê³ ì¥ ìˆ˜ë¦¬ ê¸ˆì•¡ ì–¼ë§ˆ?",
        "2021ë…„ ì§ë²Œì¹´ë©”ë¼ êµ¬ë§¤ ê¸°ì•ˆì ëˆ„êµ¬?",
        "ìŠ¤íŠœë””ì˜¤ ëª¨ë‹ˆí„° êµì²´ ê²€í† ì„œ ë‚´ìš© ìš”ì•½",
    ]
    
    print("\n í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nì§ˆë¬¸ {i}: {query}")
        print("-" * 40)
        answer = rag.answer(query)
        print(answer)
        print("-" * 40)
        
        # ìë™ ì§„í–‰ (input ì œê±°)
    
    print("\n" + "=" * 60)
    print(" í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)




    def _manage_cache_size(self, cache_dict, max_size, cache_name="cache"):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬ - LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°"""
        if len(cache_dict) > max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ë“¤ ì œê±° (FIFO)
            items_to_remove = len(cache_dict) - max_size
            for _ in range(items_to_remove):
                removed = cache_dict.popitem(last=False)  # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            print(f"  ğŸ—‘ï¸ {cache_name}ì—ì„œ {items_to_remove}ê°œ í•­ëª© ì œê±° (í˜„ì¬ í¬ê¸°: {len(cache_dict)})")

    def _add_to_cache(self, cache_dict, key, value, max_size):
        """ìºì‹œì— í•­ëª© ì¶”ê°€ with í¬ê¸° ì œí•œ"""
        # ê¸°ì¡´ í•­ëª©ì´ë©´ ì‚­ì œ í›„ ë‹¤ì‹œ ì¶”ê°€ (LRUë¥¼ ìœ„í•´)
        if key in cache_dict:
            del cache_dict[key]

        # ìƒˆ í•­ëª© ì¶”ê°€
        cache_dict[key] = {
            'data': value,
            'timestamp': time.time()
        }

        # í¬ê¸° ì œí•œ í™•ì¸
        self._manage_cache_size(cache_dict, max_size, str(type(cache_dict)))

    def clear_old_cache(self):
        """ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì œê±°"""
        current_time = time.time()

        # ê° ìºì‹œ ìˆœíšŒí•˜ë©° ì˜¤ë˜ëœ í•­ëª© ì œê±°
        for cache_name, cache_dict in [
            ('documents', self.documents_cache),
            ('metadata', self.metadata_cache),
            ('answer', self.answer_cache),
            ('pdf_text', self.pdf_text_cache)
        ]:
            if not hasattr(self, cache_name + '_cache'):
                continue

            items_to_remove = []
            for key, value in cache_dict.items():
                if isinstance(value, dict) and 'timestamp' in value:
                    if current_time - value['timestamp'] > self.CACHE_TTL:
                        items_to_remove.append(key)

            for key in items_to_remove:
                del cache_dict[key]

            if items_to_remove:
                print(f"  ğŸ—‘ï¸ {cache_name}_cacheì—ì„œ {len(items_to_remove)}ê°œ ë§Œë£Œ í•­ëª© ì œê±°")

    def get_cache_stats(self):
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        stats = {
            'documents_cache': len(self.documents_cache),
            'metadata_cache': len(self.metadata_cache),
            'answer_cache': len(self.answer_cache) if hasattr(self, 'answer_cache') else 0,
            'pdf_text_cache': len(self.pdf_text_cache) if hasattr(self, 'pdf_text_cache') else 0,
        }

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (ëŒ€ëµì )
        import sys
        total_size = 0
        for cache_dict in [self.documents_cache, self.metadata_cache,
                          getattr(self, 'answer_cache', {}),
                          getattr(self, 'pdf_text_cache', {})]:
            total_size += sys.getsizeof(cache_dict)

        stats['estimated_memory_mb'] = total_size / (1024 * 1024)

        return stats
if __name__ == "__main__":
    main()