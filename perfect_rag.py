#!/usr/bin/env python3
"""
Perfect RAG - ì‹¬í”Œí•˜ì§€ë§Œ ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
ë™ì ìœ¼ë¡œ ëª¨ë“  PDF ë¬¸ì„œì™€ ìì‚° ë°ì´í„°ë¥¼ ì •í™•í•˜ê²Œ ì²˜ë¦¬
"""

import re
import warnings
import logging
import pdfplumber
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import json
import sys
import os
from functools import lru_cache
import hashlib
import time
from contextlib import nullcontext

# ë¡œê¹… ì‹œìŠ¤í…œ ì¶”ê°€
try:
    from log_system import get_logger, TimerContext
    logger = get_logger()
except ImportError:
    logger = None
    TimerContext = None
    
# query_loggerëŠ” log_systemìœ¼ë¡œ í†µí•©ë¨

# ì‘ë‹µ í¬ë§·í„° ì¶”ê°€
try:
    from response_formatter import ResponseFormatter
except ImportError:
    ResponseFormatter = None

# FontBBox ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", message=".*FontBBox.*")
warnings.filterwarnings("ignore", category=UserWarning, module="pdfplumber")

# pdfplumber ë¡œê¹… ë ˆë²¨ ì¡°ì •
logging.getLogger("pdfplumber").setLevel(logging.ERROR)
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ê¸°
current_dir = Path(__file__).parent.absolute()
sys.path.insert(0, str(current_dir))

# ì„¤ì • íŒŒì¼ import
try:
    from config_manager import config_manager as cfg
    USE_YAML_CONFIG = True
except ImportError:
    import config as cfg
    USE_YAML_CONFIG = False

from rag_system.qwen_llm import QwenLLM
from rag_system.llm_singleton import LLMSingleton

# ìƒˆë¡œìš´ ëª¨ë“ˆ import (ì œê±°ë¨ - ë°±ì—… í´ë”ë¡œ ì´ë™)
# from pdf_parallel_processor import PDFParallelProcessor
# from error_handler import RAGErrorHandler, ErrorRecovery, DetailedError, safe_execute


class PerfectRAG:
    """ì •í™•í•˜ê³  ì‹¬í”Œí•œ RAG ì‹œìŠ¤í…œ"""

    def _load_performance_config(self):
        """performance_config.yaml ë¡œë“œ"""
        import yaml
        perf_config_path = Path(__file__).parent / 'performance_config.yaml'
        if perf_config_path.exists():
            try:
                with open(perf_config_path, 'r', encoding='utf-8') as f:
                    self.perf_config = yaml.safe_load(f)
                    if logger:
                        logger.system_logger.info(f"Performance config loaded from {perf_config_path}")
            except Exception as e:
                if logger:
                    logger.system_logger.warning(f"Failed to load performance config: {e}")
                self.perf_config = {}
        else:
            self.perf_config = {}

    # í´ë˜ìŠ¤ ë ˆë²¨ ìƒìˆ˜ - config.yamlì—ì„œ ë¡œë“œ
    def _get_manufacturer_pattern(self):
        if USE_YAML_CONFIG:
            manufacturers = cfg.get('patterns.manufacturers', [])
            if manufacturers:
                pattern = '|'.join(manufacturers)
                return rf'\b({pattern})\b'
        return r'\b(SONY|Sony|HARRIS|Harris|HP|TOSHIBA|Toshiba|PANASONIC|Panasonic|CANON|Canon|NIKON|Nikon|DELL|Dell|APPLE|Apple|SAMSUNG|Samsung|LG)\b'

    def _get_model_pattern(self):
        if USE_YAML_CONFIG:
            return cfg.get('patterns.model_regex', r'\b[A-Z]{2,}[-\s]?\d+')
        return r'\b[A-Z]{2,}[-\s]?\d+'
    
    def __init__(self, preload_llm=False):
        # performance_config.yaml ë¡œë“œ ì‹œë„
        self._load_performance_config()

        # ì„¤ì • ë¡œë“œ (YAML ìš°ì„ )
        if USE_YAML_CONFIG:
            self.docs_dir = Path(cfg.get('paths.documents_dir', './docs'))
            self.model_path = cfg.get('models.qwen.path', './models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf')
            self.max_cache_size = self.perf_config.get('cache', {}).get('max_size', 500)
            self.cache_ttl = self.perf_config.get('cache', {}).get('response_ttl', 7200)
            self.max_text_length = self.perf_config.get('memory', {}).get('max_document_length', 8000)
            self.max_pdf_pages = cfg.get('limits.max_pdf_pages', 10)
            self.pdf_workers = self.perf_config.get('parallel', {}).get('pdf_workers', 4)
            self.chunk_size = self.perf_config.get('parallel', {}).get('chunk_size', 5)
        else:
            self.docs_dir = Path(cfg.DOCS_DIR)
            self.model_path = cfg.QWEN_MODEL_PATH
            self.max_cache_size = self.perf_config.get('cache', {}).get('max_size', 500)
            self.cache_ttl = self.perf_config.get('cache', {}).get('response_ttl', 7200)
            self.max_text_length = self.perf_config.get('memory', {}).get('max_document_length', 8000)
            self.max_pdf_pages = getattr(cfg, 'MAX_PDF_PAGES', 10)
            self.pdf_workers = self.perf_config.get('parallel', {}).get('pdf_workers', 4)
            self.chunk_size = self.perf_config.get('parallel', {}).get('chunk_size', 5)

        self.llm = None
        # ìºì‹œ ê´€ë¦¬ ê°œì„  (í¬ê¸° ì œí•œ ë° TTL)
        from collections import OrderedDict
        self.documents_cache = OrderedDict()  # LRU ìºì‹œì²˜ëŸ¼ ë™ì‘
        self.metadata_cache = OrderedDict()  # ë©”íƒ€ë°ì´í„° ìºì‹œ
        self.search_mode = 'document'  # ê²€ìƒ‰ ëª¨ë“œ: 'document', 'asset', 'auto' (ê¸°ë³¸ê°’: document)
        self.answer_cache = OrderedDict()  # ë‹µë³€ ìºì‹œ (LRU)
        self.pdf_text_cache = OrderedDict()  # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”)

        # PDF ë³‘ë ¬ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (ì œê±°ë¨ - ë°±ì—… í´ë”ë¡œ ì´ë™)
        # self.pdf_processor = PDFParallelProcessor(config_manager=cfg if USE_YAML_CONFIG else None)
        # self.error_handler = RAGErrorHandler()
        # self.error_recovery = ErrorRecovery()
        self.pdf_processor = None
        self.error_handler = None
        self.error_recovery = None
        
        # ì‘ë‹µ í¬ë§·í„° ì´ˆê¸°í™”
        self.formatter = ResponseFormatter() if ResponseFormatter else None
        
        # Asset LLM ê°œì„  ëª¨ë“ˆ ì´ˆê¸°í™”
        self.asset_enhancer = None  # í•„ìš”ì‹œ ë¡œë“œ

        # ìì‚° ë°ì´í„° ì œê±° (ê¸°ì•ˆì„œ ì¤‘ì‹¬ ì‹œìŠ¤í…œìœ¼ë¡œ ì „í™˜)
        self.asset_data_cache = None
        self.asset_data = []
        
        # ëª¨ë“  PDFì™€ TXT íŒŒì¼ ëª©ë¡ (ìƒˆë¡œìš´ í´ë” êµ¬ì¡° í¬í•¨)
        self.pdf_files = []
        self.txt_files = []

        # ë£¨íŠ¸ í´ë” íŒŒì¼
        self.pdf_files.extend(list(self.docs_dir.glob('*.pdf')))
        self.txt_files.extend(list(self.docs_dir.glob('*.txt')))

        # ì—°ë„ë³„ í´ë” (year_2014 ~ year_2025)
        for year in range(2014, 2026):
            year_folder = self.docs_dir / f"year_{year}"
            if year_folder.exists():
                self.pdf_files.extend(list(year_folder.glob('*.pdf')))
                self.txt_files.extend(list(year_folder.glob('*.txt')))

        # ì¹´í…Œê³ ë¦¬ë³„ í´ë”ëŠ” ì‹¬ë³¼ë¦­ ë§í¬ë§Œ ìˆìœ¼ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
        # ì‹¤ì œ íŒŒì¼ì€ ëª¨ë‘ year_* í´ë”ì— ìˆìŒ
        # category_folders = ['category_purchase', 'category_repair', 'category_review',
        #                   'category_disposal', 'category_consumables']
        # for folder in category_folders:
        #     cat_folder = self.docs_dir / folder
        #     if cat_folder.exists():
        #         self.pdf_files.extend(list(cat_folder.glob('*.pdf')))
        #         self.txt_files.extend(list(cat_folder.glob('*.txt')))

        # íŠ¹ë³„ í´ë” (ìì‚° ê´€ë ¨ í´ë” ì œê±°)
        special_folders = ['recent', 'archive']
        for folder in special_folders:
            special_folder = self.docs_dir / folder
            if special_folder.exists():
                self.pdf_files.extend(list(special_folder.glob('*.pdf')))
                self.txt_files.extend(list(special_folder.glob('*.txt')))

        # ì¤‘ë³µ ì œê±° (ê°™ì€ íŒŒì¼ì´ ì—¬ëŸ¬ í´ë”ì— ìˆì„ ìˆ˜ ìˆìŒ)
        self.pdf_files = list(set(self.pdf_files))
        self.txt_files = list(set(self.txt_files))
        self.all_files = self.pdf_files + self.txt_files

        print(f"âœ… {len(self.pdf_files)}ê°œ PDF, {len(self.txt_files)}ê°œ TXT ë¬¸ì„œ ë°œê²¬")
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì‚¬ì „ ì¶”ì¶œ (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
        self._build_metadata_cache()
        
        # LLM ì‚¬ì „ ë¡œë“œ ì˜µì…˜
        if preload_llm:
            self._preload_llm()

    def _load_asset_data(self):
        """ìì‚° ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ìºì‹±"""
        try:
            asset_file = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if asset_file.exists():
                with open(asset_file, 'r', encoding='utf-8') as f:
                    self.asset_data_cache = f.read()
                    # ìì‚° ê°œìˆ˜ íŒŒì•…
                    asset_count = len([line for line in self.asset_data_cache.split('\n') if line.strip().startswith('[')])
                    print(f"âœ… ìì‚° ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {asset_count:,}ê°œ ì¥ë¹„")
            else:
                print(f"âš ï¸ ìì‚° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {asset_file}")
                self.asset_data_cache = ""
        except Exception as e:
            print(f"âŒ ìì‚° ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.asset_data_cache = ""
    
    def _preload_llm(self):
        """LLMì„ ë¯¸ë¦¬ ë¡œë“œ (ì‹±ê¸€í†¤ ì‚¬ìš©)"""
        if self.llm is None:
            if not LLMSingleton.is_loaded():
                print("ğŸ¤– LLM ëª¨ë¸ ìµœì´ˆ ë¡œë”© ì¤‘...")
            else:
                print("â™¾ï¸ LLM ëª¨ë¸ ì¬ì‚¬ìš©")
            
            try:
                start = time.time()
                self.llm = LLMSingleton.get_instance(model_path=self.model_path)
                elapsed = time.time() - start
                if elapsed > 1.0:  # 1ì´ˆ ì´ìƒ ê±¸ë¦° ê²½ìš°ë§Œ í‘œì‹œ
                    print(f"âœ… LLM ë¡œë“œ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")
            except Exception as e:
                print(f"âš ï¸ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _manage_cache(self, cache_dict, key, value):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬ - LRU ë°©ì‹"""
        if key in cache_dict:
            # ê¸°ì¡´ í•­ëª©ì„ ëìœ¼ë¡œ ì´ë™ (ê°€ì¥ ìµœê·¼ ì‚¬ìš©)
            cache_dict.move_to_end(key)
        else:
            # ìƒˆ í•­ëª© ì¶”ê°€
            if len(cache_dict) >= self.max_cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                cache_dict.popitem(last=False)
            cache_dict[key] = (value, time.time())  # ê°’ê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
    
    def _get_from_cache(self, cache_dict, key):
        """ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸° (TTL ì²´í¬ í¬í•¨)"""
        if key in cache_dict:
            value, timestamp = cache_dict[key]
            if time.time() - timestamp < self.cache_ttl:
                cache_dict.move_to_end(key)  # ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                return value
            else:
                # TTL ë§Œë£Œ - ì‚­ì œ
                del cache_dict[key]
        return None
    
    def _parse_pdf_result(self, result: Dict) -> Dict:
        """ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return {
            'text': result.get('text', ''),
            'page_count': result.get('page_count', 0),
            'metadata': result.get('metadata', {}),
            'method': result.get('method', 'parallel')
        }

    def process_pdfs_in_batch(self, pdf_paths: List[Path], batch_size: int = 5) -> Dict:
        """ì—¬ëŸ¬ PDFë¥¼ ë°°ì¹˜ë¡œ ë³‘ë ¬ ì²˜ë¦¬"""
        all_results = {}

        for i in range(0, len(pdf_paths), batch_size):
            batch = pdf_paths[i:i + batch_size]
            if logger:
                print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)")
            else:
                print(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)")

            batch_results = self.pdf_processor.process_multiple_pdfs(batch)
            all_results.update(batch_results)

            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            if len(self.pdf_processor.extraction_cache) > 50:
                self.pdf_processor.clear_cache()

        return all_results

    def _find_metadata_by_filename(self, filename: str) -> Optional[Dict]:
        """íŒŒì¼ëª…ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì°¾ê¸° (ìƒˆë¡œìš´ ìºì‹œ êµ¬ì¡° ì§€ì›)"""
        # ë¨¼ì € ì •í™•í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì°¾ê¸°
        if filename in self.metadata_cache:
            return self.metadata_cache[filename]

        # ìƒëŒ€ ê²½ë¡œ í¬í•¨í•œ í‚¤ì—ì„œ ì°¾ê¸°
        for cache_key, metadata in self.metadata_cache.items():
            if metadata.get('filename') == filename:
                return metadata

        return None

    def _build_metadata_cache(self):
        """ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ (ìºì‹± ì§€ì›)"""

        print("ğŸ“Š ë¬¸ì„œ ë©”íƒ€ë°ì´í„° êµ¬ì¶• ì¤‘...")

        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • í™•ì¸
        use_parallel = USE_YAML_CONFIG and cfg.get('parallel_processing.enabled', True)

        if use_parallel and self.pdf_files:
            print(f"ğŸš€ {len(self.pdf_files)}ê°œ PDF ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
            pdf_results = self.process_pdfs_in_batch(self.pdf_files)

            # ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë©”íƒ€ë°ì´í„° ìºì‹œì— ì €ì¥
            for pdf_path, result in pdf_results.items():
                pdf_path_obj = Path(pdf_path)
                filename = pdf_path_obj.name
                # ìƒëŒ€ ê²½ë¡œë¥¼ í‚¤ë¡œ ì‚¬ìš© (ì¤‘ë³µ íŒŒì¼ëª… ì²˜ë¦¬)
                try:
                    relative_path = pdf_path_obj.relative_to(self.docs_dir)
                    cache_key = str(relative_path)
                except ValueError:
                    cache_key = filename

                if 'error' not in result:
                    self.metadata_cache[cache_key] = {
                        'path': pdf_path_obj,
                        'filename': filename,
                        'text': result.get('text', '')[:1000],  # ìš”ì•½ë§Œ ì €ì¥
                        'page_count': result.get('page_count', 0),
                        'metadata': result.get('metadata', {})
                    }
        
        # PDFì™€ TXT íŒŒì¼ ëª¨ë‘ ì²˜ë¦¬
        for file_path in self.all_files:
            filename = file_path.name

            # ìƒëŒ€ ê²½ë¡œë¥¼ ìºì‹œ í‚¤ë¡œ ì‚¬ìš©
            try:
                relative_path = file_path.relative_to(self.docs_dir)
                cache_key = str(relative_path)
            except ValueError:
                cache_key = filename

            # íŒŒì¼ëª…ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            date_match = re.match(r'(\d{4}-\d{2}-\d{2})', filename)
            date = date_match.group(1) if date_match else ""
            
            # ì œëª© ì¶”ì¶œ (ë‚ ì§œ ë’¤ ë¶€ë¶„, í™•ì¥ì ì œê±°)
            if filename.endswith('.pdf'):
                title = filename.replace(date + '_', '').replace('.pdf', '') if date else filename.replace('.pdf', '')
            else:
                title = filename.replace(date + '_', '').replace('.txt', '') if date else filename.replace('.txt', '')
            
            # ì—°ë„ ì¶”ì¶œ
            year = date[:4] if date else ""
            
            # íŒŒì¼ëª…ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
            keywords = []
            
            # íŒŒì¼ëª…ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            # í•œê¸€, ì˜ë¬¸, ìˆ«ìë¡œ ë‹¨ì–´ ì¶”ì¶œ
            words = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', filename)
            
            # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ë‹¨ì–´ë“¤ì„ í‚¤ì›Œë“œë¡œ ì¶”ê°€ (2ê¸€ì ì´ìƒ)
            for word in words:
                if len(word) >= 2 and word not in ['pdf', 'PDF', 'txt', 'TXT', 'ì˜', 'ë°', 'ê±´', 'ê²€í† ì„œ']:
                    keywords.append(word)
            
            self.metadata_cache[cache_key] = {
                'path': file_path,
                'filename': filename,  # ì‹¤ì œ íŒŒì¼ëª… ì €ì¥
                'date': date,
                'year': year,
                'title': title,
                'keywords': keywords,
                'full_text': None,  # ë‚˜ì¤‘ì— í•„ìš”ì‹œ ë¡œë“œ
                'is_txt': filename.endswith('.txt'),  # TXT íŒŒì¼ ì—¬ë¶€
                'is_pdf': filename.endswith('.pdf')  # PDF íŒŒì¼ ì—¬ë¶€ ì¶”ê°€
            }

        print(f"âœ… {len(self.metadata_cache)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° êµ¬ì¶• ì™„ë£Œ")
    
    def _extract_txt_info(self, txt_path: Path) -> Dict:
        """TXT íŒŒì¼ì—ì„œ ì •ë³´ ë™ì  ì¶”ì¶œ"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            info = {'text': text[:3000]}  # ì²˜ìŒ 3000ìë§Œ
            
            # ìì‚° íŒŒì¼ì¸ ê²½ìš° í†µê³„ ì •ë³´ ë™ì  ì¶”ì¶œ
            if 'ìì‚°' in txt_path.name or '7904' in txt_path.name:
                # íŒ¨í„´: "XXX: ìˆ«ìê°œ" ë˜ëŠ” "XXX - ì´ ìˆ«ìê°œ" í˜•ì‹ ìë™ ê°ì§€
                # ë™ì ìœ¼ë¡œ ëª¨ë“  "ì¹´í…Œê³ ë¦¬: ìˆ˜ëŸ‰" íŒ¨í„´ ì°¾ê¸°
                patterns = [
                    r'([ê°€-í£A-Za-z]+).*?[:\-]\s*ì´?\s*(\d+[,\d]*)\s*[ê°œëŒ€]',
                    r'â€¢\s*([ê°€-í£A-Za-z]+):\s*(\d+[,\d]*)\s*[ê°œëŒ€]'
                ]
                
                for pattern in patterns:
                    matches = re.findall(pattern, text[:5000])  # ì²˜ìŒ 5000ìì—ì„œ ì°¾ê¸°
                    for category, count in matches:
                        # ì¹´í…Œê³ ë¦¬ëª…ì„ í‚¤ë¡œ ì‚¬ìš©
                        key = f"{category}ìˆ˜ëŸ‰"
                        info[key] = f"{count}ê°œ"
                
                # ì´ ë³´ìœ  ì¥ë¹„ ì°¾ê¸°
                total_match = re.search(r'ì´\s*ë³´ìœ \s*ì¥ë¹„:\s*(\d+[,\d]*)ê°œ', text)
                if total_match:
                    info['ì´ì¥ë¹„ìˆ˜'] = total_match.group(1) + "ê°œ"
            
            return info

        except Exception as e:
            if logger:
                print(f"âš ï¸ TXT ì½ê¸° ì˜¤ë¥˜ ({txt_path.name}): {e}")
            else:
                print(f"âš ï¸ TXT ì½ê¸° ì˜¤ë¥˜ ({txt_path.name}): {e}")
            # ì—ëŸ¬ í•¸ë“¤ëŸ¬ë¡œ ì•ˆì „í•˜ê²Œ íŒŒì¼ ì½ê¸° ì‹œë„
            # error_handler ì œê±° - ì§ì ‘ íŒŒì¼ ì½ê¸°
            try:
                with open(txt_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                if content:
                    return {'text': content[:3000]}
            except Exception:
                pass
            return {}

    # ë°ì½”ë ˆì´í„° ì œê±° (error_handler ë°±ì—… í´ë”ë¡œ ì´ë™)
    # @RAGErrorHandler.retry_with_backoff(max_retries=3, backoff_factor=1.5)
    # @RAGErrorHandler.handle_pdf_extraction_error

    def _optimize_context(self, text: str, query: str, max_length: int = 3000) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìµœì í™” - ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
        if not text or len(text) <= max_length:
            return text

        # ì¿¼ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query.lower())
        keywords = [k for k in keywords if len(k) >= 2]

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = re.split(r'[.!?\n]+', text)

        # ê° ë¬¸ì¥ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        scored_sentences = []
        for i, sentence in enumerate(sentences):
            if not sentence.strip():
                continue

            score = 0
            sentence_lower = sentence.lower()

            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            for keyword in keywords:
                if keyword in sentence_lower:
                    score += 10

            # ì¤‘ìš” íŒ¨í„´ ë³´ë„ˆìŠ¤
            if re.search(r'\d+[,\d]*\s*ì›', sentence):  # ê¸ˆì•¡
                score += 5
            if re.search(r'\d{4}[-ë…„]', sentence):  # ì—°ë„
                score += 3
            if re.search(r'ì´|í•©ê³„|ì „ì²´', sentence):  # ìš”ì•½ ì •ë³´
                score += 3

            # ìœ„ì¹˜ ì ìˆ˜ (ë¬¸ì„œ ì•ë¶€ë¶„ ì„ í˜¸)
            position_score = max(0, 5 - i * 0.1)
            score += position_score

            scored_sentences.append((sentence, score))

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        # ìƒìœ„ ë¬¸ì¥ë“¤ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        result = []
        current_length = 0

        for sentence, score in scored_sentences:
            if current_length + len(sentence) > max_length:
                break
            result.append(sentence)
            current_length += len(sentence)

        # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬
        result_text = '. '.join(result)
        return result_text if result_text else text[:max_length]

    def _extract_pdf_info_with_retry(self, pdf_path: Path) -> Dict:
        """PDF ì •ë³´ ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬ ë° ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)"""
        # ë³‘ë ¬ ì²˜ë¦¬ê¸° ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        use_parallel = USE_YAML_CONFIG and cfg.get('parallel_processing.enabled', True)

        if use_parallel:
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ PDF ì¶”ì¶œ
            results = self.pdf_processor.process_multiple_pdfs([pdf_path])
            result = results.get(str(pdf_path), {})

            if 'error' not in result:
                return self._parse_pdf_result(result)
            else:
                # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
                if logger:
                    print(f"âš ï¸ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨, ìˆœì°¨ ì²˜ë¦¬ë¡œ í´ë°±: {pdf_path.name}")
                else:
                    print(f"âš ï¸ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨, ìˆœì°¨ ì²˜ë¦¬ë¡œ í´ë°±: {pdf_path.name}")
                return self._extract_pdf_info(pdf_path)
        else:
            # ê¸°ì¡´ ìˆœì°¨ ì²˜ë¦¬
            return self._extract_pdf_info(pdf_path)
    
    def _extract_pdf_info(self, pdf_path: Path) -> Dict:
        """ê¸°ì¡´ PDF ì¶”ì¶œ ë°©ì‹ (í´ë°±ìš©) - ìºì‹± ì ìš©"""
        # ìºì‹œ í‚¤ ìƒì„± (íŒŒì¼ ê²½ë¡œ ê¸°ë°˜)
        cache_key = str(pdf_path)

        # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        if cache_key in self.pdf_text_cache:
            # ìºì‹œ íˆíŠ¸ - LRUë¥¼ ìœ„í•´ ë§¨ ë’¤ë¡œ ì´ë™
            cached_result = self.pdf_text_cache.pop(cache_key)
            self.pdf_text_cache[cache_key] = cached_result
            return cached_result

        text = ""

        # ì—ëŸ¬ í•¸ë“¤ëŸ¬ë¡œ ì•ˆì „í•˜ê²Œ íŒŒì¼ ì½ê¸°
        def extract_with_pdfplumber():
            nonlocal text
            with pdfplumber.open(pdf_path) as pdf:
                # ë¬¸ì„œ ì „ì²´ ë˜ëŠ” ì„¤ì •ëœ ìµœëŒ€ í˜ì´ì§€ ì½ê¸°
                pages_to_read = min(len(pdf.pages), self.max_pdf_pages)
                for page in pdf.pages[:pages_to_read]:
                    # safe_execute ì œê±° (error_handler ë°±ì—… í´ë”ë¡œ ì´ë™)
                    try:
                        page_text = page.extract_text() or ""
                    except Exception:
                        page_text = ""
                    if page_text:
                        text += page_text + "\n"
                        # ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ë©´ ì¤‘ë‹¨ (ë©”ëª¨ë¦¬ ì ˆì•½)
                        if len(text) > self.max_text_length:
                            break
            return text

        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
        # error_recoveryê°€ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ ì‹œë„
        text = extract_with_pdfplumber()
        if not text and hasattr(self, '_try_ocr_extraction'):
            text = self._try_ocr_extraction(pdf_path)

        # pdfplumber ì‹¤íŒ¨ì‹œ OCR ì‹œë„
        if not text:
            try:
                from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
                ocr = EnhancedOCRProcessor()
                text, _ = ocr.extract_text_with_ocr(str(pdf_path))
            except Exception:
                pass  # OCR ì‹¤íŒ¨ì‹œ ë¬´ì‹œ

        if not text:
            return {}

        info = {}

        # ê¸°ì•ˆì ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´)
        patterns = [
            r'ê¸°ì•ˆì[\s:ï¼š]*([ê°€-í£]+)',
            r'ì‘ì„±ì[\s:ï¼š]*([ê°€-í£]+)',
            r'ë‹´ë‹¹ì[\s:ï¼š]*([ê°€-í£]+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                info['ê¸°ì•ˆì'] = match.group(1).strip()
                break
            
        # ë‚ ì§œ ì¶”ì¶œ (ë¬¸ì„œ ë‚´ìš© ìš°ì„ , íŒŒì¼ëª… fallback)
        date_patterns = [
            r'ê¸°ì•ˆì¼[\s:ï¼š]*(\d{4}[-ë…„]\s*\d{1,2}[-ì›”]\s*\d{1,2})',
            r'ì‹œí–‰ì¼ì[\s:ï¼š]*(\d{4}[-ë…„]\s*\d{1,2}[-ì›”]\s*\d{1,2})',
            r'(\d{4}-\d{2}-\d{2})\s*\d{2}:\d{2}',
            r'ì¼ì[\s:ï¼š]*(\d{4}[-./ë…„]\s*\d{1,2}[-./ì›”]\s*\d{1,2})',
            r'ë‚ ì§œ[\s:ï¼š]*(\d{4}[-./ë…„]\s*\d{1,2}[-./ì›”]\s*\d{1,2})',
            r'(\d{4}[./-]\d{1,2}[./-]\d{1,2})'  # ì¼ë°˜ì ì¸ ë‚ ì§œ í˜•ì‹
        ]

        date_found = False
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                info['ë‚ ì§œ'] = match.group(1).strip()
                date_found = True
                break

        # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
        if not date_found:
            filename = pdf_path.name
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (YYYY-MM-DD, YYYY.MM.DD, YYYY_MM_DD ë“±)
            filename_date_patterns = [
                r'(20\d{2}[-_.]0?[1-9]|1[0-2][-_.][0-2]?\d|3[01])',  # YYYY-MM-DD
                r'(20\d{2})[-_.]?(\d{1,2})[-_.]?(\d{1,2})',  # YYYY MM DD (ë¶„ë¦¬ëœ í˜•íƒœ)
            ]

            for pattern in filename_date_patterns:
                match = re.search(pattern, filename)
                if match:
                    if len(match.groups()) == 1:
                        # ì „ì²´ ë§¤ì¹˜ì¸ ê²½ìš°
                        date_str = match.group(1)
                        date_str = date_str.replace('_', '-').replace('.', '-')
                        info['ë‚ ì§œ'] = date_str
                        date_found = True
                        break
                    elif len(match.groups()) == 3:
                        # ë¶„ë¦¬ëœ ê·¸ë£¹ì¸ ê²½ìš°
                        year, month, day = match.groups()
                        try:
                            normalized_date = f"{year}-{int(month):02d}-{int(day):02d}"
                            info['ë‚ ì§œ'] = normalized_date
                            date_found = True
                            break
                        except ValueError:
                            continue
            
            # ë¶€ì„œ ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            # íŒ¨í„´ 1: ê¸°ì•ˆë¶€ì„œ ë¼ë²¨
            dept_match = re.search(r'ê¸°ì•ˆë¶€ì„œ[\s:ï¼š]*([^\nì‹œí–‰]+)', text)
            if dept_match:
                dept = dept_match.group(1).strip()
                dept = dept.split('ì‹œí–‰')[0].strip()
                info['ë¶€ì„œ'] = dept
            
            # íŒ¨í„´ 2: íŒ€-íŒŒíŠ¸ í˜•ì‹ (ì±„ë„A ìŠ¤íƒ€ì¼)
            if 'ë¶€ì„œ' not in info:
                team_match = re.search(r'([ê°€-í£]+íŒ€[\-ê°€-í£]+íŒŒíŠ¸)', text)
                if team_match:
                    info['ë¶€ì„œ'] = team_match.group(1)
            
            # ê¸ˆì•¡ ì¶”ì¶œ (ê°€ì¥ í° ê¸ˆì•¡)
            amounts = re.findall(r'(\d{1,3}(?:,\d{3})*)\s*ì›', text)
            if amounts:
                numeric_amounts = []
                for amt in amounts:
                    try:
                        num = int(amt.replace(',', ''))
                        numeric_amounts.append((num, amt))
                    except (ValueError, AttributeError):
                        pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
                if numeric_amounts:
                    numeric_amounts.sort(reverse=True)
                    info['ê¸ˆì•¡'] = f"{numeric_amounts[0][1]}ì›"
            
            # ì œëª© ì¶”ì¶œ
            title_match = re.search(r'ì œëª©[\s:ï¼š]*([^\n]+)', text)
            if title_match:
                info['ì œëª©'] = title_match.group(1).strip()
            
            # í…ìŠ¤íŠ¸ ì €ì¥
            info['text'] = text[:3000]  # ì²˜ìŒ 3000ìë§Œ

        # ìºì‹œì— ì €ì¥ (í¬ê¸° ì œí•œ ì ìš©)
        MAX_PDF_CACHE_SIZE = 50  # ìµœëŒ€ 50ê°œ PDF ìºì‹±
        if len(self.pdf_text_cache) >= MAX_PDF_CACHE_SIZE:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)
            self.pdf_text_cache.popitem(last=False)

        # ìƒˆ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        self.pdf_text_cache[cache_key] = info

        return info
    
    def find_best_document(self, query: str) -> Optional[Path]:
        """ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë¬¸ì„œ ì°¾ê¸° - ë™ì  ë§¤ì¹­"""
        
        query_lower = query.lower()
        
        # PDF ë¬¸ì„œ ìš°ì„  ì²˜ë¦¬ í‚¤ì›Œë“œ í™•ì¥
        pdf_priority_keywords = [
            'ìˆ˜ë¦¬', 'ë³´ìˆ˜', 'ë‚´ìš©', 'ìš”ì•½', 'ê²€í† ì„œ', 'ê¸°ìˆ ê²€í† ',
            'êµì²´', 'êµ¬ë§¤', 'íê¸°', 'ì†Œëª¨í’ˆ', 'ê¸°ì•ˆ', 'ê²€í† ',
            'ì–´ë–¤', 'ë¬´ì—‡', 'ë­', 'ë­˜ë¡œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€'
        ]
        
        # PDF ë¬¸ì„œê°€ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ê²½ìš° ìš°ì„  ì²˜ë¦¬
        if any(keyword in query for keyword in pdf_priority_keywords):
            # PDF ë¬¸ì„œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¾ê¸°
            is_asset_query = False
        else:
            # ìì‚° ê´€ë ¨ ì§ˆë¬¸ íŒ¨í„´ ë™ì  ê°ì§€ (í•˜ë“œì½”ë”© ì—†ì´)
            is_asset_query = False
            
            # ì¥ë¹„ ì œì¡°ì‚¬ëª… íŒ¨í„´ (ëŒ€ë¬¸ì 3ê¸€ì ì´ìƒ ë˜ëŠ” ì•Œë ¤ì§„ ì œì¡°ì‚¬)
            manufacturer_pattern = r'\b[A-Z]{3,}\b|SONY|Sony|Harris|Toshiba|Panasonic|Canon|Nikon'
            
            # ìì‚° ë°ì´í„° íŠ¹ì • í‚¤ì›Œë“œë§Œ
            asset_specific_keywords = ['ì‹œë¦¬ì–¼', 'S/N', 'ë‹´ë‹¹ìë³„', 'ìœ„ì¹˜ë³„', 'ì œì¡°ì‚¬ë³„']
            
            if ('ëª‡' in query and 'ëŒ€' in query) or \
               ('ìˆ˜ëŸ‰' in query and 'ì „ì²´' in query) or \
               ('ìì‚°' in query and 'ë°ì´í„°' in query) or \
               re.search(r'\d{6,}', query) or \
               any(keyword in query for keyword in asset_specific_keywords):
                is_asset_query = True
        
        # ìì‚° ê´€ë ¨ ì§ˆë¬¸ì´ë©´ ìì‚° íŒŒì¼ ìë™ ì°¾ê¸°
        if is_asset_query:
            for cache_key, metadata in self.metadata_cache.items():
                # TXT íŒŒì¼ì´ê³  íŒŒì¼ëª…ì— 'ìì‚°' ë˜ëŠ” í° ìˆ«ì(7904 ë“±)ê°€ ìˆìœ¼ë©´
                if metadata.get('is_txt', False):
                    filename = metadata.get('filename', cache_key)
                    if 'ìì‚°' in filename or '7904' in filename or 'ì „ì²´' in filename:
                        print(f"ğŸ“Š ìì‚° íŒŒì¼ ì„ íƒ: {filename}")
                        return metadata['path']
        
        candidates = []
        
        # ê¸°ì¡´ ë¡œì§ ê³„ì†
        # ì§ˆë¬¸ ì •ê·œí™” ë° í† í°í™”
        query_tokens = set(query_lower.split())
        
        # ì—°ë„ì™€ ì›” ì¶”ì¶œ
        year_match = re.search(r'(20\d{2})', query)
        query_year = year_match.group(1) if year_match else None
        
        # ì›” ì¶”ì¶œ (1ì›”, 01ì›”, 1-ì›” ë“± ë‹¤ì–‘í•œ í˜•ì‹)
        month_match = re.search(r'(\d{1,2})\s*ì›”', query)
        query_month = None
        if month_match:
            query_month = int(month_match.group(1))
        
        for cache_key, metadata in self.metadata_cache.items():
            score = 0
            filename = metadata.get('filename', cache_key)
            filename_lower = filename.lower()
            
            # 1. ì—°ë„ì™€ ì›” ë§¤ì¹­
            if query_year:
                if metadata['year'] == query_year:
                    score += 20
                    
                    # ì›”ë„ ì§€ì •ëœ ê²½ìš° ì›”ê¹Œì§€ ì²´í¬
                    if query_month:
                        # íŒŒì¼ëª…ì—ì„œ ì›” ì¶”ì¶œ (YYYY-MM-DD í˜•ì‹)
                        file_month_match = re.search(r'\d{4}-(\d{2})-\d{2}', filename)
                        if file_month_match:
                            file_month = int(file_month_match.group(1))
                            if file_month == query_month:
                                score += 30  # ì›”ê¹Œì§€ ì¼ì¹˜í•˜ë©´ ë†’ì€ ì ìˆ˜
                            else:
                                continue  # ì›”ì´ ë‹¤ë¥´ë©´ ì œì™¸
                else:
                    continue  # ì—°ë„ê°€ ë‹¤ë¥´ë©´ ì œì™¸
            
            # 2. íŠ¹ì • ì¥ë¹„/ì¥ì†Œëª… ì •í™• ë§¤ì¹­ (ë§¤ìš° ë†’ì€ ê°€ì¤‘ì¹˜)
            # ë™ì  í‚¤ì›Œë“œ ë§¤ì¹­ - í•˜ë“œì½”ë”© ì—†ì´ ìë™ìœ¼ë¡œ
            # ì§ˆë¬¸ì˜ ë‹¨ì–´ë“¤ì„ ì¶”ì¶œ
            query_words = re.findall(r'[ê°€-í£]+|[A-Za-z]+|[0-9]+', query_lower)
            filename_words = re.findall(r'[ê°€-í£]+|[A-Za-z]+|[0-9]+', filename_lower)
            
            # ê³µí†µ ë‹¨ì–´ ì°¾ê¸° (2ê¸€ì ì´ìƒ)
            for q_word in query_words:
                if len(q_word) >= 2:
                    for f_word in filename_words:
                        if len(f_word) >= 2:
                            # ì™„ì „ ì¼ì¹˜
                            if q_word == f_word:
                                # ë‹¨ì–´ ê¸¸ì´ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë¶€ì—¬
                                weight = len(q_word) * 2
                                score += weight
                            # ìœ ì‚¬ë„ ê²€ì‚¬ (ì˜¤íƒ€ ì²˜ë¦¬)
                            elif self._calculate_similarity(q_word, f_word) >= 0.8:
                                # 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ë§¤ì¹­ìœ¼ë¡œ ê°„ì£¼
                                weight = len(q_word) * 1.5
                                score += weight
                            # ë¶€ë¶„ ì¼ì¹˜ (ê¸´ ë‹¨ì–´ì¼ ê²½ìš°)
                            elif len(q_word) >= 3 and len(f_word) >= 3:
                                if q_word in f_word or f_word in q_word:
                                    weight = min(len(q_word), len(f_word))
                                    score += weight
            
            # 3. í‚¤ì›Œë“œ ë§¤ì¹­ (ë©”íƒ€ë°ì´í„°)
            for keyword in metadata['keywords']:
                if keyword.lower() in query_lower:
                    score += 5
            
            # 4. í† í° ê¸°ë°˜ ìœ ì‚¬ë„ (ë‹¨ì–´ ê²¹ì¹¨)
            filename_tokens = set(filename_lower.replace('_', ' ').replace('-', ' ').split())
            common_tokens = query_tokens & filename_tokens
            score += len(common_tokens) * 2
            
            # 5. ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
            # ì§ˆë¬¸ì˜ ì£¼ìš” ë‹¨ì–´ê°€ íŒŒì¼ëª…ì— í¬í•¨ë˜ëŠ”ì§€
            important_words = [w for w in query_lower.split() if len(w) > 2]
            for word in important_words:
                if word in filename_lower:
                    score += 3
            
            # 6. ë¬¸ì„œ íƒ€ì…ë³„ íŠ¹ë³„ ì²˜ë¦¬
            # "ê²€í† ì„œ", "ìš”ì²­ì˜ ê±´" ë“± ë¬¸ì„œ íƒ€ì… ë§¤ì¹­
            doc_types = ['ê²€í† ì„œ', 'ìš”ì²­ì˜ ê±´', 'ê¸°ìˆ ê²€í† ì„œ', 'êµ¬ë§¤ê²€í† ì˜ ê±´', 'ë³´ìˆ˜ê±´']
            for doc_type in doc_types:
                if doc_type in query and doc_type in filename:
                    score += 5
            
            if score > 0:
                candidates.append((score, metadata['path'], filename))
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        candidates.sort(reverse=True)
        
        # ë””ë²„ê¹… ì¶œë ¥ (ìƒìœ„ 3ê°œ)
        if candidates:
            top_score = candidates[0][0]
            # ë™ì ìê°€ ìˆëŠ”ì§€ í™•ì¸
            same_score = [c for c in candidates if c[0] == top_score]
            if len(same_score) > 1:
                # ë™ì ì¼ ë•ŒëŠ” íŒŒì¼ëª… ê¸¸ì´ê°€ ì§§ì€ ê²ƒ ìš°ì„ 
                same_score.sort(key=lambda x: len(x[2]))
                return same_score[0][1]
            return candidates[0][1]
        
        return None

    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)
        ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ + í•œê¸€ ìëª¨ ë¶„í•´ ë¹„êµ
        """
        # ê¸¸ì´ê°€ ë„ˆë¬´ ë‹¤ë¥´ë©´ ë‚®ì€ ìœ ì‚¬ë„
        if abs(len(str1) - len(str2)) > 2:
            return 0.0

        # ê°„ë‹¨í•œ ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚°
        def levenshtein_distance(s1: str, s2: str) -> int:
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)

            if len(s2) == 0:
                return len(s1)

            previous_row = range(len(s2) + 1)
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    # ì‚½ì…, ì‚­ì œ, ì¹˜í™˜ ë¹„ìš© ê³„ì‚°
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row

            return previous_row[-1]

        # ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
        distance = levenshtein_distance(str1, str2)
        max_len = max(len(str1), len(str2))
        similarity = 1 - (distance / max_len) if max_len > 0 else 1.0

        # í•œê¸€ íŠ¹ìˆ˜ ì²˜ë¦¬: ììŒ/ëª¨ìŒ í•˜ë‚˜ ì°¨ì´ëŠ” ë†’ì€ ìœ ì‚¬ë„
        # ì˜ˆ: ì¼/ìº , ì½¤/ì»´ ë“±
        if len(str1) == len(str2) and distance == 1:
            # í•œê¸€ì¸ ê²½ìš° ìœ ì‚¬ë„ ë³´ì •
            if all(ord('ê°€') <= ord(c) <= ord('í£') for c in str1 + str2):
                similarity = max(similarity, 0.85)

        return similarity

    def get_document_info(self, file_path: Path, info_type: str = "all") -> str:
        """ë¬¸ì„œì—ì„œ íŠ¹ì • ì •ë³´ ì¶”ì¶œ (PDF/TXT ëª¨ë‘ ì§€ì›)"""
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{file_path.name}_{info_type}"
        if cache_key in self.documents_cache:
            return self.documents_cache[cache_key]
        
        # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì •ë³´ ì¶”ì¶œ
        if file_path.suffix == '.txt':
            info = self._extract_txt_info(file_path)
        else:
            info = self._extract_pdf_info(file_path)
        
        if not info:
            return "âŒ ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        
        result = ""
        
        # ìì‚° íŒŒì¼ íŠ¹ë³„ ì²˜ë¦¬ (ë™ì )
        if file_path.suffix == '.txt' and ('ìì‚°' in file_path.name or '7904' in file_path.name):
            if info_type == "ìì‚°í†µê³„":
                result = f"ğŸ“Š ì±„ë„A ë°©ì†¡ì¥ë¹„ ìì‚° í˜„í™©\n"
                result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                # ë™ì ìœ¼ë¡œ ëª¨ë“  ìˆ˜ëŸ‰ ì •ë³´ í‘œì‹œ
                for key, value in info.items():
                    if 'ìˆ˜ëŸ‰' in key or 'ì¥ë¹„ìˆ˜' in key:
                        # ì¹´í…Œê³ ë¦¬ëª… ì¶”ì¶œ
                        category = key.replace('ìˆ˜ëŸ‰', '').replace('ì¥ë¹„ìˆ˜', '')
                        if category:
                            result += f"â€¢ {category}: {value}\n"
            elif info_type == "all":
                result = f"ğŸ“„ {file_path.stem}\n"
                result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                for key, value in info.items():
                    if key != 'text':
                        result += f"â€¢ {key}: {value}\n"
            else:
                # íŠ¹ì • ì •ë³´ ìš”ì²­
                if info_type in info:
                    result = f"âœ… {info_type}: {info[info_type]}"
                else:
                    result = f"âŒ {info_type} ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        # ê¸°ì¡´ PDF ì²˜ë¦¬
        elif info_type == "ê¸°ì•ˆì":
            result = f"âœ… ê¸°ì•ˆì: {info.get('ê¸°ì•ˆì', 'ì •ë³´ ì—†ìŒ')}"
        elif info_type == "ë‚ ì§œ":
            result = f"âœ… ë‚ ì§œ: {info.get('ë‚ ì§œ', 'ì •ë³´ ì—†ìŒ')}"
        elif info_type == "ë¶€ì„œ":
            result = f"âœ… ë¶€ì„œ: {info.get('ë¶€ì„œ', 'ì •ë³´ ì—†ìŒ')}"
        elif info_type == "ê¸ˆì•¡":
            amount = info.get('ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')
            result = f"âœ… ê¸ˆì•¡: {amount}"
            
            # ì§§ì€ ë‹µë³€ ë³´ì™„ - ì¶”ê°€ ì •ë³´ ì œê³µ
            if amount != 'ì •ë³´ ì—†ìŒ' and len(result) < 50:
                # ë¬¸ì„œ ì •ë³´ ì¶”ê°€
                result += f"\n\nğŸ“„ ë¬¸ì„œ ì •ë³´:\n"
                result += f"â€¢ ë¬¸ì„œëª…: {file_path.stem}\n"
                if 'ë‚ ì§œ' in info:
                    result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
                if 'ê¸°ì•ˆì' in info:
                    result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
                if 'ë¶€ì„œ' in info:
                    result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
                if 'ì œëª©' in info:
                    result += f"â€¢ ì œëª©: {info['ì œëª©']}\n"
        elif info_type == "ìš”ì•½":
            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
            result = f"ğŸ“ {file_path.stem} ìš”ì•½\n"
            result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            if 'ê¸°ì•ˆì' in info:
                result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
            if 'ë‚ ì§œ' in info:
                result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
            if 'ë¶€ì„œ' in info:
                result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
            if 'ê¸ˆì•¡' in info:
                result += f"â€¢ ê¸ˆì•¡: {info['ê¸ˆì•¡']}\n"
            if 'ì œëª©' in info:
                result += f"â€¢ ì œëª©: {info['ì œëª©']}\n"
        else:  # all
            # LLM ì‘ë‹µê³¼ ìœ ì‚¬í•œ í¬ë§·ìœ¼ë¡œ í†µì¼
            result = f"ğŸ“„ {file_path.stem}\n\n"
            result += f"ğŸ“Œ **ê¸°ë³¸ ì •ë³´**\n"
            if 'ê¸°ì•ˆì' in info:
                result += f"â€¢ ê¸°ì•ˆì: {info['ê¸°ì•ˆì']}\n"
            if 'ë‚ ì§œ' in info:
                result += f"â€¢ ë‚ ì§œ: {info['ë‚ ì§œ']}\n"
            if 'ë¶€ì„œ' in info:
                result += f"â€¢ ë¶€ì„œ: {info['ë¶€ì„œ']}\n"
            
            result += f"\nğŸ“ **ì£¼ìš” ë‚´ìš©**\n"
            
            # text í•„ë“œì—ì„œ ì£¼ìš” ë‚´ìš© ì¶”ì¶œ (ê°œì„ ëœ ìš”ì•½ ì‹œìŠ¤í…œ)
            if 'text' in info:
                summary = self._generate_smart_summary(info['text'], file_path)
                result += summary
            
            if 'ê¸ˆì•¡' in info and info['ê¸ˆì•¡'] != 'ì •ë³´ ì—†ìŒ':
                result += f"\nğŸ’° **ë¹„ìš© ì •ë³´**\n"
                result += f"â€¢ ê¸ˆì•¡: {info['ê¸ˆì•¡']}\n"
            
            result += f"\nğŸ“ ì¶œì²˜: {file_path.name}"
        
        # ìºì‹œ ì €ì¥
        self.documents_cache[cache_key] = result
        
        return result

    def _generate_smart_summary(self, text: str, file_path: Path) -> str:
        """ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì˜ë¯¸ ìˆëŠ” ìš”ì•½ ìƒì„±"""

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = text[:3000]  # ì²˜ìŒ 3000ìë§Œ ì‚¬ìš©
        lines = [line.strip() for line in text.split('\n') if line.strip()]

        summary_parts = []

        # 1. ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ì¥ë¹„ëª…, ê¸ˆì•¡, ì—…ì²´ ë“±)
        equipment_keywords = []
        financial_keywords = []
        company_keywords = []

        # ì¥ë¹„ëª… íŒ¨í„´ (ì˜ë¬¸ ëŒ€ë¬¸ì + ìˆ«ì ì¡°í•©)
        equipment_pattern = r'\b[A-Z][A-Za-z0-9\-]{2,20}\b'
        equipment_matches = re.findall(equipment_pattern, text)
        for match in equipment_matches:
            if len(match) >= 3 and not match.isdigit():
                equipment_keywords.append(match)

        # ê¸ˆì•¡ ì •ë³´
        amount_pattern = r'(\d{1,3}(?:,\d{3})*)\s*(?:ì›|ë§Œì›|ì–µ)'
        amount_matches = re.findall(amount_pattern, text)
        if amount_matches:
            financial_keywords.extend([f"{amt}ì›" for amt in amount_matches[:3]])

        # ì—…ì²´ëª… (ì£¼ì‹íšŒì‚¬, (ì£¼), ë²•ì¸ëª… ë“±)
        company_pattern = r'(?:ì£¼ì‹íšŒì‚¬\s*|ãˆœ\s*|\(ì£¼\)\s*)?([ê°€-í£A-Za-z]{2,20})(?:\s*ì£¼ì‹íšŒì‚¬|\s*ãˆœ|\s*\(ì£¼\))?'
        company_matches = re.findall(company_pattern, text)
        for match in company_matches:
            if len(match) >= 2 and match not in ['ê¸°ì•ˆì', 'ë¶€ì„œ', 'ë‚ ì§œ', 'ì‹œí–‰']:
                company_keywords.append(match)

        # 2. ë¬¸ì„œ ìœ í˜•ë³„ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        file_name = file_path.name.lower()

        if 'êµ¬ë§¤' in file_name or 'êµ¬ì…' in file_name:
            # êµ¬ë§¤ ë¬¸ì„œ
            purchase_info = []
            if equipment_keywords:
                purchase_info.append(f"êµ¬ë§¤ ì¥ë¹„: {', '.join(set(equipment_keywords[:3]))}")
            if financial_keywords:
                purchase_info.append(f"ì˜ˆìƒ ë¹„ìš©: {', '.join(financial_keywords[:2])}")
            if company_keywords:
                purchase_info.append(f"ê´€ë ¨ ì—…ì²´: {', '.join(set(company_keywords[:2]))}")

            if purchase_info:
                summary_parts.append("ğŸ›’ " + " | ".join(purchase_info))

        elif 'ìˆ˜ë¦¬' in file_name or 'ë³´ìˆ˜' in file_name:
            # ìˆ˜ë¦¬ ë¬¸ì„œ
            repair_info = []
            if equipment_keywords:
                repair_info.append(f"ìˆ˜ë¦¬ ëŒ€ìƒ: {', '.join(set(equipment_keywords[:3]))}")
            if financial_keywords:
                repair_info.append(f"ìˆ˜ë¦¬ ë¹„ìš©: {', '.join(financial_keywords[:2])}")

            if repair_info:
                summary_parts.append("ğŸ”§ " + " | ".join(repair_info))

        elif 'íê¸°' in file_name:
            # íê¸° ë¬¸ì„œ
            disposal_info = []
            if equipment_keywords:
                disposal_info.append(f"íê¸° ì¥ë¹„: {', '.join(set(equipment_keywords[:3]))}")

            if disposal_info:
                summary_parts.append("ğŸ—‘ï¸ " + " | ".join(disposal_info))

        # 3. ì¼ë°˜ì ì¸ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
        important_lines = []
        priority_keywords = [
            'ëª©ì ', 'ê°œìš”', 'ì£¼ìš”ë‚´ìš©', 'ê²€í† ê²°ê³¼', 'ê²°ë¡ ', 'ì˜ê²¬',
            'ìŠ¹ì¸', 'ë°˜ë ¤', 'ë³´ì™„', 'ì¶”ì§„', 'ê³„íš', 'ì¼ì •',
            '1.', '2.', '3.', 'â‘ ', 'â‘¡', 'â‘¢', 'â—¦', 'â–¶'
        ]

        for line in lines[:30]:  # ì²˜ìŒ 30ì¤„ ê²€í† 
            if len(line) > 15:  # ì˜ë¯¸ ìˆëŠ” ê¸¸ì´ì˜ ë¬¸ì¥ë§Œ
                # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¼ì¸
                for keyword in priority_keywords:
                    if keyword in line:
                        cleaned_line = line.replace(keyword, '').strip()
                        if len(cleaned_line) > 10:
                            important_lines.append(f"â€¢ {cleaned_line[:80]}{'...' if len(cleaned_line) > 80 else ''}")
                        break

        # ì¤‘ìš”í•œ ë¼ì¸ì´ ì—†ìœ¼ë©´ ì²˜ìŒ ëª‡ ë¼ì¸ ì‚¬ìš©
        if not important_lines:
            for line in lines[:5]:
                if len(line) > 20 and not line.isdigit():
                    important_lines.append(f"â€¢ {line[:80]}{'...' if len(line) > 80 else ''}")
                    if len(important_lines) >= 3:
                        break

        # ìµœì¢… ìš”ì•½ ì¡°í•©
        result = ""
        if summary_parts:
            result += "\n".join(summary_parts) + "\n\n"

        if important_lines:
            result += "\n".join(important_lines[:4])  # ìµœëŒ€ 4ì¤„

        return result if result else "â€¢ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ì¤‘..."

    def _remove_duplicate_documents(self, documents: list) -> list:
        """ì¤‘ë³µ ë¬¸ì„œ ì œê±° (íŒŒì¼ëª… ê¸°ì¤€)"""
        seen = set()
        unique_docs = []

        for doc in documents:
            # íŒŒì¼ëª…ì—ì„œ ê²½ë¡œ ë¶€ë¶„ ì œê±°í•˜ì—¬ ë¹„êµ
            filename = Path(doc.get('path', doc.get('filename', ''))).name
            if filename not in seen:
                seen.add(filename)
                unique_docs.append(doc)

        return unique_docs

    def _format_enhanced_response(self, results: list, query: str) -> str:
        """ê°œì„ ëœ ì‘ë‹µ í˜•ì‹"""
        if not results:
            return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ì¤‘ë³µ ì œê±°
        unique_results = self._remove_duplicate_documents(results)

        response = f"ğŸ“‹ **ê²€ìƒ‰ ê²°ê³¼** ({len(unique_results)}ê°œ ë¬¸ì„œ)\n\n"

        for i, doc in enumerate(unique_results[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            title = doc.get('title', 'ì œëª© ì—†ìŒ')
            date = doc.get('date', 'ë‚ ì§œ ë¯¸ìƒ')
            category = doc.get('category', 'ê¸°íƒ€')
            drafter = doc.get('drafter', 'ë¯¸ìƒ')

            # ë‚ ì§œ í‘œì‹œ ê°œì„ 
            if date and date != 'ë‚ ì§œ ë¯¸ìƒ' and len(date) >= 10:
                display_date = date[:10]  # YYYY-MM-DD
            elif date and len(date) >= 4:
                display_date = date[:4]  # ì—°ë„ë§Œ
            else:
                display_date = "ë‚ ì§œë¯¸ìƒ"

            response += f"**{i}. [{category}] {title}**\n"
            response += f"   ğŸ“… {display_date} | ğŸ‘¤ {drafter}\n"

            # ë¬¸ì„œ ìš”ì•½ ì¶”ê°€
            if 'path' in doc:
                try:
                    file_path = Path(doc['path'])
                    if file_path.exists():
                        summary = self._generate_smart_summary("", file_path)
                        if summary and summary != "â€¢ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ì¤‘...":
                            response += f"   {summary}\n"
                except Exception:
                    pass

            response += "\n"

        if len(unique_results) > 5:
            response += f"... ì™¸ {len(unique_results) - 5}ê°œ ë¬¸ì„œ ë” ìˆìŒ\n\n"

        response += "ğŸ’¡ **íŠ¹ì • ë¬¸ì„œë¥¼ ì„ íƒí•˜ì—¬ ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.**"

        return response

    def _classify_search_intent(self, query: str) -> str:
        """ê²€ìƒ‰ ì˜ë„ ë¶„ë¥˜: document(ê¸°ì•ˆì„œ) vs asset(ìì‚°)
        
        Returns:
            'document': PDF ê¸°ì•ˆì„œ/ê²€í† ì„œ ê²€ìƒ‰
            'asset': ìì‚° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
        """
        query_lower = query.lower()
        
        # ëª…ì‹œì  ë¬¸ì„œ ê´€ë ¨ í‚¤ì›Œë“œ (í™•ì¥)
        document_keywords = [
            'ê¸°ì•ˆ', 'ê²€í† ì„œ', 'ìˆ˜ë¦¬', 'ë³´ìˆ˜', 'êµ¬ë§¤ ê±´', 'íê¸°', 
            'ë¬¸ì„œ', 'pdf', 'ë‚´ìš© ìš”ì•½', 'ìƒì„¸ ë‚´ìš©', 'ì ˆì°¨',
            'ê¸°ì•ˆì', 'ë‹´ë‹¹ì', 'ì—…ì²´ëª…', 'ê²€í†  ê²°ê³¼',
            'êµì²´', 'ëŒ€ì²´', 'ì–´ë–¤', 'ë¬´ì—‡', 'ë­', 'ë­˜ë¡œ',
            'ë¯¸ëŸ¬í´ë©', 'ë¯¸ë¼í´ë©', 'ì‚¼ê°ëŒ€'
        ]
        
        # ëª…ì‹œì  ìì‚° ê´€ë ¨ í‚¤ì›Œë“œ
        asset_keywords = [
            'ëª‡ ëŒ€', 'ìˆ˜ëŸ‰', 'ë³´ìœ ', 'ìì‚°', 'ì‹œë¦¬ì–¼', 's/n',
            'í˜„í™©', 'í†µê³„', 'ëª©ë¡', 'ì „ì²´ ì¥ë¹„', 'ì„¤ì¹˜ ìœ„ì¹˜',
            'ë„ì… ì‹œê¸°', 'ì œì¡°ì‚¬ë³„', 'ëª¨ë¸ë³„', 'ìœ„ì¹˜ë³„'
        ]
        
        # íŒ¨í„´ ê°€ì ¸ì˜¤ê¸°
        manufacturer_pattern = self._get_manufacturer_pattern()
        model_pattern = self._get_model_pattern()
        
        # ì ìˆ˜ ê¸°ë°˜ ë¶„ë¥˜
        doc_score = 0
        asset_score = 0
        
        # ë¬¸ì„œ ì ìˆ˜ ê³„ì‚°
        for keyword in document_keywords:
            if keyword in query_lower:
                doc_score += 2
        
        # ìì‚° ì ìˆ˜ ê³„ì‚°
        for keyword in asset_keywords:
            if keyword in query_lower:
                asset_score += 2
        
        # ë‚ ì§œê°€ ìˆìœ¼ë©´ì„œ 'êµ¬ë§¤', 'ìˆ˜ë¦¬' ë“±ì´ ìˆìœ¼ë©´ ë¬¸ì„œ
        if re.search(r'20\d{2}ë…„', query) and any(w in query for w in ['êµ¬ë§¤', 'ìˆ˜ë¦¬', 'ë³´ìˆ˜', 'ê²€í† ']):
            doc_score += 3
        
        # ì œì¡°ì‚¬ë‚˜ ëª¨ë¸ëª…ì´ ìˆìœ¼ë©´ ìì‚°
        if re.search(manufacturer_pattern, query) or re.search(model_pattern, query, re.IGNORECASE):
            asset_score += 2

        # ì¥ë¹„ëª…ì´ ìˆìœ¼ë©´ ìì‚° ê°€ì¤‘ì¹˜ ì¦ê°€ (DVR, CCU ë“±)
        # í•˜ì§€ë§Œ 'ê´€ë ¨ ë¬¸ì„œ', 'ì°¾ì•„ì¤˜' ê°™ì€ ë¬¸ì„œ ê²€ìƒ‰ í‘œí˜„ì´ ìˆìœ¼ë©´ ì œì™¸
        if not any(w in query_lower for w in ['ë¬¸ì„œ', 'ì°¾ì•„ì¤˜', 'ê²€ìƒ‰', 'ê¸°ì•ˆ', 'ê²€í† ']):
            equipment_names = ['dvr', 'ccu', 'ì¹´ë©”ë¼', 'ë Œì¦ˆ', 'ëª¨ë‹ˆí„°', 'ìŠ¤ìœ„ì²˜', 'ë§ˆì´í¬', 'ë¯¹ì„œ']
            for equipment in equipment_names:
                if equipment in query_lower:
                    asset_score += 3  # ë†’ì€ ê°€ì¤‘ì¹˜
        
        # ìˆ˜ëŸ‰ ê´€ë ¨ í‘œí˜„ì´ ìˆìœ¼ë©´ ìì‚°
        if re.search(r'\d+ëŒ€|\d+ê°œ|ëª‡\s*ëŒ€|ëª‡\s*ê°œ', query):
            asset_score += 3
        
        # ìµœì¢… ê²°ì •
        if doc_score > asset_score:
            return 'document'
        elif asset_score > doc_score:
            return 'asset'
        else:
            # ì ìˆ˜ê°€ ê°™ìœ¼ë©´ ë¬¸ë§¥ìœ¼ë¡œ íŒë‹¨
            if 'ë¬¸ì„œ' in query or 'ë‚´ìš©' in query or 'ìš”ì•½' in query:
                return 'document'
            elif 'ì¥ë¹„' in query or 'í˜„í™©' in query:
                return 'asset'
            else:
                return 'document'  # ê¸°ë³¸ê°’
    
    def answer_from_specific_document(self, query: str, filename: str) -> str:
        """íŠ¹ì • ë¬¸ì„œì— ëŒ€í•´ì„œë§Œ ë‹µë³€ ìƒì„± (ë¬¸ì„œ ì „ìš© ëª¨ë“œ) - ì´ˆìƒì„¸ ë²„ì „
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            filename: íŠ¹ì • ë¬¸ì„œ íŒŒì¼ëª…
        """
        print(f"ğŸ“„ ë¬¸ì„œ ì „ìš© ëª¨ë“œ: {filename}")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ í•´ë‹¹ ë¬¸ì„œ ì°¾ê¸°
        doc_metadata = self._find_metadata_by_filename(filename)
        if not doc_metadata:
            return f"âŒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
        doc_path = doc_metadata['path']
        
        # PDFì¸ì§€ TXTì¸ì§€ í™•ì¸
        if filename.endswith('.pdf'):
            # PDF ë¬¸ì„œ ì²˜ë¦¬ - ì „ì²´ ë‚´ìš© ì¶”ì¶œ
            info = self._extract_pdf_info_with_retry(doc_path)
            if not info.get('text'):
                return f"âŒ PDF ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
            
            # LLM ì´ˆê¸°í™”
            if self.llm is None:
                print("ğŸ¤– LLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
                self._preload_llm()
            
            # ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì‚¬ìš© (15000ìë¡œ í™•ëŒ€)
            full_text = info['text'][:15000]
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
            if any(word in query for word in ['ìš”ì•½', 'ì •ë¦¬', 'ê°œìš”', 'ë‚´ìš©']):
                prompt = self._create_detailed_summary_prompt(query, full_text, filename)
            elif any(word in query for word in ['ìƒì„¸', 'ìì„¸íˆ', 'êµ¬ì²´ì ', 'ì„¸ì„¸íˆ', 'ì„¸ë¶€']):
                prompt = self._create_ultra_detailed_prompt(query, full_text, filename)
            elif any(word in query for word in ['í’ˆëª©', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'í•­ëª©']):
                prompt = self._create_itemized_list_prompt(query, full_text, filename)
            else:
                prompt = self._create_document_specific_prompt(query, full_text, filename)
            
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ë” ê¸´ ë‹µë³€ í—ˆìš©)
            try:
                # ë¬¸ì„œ ì „ìš© ëª¨ë“œì—ì„œëŠ” ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
                context_chunks = [
                    {
                        'content': full_text,
                        'source': filename,
                        'metadata': {
                            'filename': filename,
                            'date': doc_metadata.get('date', ''),
                            'title': doc_metadata.get('title', ''),
                            'is_document_only_mode': True  # ë¬¸ì„œ ì „ìš© ëª¨ë“œ í‘œì‹œ
                        }
                    }
                ]
                
                response = self.llm.generate_response(query, context_chunks)
                answer = response.answer if hasattr(response, 'answer') else str(response)
                
                # ë‹µë³€ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë³´ê°•
                if len(answer) < 200 and 'ìì„¸íˆ' in query:
                    answer = self._enhance_short_answer(answer, full_text, query)
                    
            except Exception as e:
                print(f"LLM ì˜¤ë¥˜: {e}")
                # í´ë°±: ìƒì„¸í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€
                answer = self._detailed_text_search(info['text'], query, filename)
            
            # ì¶œì²˜ ì¶”ê°€
            answer += f"\n\nğŸ“„ **ì¶œì²˜**: {filename}"
            
        elif filename.endswith('.txt'):
            # TXT íŒŒì¼ ì²˜ë¦¬ (ìì‚° ë°ì´í„°)
            return self._search_asset_file(doc_path, query)
        else:
            return f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {filename}"
        
        return answer
    
    def _simple_text_search(self, text: str, query: str, filename: str) -> str:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (LLM ì—†ì´)"""
        lines = text.split('\n')
        relevant_lines = []
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        
        # ê´€ë ¨ ì¤„ ì°¾ê¸°
        for line in lines:
            if any(keyword in line for keyword in keywords if len(keyword) > 1):
                relevant_lines.append(line.strip())
        
        if relevant_lines:
            return f"ğŸ“„ {filename}ì—ì„œ ì°¾ì€ ê´€ë ¨ ë‚´ìš©:\n\n" + '\n'.join(relevant_lines[:20])
        else:
            return f"ğŸ“„ {filename}ì—ì„œ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _detailed_text_search(self, text: str, query: str, filename: str) -> str:
        """ìƒì„¸í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (LLM ì—†ì´)"""
        lines = text.split('\n')
        relevant_sections = []
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        
        # ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸° (ì•ë’¤ ë¬¸ë§¥ í¬í•¨)
        for i, line in enumerate(lines):
            if any(keyword in line for keyword in keywords if len(keyword) > 1):
                # ì•ë’¤ 2ì¤„ì”© í¬í•¨
                start = max(0, i-2)
                end = min(len(lines), i+3)
                section = '\n'.join(lines[start:end])
                if section not in relevant_sections:
                    relevant_sections.append(section)
        
        if relevant_sections:
            return f"ğŸ“„ **{filename}** ìƒì„¸ ë¶„ì„:\n\n" + '\n---\n'.join(relevant_sections[:10])
        else:
            return f"ğŸ“„ {filename}ì—ì„œ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _enhance_short_answer(self, answer: str, full_text: str, query: str) -> str:
        """ì§§ì€ ë‹µë³€ì„ ë³´ê°•"""
        enhanced = answer + "\n\nğŸ“‹ **ì¶”ê°€ ìƒì„¸ ì •ë³´**:\n"
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', query)
        lines = full_text.split('\n')
        
        additional_info = []
        for line in lines:
            if any(keyword in line for keyword in keywords if len(keyword) > 2):
                if line.strip() and line not in answer:
                    additional_info.append(f"â€¢ {line.strip()}")
        
        if additional_info:
            enhanced += '\n'.join(additional_info[:10])
        
        return enhanced
    
    def _create_detailed_summary_prompt(self, query: str, context: str, filename: str) -> str:
        """ìƒì„¸ ìš”ì•½ ì „ìš© í”„ë¡¬í”„íŠ¸ - í†µì¼ëœ í¬ë§·"""
        return f"""
[ë¬¸ì„œ ì „ì²´ ìš”ì•½ ëª¨ë“œ]

ë¬¸ì„œ: {filename}

ğŸ“‹ **ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ì¤€ìˆ˜)**:

ğŸ“„ [ë¬¸ì„œ ì œëª©]

ğŸ“Œ **ê¸°ë³¸ ì •ë³´**
â€¢ ê¸°ì•ˆì: [ê¸°ì•ˆìëª…]
â€¢ ë‚ ì§œ: [ë¬¸ì„œ ë‚ ì§œ]
â€¢ ë¶€ì„œ: [ë‹´ë‹¹ ë¶€ì„œ]
â€¢ ë¬¸ì„œ ì¢…ë¥˜: [ê¸°ì•ˆì„œ/ê²€í† ì„œ/ë³´ê³ ì„œ ë“±]

ğŸ“ **ì£¼ìš” ë‚´ìš©**
[í•µì‹¬ ë‚´ìš©ì„ êµ¬ì¡°í™”í•˜ì—¬ ìƒì„¸íˆ]
â€¢ [ë°°ê²½/ëª©ì ]
â€¢ [í˜„í™©/ë¬¸ì œì ]
â€¢ [ì œì•ˆ/í•´ê²°ë°©ì•ˆ]
â€¢ [ì„¸ë¶€ ë‚´ìš©ë“¤...]

ğŸ’° **ë¹„ìš© ì •ë³´** (í•´ë‹¹ ì‹œ)
â€¢ ì´ì•¡: [ê¸ˆì•¡]
â€¢ ì„¸ë¶€ ë‚´ì—­:
  - [í’ˆëª©/í•­ëª©]: [ê¸ˆì•¡]
  - [ì¶”ê°€ ë¹„ìš©]: [ê¸ˆì•¡]

ğŸ“‹ **ê²€í†  ì˜ê²¬**
â€¢ [ê²€í† ì‚¬í•­ 1]
â€¢ [ê²€í† ì‚¬í•­ 2]
â€¢ ê²°ë¡ : [ìµœì¢… ì˜ê²¬/ìŠ¹ì¸ì‚¬í•­]

ğŸ“ ì¶œì²˜: {filename}

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

âš ï¸ ì¤‘ìš”: ìœ„ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥´ê³ , ë¬¸ì„œì˜ ëª¨ë“  ì •ë³´ë¥¼ ìƒì„¸í•˜ê²Œ í¬í•¨í•˜ì„¸ìš”.
"""
    
    def _create_ultra_detailed_prompt(self, query: str, context: str, filename: str) -> str:
        """ì´ˆìƒì„¸ ë‹µë³€ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
[ì´ˆìƒì„¸ ë¶„ì„ ëª¨ë“œ] - ëª¨ë“  ì„¸ë¶€ì‚¬í•­ í¬í•¨

ë¬¸ì„œ: {filename}
ìš”ì²­: {query}

âš¡ **ìµœëŒ€í•œ ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”**:

1. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ì°¾ì•„ì„œ ì œê³µ
2. ë¬¸ì„œì˜ ì•ë’¤ ë¬¸ë§¥ê¹Œì§€ í¬í•¨í•˜ì—¬ ì„¤ëª…
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë‚ ì§œ, ì´ë¦„, ëª¨ë¸ëª… ë“± ëª¨ë‘ ëª…ì‹œ
4. ê´€ë ¨ ë°°ê²½ ì •ë³´ë„ í•¨ê»˜ ì œê³µ
5. ë¬¸ì„œì— ì•”ì‹œëœ ë‚´ìš©ë„ í•´ì„í•˜ì—¬ ì„¤ëª…

ë¬¸ì„œ ì „ì²´ ë‚´ìš©:
{context}

ë‹µë³€ ê·œì¹™:
âœ… ìµœì†Œ 500ì ì´ìƒ ìƒì„¸ ë‹µë³€
âœ… ëª¨ë“  ê´€ë ¨ ì •ë³´ ë‚˜ì—´
âœ… í‘œë‚˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬
âœ… ì¤‘ìš” ì •ë³´ëŠ” **êµµê²Œ** í‘œì‹œ
âœ… ë¬¸ì„œì˜ ëª¨ë“  ê´€ë ¨ ë¶€ë¶„ ì¸ìš©
"""
    
    def _create_itemized_list_prompt(self, query: str, context: str, filename: str) -> str:
        """í’ˆëª© ë¦¬ìŠ¤íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
[í’ˆëª©/í•­ëª© ìƒì„¸ ë¶„ì„ ëª¨ë“œ]

ë¬¸ì„œ: {filename}

ë¬¸ì„œì— ìˆëŠ” ëª¨ë“  í’ˆëª©/í•­ëª©ì„ ì™„ì „í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”:

ğŸ“‹ **ì¶”ì¶œ í˜•ì‹**:
1. í’ˆëª©ëª…/ëª¨ë¸ëª…
   - ì œì¡°ì‚¬: 
   - ëª¨ë¸ë²ˆí˜¸:
   - ìˆ˜ëŸ‰:
   - ë‹¨ê°€:
   - ê¸ˆì•¡:
   - ìš©ë„:
   - íŠ¹ì§•:
   - ê¸°íƒ€ì‚¬í•­:

2. (ë‹¤ìŒ í’ˆëª©...)

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

ì¤‘ìš”: 
- ë¬¸ì„œì— ë‚˜ì˜¨ ëª¨ë“  í’ˆëª©ì„ ë¹ ì§ì—†ì´
- ê° í’ˆëª©ì˜ ëª¨ë“  ì •ë³´ë¥¼ ìƒì„¸íˆ
- ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ì„œ ì •ë¦¬
"""
    
    def _create_document_specific_prompt(self, query: str, context: str, filename: str) -> str:
        """íŠ¹ì • ë¬¸ì„œ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± - í†µì¼ëœ í¬ë§·"""
        return f"""
[ë¬¸ì„œ ì „ìš© ì •ë°€ ë¶„ì„ ëª¨ë“œ] 

ğŸ“„ ë¶„ì„ ëŒ€ìƒ ë¬¸ì„œ: {filename}

ì´ ë¬¸ì„œë§Œì„ ë¶„ì„í•˜ì—¬ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ğŸ“‹ **ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ì¤€ìˆ˜)**:

ğŸ“„ [ë¬¸ì„œ ì œëª©]

ğŸ“Œ **ê¸°ë³¸ ì •ë³´**
â€¢ ê¸°ì•ˆì: [ë¬¸ì„œì—ì„œ ì°¾ì€ ê¸°ì•ˆìëª…]
â€¢ ë‚ ì§œ: [ë¬¸ì„œ ë‚ ì§œ]
â€¢ ë¬¸ì„œ ì¢…ë¥˜: [ê¸°ì•ˆì„œ/ê²€í† ì„œ/ë³´ê³ ì„œ ë“±]

ğŸ“ **ì£¼ìš” ë‚´ìš©**
[ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ êµ¬ì¡°í™”í•˜ì—¬ í‘œì‹œ]
â€¢ [ì£¼ìš” ì‚¬í•­ 1]
â€¢ [ì£¼ìš” ì‚¬í•­ 2]
â€¢ [ì„¸ë¶€ ë‚´ìš©ë“¤...]

ğŸ’° **ë¹„ìš© ì •ë³´** (ë¹„ìš© ê´€ë ¨ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°)
â€¢ ì´ì•¡: [ê¸ˆì•¡]
â€¢ ì„¸ë¶€ ë‚´ì—­:
  - [í’ˆëª©1]: [ê¸ˆì•¡]
  - [í’ˆëª©2]: [ê¸ˆì•¡]

ğŸ“‹ **ê²€í†  ì˜ê²¬** (ê²€í†  ì˜ê²¬ì´ ìˆëŠ” ê²½ìš°)
â€¢ [ê²€í† ì‚¬í•­ 1]
â€¢ [ê²€í† ì‚¬í•­ 2]
â€¢ ê²°ë¡ : [ìµœì¢… ì˜ê²¬]

ğŸ“ ì¶œì²˜: {filename}

ğŸ“‹ **ë¬¸ì„œ ì „ì²´ ë‚´ìš©**:
{context}

â“ **ì‚¬ìš©ì ì§ˆë¬¸**: {query}

âš ï¸ ì£¼ì˜ì‚¬í•­:
- ìœ„ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ
- ì´ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
- ë¬¸ì„œì˜ ì •í™•í•œ í‘œí˜„ ì‚¬ìš©
- ê°€ëŠ¥í•œ í•œ ë§ì€ ì„¸ë¶€ì‚¬í•­ í¬í•¨
"""
    
    def _get_enhanced_cache_key(self, query: str, mode: str) -> str:
        """í–¥ìƒëœ ìºì‹œ í‚¤ ìƒì„± - ìœ ì‚¬ ì§ˆë¬¸ë„ ìºì‹œ íˆíŠ¸"""

        # 1. ì¿¼ë¦¬ ì •ê·œí™”
        normalized = query.strip().lower()

        # 2. ì¡°ì‚¬ ì œê±° (í•œêµ­ì–´ íŠ¹í™”) - ê°œì„ ëœ ë²„ì „
        # ê¸´ ì¡°ì‚¬ë¶€í„° ë¨¼ì € ì œê±° (ìœ¼ë¡œ, ì—ì„œ ë“±ì´ ë¡œ, ì— ë³´ë‹¤ ë¨¼ì €)
        particles = ['ì—ì„œ', 'ìœ¼ë¡œ', 'ì´ë‚˜', 'ì´ë“ ', 'ì´ë©´', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜ì„œ',
                     'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì™€', 'ê³¼', 'ë¡œ', 'ì—', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°']
        for particle in particles:
            # ì¡°ì‚¬ ë’¤ì— ê³µë°±ì´ ìˆëŠ” ê²½ìš°
            normalized = normalized.replace(particle + ' ', ' ')
            # ë¬¸ì¥ ëì— ì¡°ì‚¬ê°€ ìˆëŠ” ê²½ìš°
            if normalized.endswith(particle):
                normalized = normalized[:-len(particle)]

        # 3. ê³µë°± ì •ê·œí™”
        normalized = ' '.join(normalized.split())

        # 4. í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        keywords = []
        for word in normalized.split():
            if len(word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ
                keywords.append(word)

        # 5. ì •ë ¬í•˜ì—¬ ìˆœì„œ ë¬´ê´€í•˜ê²Œ
        keywords.sort()

        # 6. í•´ì‹œ ìƒì„±
        cache_str = f"{mode}:{'_'.join(keywords)}"
        return hashlib.md5(cache_str.encode()).hexdigest()

    def answer_with_logging(self, query: str, mode: str = 'auto') -> str:
        """ë¡œê¹…ì´ í†µí•©ëœ answer ë©”ì„œë“œ (ìºì‹± í¬í•¨)"""
        # í–¥ìƒëœ ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._get_enhanced_cache_key(query, mode)
        
        # ìºì‹œ í™•ì¸
        if cache_key in self.answer_cache:
            cached_response, cached_time = self.answer_cache[cache_key]
            # TTL í™•ì¸ (ê¸°ë³¸ 1ì‹œê°„)
            if time.time() - cached_time < self.cache_ttl:
                print(f"ğŸ’¾ ìºì‹œ íˆíŠ¸! (í‚¤: {cache_key[:8]}...)")
                # LRU ì—…ë°ì´íŠ¸ (ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ ì´ë™)
                self.answer_cache.move_to_end(cache_key)
                return cached_response
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del self.answer_cache[cache_key]
        
        # ì‹¤ì œ ë‹µë³€ ìƒì„± (_answer_internalì—ì„œ ëª¨ë“  ë¡œê¹… ì²˜ë¦¬)
        start_time = time.time()
        response = self._answer_internal(query, mode)
        generation_time = time.time() - start_time
        
        # ìºì‹œ ì €ì¥
        self.answer_cache[cache_key] = (response, time.time())
        print(f"â±ï¸ ë‹µë³€ ìƒì„±: {generation_time:.1f}ì´ˆ (ìºì‹œ ì €ì¥ë¨)")
        
        # ìºì‹œ í¬ê¸° ì œí•œ (LRU ë°©ì‹)
        if len(self.answer_cache) > self.max_cache_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            self.answer_cache.popitem(last=False)
        
        return response
    
    def answer(self, query: str, mode: str = 'auto') -> str:
        """ë¡œê¹…ì´ í†µí•©ëœ ë‹µë³€ ìƒì„±"""
        return self.answer_with_logging(query, mode)
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.answer_cache.clear()
        self.documents_cache.clear()
        self.metadata_cache.clear()
        print("ğŸ—‘ï¸ ëª¨ë“  ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_cache_stats(self) -> Dict:
        """ìºì‹œ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        response_cache_size = len(self.response_cache) if hasattr(self, 'response_cache') else 0
        total_size = len(self.answer_cache) + len(self.documents_cache) + len(self.metadata_cache) + response_cache_size
        return {
            'size': response_cache_size,  # For compatibility with test
            'hits': self.cache_hits,
            'misses': self.cache_misses,
            'answer_cache_size': len(self.answer_cache),
            'document_cache_size': len(self.documents_cache),
            'metadata_cache_size': len(self.metadata_cache),
            'response_cache_size': response_cache_size,
            'total_cache_size': total_size,
            'max_cache_size': self.max_cache_size,
            'cache_ttl_seconds': self.cache_ttl
        }
    
    def _answer_internal(self, query: str, mode: str = 'auto') -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            mode: ê²€ìƒ‰ ëª¨ë“œ ('document', 'asset', 'auto')
                - document: PDF ë¬¸ì„œ ê²€ìƒ‰
                - asset: ìì‚° ë°ì´í„° ê²€ìƒ‰
                - auto: ìë™ íŒë‹¨
        """
        
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()
        error_msg = None
        success = True
        response = ""
        metadata = {}
        
        try:
            # ë¡œê¹… ì‹œìŠ¤í…œ ì‹œì‘
            if logger:
                logger.system_logger.info(f"=== Query Start: {query[:100]}...")
            
            # ê²€ìƒ‰ ì˜ë„ ë¶„ë¥˜
            if mode == 'auto':
                with TimerContext(logger, "classify_intent") if logger else nullcontext():
                    mode = self._classify_search_intent(query)
                print(f"ğŸ” ê²€ìƒ‰ ëª¨ë“œ: {mode}")
            
            self.search_mode = mode
            metadata['search_mode'] = mode
            
            # ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            query_lower = query.lower()
            
            if self.search_mode == 'asset':
                # LLM ì‚¬ì „ ë¡œë“œ (Asset ëª¨ë“œì—ì„œ í•„ìš”)
                if self.llm is None:
                    if not LLMSingleton.is_loaded():
                        print("ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘ (Asset ë‹µë³€ ê°œì„ ìš©)...")
                    self.llm = LLMSingleton.get_instance(model_path=self.model_path)
                
                # ìì‚° íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
                doc_path = None
                
                # ìì‚° íŒŒì¼ ìë™ ê°ì§€ (íŒŒì¼ëª… íŒ¨í„´ ê¸°ë°˜)
                for cache_key, file_meta in self.metadata_cache.items():
                    # ìì‚°/ì „ì²´/7904 ë“±ì˜ í‚¤ì›Œë“œê°€ íŒŒì¼ëª…ì— ìˆìœ¼ë©´ ìì‚° íŒŒì¼
                    if file_meta.get('is_txt', False):
                        filename = file_meta.get('filename', cache_key)
                        # Path ê°ì²´ë¡œ ë³€í™˜
                        path_obj = Path(file_meta['path']) if isinstance(file_meta['path'], str) else file_meta['path']
                        filename_str = path_obj.name
                        if 'ìì‚°' in filename_str or 'ì „ì²´' in filename_str or '7904' in filename_str:
                            doc_path = path_obj
                            print(f"ğŸ“„ ìì‚° íŒŒì¼ ë°œê²¬: {filename_str}")
                            break
                
                # ìì‚° íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©
                if not doc_path:
                    # ì—¬ëŸ¬ ê²½ë¡œì—ì„œ ìì‚° íŒŒì¼ ì°¾ê¸°
                    asset_paths = [
                        self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt",
                        self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt",
                        self.docs_dir / "asset_complete" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì¢…í•©í˜„í™©_ì „ì²´.txt",
                        self.docs_dir / "asset_reports" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì¢…í•©í˜„í™©.txt"
                    ]

                    for asset_file in asset_paths:
                        if asset_file.exists():
                            doc_path = asset_file
                            print(f"ğŸ“„ ìì‚° íŒŒì¼ ì‚¬ìš©: {asset_file.name}")
                            break

                    # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ìºì‹œì—ì„œ ì°¾ê¸°
                    if not doc_path:
                        for cache_key, file_meta in self.metadata_cache.items():
                            if file_meta.get('is_txt', False):
                                filename = file_meta.get('filename', cache_key)
                                path_obj = Path(file_meta['path']) if isinstance(file_meta['path'], str) else file_meta['path']
                                if path_obj.exists():
                                    doc_path = path_obj
                                    print(f"ğŸ“„ ìºì‹œì—ì„œ ìì‚° íŒŒì¼ ë°œê²¬: {path_obj.name}")
                                    break
                
                if doc_path:
                    # ê°œì„ ëœ ì¿¼ë¦¬ íŒŒì‹± ì‚¬ìš©
                    query_intent = self._parse_asset_query(query)
                    
                    # ë³µí•© ì¡°ê±´ ê²€ìƒ‰ ì²˜ë¦¬
                    if query_intent.get('has_multiple_conditions'):
                        response = self._search_asset_complex(doc_path, query_intent)
                    # ë‹´ë‹¹ì/ê´€ë¦¬ì ê²€ìƒ‰ - ìš°ì„ ìˆœìœ„ ë†’ì„
                    elif query_intent.get('search_type') == 'manager':
                        response = self._search_asset_by_manager(doc_path, query)
                    # ê¸ˆì•¡/ê°€ê²© ë²”ìœ„ ê²€ìƒ‰
                    elif query_intent.get('search_type') == 'price':
                        response = self._search_asset_by_price_range(doc_path, query)
                    # ì‹œë¦¬ì–¼ ë²ˆí˜¸ ê²€ìƒ‰
                    elif query_intent.get('search_type') == 'serial' or 'ì‹œë¦¬ì–¼' in query_lower or 's/n' in query_lower:
                        response = self._search_asset_detail(doc_path, query)
                    # êµ¬ì…ì—°ë„ ê²€ìƒ‰ - ë²”ìœ„ ê²€ìƒ‰ í¬í•¨
                    elif query_intent.get('search_type') == 'year':
                        response = self._search_asset_by_year_range(doc_path, query)
                    # ì œì¡°ì‚¬ ê²€ìƒ‰
                    elif query_intent.get('search_type') == 'manufacturer':
                        response = self._search_asset_by_manufacturer(doc_path, query)
                    # ëª¨ë¸ ê²€ìƒ‰
                    elif query_intent.get('search_type') == 'model':
                        response = self._search_asset_by_model(doc_path, query)
                    # ìœ„ì¹˜ë³„ ê²€ìƒ‰
                    elif query_intent.get('search_type') == 'location':
                        response = self._search_location_unified(doc_path, query)
                    # ìœ„ì¹˜+ì¥ë¹„ ë³µí•© ê²€ìƒ‰
                    elif query_intent.get('search_type') == 'location_equipment':
                        response = self._search_location_equipment_combo(doc_path, query)
                    # ì¥ë¹„ ìœ í˜•ë³„ ê²€ìƒ‰
                    elif query_intent.get('search_type') == 'equipment':
                        response = self._search_asset_by_equipment_type(doc_path, query)
                    # "ì–´ë””" í‚¤ì›Œë“œê°€ ìˆì§€ë§Œ ìœ„ì¹˜ê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš° - ì œì¡°ì‚¬/ì¥ë¹„ëª…ìœ¼ë¡œ ê²€ìƒ‰
                    elif 'ì–´ë””' in query_lower:
                        # ì œì¡°ì‚¬ë‚˜ ì¥ë¹„ëª…ì´ ìˆìœ¼ë©´ í•´ë‹¹ ê²€ìƒ‰ ìˆ˜í–‰
                        if re.search(self._get_manufacturer_pattern(), query):
                            response = self._search_asset_by_manufacturer(doc_path, query)
                        else:
                            response = self._search_asset_with_llm(doc_path, query)
                    # ì „ì²´/í˜„í™© ìš”ì²­ì¸ ê²½ìš°
                    elif "ì „ì²´" in query_lower or "í˜„í™©" in query_lower:
                        # íŠ¹ì • ì¥ë¹„ê°€ ì–¸ê¸‰ëœ ê²½ìš°
                        equipment_keywords = ["CCU", "ì¹´ë©”ë¼", "ëª¨ë‹ˆí„°", "ì˜¤ë””ì˜¤", "ë¹„ë””ì˜¤", "ì„œë²„", "ìŠ¤ìœ„ì¹˜", "ë¼ìš°í„°"]
                        found_equipment = None
                        for eq in equipment_keywords:
                            if eq.upper() in query.upper():
                                found_equipment = eq
                                break
                        
                        if found_equipment:
                            # ìœ„ì¹˜ë³„ë¡œ ì •ë¦¬
                            response = self._search_equipment_all_locations(doc_path, found_equipment)
                        else:
                            response = self._search_asset_with_llm(doc_path, query)
                    # ì¼ë°˜ ìì‚° ê²€ìƒ‰ - LLM í™œìš©
                    else:
                        response = self._search_asset_with_llm(doc_path, query)
                else:
                    response = "âŒ ìì‚° ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # document ëª¨ë“œ ë˜ëŠ” ê¸°ë³¸ ëª¨ë“œì¸ ê²½ìš° ë¬¸ì„œ ê²€ìƒ‰ ì§„í–‰
            elif self.search_mode == 'document' or self.search_mode not in ['asset']:
                # ë¬¸ì„œ ì½ê³  ì •ë¦¬ ìš”ì²­ (ë‹¤ ì½ê³ , ì •ë¦¬í•´ì¤˜)
                if any(keyword in query for keyword in ["ë‹¤ ì½ê³ ", "ì „ë¶€ ì½ê³ ", "ëª¨ë‘ ì½ê³ ", "ì •ë¦¬í•´", "ì¢…í•©í•´", "ë¶„ì„í•´"]) \
                   and any(keyword in query for keyword in ["ê´€ë ¨", "ë¬¸ì„œ"]):
                    return self._read_and_summarize_documents(query)

                # íŠ¹ì • ë‚´ìš© ì–¸ê¸‰ ì‹œ ê´€ë ¨ ë¬¸ì„œë“¤ë„ í•¨ê»˜ ì°¾ì•„ì„œ ì •ë¦¬
                # ì˜ˆ: "DVR êµì²´ ê²€í†  ë‚´ìš© ì •ë¦¬í•´ì¤˜" â†’ DVR ê´€ë ¨ ëª¨ë“  ë¬¸ì„œ ì°¾ì•„ì„œ ì •ë¦¬
                content_keywords = ['êµì²´', 'ê²€í† ', 'êµ¬ë§¤', 'ìˆ˜ë¦¬', 'ë³´ìˆ˜', 'íê¸°', 'ë„ì…', 'ì—…ê·¸ë ˆì´ë“œ']
                if any(keyword in query for keyword in content_keywords):
                    # ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë“¤ ì°¾ê¸°
                    return self._search_and_analyze_by_content(query)

                # ì—¬ëŸ¬ ë¬¸ì„œ ê²€ìƒ‰ ìš”ì²­ (ì°¾ì•„ì¤˜, ê´€ë ¨ ë¬¸ì„œ, ìˆì–´? ë“±)
                # "ë¬¸ì„œ ì°¾ì•„ì¤˜", "ë¬¸ì„œ ë‚´ìš© ì°¾ì•„ì¤˜" ëª¨ë‘ ë¬¸ì„œ ëª©ë¡ í‘œì‹œ
                # DVR, CCU ë“± ì¥ë¹„ëª…ê³¼ í•¨ê»˜ ë¬¸ì„œ/ì°¾ì•„ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ í‘œì‹œ
                if (any(keyword in query for keyword in ["ì°¾ì•„", "ê´€ë ¨ ë¬¸ì„œ", "ê´€ë ¨ëœ", "ì–´ë–¤", "ìˆì–´", "ìˆë‚˜", "ë¦¬ìŠ¤íŠ¸", "ëª¨ë‘", "ì „ë¶€", "ë¬¸ì„œë“¤", "ë³´ì—¬ì¤˜"]) \
                   or any(equipment in query.lower() for equipment in ['dvr', 'ccu', 'ì¹´ë©”ë¼', 'ë Œì¦ˆ', 'ëª¨ë‹ˆí„°'])) \
                   and not any(keyword in query for keyword in ["ìš”ì•½", "ì•Œë ¤", "ì„¤ëª…", "ì •ë¦¬"]):
                    return self._search_multiple_documents(query)
                
                # ì›” ë‹¨ìœ„ ê²€ìƒ‰ë„ ì—¬ëŸ¬ ë¬¸ì„œ ë°˜í™˜ (ìš”ì•½ ìš”ì²­ ì œì™¸)
                if re.search(r'\d{1,2}\s*ì›”', query) and any(word in query for word in ["ë¬¸ì„œ", "ì‘ì„±", "êµ¬ë§¤", "ê²€í† "]) \
                   and not any(keyword in query for keyword in ["ìš”ì•½", "ë‚´ìš©", "ì•Œë ¤", "ì„¤ëª…"]):
                    return self._search_multiple_documents(query)
                
                # í†µê³„ ê´€ë ¨ ì§ˆë¬¸ì€ ë¬¸ì„œ ì°¾ê¸° ì—†ì´ ë°”ë¡œ ì²˜ë¦¬
                if any(keyword in query for keyword in ["ì „ì²´ í†µê³„", "ì „ì²´ í˜„í™©", "ì—°ë„ë³„", "ê¸°ì•ˆìë³„", "ì›”ë³„", "ì¹´í…Œê³ ë¦¬ë³„", "ë¶€í„°", "ê¹Œì§€"]):
                    if any(word in query for word in ["ë‚´ì—­", "ì •ë¦¬", "ëª©ë¡", "ì´", "êµ¬ë§¤", "ìš”ì•½", "ë‚´ìš©", "ì•Œë ¤", "í‘œ", "ë¦¬ìŠ¤íŠ¸", "í†µê³„", "í˜„í™©", "ë¶„ì„"]):
                        return self._generate_statistics_report(query)
                
                # 1. ê°€ì¥ ì í•©í•œ ë¬¸ì„œ ì°¾ê¸°
                doc_path = self.find_best_document(query)
                
                if not doc_path:
                    return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                
                print(f"ğŸ“„ ì„ íƒëœ ë¬¸ì„œ: {doc_path.name}")
                
                # 2. ì§ˆë¬¸ íƒ€ì… íŒŒì•…
                # íŠ¹ì • ì •ë³´ ì¶”ì¶œ ì§ˆë¬¸ (êµì²´, ëŒ€ì²´ ì¥ë¹„ ë“±)
                if ('êµì²´' in query or 'ëŒ€ì²´' in query) and ('ì–´ë–¤' in query or 'ë­' in query or 'ë¬´ì—‡' in query):
                    # PDF ë‚´ìš© ì½ê¸°
                    pdf_info = self._extract_pdf_metadata(doc_path)
                    context = pdf_info.get('ì „ì²´í…ìŠ¤íŠ¸', pdf_info.get('text', ''))
                    
                    # êµì²´/ê²€í†  ì¥ë¹„ ì°¾ê¸°
                    models = []
                    if 'Leofoto' in context:
                        leofoto_match = re.search(r'Leofoto\s+([A-Za-z0-9\-\(\)]+)', context)
                        if leofoto_match:
                            # ê¸ˆì•¡ ì°¾ê¸°
                            price_match = re.search(r'Leofoto[^\\n]*?([0-9,]+)\s*ì›', context)
                            price = price_match.group(1) if price_match else "ê¸ˆì•¡ ë¯¸ìƒ"
                            models.append(f"**Leofoto {leofoto_match.group(1)}** - {price}ì› (ì¹´ë³¸ êµ¬ì¡°, ê²½ëŸ‰)")
                    
                    if 'COMAN' in context:
                        coman_match = re.search(r'COMAN\s+([A-Za-z0-9\-\(\)]+)', context)
                        if coman_match:
                            price_match = re.search(r'COMAN[^\\n]*?([0-9,]+)\s*ì›', context)
                            price = price_match.group(1) if price_match else "ê¸ˆì•¡ ë¯¸ìƒ"
                            models.append(f"**COMAN {coman_match.group(1)}** - {price}ì› (ì•Œë£¨ë¯¸ëŠ„, ê°€ê²© ê²½ìŸë ¥)")
                    
                    if models:
                        answer = f"ğŸ“‹ **ë¯¸ëŸ¬í´ë© ì¹´ë©”ë¼ ì‚¼ê°ëŒ€ êµì²´ ê²€í†  ì¥ë¹„**\n\n"
                        for i, model in enumerate(models, 1):
                            answer += f"{i}. {model}\n"
                        answer += f"\nğŸ“„ ì¶œì²˜: {doc_path.name}"
                        return answer
                
                # ë‹¨ìˆœ ì •ë³´ ì¶”ì¶œ ì§ˆë¬¸ë“¤
                elif "ê¸°ì•ˆì" in query or "ëˆ„êµ¬" in query:
                    return self.get_document_info(doc_path, "ê¸°ì•ˆì")
                elif "ë‚ ì§œ" in query or "ì–¸ì œ" in query:
                    return self.get_document_info(doc_path, "ë‚ ì§œ")
                elif "ë¶€ì„œ" in query:
                    return self.get_document_info(doc_path, "ë¶€ì„œ")
                elif ("ê¸ˆì•¡" in query or "ì–¼ë§ˆ" in query or "ë¹„ìš©" in query) and not any(word in query for word in ["ë‚´ì—­", "ì •ë¦¬", "ëª©ë¡", "ì´"]):
                    # ë‹¨ìˆœ ê¸ˆì•¡ ì§ˆë¬¸ë§Œ (ë‚´ì—­ ì •ë¦¬ê°€ ì•„ë‹Œ ê²½ìš°)
                    return self.get_document_info(doc_path, "ê¸ˆì•¡")
                
                # LLMì´ í•„ìš”í•œ ë³µì¡í•œ ì§ˆë¬¸ë“¤ (ë³´ê³ ì„œ, ì •ë¦¬, ìš”ì•½ ë“±)
                elif any(word in query for word in ["ë‚´ì—­", "ì •ë¦¬", "ëª©ë¡", "ì´", "êµ¬ë§¤", "ìš”ì•½", "ë‚´ìš©", "ì•Œë ¤", "í‘œ", "ë¦¬ìŠ¤íŠ¸", "í†µê³„", "í˜„í™©", "ë¶„ì„"]):
                    # ì „ì²´ í†µê³„ ìš”ì²­ì¸ ê²½ìš° ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬
                    if any(keyword in query for keyword in ["ì „ì²´ í†µê³„", "ì „ì²´ í˜„í™©", "ì—°ë„ë³„", "ê¸°ì•ˆìë³„", "ì›”ë³„", "ì¹´í…Œê³ ë¦¬ë³„", "ë¶€í„°", "ê¹Œì§€"]):
                        return self._generate_statistics_report(query)
                    # ë‹¨ì¼ ë¬¸ì„œ ìš”ì•½
                    else:
                        # LLM ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
                        return self._generate_llm_summary(doc_path, query)
                
                # ê¸´ê¸‰ ìƒí™©, ìˆ˜ë¦¬, ê³ ì¥ ê´€ë ¨
                elif any(word in query for word in ["ê¸´ê¸‰", "ìˆ˜ë¦¬", "ê³ ì¥", "ì—…ì²´", "ì—°ë½ì²˜"]):
                    # LLM ì‚¬ìš©í•˜ì—¬ ê¸´ê¸‰ ì •ë³´ ì œê³µ
                    return self._generate_llm_summary(doc_path, query)
                
                # ê°ì‚¬, ì ˆì°¨ ê´€ë ¨
                elif any(word in query for word in ["ê°ì‚¬", "ì ˆì°¨", "ìŠ¹ì¸", "íê¸°", "í”„ë¡œì„¸ìŠ¤"]):
                    # LLM ì‚¬ìš©í•˜ì—¬ ì²´í¬ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ë‹µë³€
                    answer = self._generate_llm_summary(doc_path, query)
                    
                    # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° ë° ë¡œê¹…
                    processing_time = time.time() - start_time
                    if logger:
                        logger.log_query(
                            query=query,
                            response=answer,
                            search_mode=self.search_mode,
                            processing_time=processing_time,
                            metadata={'selected_doc': doc_path.name if doc_path else None}
                        )
                        logger.system_logger.info(f"Query completed successfully in {processing_time:.2f}s")
                    return answer
                
                else:
                    # LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ë° ë‹µë³€ ìƒì„±
                    answer = self._generate_llm_summary(doc_path, query)
                    
                    # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° ë° ë¡œê¹…
                    processing_time = time.time() - start_time
                    if logger:
                        logger.log_query(
                            query=query,
                            response=answer,
                            search_mode=self.search_mode,
                            processing_time=processing_time,
                            metadata={'selected_doc': doc_path.name if doc_path else None}
                        )
                        logger.system_logger.info(f"Query completed successfully in {processing_time:.2f}s")
                    return answer
            
            # asset ëª¨ë“œì—ì„œ responseê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
            if self.search_mode == 'asset' and 'response' in locals():
                # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° ë° ë¡œê¹…
                processing_time = time.time() - start_time
                if logger:
                    # metadataì—ì„œ Path ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                    safe_metadata = {}
                    if 'metadata' in locals():
                        for key, value in metadata.items():
                            if isinstance(value, Path):
                                safe_metadata[key] = str(value)
                            else:
                                safe_metadata[key] = value
                    
                    logger.log_query(
                        query=query,
                        response=response,
                        search_mode=self.search_mode,
                        processing_time=processing_time,
                        metadata=safe_metadata
                    )
                    logger.system_logger.info(f"Asset query completed in {processing_time:.2f}s")
                return response
                
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ
            error_msg = str(e)
            success = False
            response = f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}"
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # ë¡œê¹… ì‹œìŠ¤í…œ
            if logger:
                logger.log_error(
                    error_type=type(e).__name__,
                    error_msg=error_msg,
                    query=query
                )
            
            return response
    
    def _get_detail_only_prompt(self, query: str, context: str, filename: str) -> str:
        """ê¸°ë³¸ ì •ë³´ ì œì™¸í•œ ìƒì„¸ ë‚´ìš©ë§Œ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë¬¸ì„œì—ì„œ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì„¸ìš”. 
âš ï¸ ê¸°ì•ˆì, ë‚ ì§œ, ë¬¸ì„œë²ˆí˜¸ ë“± ê¸°ë³¸ ì •ë³´ëŠ” ì œì™¸í•˜ê³  ì‹¤ì§ˆì ì¸ ë‚´ìš©ë§Œ ì‘ì„±í•˜ì„¸ìš”.

ğŸ“ **êµ¬ë§¤/ìˆ˜ë¦¬ ì‚¬ìœ  ë° í˜„í™©**
â€¢ ì–´ë–¤ ë¬¸ì œê°€ ìˆì—ˆëŠ”ì§€
â€¢ í˜„ì¬ ìƒí™©ì€ ì–´ë–¤ì§€
â€¢ ì œì•ˆí•˜ëŠ” í•´ê²°ì±…ì€ ë¬´ì—‡ì¸ì§€

ğŸ”§ **ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­** (ìˆëŠ” ê²½ìš°ë§Œ)
â€¢ ì¥ë¹„ ì‚¬ì–‘ì´ë‚˜ ëª¨ë¸
â€¢ ê²€í† í•œ ëŒ€ì•ˆë“¤
â€¢ ì„ íƒ ê·¼ê±°

ğŸ’° **ë¹„ìš© ê´€ë ¨** (ìˆëŠ” ê²½ìš°ë§Œ)
â€¢ ì˜ˆìƒ ë¹„ìš©
â€¢ ì—…ì²´ ì •ë³´

ë¬¸ì„œ: {filename}
ë‚´ìš©: {context[:5000]}

ì§ˆë¬¸: {query}

ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
"""

    def _get_optimized_prompt(self, query: str, context: str, filename: str) -> str:
        """ê¸°ìˆ ê´€ë¦¬íŒ€ ì‹¤ë¬´ ìµœì í™” í”„ë¡¬í”„íŠ¸"""
        
        # ê¸´ê¸‰ ìƒí™© í‚¤ì›Œë“œ
        if any(word in query for word in ["ê¸´ê¸‰", "ìˆ˜ë¦¬", "ê³ ì¥", "ì—…ì²´", "ì—°ë½ì²˜"]):
            return f"""
[ê¸´ê¸‰ ì¥ë¹„ ì¡°íšŒ] ë°©ì†¡ ì¥ë¹„ ê´€ë¦¬ ì‹œìŠ¤í…œ

ìƒí™©: ê¸´ê¸‰ ëŒ€ì‘ í•„ìš”
ë¬¸ì„œ: {filename}

ë‹¤ìŒ ì •ë³´ë¥¼ ì¦‰ì‹œ ì œê³µí•˜ì„¸ìš”:
â€¢ ìˆ˜ë¦¬ ì—…ì²´ëª…:
â€¢ ë‹´ë‹¹ì ì—°ë½ì²˜:
â€¢ ì´ì „ ì²˜ë¦¬ ë¹„ìš©:
â€¢ ì²˜ë¦¬ ê¸°ê°„:

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

âš ï¸ 30ì´ˆ ë‚´ ë‹µë³€ í•„ìš”
âš ï¸ ì—†ëŠ” ì •ë³´ëŠ” "í™•ì¸ í•„ìš”"ë¡œ í‘œì‹œ
"""
        
        # ë³´ê³ ì„œ/ë‚´ì—­ ì •ë¦¬/ê¸°ìˆ ê²€í† ì„œ
        elif any(word in query for word in ["ë‚´ì—­", "ì •ë¦¬", "ì´", "ëª©ë¡", "êµ¬ë§¤", "í’ˆëª©", "ê²€í† ì„œ", "ë‚´ìš©", "ìš”ì•½", "ì•Œë ¤"]):
            # ê¸°ë³¸ ì •ë³´ê°€ ì´ë¯¸ ì¶”ì¶œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            has_basic_info = 'basic_summary' in locals() if 'locals' in dir() else False
            
            if has_basic_info:
                # ê¸°ë³¸ ì •ë³´ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ìƒì„¸ ë‚´ìš©ë§Œ ìš”ì²­
                return f"""
[ë¬¸ì„œ ìƒì„¸ ë¶„ì„] {filename}

âš ï¸ ê¸°ë³¸ ì •ë³´(ê¸°ì•ˆì, ë‚ ì§œ ë“±)ëŠ” ì´ë¯¸ ì¶”ì¶œë¨. ì•„ë˜ ë‚´ìš©ë§Œ ì‘ì„±í•˜ì„¸ìš”:

ğŸ” **í•µì‹¬ ë‚´ìš©**
â€¢ êµ¬ë§¤/ìˆ˜ë¦¬ ì‚¬ìœ : [íŒŒì† ìƒíƒœ, ë¬¸ì œì  ë“± êµ¬ì²´ì ìœ¼ë¡œ]
â€¢ í˜„ì¬ ìƒí™©: [í˜„í™© ì„¤ëª…]  
â€¢ í•´ê²° ë°©ì•ˆ: [ì œì•ˆ ë‚´ìš©]

ğŸ”§ **ê¸°ìˆ  ê²€í†  ë‚´ìš©** (í•´ë‹¹ì‹œ)
â€¢ ê¸°ì¡´ ì¥ë¹„ ë¬¸ì œì : [êµ¬ì²´ì  ë¬¸ì œ ì„¤ëª…]
â€¢ ëŒ€ì²´ ì¥ë¹„: [ëª¨ë¸ëª…, ì œì¡°ì‚¬]
â€¢ ì£¼ìš” ì‚¬ì–‘: [í•µì‹¬ ìŠ¤í™]
â€¢ ì„ ì • ì´ìœ : [ì„ íƒ ê·¼ê±°]

ğŸ’° **ë¹„ìš© ì •ë³´**
â€¢ ì´ì•¡: [ê¸ˆì•¡] (ë¶€ê°€ì„¸ í¬í•¨/ë³„ë„)
â€¢ ì„¸ë¶€ ë‚´ì—­:
  - [í’ˆëª©1]: [ëª¨ë¸ëª…] - [ìˆ˜ëŸ‰] x [ë‹¨ê°€] = [ê¸ˆì•¡]
  - [í’ˆëª©2]: [ëª¨ë¸ëª…] - [ìˆ˜ëŸ‰] x [ë‹¨ê°€] = [ê¸ˆì•¡]
â€¢ ë‚©í’ˆì—…ì²´: [ì—…ì²´ëª…]

ğŸ“‹ **ê²€í†  ì˜ê²¬**
â€¢ [ê²€í† ì‚¬í•­ 1]
â€¢ [ê²€í† ì‚¬í•­ 2]
â€¢ ê²°ë¡ : [ìµœì¢… ì˜ê²¬/ìŠ¹ì¸ì‚¬í•­]

ğŸ“ ì¶œì²˜: {filename}

ë¬¸ì„œ ë‚´ìš©:
{context}

ìš”ì²­: {query}

âš ï¸ ì£¼ì˜ì‚¬í•­:
- ìœ„ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ
- ëª¨ë“  í’ˆëª©/ì¥ë¹„ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨
- ê¸ˆì•¡ì€ ì²œë‹¨ìœ„ ì½¤ë§ˆ í¬í•¨ (ì˜ˆ: 820,000ì›)
- ëª¨ë¸ëª…, ì—…ì²´ëª… ë“± ê³ ìœ ëª…ì‚¬ëŠ” ì •í™•íˆ í‘œê¸°
"""
        
        # ê°ì‚¬/ì ˆì°¨ í™•ì¸
        elif any(word in query for word in ["ê°ì‚¬", "ì ˆì°¨", "ìŠ¹ì¸", "íê¸°"]):
            return f"""
[ê°ì‚¬ ëŒ€ì‘ ìë£Œ] ê¸°ìˆ ê´€ë¦¬íŒ€ ë¬¸ì„œ ì‹œìŠ¤í…œ

ë¬¸ì„œ: {filename}
ìš”ì²­: {query}

í•„ìˆ˜ í™•ì¸ ì‚¬í•­:
â–¡ ìš”ì²­ ì¼ì:
â–¡ í’ˆì˜ì„œ ë²ˆí˜¸:
â–¡ ëŒ€ìƒ ì¥ë¹„:
â–¡ ì²˜ë¦¬ ì‚¬ìœ :
â–¡ ìŠ¹ì¸ ë¼ì¸:
  - 1ì°¨:
  - 2ì°¨:
  - ìµœì¢…:
â–¡ ì²˜ë¦¬ ì™„ë£Œì¼:

ë¬¸ì„œ ë‚´ìš©:
{context}

âš ï¸ ê°ì‚¬ ì§€ì  ë°©ì§€ë¥¼ ìœ„í•´ ì •í™•íˆ í™•ì¸
"""
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (í†µì¼ëœ í¬ë§·)
        else:
            return f"""
[ê¸°ìˆ ê´€ë¦¬íŒ€ ë¬¸ì„œ ë¶„ì„]

ğŸ“‹ **ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ì¤€ìˆ˜)**:

ğŸ“„ {filename.replace('.pdf', '')}

ğŸ“Œ **ê¸°ë³¸ ì •ë³´**
â€¢ ê¸°ì•ˆì: [ë¬¸ì„œì—ì„œ ì°¾ì€ ê¸°ì•ˆìëª…]
â€¢ ë‚ ì§œ: [ë¬¸ì„œ ë‚ ì§œ]
â€¢ ë¬¸ì„œ ì¢…ë¥˜: [ê¸°ì•ˆì„œ/ê²€í† ì„œ/ë³´ê³ ì„œ ë“±]

ğŸ“ **ì£¼ìš” ë‚´ìš©**
[ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ êµ¬ì¡°í™”í•˜ì—¬ í‘œì‹œ]
â€¢ [ì£¼ìš” ì‚¬í•­ 1]
â€¢ [ì£¼ìš” ì‚¬í•­ 2]
â€¢ [ì„¸ë¶€ ë‚´ìš©ë“¤...]

ğŸ’° **ë¹„ìš© ì •ë³´** (ë¹„ìš© ê´€ë ¨ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°)
â€¢ ì´ì•¡: [ê¸ˆì•¡]
â€¢ ì„¸ë¶€ ë‚´ì—­:
  - [í’ˆëª©1]: [ê¸ˆì•¡]
  - [í’ˆëª©2]: [ê¸ˆì•¡]

ğŸ“‹ **ê²€í†  ì˜ê²¬** (ê²€í†  ì˜ê²¬ì´ ìˆëŠ” ê²½ìš°)
â€¢ [ê²€í† ì‚¬í•­ 1]
â€¢ [ê²€í† ì‚¬í•­ 2]
â€¢ ê²°ë¡ : [ìµœì¢… ì˜ê²¬]

ğŸ“ ì¶œì²˜: {filename}

ë¬¸ì„œ ë‚´ìš©:
{context}

ìš”ì²­: {query}

âš ï¸ ì£¼ì˜ì‚¬í•­:
- ìœ„ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ
- ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ìƒì„¸íˆ í¬í•¨
- ì‹¤ë¬´ìê°€ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€
- ëª¨ë“  ê³ ìœ ëª…ì‚¬(ëª¨ë¸ëª…, ì—…ì²´ëª…, ë‹´ë‹¹ìëª…)ëŠ” ì •í™•íˆ í‘œê¸°
- ê¸ˆì•¡ì€ ì²œë‹¨ìœ„ ì½¤ë§ˆ í¬í•¨ (ì˜ˆ: 1,234,000ì›)
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
"""
    
    def _is_gian_document(self, text: str) -> bool:
        """ê¸°ì•ˆì„œ ë¬¸ì„œì¸ì§€ í™•ì¸"""
        gian_keywords = ['ì¥ë¹„êµ¬ë§¤/ìˆ˜ë¦¬ ê¸°ì•ˆì„œ', 'ê¸°ì•ˆë¶€ì„œ', 'ê¸°ì•ˆì', 'ê¸°ì•ˆì¼ì', 'ê²°ì¬', 'í•©ì˜']
        matches = sum(1 for keyword in gian_keywords if keyword in text[:500])
        return matches >= 3
    
    def _try_ocr_extraction(self, pdf_path: Path) -> str:
        """OCRì„ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„"""
        try:
            # OCR í”„ë¡œì„¸ì„œ ì„í¬íŠ¸
            from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
            
            ocr_processor = EnhancedOCRProcessor()
            text, metadata = ocr_processor.extract_text_with_ocr(str(pdf_path))
            
            if metadata.get('ocr_performed'):
                if logger:
                    logger.system_logger.info(f"OCR ì„±ê³µ: {pdf_path.name} - {metadata.get('ocr_text_length', 0)}ì ì¶”ì¶œ")
                return text
            else:
                if logger:
                    logger.system_logger.warning(f"OCR ì‹¤íŒ¨: {pdf_path.name}")
                return ""
                
        except ImportError:
            if logger:
                logger.system_logger.warning("OCR ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€ - pytesseract ë˜ëŠ” Tesseract ë¯¸ì„¤ì¹˜")
            return ""
        except Exception as e:
            if logger:
                logger.system_logger.error(f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {pdf_path.name} - {e}")
            return ""
    
    def _extract_full_pdf_content(self, pdf_path: Path) -> dict:
        """PDF ì „ì²´ ë‚´ìš© ì¶”ì¶œ ë° êµ¬ì¡°í™”"""
        try:
            import PyPDF2
            
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                
                # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)
                full_text = ""
                max_pages = min(len(reader.pages), 50)  # ìµœëŒ€ 50í˜ì´ì§€ë¡œ ì¦ê°€
                for page_num in range(max_pages):
                    page = reader.pages[page_num]
                    page_text = page.extract_text()
                    
                    # ê·¸ë£¹ì›¨ì–´ URL ì œê±°
                    page_text = re.sub(r'gw\.channela[^\n]+', '', page_text)
                    page_text = re.sub(r'\d+\.\s*\d+\.\s*\d+\.\s*ì˜¤[ì „í›„]\s*\d+:\d+\s*ì¥ë¹„êµ¬ë§¤.*?ê¸°ì•ˆì„œ', '', page_text)
                    
                    full_text += f"\n[í˜ì´ì§€ {page_num+1}]\n{page_text}\n"
                    
                    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì¤‘ë‹¨
                    if len(full_text) > 100000:  # 100K ë¬¸ìë¡œ ì¦ê°€
                        break
                
                # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ OCR ì‹œë„
                if not full_text.strip():
                    logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨, OCR ì‹œë„: {pdf_path.name}")
                    full_text = self._try_ocr_extraction(pdf_path)
                    if not full_text:
                        return None
                
                # êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
                info = {}
                
                # ê¸°ì•ˆì„œ ë¬¸ì„œì¸ì§€ í™•ì¸
                is_gian = self._is_gian_document(full_text)
                
                if is_gian:
                    # ê¸°ì•ˆì„œ ì „ìš© íŒŒì‹±
                    patterns = {
                        'ê¸°ì•ˆì': r'ê¸°ì•ˆì\s+([ê°€-í£]+)',
                        'ì œëª©': r'ì œëª©\s+(.+?)(?:\n|$)',
                        'ê¸°ì•ˆì¼ì': r'ê¸°ì•ˆì¼ì\s+(\d{4}-\d{2}-\d{2})',
                        'ê¸°ì•ˆë¶€ì„œ': r'ê¸°ì•ˆë¶€ì„œ\s+([^\s]+)',
                        'ë³´ì¡´ê¸°ê°„': r'ë³´ì¡´ê¸°ê°„\s+([^\s\n]+)',
                        'ì‹œí–‰ì¼ì': r'ì‹œí–‰ì¼ì\s+(\d{4}-\d{2}-\d{2})',
                        'ë¬¸ì„œë²ˆí˜¸': r'ë¬¸ì„œë²ˆí˜¸\s+([^\s]+)',
                    }
                else:
                    # ì¼ë°˜ ë¬¸ì„œ íŒŒì‹±
                    patterns = {
                        'ê¸°ì•ˆì': r'ê¸°ì•ˆì\s+([^\s\n]+)',
                        'ì œëª©': r'ì œëª©\s+(.+?)(?:\n|$)',
                        'ê¸°ì•ˆì¼ì': r'ê¸°ì•ˆì¼ì\s+(\d{4}-\d{2}-\d{2})',
                        'ê¸°ì•ˆë¶€ì„œ': r'ê¸°ì•ˆë¶€ì„œ\s+([^\s\n]+)',
                        'ë³´ì¡´ê¸°ê°„': r'ë³´ì¡´ê¸°ê°„\s+([^\s\n]+)'
                    }
                
                for key, pattern in patterns.items():
                    match = re.search(pattern, full_text)
                    if match:
                        info[key] = match.group(1).strip()
                
                # ê°œìš” ì¶”ì¶œ (ê¸°ì•ˆì„œ í˜•ì‹ì— ë§ì¶¤)
                if is_gian:
                    # ê¸°ì•ˆì„œëŠ” 1. ê°œìš” í˜•ì‹
                    match = re.search(r'1\.\s*ê°œìš”\s*\n(.+?)(?:\n2\.|$)', full_text, re.DOTALL)
                    if match:
                        overview = match.group(1).strip()
                        # ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì •ë¦¬
                        overview = re.sub(r'\n(?![-â€¢Â·])', ' ', overview)
                        info['ê°œìš”'] = overview[:800]  # 800ì ì œí•œ
                    
                    # 2. ë‚´ìš© ì¶”ì¶œ
                    match = re.search(r'2\.\s*ë‚´ìš©\s*\n(.+?)(?:\n3\.|$)', full_text, re.DOTALL)
                    if match:
                        content = match.group(1).strip()
                        content = re.sub(r'\n(?![-â€¢Â·\d\)])', ' ', content)
                        info['ë‚´ìš©'] = content[:1000]  # 1000ì ì œí•œ
                    
                    # 3. ê²€í†  ì˜ê²¬ ì¶”ì¶œ
                    match = re.search(r'3\.\s*ê²€í† \s*ì˜ê²¬\s*\n(.+?)(?:\n4\.|$)', full_text, re.DOTALL)
                    if match:
                        review = match.group(1).strip()
                        review = re.sub(r'\n(?![-â€¢Â·])', ' ', review)
                        info['ê²€í† ì˜ê²¬'] = review[:2500]  # 800 -> 2500
                else:
                    # ì¼ë°˜ ë¬¸ì„œëŠ” ê¸°ì¡´ ë°©ì‹
                    if 'ê°œìš”' in full_text:
                        match = re.search(r'ê°œìš”\s*\n(.+?)(?:\n\d+\.|$)', full_text, re.DOTALL)
                        if match:
                            info['ê°œìš”'] = match.group(1).strip()
                
                # ê¸ˆì•¡ ì •ë³´ (íŒ¨í„´ ê°œì„  - ë” ì •í™•í•œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì¶œ)
                # ì‹¤ì œ ê¸ˆì•¡ì´ ë‚˜ì˜¤ëŠ” ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ íŒ¨í„´
                amount_patterns = [
                    # ì´ì•¡, í•©ê³„ ë“± ëª…ì‹œì  ê¸ˆì•¡ (ì› ì—†ì´ë„ ë§¤ì¹­)
                    r'(?:ì´ì•¡|í•©ê³„|ì´\s*ì•¡|ì´\s*ë¹„ìš©|ê²€í† \s*ë¹„ìš©|ê²€í† \s*ê¸ˆì•¡)[:\s]*(\d{1,3}(?:,\d{3})*)\s*(?:ì›)?',
                    r'(?:ê¸ˆì•¡|ë¹„ìš©|ê°€ê²©)[:\s]*(\d{1,3}(?:,\d{3})*)\s*(?:ì›)?',
                    # VAT ê´€ë ¨ ê¸ˆì•¡
                    r'(\d{1,3}(?:,\d{3})*)\s*ì›\s*\(?(?:VAT|ë¶€ê°€ì„¸)',
                    r'(\d{1,3}(?:,\d{3})*)\s*\(?(?:VAT|ë¶€ê°€ì„¸)',  # ì› ì—†ì´
                    # ë°œìƒ ë¹„ìš© íŒ¨í„´
                    r'ë°œìƒ\s*ë¹„ìš©\s*(\d{1,3}(?:,\d{3})*)\s*ì›',
                    # ë°±ë§Œì›, ì²œë§Œì› ë‹¨ìœ„ í‘œê¸°
                    r'(\d{1,3}(?:,\d{3})*)\s*(?:ë°±ë§Œ|ì²œë§Œ|ì–µ)\s*ì›',
                    # ì¼ë°˜ì ì¸ ê¸ˆì•¡ íŒ¨í„´ (ì²œë§Œì› ì´ìƒë§Œ)
                    r'(\d{1,3}(?:,\d{3})*)\s*ì›',
                ]
                
                amounts = []
                amount_contexts = []  # ê¸ˆì•¡ê³¼ ì»¨í…ìŠ¤íŠ¸ í•¨ê»˜ ì €ì¥
                
                for pattern in amount_patterns:
                    matches = re.finditer(pattern, full_text, re.IGNORECASE)
                    for match in matches:
                        amount = match.group(1)
                        # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸ˆì•¡ ì£¼ë³€ í…ìŠ¤íŠ¸)
                        start = max(0, match.start() - 50)
                        end = min(len(full_text), match.end() + 50)
                        context = full_text[start:end].strip()
                        
                        # ê¸ˆì•¡ì´ ìœ ì˜ë¯¸í•œì§€ ê²€ì¦ (10ë§Œì› ì´ìƒ)
                        try:
                            amount_int = int(amount.replace(',', ''))
                            if amount_int >= 100000:  # 10ë§Œì› ì´ìƒë§Œ
                                amounts.append(amount)
                                amount_contexts.append({
                                    'amount': amount,
                                    'context': context
                                })
                        except:
                            pass
                
                # ê°€ì¥ í° ê¸ˆì•¡ì„ ì£¼ìš” ê¸ˆì•¡ìœ¼ë¡œ íŒë‹¨
                if amounts:
                    # ê¸ˆì•¡ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬
                    sorted_amounts = sorted(amounts, 
                                          key=lambda x: int(x.replace(',', '')), 
                                          reverse=True)
                    # ìƒìœ„ 3ê°œë§Œ ì €ì¥
                    info['ê¸ˆì•¡ì •ë³´'] = sorted_amounts[:3]
                    info['ê¸ˆì•¡ì»¨í…ìŠ¤íŠ¸'] = amount_contexts[:3]
                
                # ì—…ì²´ ì •ë³´
                if 'ì—…ì²´' in full_text or 'ë²¤ë”' in full_text:
                    vendor_match = re.search(r'(?:ì—…ì²´|ë²¤ë”)[:\s]*([^\n]+)', full_text)
                    if vendor_match:
                        info['ì—…ì²´'] = vendor_match.group(1).strip()
                
                # ê²€í†  ì˜ê²¬ ì¶”ì¶œ (ìƒˆë¡œ ì¶”ê°€)
                if 'ê²€í†  ì˜ê²¬' in full_text:
                    match = re.search(r'ê²€í†  ì˜ê²¬(.+?)(?:3\.|$)', full_text, re.DOTALL)
                    if match:
                        opinion = match.group(1).strip()
                        opinion = re.sub(r'\n+', ' ', opinion)
                        opinion = re.sub(r'\s+', ' ', opinion)
                        info['ê²€í† ì˜ê²¬'] = opinion[:2500]  # 500 -> 2500ìë¡œ ì¦ê°€
                
                # ì„¸ë¶€ í•­ëª© ì¶”ì¶œ (í…Œì´ë¸” ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°ë¥¼ ìœ„í•´)
                info['ì„¸ë¶€í•­ëª©'] = []
                
                # ì¤‘ê³„ì°¨ ë³´ìˆ˜ í•­ëª© ì°¾ê¸°
                if 'ì¤‘ê³„ì°¨' in full_text and 'ë³´ìˆ˜' in full_text:
                    # ë„ì–´, ë°œì „ê¸°, ë°°í„°ë¦¬ ë“± í•­ëª© ì°¾ê¸°
                    repair_items = []
                    if 'ë„ì–´' in full_text:
                        repair_items.append({'í•­ëª©': 'ë„ì–´', 'ë‚´ìš©': 'ë¶€ì‹ ë° ì‘ë™ ë¶ˆëŸ‰'})
                    if 'ë°œì „ê¸°' in full_text:
                        repair_items.append({'í•­ëª©': 'ë°œì „ê¸°', 'ë‚´ìš©': 'ëˆ„ìˆ˜ ë° ì ê²€ í•„ìš”'})
                    if 'ë ˆë²¨ì­' in full_text:
                        repair_items.append({'í•­ëª©': 'ë ˆë²¨ì­', 'ë‚´ìš©': 'ì‘ë™ ë¶ˆëŸ‰'})
                    if 'ë°°í„°ë¦¬' in full_text:
                        repair_items.append({'í•­ëª©': 'ë°°í„°ë¦¬', 'ë‚´ìš©': 'êµì²´ í•„ìš”'})
                    if repair_items:
                        info['ì„¸ë¶€í•­ëª©'] = repair_items
                
                # ì§€ë¯¸ì§‘ Control Box ìˆ˜ë¦¬ í•­ëª© ì°¾ê¸°
                elif 'Control Box' in full_text or 'ì§€ë¯¸ì§‘' in full_text:
                    repair_items = []
                    if 'Tilt ìŠ¤í”¼ë“œ' in full_text:
                        repair_items.append({'í•­ëª©': 'Tilt ìŠ¤í”¼ë“œë‹¨', 'ë‚´ìš©': 'ë¶€í’ˆ êµì²´'})
                    if repair_items:
                        info['ì„¸ë¶€í•­ëª©'] = repair_items
                    
                    # ì¥ì•  ë‚´ìš© ì¶”ì¶œ
                    if 'ì¥ì•  ë‚´ìš©' in full_text:
                        match = re.search(r'ì¥ì•  ë‚´ìš©(.+?)(?:\d+\)|$)', full_text, re.DOTALL)
                        if match:
                            info['ì¥ì• ë‚´ìš©'] = match.group(1).strip()[:300]
                
                # ë¹„ìš© ë‚´ì—­ ì¶”ì¶œ (ê°œì„  - DVR í¬í•¨)
                info['ë¹„ìš©ë‚´ì—­'] = {}
                
                # DVR ê´€ë ¨ ë¹„ìš© ì²´í¬
                if 'DVR' in full_text or '2,446,000' in full_text:
                    # DVR ë¹„ìš© í‘œ ì°¾ê¸°
                    cost_match = re.search(r'ê²€í†  ë¹„ìš©.*?ì´ì•¡\s*([\d,]+)', full_text, re.DOTALL)
                    if cost_match:
                        info['ë¹„ìš©ë‚´ì—­']['ì´ì•¡'] = cost_match.group(1) + 'ì›'
                    
                    # ì„¸ë¶€ í•­ëª© ì¶”ì¶œ
                    if '666,000' in full_text:
                        info['ë¹„ìš©ë‚´ì—­']['DVR'] = '666,000ì› (2EA)'
                    if '1,520,000' in full_text:
                        info['ë¹„ìš©ë‚´ì—­']['HDD'] = '1,520,000ì› (10TB x 4EA)'
                    if '260,000' in full_text:
                        info['ë¹„ìš©ë‚´ì—­']['ì»¨ë²„í„°'] = '260,000ì› (2EA)'
                    if '2,446,000' in full_text:
                        info['ë¹„ìš©ë‚´ì—­']['ì´ì•¡'] = '2,446,000ì›'
                
                # ê¸°ì¡´ ê¸ˆì•¡ ì²˜ë¦¬
                for amt in info.get('ê¸ˆì•¡ì •ë³´', []):
                    try:
                        amt_int = int(amt.replace(',', ''))
                        if amt_int >= 100000:  # 10ë§Œì› ì´ìƒìœ¼ë¡œ ë‚®ì¶¤
                            if '26,660,000' in amt:
                                info['ë¹„ìš©ë‚´ì—­']['ë‚´ì™¸ê´€ë³´ìˆ˜'] = amt + 'ì›'
                            elif '7,680,000' in amt:
                                info['ë¹„ìš©ë‚´ì—­']['ë°©ì†¡ì‹œìŠ¤í…œ'] = amt + 'ì›'
                            elif '34,340,000' in amt:
                                info['ë¹„ìš©ë‚´ì—­']['ì´í•©ê³„'] = amt + 'ì› (VATë³„ë„)'
                            elif '200,000' in amt:
                                info['ë¹„ìš©ë‚´ì—­']['ìˆ˜ë¦¬ë¹„ìš©'] = amt + 'ì› (VATë³„ë„)'
                            elif not info['ë¹„ìš©ë‚´ì—­']:  # ì²« ë²ˆì§¸ ê¸ˆì•¡
                                info['ë¹„ìš©ë‚´ì—­']['ê¸ˆì•¡'] = amt + 'ì›'
                    except (ValueError, AttributeError):
                        pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
                
                info['ì „ì²´í…ìŠ¤íŠ¸'] = full_text[:8000]  # LLM ì»¨í…ìŠ¤íŠ¸ ì œí•œ
                
                return info
                
        except Exception as e:
            return {'error': str(e)}
    
    def _prepare_formatted_data(self, pdf_info: Dict, pdf_path: Path) -> Dict:
        """í¬ë§·í„°ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        formatted_info = {
            'ì œëª©': pdf_info.get('ì œëª©', pdf_path.stem),
            'ê¸°ì•ˆì': pdf_info.get('ê¸°ì•ˆì', ''),
            'ê¸°ì•ˆì¼ì': pdf_info.get('ê¸°ì•ˆì¼ì', ''),
            'ê¸°ì•ˆë¶€ì„œ': pdf_info.get('ê¸°ì•ˆë¶€ì„œ', '')
        }
        
        # í•µì‹¬ ìš”ì•½ ì¶”ì¶œ (3ì¤„)
        if self.formatter and pdf_info.get('ì „ì²´í…ìŠ¤íŠ¸'):
            key_points = self.formatter.extract_key_points(pdf_info['ì „ì²´í…ìŠ¤íŠ¸'])
            formatted_info['í•µì‹¬ìš”ì•½'] = key_points
        
        # ìƒì„¸ ë‚´ìš© êµ¬ì¡°í™”
        detail_content = []
        
        # ê°œìš”ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if pdf_info.get('ê°œìš”'):
            detail_content.append({
                'í•­ëª©': 'ê°œìš”',
                'ë‚´ìš©': pdf_info['ê°œìš”'][:200]
            })
        
        # ì¥ì• /ìˆ˜ë¦¬ ë‚´ìš©
        if any(k in pdf_info for k in ['ì¥ì• ë‚´ìš©', 'ìˆ˜ë¦¬ë‚´ìš©', 'êµ¬ë§¤ì‚¬ìœ ']):
            for key in ['ì¥ì• ë‚´ìš©', 'ìˆ˜ë¦¬ë‚´ìš©', 'êµ¬ë§¤ì‚¬ìœ ']:
                if pdf_info.get(key):
                    detail_content.append({
                        'í•­ëª©': key,
                        'ë‚´ìš©': pdf_info[key][:200]
                    })
        
        formatted_info['ìƒì„¸ë‚´ìš©'] = detail_content
        
        # ë¹„ìš© ì •ë³´
        if pdf_info.get('ë¹„ìš©ë‚´ì—­'):
            formatted_info['ë¹„ìš©ì •ë³´'] = pdf_info['ë¹„ìš©ë‚´ì—­']
        
        # ê²€í†  ì˜ê²¬
        opinions = []
        if pdf_info.get('ê²€í† ê²°ê³¼'):
            opinions.append(pdf_info['ê²€í† ê²°ê³¼'])
        if pdf_info.get('ì¶”ì²œì‚¬í•­'):
            opinions.append(pdf_info['ì¶”ì²œì‚¬í•­'])
        if opinions:
            formatted_info['ê²€í† ì˜ê²¬'] = opinions
        
        # ê´€ë ¨ ì •ë³´
        related = []
        if pdf_info.get('ì—…ì²´'):
            related.append(f"ì—…ì²´: {pdf_info['ì—…ì²´']}")
        if pdf_info.get('ë„ì…ì—°ë„'):
            related.append(f"ë„ì…: {pdf_info['ë„ì…ì—°ë„']}ë…„")
        if related:
            formatted_info['ê´€ë ¨ì •ë³´'] = related
        
        return formatted_info
    
    def _analyze_user_intent(self, query: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„ì„"""
        query_lower = query.lower()
        
        intent = {
            'type': 'general',
            'needs_detail': False,
            'wants_comparison': False,
            'wants_recommendation': False,
            'is_urgent': False,
            'tone': 'informative',
            'context_keywords': [],
            'response_style': 'conversational'
        }
        
        # ì˜ë„ íŒŒì•…
        if any(word in query_lower for word in ['ìš”ì•½', 'ì •ë¦¬', 'ì•Œë ¤', 'ì„¤ëª…']):
            intent['type'] = 'summary'
            intent['needs_detail'] = True
        elif any(word in query_lower for word in ['ë¹„êµ', 'ì°¨ì´', 'ì–´ë–¤ê²Œ ë‚˜ì€', 'ë­ê°€ ì¢‹']):
            intent['type'] = 'comparison'
            intent['wants_comparison'] = True
            intent['wants_recommendation'] = True
        elif any(word in query_lower for word in ['ì¶”ì²œ', 'ê¶Œì¥', 'ì–´ë–»ê²Œ', 'ë°©ë²•']):
            intent['type'] = 'recommendation'
            intent['wants_recommendation'] = True
        elif any(word in query_lower for word in ['ê¸´ê¸‰', 'ë¹¨ë¦¬', 'ê¸‰í•´', 'ë°”ë¡œ']):
            intent['type'] = 'urgent'
            intent['is_urgent'] = True
            intent['tone'] = 'direct'
        elif any(word in query_lower for word in ['ì–¼ë§ˆ', 'ë¹„ìš©', 'ê°€ê²©', 'ê¸ˆì•¡']):
            intent['type'] = 'cost'
            intent['needs_detail'] = True
        elif any(word in query_lower for word in ['ë¬¸ì œ', 'ê³ ì¥', 'ìˆ˜ë¦¬', 'ì¥ì• ']):
            intent['type'] = 'problem'
            intent['wants_recommendation'] = True
        
        # ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        important_words = ['DVR', 'ì¤‘ê³„ì°¨', 'ì¹´ë©”ë¼', 'ì‚¼ê°ëŒ€', 'ë°©ì†¡', 'ì¥ë¹„', 'êµ¬ë§¤', 'ìˆ˜ë¦¬', 'êµì²´', 'ì—…ê·¸ë ˆì´ë“œ']
        intent['context_keywords'] = [word for word in important_words if word.lower() in query_lower]
        
        # ì‘ë‹µ ìŠ¤íƒ€ì¼ ê²°ì •
        if '?' in query:
            intent['response_style'] = 'explanatory'
        elif any(word in query_lower for word in ['í•´ì¤˜', 'ë¶€íƒ', 'ì¢€']):
            intent['response_style'] = 'helpful'
        
        return intent
    
    def _generate_conversational_response(self, context: str, query: str, intent: Dict[str, Any], 
                                         pdf_info: Dict[str, Any] = None) -> str:
        """ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± (ChatGPT/Claude ìŠ¤íƒ€ì¼)"""
        
        # LLMì—ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”í˜• ì‘ë‹µì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ì¤‘ìš” ì›ì¹™:
1. ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”í•˜ë“¯ ë‹µë³€í•˜ì„¸ìš”
2. í…œí”Œë¦¿ì´ë‚˜ ì •í˜•í™”ëœ í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ í•„ìš”ë¡œ í•˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
4. ì¶”ê°€ë¡œ ë„ì›€ì´ ë  ë§Œí•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì œì•ˆí•˜ì„¸ìš”
5. ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”"""
        
        # ì˜ë„ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
        if intent['type'] == 'summary':
            user_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ê°„ë‹¨íˆ ì„¤ëª…
- ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ì„¤ëª…
- ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì„ ì •ë³´ ì œì•ˆ
- ë”±ë”±í•œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹Œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì—°ê²°"""
        
        elif intent['type'] == 'comparison':
            user_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹„êµ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- ë¹„êµ ëŒ€ìƒë“¤ì˜ ì£¼ìš” ì°¨ì´ì ì„ ë¨¼ì € ì„¤ëª…
- ê°ê°ì˜ ì¥ë‹¨ì ì„ ì‹¤ìš©ì  ê´€ì ì—ì„œ ì„¤ëª…
- ìƒí™©ì— ë”°ë¥¸ ì¶”ì²œ ì œê³µ
- "ì´ëŸ° ê²½ìš°ì—” Aê°€ ì¢‹ê³ , ì €ëŸ° ê²½ìš°ì—” Bê°€ ë‚«ë‹¤"ëŠ” ì‹ìœ¼ë¡œ ì„¤ëª…"""
        
        elif intent['type'] == 'recommendation':
            user_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ìš©ì ì¸ ì¶”ì²œì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- ì¶”ì²œ ì‚¬í•­ì„ ëª…í™•í•˜ê²Œ ì œì‹œ
- ì¶”ì²œ ì´ìœ ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…
- ê³ ë ¤ì‚¬í•­ì´ë‚˜ ì£¼ì˜ì ë„ í•¨ê»˜ ì–¸ê¸‰
- ëŒ€ì•ˆì´ ìˆë‹¤ë©´ ê°„ë‹¨íˆ ì†Œê°œ"""
        
        elif intent['type'] == 'cost':
            user_prompt = f"""ë‹¤ìŒ ì •ë³´ì—ì„œ ë¹„ìš© ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- êµ¬ì²´ì ì¸ ê¸ˆì•¡ì„ ë¨¼ì € ì œì‹œ
- ë¹„ìš© êµ¬ì„±ì´ë‚˜ ë‚´ì—­ ì„¤ëª…
- ë¹„ìš© ëŒ€ë¹„ ê°€ì¹˜ë‚˜ íš¨ê³¼ ì–¸ê¸‰
- ì˜ˆì‚° ê´€ë ¨ ì¡°ì–¸ì´ ìˆë‹¤ë©´ ì¶”ê°€"""
        
        elif intent['is_urgent']:
            user_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹ ë¥´ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- í•µì‹¬ ì •ë³´ë¥¼ ë°”ë¡œ ì œì‹œ
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜ ì‚¬í•­ ì œê³µ
- ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­ ëª…ì‹œ"""
        
        else:
            user_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹µë³€ ë°©ì‹:
- ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€
- ê´€ë ¨ëœ ì¶”ê°€ ì •ë³´ ì œê³µ
- ê¶ê¸ˆí•  ë§Œí•œ ë‹¤ë¥¸ ì‚¬í•­ ì–¸ê¸‰
- ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í†¤ ìœ ì§€"""
        
        # LLM í˜¸ì¶œ
        if self.llm:
            try:
                # ëŒ€í™”í˜• ì‘ë‹µ ìƒì„±
                context_chunks = [{
                    'content': context,
                    'source': pdf_info.get('ì œëª©', 'document') if pdf_info else 'document',
                    'score': 1.0
                }]
                
                # ëŒ€í™”í˜• ì‘ë‹µ ë©”ì„œë“œ í˜¸ì¶œ
                response = self.llm.generate_conversational_response(query, context_chunks)
                
                if response and hasattr(response, 'answer'):
                    answer = response.answer
                else:
                    answer = str(response)
                
                # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë‚˜ ì¶”ì²œ ì‚¬í•­ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€
                if intent['wants_recommendation'] and 'ì¶”ì²œ' not in answer:
                    answer += "\n\nì°¸ê³ ë¡œ, ì´ì™€ ê´€ë ¨í•´ì„œ ì¶”ê°€ë¡œ ê²€í† í•˜ì‹œë©´ ì¢‹ì„ ì‚¬í•­ë“¤ë„ ìˆìŠµë‹ˆë‹¤. í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
                
                return answer
                
            except Exception as e:
                print(f"LLM ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
                # í´ë°±: ê¸°ë³¸ ì‘ë‹µ ìƒì„±
                return self._generate_fallback_response(context, query, intent)
        
        return self._generate_fallback_response(context, query, intent)
    
    def _generate_fallback_response(self, context: str, query: str, intent: Dict[str, Any]) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ í´ë°± ì‘ë‹µ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        lines = context.split('\n')
        key_info = []
        
        for line in lines:
            if any(keyword in line for keyword in ['ê¸ˆì•¡', 'ë¹„ìš©', 'ì›', 'ì œëª©', 'ê¸°ì•ˆ', 'ë‚ ì§œ']):
                key_info.append(line.strip())
        
        response = f"ë¬¸ì„œë¥¼ í™•ì¸í•œ ê²°ê³¼, "
        
        if intent['type'] == 'summary':
            response += f"ìš”ì²­í•˜ì‹  ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. "
            if key_info:
                response += ' '.join(key_info[:3])
        elif intent['type'] == 'cost':
            cost_info = [line for line in key_info if 'ì›' in line or 'ê¸ˆì•¡' in line]
            if cost_info:
                response += f"ë¹„ìš© ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤. {cost_info[0]}"
        else:
            if key_info:
                response += f"ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. {key_info[0]}"
        
        return response
    
    def _generate_llm_summary(self, pdf_path: Path, query: str) -> str:
        """LLMì„ ì‚¬ìš©í•œ ìƒì„¸ ìš”ì•½ - ëŒ€í™”í˜• ìŠ¤íƒ€ì¼"""
        
        # ì‚¬ìš©ì ì˜ë„ ë¶„ì„
        intent = self._analyze_user_intent(query)
        
        # PDF íŒŒì¼ì¸ ê²½ìš° ë¨¼ì € êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ ì‹œë„
        if pdf_path.suffix.lower() == '.pdf':
            pdf_info = self._extract_full_pdf_content(pdf_path)
            
            # ëŒ€í™”í˜• ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            context_parts = []
            summary = []
            
            # ìš”ì•½ì´ë‚˜ ë‚´ìš© ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°
            if pdf_info and 'error' not in pdf_info:
                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ
                if 'ì œëª©' in pdf_info:
                    context_parts.append(f"ì œëª©: {pdf_info['ì œëª©']}")
                if 'ê¸°ì•ˆì' in pdf_info:
                    context_parts.append(f"ê¸°ì•ˆì: {pdf_info['ê¸°ì•ˆì']}")
                if 'ê¸°ì•ˆì¼ì' in pdf_info:
                    context_parts.append(f"ì‘ì„±ì¼: {pdf_info['ê¸°ì•ˆì¼ì']}")
                if 'ê¸°ì•ˆë¶€ì„œ' in pdf_info:
                    context_parts.append(f"ë‹´ë‹¹ë¶€ì„œ: {pdf_info['ê¸°ì•ˆë¶€ì„œ']}")
                
                # ê°œìš”
                if 'ê°œìš”' in pdf_info:
                    overview = pdf_info['ê°œìš”'].replace('\n', ' ').strip()
                    if len(overview) > 300:
                        overview = overview[:300] + "..."
                    context_parts.append(f"\nê°œìš”: {overview}")
                
                # ì¥ì•  ë‚´ìš©
                if 'ì¥ì• ë‚´ìš©' in pdf_info and pdf_info['ì¥ì• ë‚´ìš©']:
                    ì¥ì• _text = pdf_info['ì¥ì• ë‚´ìš©'].replace('\n', ' ').strip()
                    if len(ì¥ì• _text) > 300:
                        ì¥ì• _text = ì¥ì• _text[:300] + "..."
                    context_parts.append(f"\nì¥ì•  ë‚´ìš©: {ì¥ì• _text}")
                
                # ì„¸ë¶€ í•­ëª© (ìƒˆë¡œ ì¶”ê°€)
                if 'ì„¸ë¶€í•­ëª©' in pdf_info and pdf_info['ì„¸ë¶€í•­ëª©']:
                    summary.append(f"\nğŸ”§ **ì„¸ë¶€ ì¥ì• /ìˆ˜ë¦¬ ë‚´ì—­**")
                    
                    # ì¤‘ê³„ì°¨ ë‚´ì™¸ê´€
                    ì¤‘ê³„ì°¨_items = [item for item in pdf_info['ì„¸ë¶€í•­ëª©'] if 'ë„ì–´' in item.get('í•­ëª©', '') or 'ë°œì „ê¸°' in item.get('í•­ëª©', '')]
                    if ì¤‘ê³„ì°¨_items:
                        summary.append("\n**[ì¤‘ê³„ì°¨ ë‚´ì™¸ê´€]**")
                        for item in ì¤‘ê³„ì°¨_items:
                            summary.append(f"â€¢ {item['í•­ëª©']}: {item['ë‚´ìš©']}")
                    
                    # ë°©ì†¡ ì‹œìŠ¤í…œ
                    ë°©ì†¡_items = [item for item in pdf_info['ì„¸ë¶€í•­ëª©'] if 'ë¹„ë””ì˜¤' in item.get('í•­ëª©', '') or 'ì˜¤ë””ì˜¤' in item.get('í•­ëª©', '')]
                    if ë°©ì†¡_items:
                        summary.append("\n**[ë°©ì†¡ ì‹œìŠ¤í…œ]**")
                        for item in ë°©ì†¡_items:
                            summary.append(f"â€¢ {item['í•­ëª©']}: {item['ë‚´ìš©']}")
                    
                    # ì§€ë¯¸ì§‘ ë“± ê¸°íƒ€ í•­ëª©
                    ê¸°íƒ€_items = [item for item in pdf_info['ì„¸ë¶€í•­ëª©'] if 'Tilt' in item.get('í•­ëª©', '') or 'Control' in item.get('í•­ëª©', '')]
                    if ê¸°íƒ€_items:
                        for item in ê¸°íƒ€_items:
                            summary.append(f"â€¢ {item['í•­ëª©']}: {item['ë‚´ìš©']}")
                
                # ë¹„ìš© ë‚´ì—­ (ê°œì„ )
                if 'ë¹„ìš©ë‚´ì—­' in pdf_info and pdf_info['ë¹„ìš©ë‚´ì—­']:
                    summary.append(f"\nğŸ’° **ë¹„ìš© ë‚´ì—­**")
                    if 'ë‚´ì™¸ê´€ë³´ìˆ˜' in pdf_info['ë¹„ìš©ë‚´ì—­']:
                        summary.append(f"â€¢ ì¤‘ê³„ì°¨ ë‚´ì™¸ê´€ ë³´ìˆ˜: {pdf_info['ë¹„ìš©ë‚´ì—­']['ë‚´ì™¸ê´€ë³´ìˆ˜']}")
                    if 'ë°©ì†¡ì‹œìŠ¤í…œ' in pdf_info['ë¹„ìš©ë‚´ì—­']:
                        summary.append(f"â€¢ ë°©ì†¡ ì‹œìŠ¤í…œ ë³´ìˆ˜: {pdf_info['ë¹„ìš©ë‚´ì—­']['ë°©ì†¡ì‹œìŠ¤í…œ']}")
                    if 'ì´í•©ê³„' in pdf_info['ë¹„ìš©ë‚´ì—­']:
                        summary.append(f"â€¢ **ì´ ë¹„ìš©: {pdf_info['ë¹„ìš©ë‚´ì—­']['ì´í•©ê³„']}**")
                # ê¸ˆì•¡ ì •ë³´ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
                elif 'ê¸ˆì•¡ì •ë³´' in pdf_info and pdf_info['ê¸ˆì•¡ì •ë³´']:
                    summary.append(f"\nğŸ’° **ì£¼ìš” ê¸ˆì•¡**")
                    # ê¸ˆì•¡ ì •ë ¬ ë° ìƒìœ„ í‘œì‹œ
                    amounts = []
                    for amt in pdf_info['ê¸ˆì•¡ì •ë³´']:
                        try:
                            amt_int = int(amt.replace(',', ''))
                            if amt_int > 1000000:  # 100ë§Œì› ì´ìƒë§Œ
                                amounts.append((amt, amt_int))
                        except (ValueError, AttributeError):
                            pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
                    amounts.sort(key=lambda x: x[1], reverse=True)
                    for amt, _ in amounts[:3]:
                        summary.append(f"â€¢ {amt}ì›")
                
                # ê²€í†  ì˜ê²¬ (ê°œì„ ëœ ì •ë¦¬)
                if 'ê²€í† ì˜ê²¬' in pdf_info and pdf_info['ê²€í† ì˜ê²¬']:
                    summary.append(f"\nğŸ“‹ **ê²€í†  ì˜ê²¬**")
                    opinion = pdf_info['ê²€í† ì˜ê²¬']
                    
                    # DVR ê´€ë ¨ ê²€í† ì¸ ê²½ìš°
                    if 'DVR' in opinion or ('1ì•ˆ' in opinion and '2ì•ˆ' in opinion):
                        # 1ì•ˆ ì¶”ì¶œ ë° ì •ë¦¬
                        if '1ì•ˆ' in opinion:
                            ì•ˆ1_text = re.search(r'1ì•ˆ[^2]*(?=2ì•ˆ|$)', opinion, re.DOTALL)
                            if ì•ˆ1_text:
                                ì•ˆ1_clean = re.sub(r'[\d]+\.\s*[\d]+\.\s*[\d]+.*?(?=\n)', '', ì•ˆ1_text.group(0))
                                ì•ˆ1_clean = re.sub(r'\[í˜ì´ì§€ \d+\]', '', ì•ˆ1_clean)
                                ì•ˆ1_clean = ' '.join(ì•ˆ1_clean.split())
                                # HD-SDI í™•ì¸ ë˜ëŠ” 1ì•ˆ ê´€ë ¨ ë‚´ìš©ì´ ìˆìœ¼ë©´ í‘œì‹œ
                                if 'HD-SDI' in ì•ˆ1_clean or 'HDê¸‰' in ì•ˆ1_clean or 'í™”ì§ˆ í–¥ìƒ' in ì•ˆ1_clean or '1ì•ˆ' in ì•ˆ1_clean:
                                    summary.append("\n**âœ… 1ì•ˆ: HD-SDI ì…ë ¥ ëª¨ë¸**")
                                    summary.append("â€¢ í™”ì§ˆ í–¥ìƒìœ¼ë¡œ ì˜ìƒ ê²€ìˆ˜ ìš©ì´")
                                    summary.append("â€¢ HDê¸‰ ë…¹í™”, ë‹¤ì–‘í•œ ì…ë ¥ ì§€ì›")
                                    summary.append("â€¢ ì¶”ê°€ ë¹„ìš© ë°œìƒ (ì»¨ë²„í„° ë“±)")
                        
                        # 2ì•ˆ ì¶”ì¶œ ë° ì •ë¦¬
                        if '2ì•ˆ' in opinion:
                            ì•ˆ2_text = re.search(r'2ì•ˆ[^ì¢…í•©]*(?=ì¢…í•©|$)', opinion, re.DOTALL)
                            if ì•ˆ2_text:
                                ì•ˆ2_clean = re.sub(r'[\d]+\.\s*[\d]+\.\s*[\d]+.*?(?=\n)', '', ì•ˆ2_text.group(0))
                                ì•ˆ2_clean = re.sub(r'\[í˜ì´ì§€ \d+\]', '', ì•ˆ2_clean)
                                if 'CVBS' in ì•ˆ2_clean or 'ê¸°ì¡´' in ì•ˆ2_clean:
                                    summary.append("\n**âšª 2ì•ˆ: ê¸°ì¡´ ë™ì¼ ëª¨ë¸**")
                                    summary.append("â€¢ í˜„ì¬ ì‹œìŠ¤í…œê³¼ í˜¸í™˜ì„± ë†’ìŒ")
                                    summary.append("â€¢ ë‚®ì€ ë¹„ìš©, ì„¤ì¹˜ ìš©ì´")
                                    summary.append("â€¢ SDê¸‰ í™”ì§ˆë¡œ ê°œì„  íš¨ê³¼ ì—†ìŒ")
                        
                        # ì¢…í•© ì˜ê²¬
                        if 'ì¢…í•©' in opinion or 'ê²°ë¡ ' in opinion:
                            summary.append("\n**ğŸ’¡ ìµœì¢… ì¶”ì²œ**")
                            if '1ì•ˆ' in opinion and ('ìœ ë¦¬' in opinion or 'ì ì ˆ' in opinion or 'ì¶”ì²œ' in opinion):
                                summary.append("â€¢ **1ì•ˆ ì±„íƒ ê¶Œì¥** - ì¥ê¸°ì  ìš´ì˜ ë° í™”ì§ˆ ê°œì„  í•„ìš”")
                            elif '2ì•ˆ' in opinion and ('ìœ ë¦¬' in opinion or 'ì ì ˆ' in opinion):
                                summary.append("â€¢ **2ì•ˆ ì±„íƒ ê¶Œì¥** - ë¹„ìš© ì ˆê° ìš°ì„ ")
                    
                    # ì¤‘ê³„ì°¨ ê´€ë ¨ì¸ ê²½ìš°
                    elif 'ì¤‘ê³„ì°¨ ì„ëŒ€' in opinion:
                        summary.append("â€¢ ì¤‘ê³„ì°¨ ì„ëŒ€: ê¸‰ì‘ìŠ¤ëŸ° íŠ¹ë³´ ìƒí™© ì‹œ ëŒ€ì‘ ì–´ë ¤ì›€")
                        if 'ì¤‘ê³„ì°¨ ì œì‘' in opinion:
                            summary.append("â€¢ ì‹ ê·œ ì œì‘: 25-30ì–µì› ê³¼ë„í•œ ë¹„ìš©, 4K ì†¡ì¶œ ì¼ì • ë¶ˆí™•ì‹¤")
                        if 'ë³´ìˆ˜í•˜ì—¬' in opinion:
                            summary.append("â€¢ **ê²°ë¡ : í˜„ ì¤‘ê³„ì°¨ ë³´ìˆ˜ë¡œ ì‹œìŠ¤í…œ ì•ˆì •ì„± í™•ë³´ê°€ ì ì ˆ**")
                    else:
                        # ì¼ë°˜ ê²€í†  ì˜ê²¬ (ê¸°ì¡´ ë°©ì‹)
                        if len(opinion) > 500:
                            opinion = opinion[:500] + "..."
                        summary.append(opinion)
                
                # ì£¼ìš” ë‚´ìš© (ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ) - ê¸°ì¡´ ì½”ë“œ ìœ ì§€
                if 'ì „ì²´í…ìŠ¤íŠ¸' in pdf_info:
                    full_text = pdf_info['ì „ì²´í…ìŠ¤íŠ¸']
                    
                    # ë„ì… ì—°ë„ ì°¾ê¸°
                    if 'ë„ì…' in query or 'ì–¸ì œ' in query:
                        ë„ì…_match = re.search(r'ë„ì…\s*ë…„ë„\s*[:ï¼š]?\s*(\d{4})', full_text)
                        if ë„ì…_match:
                            summary.append(f"\nğŸ“… **ë„ì… ì—°ë„**: {ë„ì…_match.group(1)}ë…„")
                
                # ì—…ì²´ ì •ë³´
                if 'ì—…ì²´' in pdf_info:
                    summary.append(f"\nğŸ¢ **ê´€ë ¨ ì—…ì²´**: {pdf_info['ì—…ì²´']}")
            
            # ê¸°ë³¸ ì •ë³´ë¥¼ ë³´ê´€ (if ë¸”ë¡ ë°–ìœ¼ë¡œ ì´ë™)
            basic_summary = '\n'.join(summary) if summary else ""
            
            # ëŒ€í™”í˜• ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš° ë°”ë¡œ ì²˜ë¦¬
            if 'ìš”ì•½' in query or 'ë‚´ìš©' in query or 'ì„¤ëª…' in query:
                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                context_text = ""
                if pdf_info and 'error' not in pdf_info:
                    # ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                    context_parts = []
                    if 'ì œëª©' in pdf_info:
                        context_parts.append(f"ë¬¸ì„œ ì œëª©: {pdf_info['ì œëª©']}")
                    if 'ê¸°ì•ˆì' in pdf_info:
                        context_parts.append(f"ì‘ì„±ì: {pdf_info['ê¸°ì•ˆì']}")
                    if 'ê¸°ì•ˆì¼ì' in pdf_info:
                        context_parts.append(f"ì‘ì„±ì¼: {pdf_info['ê¸°ì•ˆì¼ì']}")
                    if 'ê°œìš”' in pdf_info:
                        context_parts.append(f"\nê°œìš”: {pdf_info['ê°œìš”']}")
                    if 'ê¸ˆì•¡ì •ë³´' in pdf_info:
                        amounts = pdf_info['ê¸ˆì•¡ì •ë³´']
                        if amounts:
                            # ê°€ì¥ í° ê¸ˆì•¡ë§Œ ì£¼ìš” ê¸ˆì•¡ìœ¼ë¡œ í‘œì‹œ
                            main_amount = amounts[0] if amounts else None
                            if main_amount:
                                # ê¸ˆì•¡ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í•¨ê»˜ í‘œì‹œ
                                if 'ê¸ˆì•¡ì»¨í…ìŠ¤íŠ¸' in pdf_info and pdf_info['ê¸ˆì•¡ì»¨í…ìŠ¤íŠ¸']:
                                    context_info = pdf_info['ê¸ˆì•¡ì»¨í…ìŠ¤íŠ¸'][0].get('context', '')
                                    # ì´ì•¡, í•©ê³„ ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ëª…ì‹œ
                                    if 'ì´ì•¡' in context_info or 'í•©ê³„' in context_info:
                                        context_parts.append(f"\nì´ ê¸ˆì•¡: {main_amount}ì›")
                                    else:
                                        context_parts.append(f"\nê¸ˆì•¡: {main_amount}ì›")
                                else:
                                    context_parts.append(f"\nê¸ˆì•¡: {main_amount}ì›")
                    if 'ê²€í† ì˜ê²¬' in pdf_info:
                        context_parts.append(f"\nê²€í†  ì˜ê²¬: {pdf_info['ê²€í† ì˜ê²¬'][:500]}")
                    
                    context_text = "\n".join(context_parts)
                
                # LLM ë¡œë“œ
                if self.llm is None:
                    if not LLMSingleton.is_loaded():
                        print("ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
                    self.llm = LLMSingleton.get_instance(model_path=self.model_path)
                
                # ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± - ì „ì²´ í…ìŠ¤íŠ¸ í¬í•¨
                if self.llm and context_text:
                    # ì „ì²´ í…ìŠ¤íŠ¸ë„ í¬í•¨ (ì¤‘ìš” ì •ë³´ ëˆ„ë½ ë°©ì§€)
                    if 'ì „ì²´í…ìŠ¤íŠ¸' in pdf_info and pdf_info['ì „ì²´í…ìŠ¤íŠ¸']:
                        full_context = f"{context_text}\n\n[ì „ì²´ ë¬¸ì„œ ë‚´ìš©]\n{pdf_info['ì „ì²´í…ìŠ¤íŠ¸'][:3000]}"
                    else:
                        full_context = context_text
                    
                    context_chunks = [{
                        'content': full_context,
                        'source': pdf_path.name,
                        'score': 1.0
                    }]
                    
                    response = self.llm.generate_conversational_response(query, context_chunks)
                    
                    if response and hasattr(response, 'answer'):
                        answer = response.answer
                    else:
                        answer = str(response)
                    
                    # ì¶œì²˜ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€
                    if pdf_path.name not in answer:
                        answer += f"\n\n(ì°¸ê³ : {pdf_path.name})"
                    
                    return answer
            
            # ëŒ€í™”í˜•ì´ ì•„ë‹Œ ê²½ìš° ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        
        # LLM ë¡œë“œ (í•„ìš”ì‹œ) - ì‹±ê¸€í†¤ ì‚¬ìš©
        if self.llm is None:
            if not LLMSingleton.is_loaded():
                print("ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.llm = LLMSingleton.get_instance(model_path=self.model_path)
        
        # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì½ê¸°
        try:
            text = ""
            
            # TXT íŒŒì¼ì¸ ê²½ìš°
            if pdf_path.suffix.lower() == '.txt':
                with open(pdf_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            # PDF íŒŒì¼ì¸ ê²½ìš°
            else:
                with pdfplumber.open(pdf_path) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                
                # pdfplumber ì‹¤íŒ¨ì‹œ OCR ì‹œë„
                if not text:
                    try:
                        from rag_system.enhanced_ocr_processor import EnhancedOCRProcessor
                        ocr = EnhancedOCRProcessor()
                        text, _ = ocr.extract_text_with_ocr(str(pdf_path))
                    except Exception as e:
                        pass
            
            if not text:
                return "âŒ ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            
            # ê°„ë‹¨í•˜ê³  ëª…í™•í•œ í”„ë¡¬í”„íŠ¸
            # ê¸°ìˆ ê´€ë¦¬íŒ€ ì‹¤ë¬´ ìµœì í™” í”„ë¡¬í”„íŠ¸
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ì¦ê°€ (5000 -> 15000)
            max_text_length = 15000  # 15,000ìë¡œ ì¦ê°€
            
            # ê¸°ë³¸ ì •ë³´ê°€ ì´ë¯¸ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ì¡°ì •
            if 'basic_summary' in locals():
                # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸
                prompt = self._get_detail_only_prompt(query, text[:max_text_length], pdf_path.name)
            else:
                prompt = self._get_optimized_prompt(query, text[:max_text_length], pdf_path.name)
            
            # LLM í˜¸ì¶œ - ëŒ€í™”í˜• ì‘ë‹µ ìš°ì„ 
            context_chunks = [{'content': text[:max_text_length], 'metadata': {'source': pdf_path.name}, 'score': 1.0}]
            
            # ìš”ì•½/ë‚´ìš© ìš”ì²­ì¸ ê²½ìš° ëŒ€í™”í˜• ì‘ë‹µ
            if 'ìš”ì•½' in query or 'ë‚´ìš©' in query or 'ì„¤ëª…' in query:
                response = self.llm.generate_conversational_response(query, context_chunks)
            else:
                # ê¸°ì¡´ ë°©ì‹
                response = self.llm.generate_response(prompt, context_chunks)
            
            answer = response.answer if hasattr(response, 'answer') else str(response)
            
            # ëŒ€í™”í˜• ì‘ë‹µì¸ ê²½ìš° ì¶œì²˜ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€
            if 'ìš”ì•½' in query or 'ë‚´ìš©' in query or 'ì„¤ëª…' in query:
                # ì¶œì²˜ê°€ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì¶”ê°€
                if pdf_path.name not in answer:
                    answer += f"\n\n(ì°¸ê³  ë¬¸ì„œ: {pdf_path.name})"
                return answer
            else:
                # ê¸°ì¡´ ë°©ì‹ (í…œí”Œë¦¿ í˜•ì‹)
                if 'basic_summary' in locals() and basic_summary:
                    combined_answer = f"{basic_summary}\n\nğŸ“‹ **ìƒì„¸ ë‚´ìš©**\n{answer}"
                    return f"{combined_answer}\n\nğŸ“„ ì¶œì²˜: {pdf_path.name}"
                else:
                    return f"{answer}\n\nğŸ“„ ì¶œì²˜: {pdf_path.name}"
            
        except Exception as e:
            return f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _collect_statistics_data(self, query: str) -> Dict:
        """í†µê³„ ë°ì´í„° ìˆ˜ì§‘ ë° êµ¬ì¡°í™”"""
        stats_data = {
            'title': '',
            'headers': [],
            'table_data': [],
            'ì´ê³„': '',
            'ë¶„ì„': {},
            'ì¶”ì²œ': []
        }
        
        # ì—°ë„ ì¶”ì¶œ
        year_match = re.search(r'(20\d{2})', query)
        target_year = year_match.group(1) if year_match else None
        
        if "ì—°ë„ë³„" in query and "êµ¬ë§¤" in query:
            stats_data['title'] = "ì—°ë„ë³„ êµ¬ë§¤ í˜„í™©"
            stats_data['headers'] = ['ì—°ë„', 'ê±´ìˆ˜', 'ì´ ê¸ˆì•¡', 'ì£¼ìš” í’ˆëª©']
            
            yearly_data = {}
            for filename, metadata in self.metadata_cache.items():
                if 'êµ¬ë§¤' in filename or 'êµ¬ì…' in filename:
                    year = metadata['year']
                    if year not in yearly_data:
                        yearly_data[year] = {'count': 0, 'total': 0, 'items': []}
                    
                    yearly_data[year]['count'] += 1
                    # ê¸ˆì•¡ ì¶”ì¶œ ë¡œì§
                    pdf_path = self.docs_dir / filename
                    info = self._extract_pdf_info(pdf_path)
                    if info.get('ê¸ˆì•¡'):
                        amount = self._parse_amount(info['ê¸ˆì•¡'])
                        yearly_data[year]['total'] += amount
                    if info.get('í’ˆëª©'):
                        yearly_data[year]['items'].append(info['í’ˆëª©'])
            
            # í…Œì´ë¸” ë°ì´í„° ìƒì„±
            total_amount = 0
            for year in sorted(yearly_data.keys()):
                data = yearly_data[year]
                total_amount += data['total']
                items_str = ', '.join(data['items'][:2])  # ìƒìœ„ 2ê°œë§Œ
                if len(data['items']) > 2:
                    items_str += f" ì™¸ {len(data['items'])-2}ê±´"
                
                stats_data['table_data'].append([
                    year,
                    f"{data['count']}ê±´",
                    f"{data['total']:,}ì›",
                    items_str
                ])
            
            stats_data['ì´ê³„'] = f"{total_amount:,}ì›"
            stats_data['ë¶„ì„']['í‰ê·  ì—°ê°„ êµ¬ë§¤ì•¡'] = f"{total_amount // len(yearly_data):,}ì›"
            stats_data['ì¶”ì²œ'].append("êµ¬ë§¤ ì§‘ì¤‘ ì‹œê¸°ë¥¼ íŒŒì•…í•˜ì—¬ ì˜ˆì‚° ê³„íš ìˆ˜ë¦½")
            
        elif target_year:
            # íŠ¹ì • ì—°ë„ ì „ì²´ í†µê³„
            stats_data['title'] = f"{target_year}ë…„ ì „ì²´ í˜„í™©"
            stats_data['headers'] = ['êµ¬ë¶„', 'ê±´ìˆ˜', 'ì´ ê¸ˆì•¡', 'ë¹„ìœ¨']
            
            categories = {'êµ¬ë§¤': 0, 'ìˆ˜ë¦¬': 0, 'íê¸°': 0, 'ê¸°íƒ€': 0}
            amounts = {'êµ¬ë§¤': 0, 'ìˆ˜ë¦¬': 0, 'íê¸°': 0, 'ê¸°íƒ€': 0}
            
            for filename, metadata in self.metadata_cache.items():
                if metadata['year'] == target_year:
                    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                    if 'êµ¬ë§¤' in filename or 'êµ¬ì…' in filename:
                        cat = 'êµ¬ë§¤'
                    elif 'ìˆ˜ë¦¬' in filename or 'ë³´ìˆ˜' in filename:
                        cat = 'ìˆ˜ë¦¬'
                    elif 'íê¸°' in filename:
                        cat = 'íê¸°'
                    else:
                        cat = 'ê¸°íƒ€'
                    
                    categories[cat] += 1
                    
                    # ê¸ˆì•¡ ì¶”ì¶œ
                    pdf_path = self.docs_dir / filename
                    info = self._extract_pdf_info(pdf_path)
                    if info.get('ê¸ˆì•¡'):
                        amounts[cat] += self._parse_amount(info['ê¸ˆì•¡'])
            
            # ì´ê³„ ê³„ì‚°
            total_docs = sum(categories.values())
            total_amount = sum(amounts.values())
            
            # í…Œì´ë¸” ë°ì´í„° ìƒì„±
            for cat in ['êµ¬ë§¤', 'ìˆ˜ë¦¬', 'íê¸°', 'ê¸°íƒ€']:
                if categories[cat] > 0:
                    ratio = (categories[cat] / total_docs * 100) if total_docs > 0 else 0
                    stats_data['table_data'].append([
                        cat,
                        f"{categories[cat]}ê±´",
                        f"{amounts[cat]:,}ì›",
                        f"{ratio:.1f}%"
                    ])
            
            stats_data['ì´ê³„'] = f"ë¬¸ì„œ {total_docs}ê±´, ê¸ˆì•¡ {total_amount:,}ì›"
            stats_data['ë¶„ì„']['êµ¬ë§¤ ë¹„ì¤‘'] = f"{amounts['êµ¬ë§¤']/total_amount*100:.1f}%" if total_amount > 0 else "0%"
            stats_data['ë¶„ì„']['ìˆ˜ë¦¬ ë¹„ì¤‘'] = f"{amounts['ìˆ˜ë¦¬']/total_amount*100:.1f}%" if total_amount > 0 else "0%"
        
        return stats_data
    
    def _generate_statistics_report(self, query: str) -> str:
        """ì „ì²´ ë¬¸ì„œì— ëŒ€í•œ í†µê³„ ë³´ê³ ì„œ ìƒì„± - êµ¬ì¡°í™”ëœ í¬ë§·"""
        try:
            # formatter ì‚¬ìš© ê°€ëŠ¥ ì‹œ êµ¬ì¡°í™”ëœ í¬ë§· ì ìš©
            if self.formatter:
                stats_data = self._collect_statistics_data(query)
                return self.formatter.format_statistics_response(stats_data, query)
            
            # ê¸°ì¡´ ë°©ì‹ (formatter ì—†ì„ ë•Œ)
            # í†µê³„ íƒ€ì… íŒŒì•…
            if "ì—°ë„ë³„" in query and "êµ¬ë§¤" in query:
                return self._generate_yearly_purchase_report(query)
            elif "ê¸°ì•ˆìë³„" in query:
                return self._generate_drafter_report(query)
            elif "ì›”ë³„" in query and "ìˆ˜ë¦¬" in query:
                return self._generate_monthly_repair_report(query)
            
            # ê¸°ë³¸: íŠ¹ì • ì—°ë„ ì „ì²´ í†µê³„
            year_match = re.search(r'(20\d{2})', query)
            target_year = year_match.group(1) if year_match else None
            
            # í†µê³„ ë°ì´í„° ìˆ˜ì§‘
            stats = {
                'êµ¬ë§¤': [],
                'ìˆ˜ë¦¬': [],
                'íê¸°': [],
                'ì†Œëª¨í’ˆ': [],
                'ê¸°íƒ€': []
            }
            
            drafters = {}  # ê¸°ì•ˆìë³„ í†µê³„
            monthly = {}  # ì›”ë³„ í†µê³„
            total_amount = 0
            doc_count = 0
            
            for filename, metadata in self.metadata_cache.items():
                # ì—°ë„ í•„í„°ë§
                if target_year and metadata['year'] != target_year:
                    continue
                
                doc_count += 1
                pdf_path = self.docs_dir / filename
                info = self._extract_pdf_info(pdf_path)
                
                # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                category = 'ê¸°íƒ€'
                if 'êµ¬ë§¤' in filename:
                    category = 'êµ¬ë§¤'
                elif 'ìˆ˜ë¦¬' in filename or 'ë³´ìˆ˜' in filename:
                    category = 'ìˆ˜ë¦¬'
                elif 'íê¸°' in filename:
                    category = 'íê¸°'
                elif 'ì†Œëª¨í’ˆ' in filename:
                    category = 'ì†Œëª¨í’ˆ'
                
                # í†µê³„ì— ì¶”ê°€
                doc_info = {
                    'filename': filename,
                    'date': info.get('ë‚ ì§œ', ''),
                    'drafter': info.get('ê¸°ì•ˆì', 'ë¯¸ìƒ'),
                    'amount': info.get('ê¸ˆì•¡', ''),
                    'title': info.get('ì œëª©', filename.replace('.pdf', ''))
                }
                
                stats[category].append(doc_info)
                
                # ê¸°ì•ˆìë³„ í†µê³„
                drafter = doc_info['drafter']
                if drafter not in drafters:
                    drafters[drafter] = 0
                drafters[drafter] += 1
                
                # ì›”ë³„ í†µê³„ (ë‚ ì§œê°€ ìˆëŠ” ê²½ìš°)
                if doc_info['date']:
                    month_match = re.search(r'-(\d{2})-', doc_info['date'])
                    if month_match:
                        month = month_match.group(1)
                        if month not in monthly:
                            monthly[month] = 0
                        monthly[month] += 1
                
                # ê¸ˆì•¡ í•©ê³„
                if doc_info['amount']:
                    amount_num = re.search(r'(\d+(?:,\d+)*)', doc_info['amount'])
                    if amount_num:
                        try:
                            total_amount += int(amount_num.group(1).replace(',', ''))
                        except (ValueError, AttributeError):
                            pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
            
            # ë³´ê³ ì„œ ìƒì„±
            report = []
            
            if target_year:
                report.append(f"ğŸ“Š {target_year}ë…„ ê¸°ìˆ ê´€ë¦¬íŒ€ ë¬¸ì„œ í†µê³„ ë³´ê³ ì„œ")
            else:
                report.append("ğŸ“Š ì „ì²´ ê¸°ê°„ ê¸°ìˆ ê´€ë¦¬íŒ€ ë¬¸ì„œ í†µê³„ ë³´ê³ ì„œ")
            
            report.append("=" * 50)
            report.append("")
            
            # ì „ì²´ ìš”ì•½
            report.append("### ğŸ“Š ì „ì²´ ìš”ì•½")
            report.append(f"â€¢ ì´ ë¬¸ì„œ ìˆ˜: {doc_count}ê°œ")
            if total_amount > 0:
                report.append(f"â€¢ ì´ ê¸ˆì•¡: {total_amount:,}ì›")
            report.append("")
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            report.append("### ğŸ“ ì¹´í…Œê³ ë¦¬ë³„ í˜„í™©")
            report.append("")
            
            for category, docs in stats.items():
                if docs:
                    count = len(docs)
                    ratio = (count / doc_count * 100) if doc_count > 0 else 0
                    report.append(f"â€¢ **{category}**: {count}ê±´ ({ratio:.1f}%)")
            report.append("")
            
            # ê¸°ì•ˆìë³„ í†µê³„
            if drafters:
                report.append("### ğŸ‘¥ ê¸°ì•ˆìë³„ í˜„í™©")
                report.append("")
                
                for drafter, count in sorted(drafters.items(), key=lambda x: x[1], reverse=True):
                    if drafter and drafter != 'ë¯¸ìƒ':
                        report.append(f"â€¢ **{drafter}**: {count}ê±´")
                
                report.append("")
            
            # ì›”ë³„ í†µê³„ (ì—°ë„ ì§€ì •ì‹œ)
            if target_year and monthly:
                report.append("### ğŸ“… ì›”ë³„ í˜„í™©")
                report.append("")
                
                for month in sorted(monthly.keys()):
                    count = monthly[month]
                    report.append(f"â€¢ **{int(month)}ì›”**: {count}ê±´")
                
                report.append("")
            
            # ì£¼ìš” ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            report.append("### ğŸ“„ ì£¼ìš” ë¬¸ì„œ ëª©ë¡")
            for category, docs in stats.items():
                if docs:
                    report.append(f"\nâ–¶ {category} ({len(docs)}ê±´)")
                    for doc in docs[:3]:  # ê° ì¹´í…Œê³ ë¦¬ë³„ ìµœëŒ€ 3ê°œ
                        date = doc['date'][:10] if doc['date'] else 'ë‚ ì§œì—†ìŒ'
                        drafter = doc['drafter'] if doc['drafter'] != 'ë¯¸ìƒ' else ''
                        amount = f" - {doc['amount']}" if doc['amount'] else ""
                        
                        title = doc['title'][:30] + "..." if len(doc['title']) > 30 else doc['title']
                        report.append(f"  â€¢ [{date}] {title}")
                        if drafter or amount:
                            report.append(f"    {drafter}{amount}")
            
            return "\n".join(report)
            
        except Exception as e:
            return f"âŒ í†µê³„ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _generate_yearly_purchase_report(self, query: str) -> str:
        """ì—°ë„ë³„ êµ¬ë§¤ í˜„í™© ë³´ê³ ì„œ"""
        try:
            yearly_stats = {}
            
            for filename, metadata in self.metadata_cache.items():
                if 'êµ¬ë§¤' not in filename:
                    continue
                    
                year = metadata['year']
                if year not in yearly_stats:
                    yearly_stats[year] = {'count': 0, 'amount': 0, 'items': []}
                
                pdf_path = self.docs_dir / filename
                info = self._extract_pdf_info(pdf_path)
                
                yearly_stats[year]['count'] += 1
                yearly_stats[year]['items'].append({
                    'filename': filename,
                    'date': info.get('ë‚ ì§œ', ''),
                    'drafter': info.get('ê¸°ì•ˆì', ''),
                    'amount': info.get('ê¸ˆì•¡', ''),
                    'title': info.get('ì œëª©', filename.replace('.pdf', ''))
                })
                
                # ê¸ˆì•¡ í•©ê³„
                if info.get('ê¸ˆì•¡'):
                    amount_num = re.search(r'(\d+(?:,\d+)*)', info['ê¸ˆì•¡'])
                    if amount_num:
                        try:
                            yearly_stats[year]['amount'] += int(amount_num.group(1).replace(',', ''))
                        except (ValueError, AttributeError):
                            pass  # ê¸ˆì•¡ ë³€í™˜ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
            
            # ë³´ê³ ì„œ ìƒì„±
            report = []
            report.append("ğŸ“Š ì—°ë„ë³„ êµ¬ë§¤ í˜„í™© ë³´ê³ ì„œ (2021-2025)")
            report.append("=" * 50)
            report.append("")
            
            report.append("### ğŸ“ˆ ì—°ë„ë³„ êµ¬ë§¤ í†µê³„")
            report.append("")
            
            total_count = 0
            total_amount = 0
            
            for year in sorted(yearly_stats.keys()):
                stats = yearly_stats[year]
                total_count += stats['count']
                total_amount += stats['amount']
                
                amount_str = f"{stats['amount']:,}ì›" if stats['amount'] > 0 else "ê¸ˆì•¡ë¯¸ìƒ"
                report.append(f"â€¢ **{year}ë…„**: {stats['count']}ê±´ - {amount_str}")
            
            report.append("")
            report.append(f"**ğŸ“Š ì´ê³„: {total_count}ê±´ - {total_amount:,}ì›**")
            report.append("")
            
            # ì—°ë„ë³„ ìƒì„¸ ë‚´ì—­
            report.append("### ğŸ“„ ì—°ë„ë³„ ìƒì„¸ ë‚´ì—­")
            for year in sorted(yearly_stats.keys()):
                stats = yearly_stats[year]
                if stats['items']:
                    report.append(f"\nâ–¶ {year}ë…„ ({stats['count']}ê±´)")
                    for item in stats['items']:
                        date = item['date'][:10] if item['date'] else 'ë‚ ì§œì—†ìŒ'
                        title = item['title'][:35] + "..." if len(item['title']) > 35 else item['title']
                        amount = f" - {item['amount']}" if item['amount'] else ""
                        report.append(f"  â€¢ [{date}] {title}{amount}")
            
            return "\n".join(report)
            
        except Exception as e:
            return f"âŒ ì—°ë„ë³„ êµ¬ë§¤ í˜„í™© ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _generate_drafter_report(self, query: str) -> str:
        """ê¸°ì•ˆìë³„ ë¬¸ì„œ í˜„í™© ë³´ê³ ì„œ"""
        try:
            drafter_stats = {}
            
            for filename, metadata in self.metadata_cache.items():
                pdf_path = self.docs_dir / filename
                info = self._extract_pdf_info(pdf_path)
                
                drafter = info.get('ê¸°ì•ˆì', 'ë¯¸ìƒ')
                if drafter not in drafter_stats:
                    drafter_stats[drafter] = {
                        'count': 0,
                        'categories': {'êµ¬ë§¤': 0, 'ìˆ˜ë¦¬': 0, 'íê¸°': 0, 'ê¸°íƒ€': 0},
                        'years': {},
                        'items': []
                    }
                
                # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                category = 'ê¸°íƒ€'
                if 'êµ¬ë§¤' in filename:
                    category = 'êµ¬ë§¤'
                elif 'ìˆ˜ë¦¬' in filename or 'ë³´ìˆ˜' in filename:
                    category = 'ìˆ˜ë¦¬'
                elif 'íê¸°' in filename:
                    category = 'íê¸°'
                
                drafter_stats[drafter]['count'] += 1
                drafter_stats[drafter]['categories'][category] += 1
                
                # ì—°ë„ë³„ ì§‘ê³„
                year = metadata['year']
                if year not in drafter_stats[drafter]['years']:
                    drafter_stats[drafter]['years'][year] = 0
                drafter_stats[drafter]['years'][year] += 1
                
                drafter_stats[drafter]['items'].append({
                    'filename': filename,
                    'date': info.get('ë‚ ì§œ', ''),
                    'category': category,
                    'title': info.get('ì œëª©', filename.replace('.pdf', ''))
                })
            
            # ë³´ê³ ì„œ ìƒì„±
            report = []
            report.append("ğŸ“Š ê¸°ì•ˆìë³„ ë¬¸ì„œ ì‘ì„± í˜„í™©")
            report.append("=" * 50)
            report.append("")
            
            report.append("### ğŸ“Š ê¸°ì•ˆìë³„ ì „ì²´ í†µê³„")
            report.append("")
            
            for drafter in sorted(drafter_stats.keys()):
                if drafter and drafter != 'ë¯¸ìƒ':
                    stats = drafter_stats[drafter]
                    cat_str = f"êµ¬ë§¤ {stats['categories']['êµ¬ë§¤']}ê±´, ìˆ˜ë¦¬ {stats['categories']['ìˆ˜ë¦¬']}ê±´"
                    if stats['categories']['íê¸°'] > 0:
                        cat_str += f", íê¸° {stats['categories']['íê¸°']}ê±´"
                    if stats['categories']['ê¸°íƒ€'] > 0:
                        cat_str += f", ê¸°íƒ€ {stats['categories']['ê¸°íƒ€']}ê±´"
                    report.append(f"â€¢ **{drafter}**: ì´ {stats['count']}ê±´ ({cat_str})")
            
            report.append("")
            
            # ê¸°ì•ˆìë³„ ì—°ë„ ë¶„í¬
            report.append("### ğŸ“… ê¸°ì•ˆìë³„ ì—°ë„ ë¶„í¬")
            for drafter in sorted(drafter_stats.keys()):
                if drafter and drafter != 'ë¯¸ìƒ':
                    stats = drafter_stats[drafter]
                    year_str = ", ".join([f"{year}ë…„({count}ê±´)" for year, count in sorted(stats['years'].items())])
                    report.append(f"â€¢ {drafter}: {year_str}")
            report.append("")
            
            # ê¸°ì•ˆìë³„ ëª¨ë“  ë¬¸ì„œ
            report.append("### ğŸ“ ê¸°ì•ˆìë³„ ë‹´ë‹¹ ë¬¸ì„œ (ì „ì²´)")
            report.append("*ğŸ’¡ ì‹¤ë¬´ ë‹´ë‹¹ìì—ê²Œ ì§ì ‘ ë¬¸ì˜ ê°€ëŠ¥*")
            report.append("")
            
            for drafter in sorted(drafter_stats.keys()):
                if drafter and drafter != 'ë¯¸ìƒ':
                    stats = drafter_stats[drafter]
                    report.append(f"#### ğŸ‘¤ **{drafter}** ({stats['count']}ê±´)")
                    
                    # ì—°ë„ë³„ë¡œ ê·¸ë£¹í™”
                    docs_by_year = {}
                    for item in sorted(stats['items'], key=lambda x: x['date'], reverse=True):
                        year = item['date'][:4] if item['date'] else 'ì—°ë„ì—†ìŒ'
                        if year not in docs_by_year:
                            docs_by_year[year] = []
                        docs_by_year[year].append(item)
                    
                    # ì—°ë„ë³„ë¡œ í‘œì‹œ
                    for year in sorted(docs_by_year.keys(), reverse=True):
                        report.append(f"\n**{year}ë…„:**")
                        for item in docs_by_year[year]:
                            date = item['date'][5:10] if item['date'] and len(item['date']) >= 10 else 'ë‚ ì§œì—†ìŒ'
                            cat_emoji = {
                                'êµ¬ë§¤': 'ğŸ›’',
                                'ìˆ˜ë¦¬': 'ğŸ”§', 
                                'íê¸°': 'ğŸ—‘ï¸',
                                'ê¸°íƒ€': 'ğŸ“‹'
                            }.get(item['category'], 'ğŸ“‹')
                            
                            # ì „ì²´ ì œëª© í‘œì‹œ (ì¶•ì•½ ì—†ì´)
                            title = item['title']
                            report.append(f"  â€¢ [{date}] {cat_emoji} {title}")
                    report.append("")
            
            return "\n".join(report)
            
        except Exception as e:
            return f"âŒ ê¸°ì•ˆìë³„ í˜„í™© ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _generate_monthly_repair_report(self, query: str) -> str:
        """ì›”ë³„ ìˆ˜ë¦¬ ë‚´ì—­ ë³´ê³ ì„œ"""
        try:
            monthly_stats = {}
            total_amount = 0
            
            for filename, metadata in self.metadata_cache.items():
                if 'ìˆ˜ë¦¬' not in filename and 'ë³´ìˆ˜' not in filename:
                    continue
                
                pdf_path = self.docs_dir / filename
                info = self._extract_pdf_info(pdf_path)
                
                # ì›” ì¶”ì¶œ
                date = info.get('ë‚ ì§œ', '')
                if date:
                    month_match = re.search(r'-(\d{2})-', date)
                    if month_match:
                        month = int(month_match.group(1))
                        year = metadata['year']
                        month_key = f"{year}-{month:02d}"
                        
                        if month_key not in monthly_stats:
                            monthly_stats[month_key] = {'count': 0, 'amount': 0, 'items': []}
                        
                        monthly_stats[month_key]['count'] += 1
                        monthly_stats[month_key]['items'].append({
                            'filename': filename,
                            'date': date,
                            'drafter': info.get('ê¸°ì•ˆì', ''),
                            'amount': info.get('ê¸ˆì•¡', ''),
                            'title': info.get('ì œëª©', filename.replace('.pdf', ''))
                        })
                        
                        # ê¸ˆì•¡ í•©ê³„
                        if info.get('ê¸ˆì•¡'):
                            amount_num = re.search(r'(\d+(?:,\d+)*)', info['ê¸ˆì•¡'])
                            if amount_num:
                                try:
                                    amount = int(amount_num.group(1).replace(',', ''))
                                    monthly_stats[month_key]['amount'] += amount
                                    total_amount += amount
                                except (ValueError, KeyError):
                                    pass  # ê¸ˆì•¡ ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
            
            # ë³´ê³ ì„œ ìƒì„±
            report = []
            report.append("ğŸ“Š ì›”ë³„ ìˆ˜ë¦¬ ë‚´ì—­ ë° ë¹„ìš© ë¶„ì„")
            report.append("=" * 50)
            report.append("")
            
            report.append("### ğŸ“Š ì „ì²´ ìš”ì•½")
            total_count = sum(stats['count'] for stats in monthly_stats.values())
            report.append(f"â€¢ ì´ ìˆ˜ë¦¬ ê±´ìˆ˜: {total_count}ê±´")
            if total_amount > 0:
                report.append(f"â€¢ ì´ ìˆ˜ë¦¬ ë¹„ìš©: {total_amount:,}ì›")
                report.append(f"â€¢ í‰ê·  ìˆ˜ë¦¬ ë¹„ìš©: {total_amount // total_count:,}ì›")
            report.append("")
            
            report.append("### ğŸ“Š ì›”ë³„ ìˆ˜ë¦¬ í˜„í™©")
            report.append("")
            
            for month_key in sorted(monthly_stats.keys()):
                stats = monthly_stats[month_key]
                year, month = month_key.split('-')
                amount_str = f"{stats['amount']:,}ì›" if stats['amount'] > 0 else "ê¸ˆì•¡ë¯¸ìƒ"
                report.append(f"â€¢ **{year}ë…„ {int(month)}ì›”**: {stats['count']}ê±´ - {amount_str}")
            
            report.append("")
            
            # ì›”ë³„ ìƒì„¸ ë‚´ì—­
            report.append("### ğŸ“ ì›”ë³„ ìƒì„¸ ìˆ˜ë¦¬ ë‚´ì—­")
            for month_key in sorted(monthly_stats.keys()):
                stats = monthly_stats[month_key]
                year, month = month_key.split('-')
                report.append(f"\nâ–¶ {year}ë…„ {int(month)}ì›” ({stats['count']}ê±´)")
                
                for item in stats['items']:
                    date = item['date'][:10] if item['date'] else 'ë‚ ì§œì—†ìŒ'
                    title = item['title'][:35] + "..." if len(item['title']) > 35 else item['title']
                    amount = f" - {item['amount']}" if item['amount'] else ""
                    report.append(f"  â€¢ [{date}] {title}{amount}")
            
            return "\n".join(report)
            
        except Exception as e:
            return f"âŒ ì›”ë³„ ìˆ˜ë¦¬ ë‚´ì—­ ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _enhance_asset_response(self, raw_response: str, query: str) -> str:
        """Asset ì‘ë‹µì„ LLMìœ¼ë¡œ ê°œì„ í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        try:
            # LLMì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            if not self.llm:
                return raw_response
            
            # Asset LLM Enhancer ë¡œë“œ
            if not self.asset_enhancer:
                from asset_llm_enhancer import AssetLLMEnhancer
                self.asset_enhancer = AssetLLMEnhancer(self.llm)
            
            # ë‹µë³€ ê°œì„ 
            enhanced_response = self.asset_enhancer.enhance_asset_response(
                raw_data=raw_response,
                query=query,
                llm=self.llm
            )
            
            # ê°œì„ ëœ ë‹µë³€ì´ ë” ì¢‹ìœ¼ë©´ ë°˜í™˜
            if enhanced_response and len(enhanced_response) > 50:
                return enhanced_response
            
            return raw_response
            
        except Exception as e:
            print(f"Asset ì‘ë‹µ ê°œì„  ì‹¤íŒ¨: {e}")
            return raw_response
    
    def _search_asset_detail(self, txt_path: Path, query: str) -> str:
        """ìì‚° íŒŒì¼ì—ì„œ íŠ¹ì • ì‹œë¦¬ì–¼ ë²ˆí˜¸ë‚˜ ì„¸ë¶€ ì •ë³´ ê²€ìƒ‰ - ì™„ì „í•œ ì •ë³´ í‘œì‹œ"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì‹œë¦¬ì–¼ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ë¬¸+ìˆ«ì ì¡°í•©ë„ ì¸ì‹)
            serial_patterns = [
                r'([A-Z0-9]{6,})',  # ëŒ€ë¬¸ìì™€ ìˆ«ì ì¡°í•© (CZC5134KXL ê°™ì€ í˜•ì‹)
                r'(\d{5,})',  # 5ìë¦¬ ì´ìƒ ìˆ«ì (10038 ê°™ì€ í˜•ì‹)
                r'([A-Za-z]+\d+[A-Za-z0-9]*)'  # ì˜ë¬¸ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì‹œë¦¬ì–¼
            ]
            
            serial = None
            for pattern in serial_patterns:
                serial_match = re.search(pattern, query, re.IGNORECASE)
                if serial_match:
                    serial = serial_match.group(1).upper()
                    break
            
            if serial:
                lines = content.split('\n')
                results = []
                
                # ê° ì¥ë¹„ í•­ëª©ì„ ì°¾ê¸° ([0001] í˜•ì‹ìœ¼ë¡œ ì‹œì‘)
                current_item = []
                in_matching_item = False
                
                for line in lines:
                    # ìƒˆë¡œìš´ ì¥ë¹„ í•­ëª© ì‹œì‘
                    if re.match(r'^\[\d{4}\]', line):
                        # ì´ì „ í•­ëª©ì´ ë§¤ì¹­ë˜ì—ˆìœ¼ë©´ ì €ì¥
                        if in_matching_item and current_item:
                            results.append('\n'.join(current_item))
                        # ìƒˆ í•­ëª© ì‹œì‘
                        current_item = [line]
                        in_matching_item = False
                    elif current_item:
                        # í˜„ì¬ í•­ëª©ì— ë¼ì¸ ì¶”ê°€
                        current_item.append(line)
                        # ì‹œë¦¬ì–¼ ë²ˆí˜¸ í™•ì¸
                        if serial in line.upper():
                            in_matching_item = True
                
                # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
                if in_matching_item and current_item:
                    results.append('\n'.join(current_item))
                
                if results:
                    response = f"ğŸ“Š ì‹œë¦¬ì–¼ ë²ˆí˜¸ {serial} ì¥ë¹„ ìƒì„¸ì •ë³´:\n"
                    response += "â”" * 50 + "\n\n"
                    
                    for i, result in enumerate(results[:5], 1):  # ìµœëŒ€ 5ê°œ
                        # ê²°ê³¼ë¥¼ ë” ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
                        result_lines = result.split('\n')
                        formatted = []
                        
                        for line in result_lines:
                            line = line.strip()
                            if line and not line.startswith('='):
                                # ì¤‘ìš” ì •ë³´ í•˜ì´ë¼ì´íŠ¸
                                if 'ê¸°ë³¸ì •ë³´:' in line or 'êµ¬ì…ì •ë³´:' in line or \
                                   'ìœ„ì¹˜ì •ë³´:' in line or 'ê´€ë¦¬ì •ë³´:' in line or \
                                   'ìœ ì§€ë³´ìˆ˜:' in line:
                                    formatted.append(line)
                                elif line.startswith('['):
                                    formatted.append(f"ğŸ“Œ {line}")
                                else:
                                    formatted.append(f"  {line}")
                        
                        response += '\n'.join(formatted)
                        if i < len(results):
                            response += "\n" + "-" * 40 + "\n"
                    
                    response += f"\n\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                    response += f"\nâœ… ì´ {len(results)}ê°œ í•­ëª©ì—ì„œ ì‹œë¦¬ì–¼ {serial} ë°œê²¬"
                    
                    # ë‹´ë‹¹ì, ë²¤ë”ì‚¬ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¶”ê°€ë¡œ ì°¾ê¸°
                    if len(results) == 1 and 'ë‹´ë‹¹ì' not in results[0]:
                        response += "\n\nâš ï¸ ê´€ë¦¬ì •ë³´ê°€ ì¼ë¶€ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                    
                    # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                    return self._enhance_asset_response(response, query)
                else:
                    return f"âŒ ì‹œë¦¬ì–¼ ë²ˆí˜¸ {serial}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return "âŒ ì‹œë¦¬ì–¼ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _parse_asset_query(self, query: str) -> dict:
        """Asset ì¿¼ë¦¬ë¥¼ íŒŒì‹±í•˜ì—¬ ê²€ìƒ‰ ì˜ë„ì™€ ì¡°ê±´ì„ ì¶”ì¶œ
        
        Returns:
            dict: íŒŒì‹±ëœ ì¿¼ë¦¬ ì •ë³´
                - search_type: ê²€ìƒ‰ ìœ í˜• (location, manufacturer, year, price, manager, model, etc.)
                - conditions: ì¶”ì¶œëœ ì¡°ê±´ë“¤
                - has_multiple_conditions: ë³µí•© ì¡°ê±´ ì—¬ë¶€
        """
        query_lower = query.lower()
        result = {
            'search_type': None,
            'conditions': {},
            'has_multiple_conditions': False,
            'original_query': query
        }
        
        # 1. ìœ„ì¹˜ ì¡°ê±´ ì¶”ì¶œ
        location_keywords = {
            'ì¤‘ê³„ì°¨': 'ì¤‘ê³„ì°¨',
            'ëŒ€í˜•ìŠ¤íŠœë””ì˜¤': 'ëŒ€í˜•ìŠ¤íŠœë””ì˜¤',
            'ì†Œí˜•ë¶€ì¡°ì •ì‹¤': 'ì†Œí˜•ë¶€ì¡°ì •ì‹¤',
            'ì¤‘í˜•ë¶€ì¡°ì •ì‹¤': 'ì¤‘í˜•ë¶€ì¡°ì •ì‹¤',
            'news van': 'News VAN',
            'van': 'News VAN',
            'ê´‘í™”ë¬¸': 'ê´‘í™”ë¬¸',
            'í¸ì§‘ì‹¤': 'í¸ì§‘ì‹¤',
            'ë”ë¹™ì‹¤': 'ë”ë¹™ì‹¤'
        }
        
        for keyword, location in location_keywords.items():
            if keyword in query_lower:
                result['conditions']['location'] = location
                break
        
        # 2. ì œì¡°ì‚¬ ì¡°ê±´ ì¶”ì¶œ
        manufacturer_match = re.search(self._get_manufacturer_pattern(), query, re.IGNORECASE)
        if manufacturer_match:
            result['conditions']['manufacturer'] = manufacturer_match.group(0).upper()
        
        # 3. ì—°ë„ ì¡°ê±´ ì¶”ì¶œ
        year_match = re.search(r'(20\d{2})ë…„', query)
        if year_match:
            result['conditions']['year'] = int(year_match.group(1))
            
            # ì—°ë„ ë²”ìœ„ í‚¤ì›Œë“œ í™•ì¸
            if 'ì´ì „' in query_lower:
                result['conditions']['year_range'] = 'before'
            elif 'ì´í›„' in query_lower:
                result['conditions']['year_range'] = 'after'
            elif 'ë¶€í„°' in query_lower and 'ê¹Œì§€' in query_lower:
                # ë²”ìœ„ ì¶”ì¶œ
                range_match = re.search(r'(20\d{2})ë…„ë¶€í„°\s*(20\d{2})ë…„ê¹Œì§€', query)
                if range_match:
                    result['conditions']['year_start'] = int(range_match.group(1))
                    result['conditions']['year_end'] = int(range_match.group(2))
                    result['conditions']['year_range'] = 'between'
        
        # 4. ê¸ˆì•¡ ì¡°ê±´ ì¶”ì¶œ
        price_keywords = ['ì–µì›', 'ì²œë§Œì›', 'ë°±ë§Œì›', 'ë§Œì›']
        for keyword in price_keywords:
            if keyword in query_lower:
                # ê¸ˆì•¡ ìˆ«ì ì¶”ì¶œ
                price_match = re.search(r'(\d+(?:\.\d+)?)\s*' + keyword, query_lower)
                if price_match:
                    amount = float(price_match.group(1))
                    if keyword == 'ì–µì›':
                        amount *= 100000000
                    elif keyword == 'ì²œë§Œì›':
                        amount *= 10000000
                    elif keyword == 'ë°±ë§Œì›':
                        amount *= 1000000
                    elif keyword == 'ë§Œì›':
                        amount *= 10000
                    result['conditions']['price'] = amount
                    
                    # ë²”ìœ„ í‚¤ì›Œë“œ
                    if 'ì´ìƒ' in query_lower:
                        result['conditions']['price_range'] = 'above'
                    elif 'ì´í•˜' in query_lower:
                        result['conditions']['price_range'] = 'below'
                    elif 'ë¯¸ë§Œ' in query_lower:
                        result['conditions']['price_range'] = 'under'
                    elif 'ì´ˆê³¼' in query_lower:
                        result['conditions']['price_range'] = 'over'
                break
        
        # 5. ë‹´ë‹¹ì ì¡°ê±´ ì¶”ì¶œ
        manager_keywords = ['ë‹´ë‹¹', 'ê´€ë¦¬', 'ì°¨ì¥', 'ë¶€ì¥', 'ê³¼ì¥', 'ëŒ€ë¦¬', 'ì‚¬ì›']
        for keyword in manager_keywords:
            if keyword in query_lower:
                result['conditions']['has_manager'] = True
                result['search_type'] = 'manager'
                break
        
        # 6. ì¥ë¹„ ìœ í˜• ì¶”ì¶œ
        equipment_types = ['CCU', 'ì¹´ë©”ë¼', 'ëª¨ë‹ˆí„°', 'ì˜¤ë””ì˜¤', 'ë¹„ë””ì˜¤', 'ì„œë²„', 'ìŠ¤ìœ„ì¹˜', 'ë¼ìš°í„°', 'ë Œì¦ˆ']
        for eq_type in equipment_types:
            if eq_type.lower() in query_lower:
                result['conditions']['equipment_type'] = eq_type
                break
        
        # 7. ëª¨ë¸ëª… ì¶”ì¶œ
        model_match = re.search(r'[A-Z]+[-\s]?\d+', query, re.IGNORECASE)
        if model_match:
            result['conditions']['model'] = model_match.group(0).upper()
        
        # ë³µí•© ì¡°ê±´ ì—¬ë¶€ íŒë‹¨
        condition_count = len([k for k in result['conditions'].keys() if not k.endswith('_range')])
        result['has_multiple_conditions'] = condition_count >= 2
        
        # ê²€ìƒ‰ ìœ í˜• ê²°ì • (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)
        if not result['search_type']:
            if result['conditions'].get('has_manager'):
                result['search_type'] = 'manager'
            elif 'price' in result['conditions']:
                result['search_type'] = 'price'
            elif 'year' in result['conditions']:
                result['search_type'] = 'year'
            elif 'location' in result['conditions'] and 'equipment_type' in result['conditions']:
                result['search_type'] = 'location_equipment'
            elif 'location' in result['conditions']:
                result['search_type'] = 'location'
            elif 'manufacturer' in result['conditions']:
                result['search_type'] = 'manufacturer'
            elif 'model' in result['conditions']:
                result['search_type'] = 'model'
            elif 'equipment_type' in result['conditions']:
                result['search_type'] = 'equipment'
            else:
                result['search_type'] = 'general'
        
        return result
    
    def _search_asset_complex(self, txt_path: Path, query_intent: dict) -> str:
        """ë³µí•© ì¡°ê±´ì„ ì²˜ë¦¬í•˜ëŠ” Asset ê²€ìƒ‰
        
        Args:
            txt_path: ìì‚° íŒŒì¼ ê²½ë¡œ
            query_intent: íŒŒì‹±ëœ ì¿¼ë¦¬ ì •ë³´
        """
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            conditions = query_intent['conditions']
            matched_items = []
            
            # ê° ë¼ì¸ ê²€ì‚¬
            for i in range(len(lines)):
                line = lines[i]
                if not line.strip():
                    continue
                
                # ëª¨ë“  ì¡°ê±´ í™•ì¸
                all_conditions_met = True
                
                # ìœ„ì¹˜ ì¡°ê±´
                if 'location' in conditions:
                    if conditions['location'] not in line:
                        all_conditions_met = False
                
                # ì œì¡°ì‚¬ ì¡°ê±´
                if 'manufacturer' in conditions and all_conditions_met:
                    if conditions['manufacturer'] not in line.upper():
                        all_conditions_met = False
                
                # ì—°ë„ ì¡°ê±´
                if 'year' in conditions and all_conditions_met:
                    year_in_line = re.search(r'20\d{2}', line)
                    if year_in_line:
                        line_year = int(year_in_line.group(0))
                        target_year = conditions['year']
                        
                        if conditions.get('year_range') == 'before':
                            if line_year > target_year:
                                all_conditions_met = False
                        elif conditions.get('year_range') == 'after':
                            if line_year < target_year:
                                all_conditions_met = False
                        elif conditions.get('year_range') == 'between':
                            if not (conditions.get('year_start', 0) <= line_year <= conditions.get('year_end', 9999)):
                                all_conditions_met = False
                        else:
                            if line_year != target_year:
                                all_conditions_met = False
                    else:
                        all_conditions_met = False
                
                # ì¥ë¹„ ìœ í˜• ì¡°ê±´
                if 'equipment_type' in conditions and all_conditions_met:
                    if conditions['equipment_type'].upper() not in line.upper():
                        all_conditions_met = False
                
                # ëª¨ë¸ ì¡°ê±´
                if 'model' in conditions and all_conditions_met:
                    if conditions['model'] not in line.upper():
                        all_conditions_met = False
                
                # ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ ì¶”ê°€
                if all_conditions_met:
                    # ì „ì²´ í•­ëª© ì •ë³´ ìˆ˜ì§‘ (ì•ë’¤ ë¼ì¸ í¬í•¨)
                    item_lines = []
                    start_idx = max(0, i - 1)
                    end_idx = min(len(lines), i + 5)
                    
                    for j in range(start_idx, end_idx):
                        if lines[j].strip():
                            item_lines.append(lines[j])
                    
                    if item_lines:
                        matched_items.append('\n'.join(item_lines))
            
            # ê²°ê³¼ ìƒì„±
            if not matched_items:
                # ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ë©”ì‹œì§€
                error_msg = "ğŸ˜” ì¡°ê±´ì— ë§ëŠ” ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                error_msg += "ğŸ“‹ **ìš”ì²­í•˜ì‹  ê²€ìƒ‰ ì¡°ê±´:**\n"
                error_msg += self._format_conditions(conditions) + "\n\n"
                error_msg += "ğŸ’¡ **ê²€ìƒ‰ íŒ:**\n"
                error_msg += "â€¢ ì¡°ê±´ì„ í•˜ë‚˜ì”© ì¤„ì—¬ì„œ ì‹œë„í•´ë³´ì„¸ìš”\n"
                error_msg += "â€¢ ì—°ë„ëŠ” 2000-2024 ë²”ìœ„ë¡œ ì…ë ¥í•˜ì„¸ìš”\n"
                error_msg += "â€¢ ìœ„ì¹˜ëª…ì€ ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì¤‘ê³„ì°¨, ëŒ€í˜•ìŠ¤íŠœë””ì˜¤)\n"
                error_msg += "â€¢ ì œì¡°ì‚¬ëª…ì€ ì˜ë¬¸ ëŒ€ë¬¸ìë¡œ (ì˜ˆ: SONY, Harris)\n"
                return error_msg
            
            # ì‘ë‹µ ìƒì„±
            response = f"âœ… ë³µí•© ì¡°ê±´ ê²€ìƒ‰ ê²°ê³¼: ì´ {len(matched_items)}ê°œ ì¥ë¹„\n\n"
            response += f"ğŸ“‹ ê²€ìƒ‰ ì¡°ê±´:\n{self._format_conditions(conditions)}\n\n"
            response += "=" * 50 + "\n\n"
            
            # ìµœëŒ€ 20ê°œê¹Œì§€ í‘œì‹œ
            for idx, item in enumerate(matched_items[:20], 1):
                response += f"[{idx}] {item}\n"
                response += "-" * 40 + "\n"
            
            if len(matched_items) > 20:
                response += f"\n... ì™¸ {len(matched_items) - 20}ê°œ ì¥ë¹„"
            
            # LLMìœ¼ë¡œ ìš”ì•½ ì¶”ê°€
            if self.llm and len(matched_items) > 0:
                response = self._enhance_asset_response(response, query_intent['original_query'])
            
            return response
            
        except Exception as e:
            return f"âŒ ë³µí•© ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _format_conditions(self, conditions: dict) -> str:
        """ê²€ìƒ‰ ì¡°ê±´ì„ ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…"""
        formatted = []
        
        if 'location' in conditions:
            formatted.append(f"â€¢ ìœ„ì¹˜: {conditions['location']}")
        
        if 'manufacturer' in conditions:
            formatted.append(f"â€¢ ì œì¡°ì‚¬: {conditions['manufacturer']}")
        
        if 'year' in conditions:
            year_str = f"â€¢ ì—°ë„: {conditions['year']}ë…„"
            if conditions.get('year_range') == 'before':
                year_str += " ì´ì „"
            elif conditions.get('year_range') == 'after':
                year_str += " ì´í›„"
            elif conditions.get('year_range') == 'between':
                year_str = f"â€¢ ì—°ë„: {conditions.get('year_start')}ë…„ ~ {conditions.get('year_end')}ë…„"
            formatted.append(year_str)
        
        if 'price' in conditions:
            price_str = f"â€¢ ê¸ˆì•¡: {conditions['price']:,.0f}ì›"
            if conditions.get('price_range') == 'above':
                price_str += " ì´ìƒ"
            elif conditions.get('price_range') == 'below':
                price_str += " ì´í•˜"
            formatted.append(price_str)
        
        if 'equipment_type' in conditions:
            formatted.append(f"â€¢ ì¥ë¹„ ìœ í˜•: {conditions['equipment_type']}")
        
        if 'model' in conditions:
            formatted.append(f"â€¢ ëª¨ë¸: {conditions['model']}")
        
        return '\n'.join(formatted) if formatted else "ì¡°ê±´ ì—†ìŒ"
    
    def _search_asset_by_equipment_type(self, txt_path: Path, query: str) -> str:
        """ì¥ë¹„ ìœ í˜•ë³„ ìì‚° ê²€ìƒ‰"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì¥ë¹„ ìœ í˜• í‚¤ì›Œë“œ ì¶”ì¶œ
            query_intent = self._parse_asset_query(query)
            equipment_type = query_intent['conditions'].get('equipment_type', '')
            
            if not equipment_type:
                # ì¿¼ë¦¬ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
                equipment_types = ['CCU', 'ì¹´ë©”ë¼', 'ëª¨ë‹ˆí„°', 'ì˜¤ë””ì˜¤', 'ë¹„ë””ì˜¤', 'ì„œë²„', 'ìŠ¤ìœ„ì¹˜', 'ë¼ìš°í„°', 'ë Œì¦ˆ']
                for eq_type in equipment_types:
                    if eq_type.lower() in query.lower():
                        equipment_type = eq_type
                        break
            
            if not equipment_type:
                return ("ğŸ¤” **ì¥ë¹„ ìœ í˜•ì„ ì§€ì •í•´ì£¼ì„¸ìš”**\n\n"
                       "ğŸ“‹ **ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ë¹„ ìœ í˜•:**\n"
                       "â€¢ ì¹´ë©”ë¼ - ë°©ì†¡ìš© ì¹´ë©”ë¼ ì¥ë¹„\n"
                       "â€¢ ëª¨ë‹ˆí„° - ë””ìŠ¤í”Œë ˆì´ ì¥ë¹„\n"
                       "â€¢ CCU - Camera Control Unit\n"
                       "â€¢ ì˜¤ë””ì˜¤ - ìŒí–¥ ê´€ë ¨ ì¥ë¹„\n"
                       "â€¢ ë¹„ë””ì˜¤ - ì˜ìƒ ì²˜ë¦¬ ì¥ë¹„\n"
                       "â€¢ ì„œë²„ - ë°©ì†¡ ì„œë²„ ì‹œìŠ¤í…œ\n"
                       "â€¢ ìŠ¤ìœ„ì¹˜ - ë„¤íŠ¸ì›Œí¬/ë¹„ë””ì˜¤ ìŠ¤ìœ„ì¹˜\n"
                       "â€¢ ë¼ìš°í„° - ì‹ í˜¸ ë¼ìš°íŒ… ì¥ë¹„\n"
                       "â€¢ ë Œì¦ˆ - ì¹´ë©”ë¼ ë Œì¦ˆ\n\n"
                       "ğŸ’¡ **ì˜ˆì‹œ:** 'ì¹´ë©”ë¼ ì¥ë¹„ í˜„í™©', 'CCU ì „ì²´ ëª©ë¡'")
            
            lines = content.split('\n')
            matched_items = []
            
            # ì¥ë¹„ ìœ í˜•ìœ¼ë¡œ ê²€ìƒ‰
            for i in range(len(lines)):
                line = lines[i]
                if equipment_type.upper() in line.upper():
                    # ì „ì²´ í•­ëª© ì •ë³´ ìˆ˜ì§‘
                    item_lines = []
                    start_idx = max(0, i - 1)
                    end_idx = min(len(lines), i + 5)
                    
                    for j in range(start_idx, end_idx):
                        if lines[j].strip():
                            item_lines.append(lines[j])
                    
                    if item_lines:
                        matched_items.append('\n'.join(item_lines))
            
            if not matched_items:
                return f"âŒ '{equipment_type}' ìœ í˜•ì˜ ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # í†µê³„ ì •ë³´ ìˆ˜ì§‘
            locations = {}
            manufacturers = {}
            years = {}
            
            for item in matched_items:
                # ìœ„ì¹˜ ì¶”ì¶œ
                for loc in ['ì¤‘ê³„ì°¨', 'ëŒ€í˜•ìŠ¤íŠœë””ì˜¤', 'ì†Œí˜•ë¶€ì¡°ì •ì‹¤', 'News VAN', 'ê´‘í™”ë¬¸']:
                    if loc in item:
                        locations[loc] = locations.get(loc, 0) + 1
                        break
                
                # ì œì¡°ì‚¬ ì¶”ì¶œ
                for mfr in ['SONY', 'Harris', 'HP', 'Panasonic', 'Canon']:
                    if mfr in item.upper():
                        manufacturers[mfr] = manufacturers.get(mfr, 0) + 1
                        break
                
                # ì—°ë„ ì¶”ì¶œ
                year_match = re.search(r'20\d{2}', item)
                if year_match:
                    year = year_match.group(0)
                    years[year] = years.get(year, 0) + 1
            
            # ì‘ë‹µ ìƒì„±
            response = f"âœ… '{equipment_type}' ì¥ë¹„ ê²€ìƒ‰ ê²°ê³¼: ì´ {len(matched_items)}ê°œ\n\n"
            
            # í†µê³„ ì •ë³´
            response += "ğŸ“Š í†µê³„ ì •ë³´:\n"
            
            if locations:
                response += "\nìœ„ì¹˜ë³„ ë¶„í¬:\n"
                for loc, count in sorted(locations.items(), key=lambda x: x[1], reverse=True)[:5]:
                    response += f"  â€¢ {loc}: {count}ê°œ\n"
            
            if manufacturers:
                response += "\nì œì¡°ì‚¬ë³„ ë¶„í¬:\n"
                for mfr, count in sorted(manufacturers.items(), key=lambda x: x[1], reverse=True)[:5]:
                    response += f"  â€¢ {mfr}: {count}ê°œ\n"
            
            if years:
                response += "\nêµ¬ì… ì—°ë„ë³„:\n"
                for year, count in sorted(years.items(), reverse=True)[:5]:
                    response += f"  â€¢ {year}ë…„: {count}ê°œ\n"
            
            response += "\n" + "=" * 50 + "\n\n"
            response += "ğŸ“‹ ìƒì„¸ ì¥ë¹„ ëª©ë¡ (ìµœëŒ€ 15ê°œ):\n\n"
            
            # ìƒì„¸ ëª©ë¡
            for idx, item in enumerate(matched_items[:15], 1):
                response += f"[{idx}] {item}\n"
                response += "-" * 40 + "\n"
            
            if len(matched_items) > 15:
                response += f"\n... ì™¸ {len(matched_items) - 15}ê°œ ì¥ë¹„"
            
            # LLMìœ¼ë¡œ ìš”ì•½ ì¶”ê°€
            if self.llm:
                response = self._enhance_asset_response(response, query)
            
            return response
            
        except Exception as e:
            return f"âŒ ì¥ë¹„ ìœ í˜• ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _search_asset_by_manager(self, txt_path: Path, query: str) -> str:
        """ë‹´ë‹¹ìë³„ ìì‚° ê²€ìƒ‰"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ë‹´ë‹¹ìëª… ì¶”ì¶œ - ì§ê¸‰ í¬í•¨ íŒ¨í„´ (ë¶™ì–´ìˆëŠ” í˜•ì‹ ìš°ì„ )
            manager_patterns = [
                r'([ê°€-í£]{2,4})(?:ì°¨ì¥|ë¶€ì¥|ê³¼ì¥|ëŒ€ë¦¬|ì‚¬ì›)',    # ë¶™ì–´ìˆëŠ” í˜•ì‹ (ì‹ ìŠ¹ë§Œì°¨ì¥)
                r'([ê°€-í£]{2,4})\s*(?:ì°¨ì¥|ë¶€ì¥|ê³¼ì¥|ëŒ€ë¦¬|ì‚¬ì›)',  # ì´ë¦„ + ê³µë°± + ì§ê¸‰
                r'([ê°€-í£]{2,4})(?:ì´|ê°€|ì˜|ë‹˜)',  # ì´ë¦„ + ì¡°ì‚¬
                r'([ê°€-í£]{2,4})\s+ë‹´ë‹¹',  # ì´ë¦„ + ë‹´ë‹¹
            ]
            
            manager_name = None
            for pattern in manager_patterns:
                match = re.search(pattern, query)
                if match:
                    manager_name = match.group(1)
                    break
            
            if not manager_name:
                # í‚¤ì›Œë“œë¡œ ì¶”ì¶œ ì‹œë„
                for word in query.split():
                    if len(word) >= 2 and len(word) <= 4 and all(ord('ê°€') <= ord(c) <= ord('í£') for c in word):
                        if word not in ['ë‹´ë‹¹', 'ê´€ë¦¬', 'ì¥ë¹„', 'ì „ì²´', 'ëª©ë¡']:
                            manager_name = word
                            break
            
            if manager_name:
                result = self._count_by_field(content, "ë‹´ë‹¹ì", manager_name)
                
                if result['count'] > 0:
                    response = f"ğŸ“Š **{manager_name} ë‹´ë‹¹ì ê´€ë¦¬ ì¥ë¹„ í˜„í™©**\n"
                    response += "=" * 50 + "\n"
                    response += f"âœ… ì´ ê´€ë¦¬ ì¥ë¹„: **{result['count']}ê°œ**\n\n"
                    
                    if result.get('sample_items'):
                        response += "ğŸ“‹ **ëŒ€í‘œ ì¥ë¹„ ëª©ë¡ (ìµœëŒ€ 10ê°œ)**:\n"
                        response += "-" * 40 + "\n"
                        for i, item in enumerate(result['sample_items'][:10], 1):
                            lines = item.split('\n')
                            equipment_info = {}
                            for line in lines:
                                if '[' in line and ']' in line:
                                    equipment_info['id'] = line.strip()
                                elif 'ë¶€í’ˆëª…:' in line:
                                    equipment_info['name'] = line.split('ë¶€í’ˆëª…:')[1].strip()
                                elif 'ëª¨ë¸:' in line:
                                    equipment_info['model'] = line.split('ëª¨ë¸:')[1].strip()
                                elif 'ìœ„ì¹˜:' in line:
                                    equipment_info['location'] = line.split('ìœ„ì¹˜:')[1].strip()
                                elif 'êµ¬ì…ì¼:' in line:
                                    equipment_info['date'] = line.split('êµ¬ì…ì¼:')[1].strip()
                            
                            if equipment_info:
                                response += f"\n**{i}. {equipment_info.get('id', '')}**\n"
                                if 'name' in equipment_info:
                                    response += f"   â€¢ ë¶€í’ˆëª…: {equipment_info['name']}\n"
                                if 'model' in equipment_info:
                                    response += f"   â€¢ ëª¨ë¸: {equipment_info['model']}\n"
                                if 'location' in equipment_info:
                                    response += f"   â€¢ ìœ„ì¹˜: {equipment_info['location']}\n"
                                if 'date' in equipment_info:
                                    response += f"   â€¢ êµ¬ì…ì¼: {equipment_info['date']}\n"
                    
                    response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                    # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                    return self._enhance_asset_response(response, query)
                else:
                    return f"âŒ {manager_name} ë‹´ë‹¹ìê°€ ê´€ë¦¬í•˜ëŠ” ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                return "âŒ ë‹´ë‹¹ì ì´ë¦„ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •í™•í•œ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _search_asset_by_price_range(self, txt_path: Path, query: str) -> str:
        """ê¸ˆì•¡ ë²”ìœ„ë³„ ìì‚° ê²€ìƒ‰"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ê¸ˆì•¡ ì¶”ì¶œ ë° ë³€í™˜
            amount_patterns = [
                (r'(\d+)\s*ì–µ\s*(\d*)\s*ì²œ?\s*ë§Œ?\s*ì›?', lambda m: int(m.group(1)) * 100000000 + (int(m.group(2)) * 10000000 if m.group(2) else 0)),
                (r'(\d+)\s*ì²œë§Œ\s*ì›?', lambda m: int(m.group(1)) * 10000000),
                (r'(\d+)\s*ë°±ë§Œ\s*ì›?', lambda m: int(m.group(1)) * 1000000),
                (r'(\d+)\s*ë§Œ\s*ì›?', lambda m: int(m.group(1)) * 10000),
            ]
            
            target_amount = None
            for pattern, converter in amount_patterns:
                match = re.search(pattern, query)
                if match:
                    target_amount = converter(match)
                    break
            
            if not target_amount:
                return "âŒ ê¸ˆì•¡ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜ˆ: 1ì–µì›, 5ì²œë§Œì›)"
            
            # ë²”ìœ„ íƒ€ì… ê²°ì •
            range_type = 'exact'
            if 'ì´ìƒ' in query or 'ì´ˆê³¼' in query:
                range_type = 'above'
            elif 'ì´í•˜' in query or 'ë¯¸ë§Œ' in query:
                range_type = 'below'
            elif 'ë¶€í„°' in query or 'ê¹Œì§€' in query:
                range_type = 'range'
            
            # ì¥ë¹„ ê²€ìƒ‰
            lines = content.split('\n')
            results = []
            current_item = []
            
            for line in lines:
                if re.match(r'^\[\d{4}\]', line):
                    # ì´ì „ í•­ëª© ì²˜ë¦¬
                    if current_item:
                        item_text = '\n'.join(current_item)
                        # ê¸ˆì•¡ ì¶”ì¶œ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ íŒ¨í„´ ì‹œë„)
                        price_match = re.search(r'ê¸ˆì•¡:\s*([\d,]+)ì›?', item_text)
                        if not price_match:
                            price_match = re.search(r'ë‹¨ê°€:\s*([\d,]+)ì›?', item_text)
                        if price_match:
                            price = int(price_match.group(1).replace(',', ''))
                            
                            # ë²”ìœ„ ì²´í¬
                            if range_type == 'above' and price >= target_amount:
                                results.append((price, item_text))
                            elif range_type == 'below' and price <= target_amount:
                                results.append((price, item_text))
                            elif range_type == 'exact' and price == target_amount:
                                results.append((price, item_text))
                    
                    current_item = [line]
                elif current_item:
                    current_item.append(line)
            
            # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
            if current_item:
                item_text = '\n'.join(current_item)
                price_match = re.search(r'ê¸ˆì•¡:\s*([\d,]+)ì›?', item_text)
                if not price_match:
                    price_match = re.search(r'ë‹¨ê°€:\s*([\d,]+)ì›?', item_text)
                if price_match:
                    price = int(price_match.group(1).replace(',', ''))
                    if range_type == 'above' and price >= target_amount:
                        results.append((price, item_text))
                    elif range_type == 'below' and price <= target_amount:
                        results.append((price, item_text))
                    elif range_type == 'exact' and price == target_amount:
                        results.append((price, item_text))
            
            # ê²°ê³¼ ì •ë ¬ (ê°€ê²© ë‚´ë¦¼ì°¨ìˆœ)
            results.sort(key=lambda x: x[0], reverse=True)
            
            if results:
                range_desc = {
                    'above': 'ì´ìƒ',
                    'below': 'ì´í•˜',
                    'exact': '',
                    'range': 'ë²”ìœ„'
                }[range_type]
                
                response = f"ğŸ’° **{target_amount:,}ì› {range_desc} ì¥ë¹„ í˜„í™©**\n"
                response += "=" * 50 + "\n"
                response += f"âœ… ì´ {len(results)}ê°œ ì¥ë¹„\n\n"
                
                # ê¸ˆì•¡ë³„ í†µê³„
                total_amount = sum(r[0] for r in results)
                avg_amount = total_amount // len(results) if results else 0
                response += f"ğŸ“Š **í†µê³„**:\n"
                response += f"   â€¢ ì´ ê¸ˆì•¡: {total_amount:,}ì›\n"
                response += f"   â€¢ í‰ê·  ê¸ˆì•¡: {avg_amount:,}ì›\n"
                response += f"   â€¢ ìµœê³ ê°€: {results[0][0]:,}ì›\n"
                response += f"   â€¢ ìµœì €ê°€: {results[-1][0]:,}ì›\n\n"
                
                response += "ğŸ“‹ **ìƒìœ„ 10ê°œ ì¥ë¹„**:\n"
                response += "-" * 40 + "\n"
                
                for i, (price, item) in enumerate(results[:10], 1):
                    lines = item.split('\n')
                    for line in lines:
                        if '[' in line and ']' in line:
                            response += f"\n**{i}. {line.strip()}** - {price:,}ì›\n"
                        elif 'ë¶€í’ˆëª…:' in line:
                            response += f"   â€¢ {line.strip()}\n"
                        elif 'ëª¨ë¸:' in line:
                            response += f"   â€¢ {line.strip()}\n"
                
                response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                return self._enhance_asset_response(response, query)
            else:
                range_desc = {
                    'above': 'ì´ìƒ',
                    'below': 'ì´í•˜',
                    'exact': '',
                    'range': 'ë²”ìœ„'
                }.get(range_type, '')
                return f"âŒ {target_amount:,}ì› {range_desc} ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _search_asset_by_year_range(self, txt_path: Path, query: str) -> str:
        """ì—°ë„ ë²”ìœ„ë³„ ìì‚° ê²€ìƒ‰ (ì´ì „/ì´í›„/ë²”ìœ„)"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì—°ë„ ì¶”ì¶œ
            year_matches = re.findall(r'20\d{2}', query)
            
            # ë²”ìœ„ íƒ€ì… ê²°ì •
            range_type = 'exact'
            if 'ì´ì „' in query or 'ì „' in query:
                range_type = 'before'
            elif 'ì´í›„' in query or 'í›„' in query:
                range_type = 'after'
            elif 'ë¶€í„°' in query and 'ê¹Œì§€' in query:
                range_type = 'between'
            elif 'ìµœê·¼' in query:
                range_type = 'recent'
                # ìµœê·¼ Në…„ ì¶”ì¶œ
                recent_match = re.search(r'ìµœê·¼\s*(\d+)\s*ë…„', query)
                if recent_match:
                    recent_years = int(recent_match.group(1))
                    current_year = 2025
                    year_matches = [str(current_year - recent_years)]
                    range_type = 'after'
            
            if not year_matches and range_type != 'recent':
                return "âŒ ì—°ë„ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜ˆ: 2023ë…„, 2020ë…„ ì´ì „)"
            
            # ì—°ë„ ë²”ìœ„ ì„¤ì •
            target_year = None
            if range_type == 'between' and len(year_matches) >= 2:
                start_year = int(year_matches[0])
                end_year = int(year_matches[1])
            elif year_matches:
                target_year = int(year_matches[0])
                if range_type == 'exact':
                    start_year = target_year
                    end_year = target_year
                elif range_type == 'after':
                    start_year = target_year
                    end_year = 2025
                elif range_type == 'before':
                    start_year = 2000
                    end_year = target_year
            else:
                # ìµœê·¼ 3ë…„ ê¸°ë³¸ê°’
                current_year = 2025
                start_year = current_year - 3
                end_year = current_year
            
            # ì¥ë¹„ ê²€ìƒ‰
            lines = content.split('\n')
            results = []
            year_stats = {}
            
            for line in lines:
                if 'êµ¬ì…ì¼:' in line:
                    date_match = re.search(r'êµ¬ì…ì¼:\s*(\d{4})', line)
                    if date_match:
                        year = int(date_match.group(1))
                        
                        # ë²”ìœ„ ì²´í¬
                        if start_year <= year <= end_year:
                            results.append(line)
                            year_stats[year] = year_stats.get(year, 0) + 1
            
            if results:
                # ë²”ìœ„ ì„¤ëª…
                if range_type == 'exact':
                    range_desc = f"{target_year}ë…„"
                elif range_type == 'before':
                    range_desc = f"{end_year}ë…„ ì´ì „"
                elif range_type == 'after':
                    range_desc = f"{start_year}ë…„ ì´í›„"
                elif range_type == 'between':
                    range_desc = f"{start_year}ë…„ ~ {end_year}ë…„"
                else:
                    range_desc = "ì§€ì • ê¸°ê°„"
                
                response = f"ğŸ“… **{range_desc} êµ¬ì… ì¥ë¹„ í˜„í™©**\n"
                response += "=" * 50 + "\n"
                response += f"âœ… ì´ {len(results)}ê°œ ì¥ë¹„\n\n"
                
                # ì—°ë„ë³„ í†µê³„
                if year_stats:
                    response += "ğŸ“Š **ì—°ë„ë³„ ë¶„í¬**:\n"
                    for year in sorted(year_stats.keys(), reverse=True):
                        response += f"   â€¢ {year}ë…„: {year_stats[year]}ê°œ\n"
                    response += "\n"
                
                # ìƒ˜í”Œ í‘œì‹œ
                response += "ğŸ“‹ **ìƒ˜í”Œ ì¥ë¹„ (ìµœëŒ€ 15ê°œ)**:\n"
                response += "-" * 40 + "\n"
                for i, result in enumerate(results[:15], 1):
                    response += f"{i}. {result}\n"
                
                response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                return self._enhance_asset_response(response, query)
            else:
                if range_type == 'exact' and target_year:
                    return f"âŒ {target_year}ë…„ì— êµ¬ì…í•œ ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                elif start_year and end_year:
                    return f"âŒ {start_year}ë…„ ~ {end_year}ë…„ ê¸°ê°„ì— êµ¬ì…í•œ ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                else:
                    return f"âŒ í•´ë‹¹ ê¸°ê°„ì— êµ¬ì…í•œ ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _search_asset_by_year(self, txt_path: Path, query: str) -> str:
        """êµ¬ì…ì—°ë„ë³„ ìì‚° ê²€ìƒ‰"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì—°ë„ ì¶”ì¶œ
            year_match = re.search(r'(20\d{2})', query)
            if year_match:
                year = year_match.group(1)
                
                # êµ¬ì…ì¼ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
                pattern = f"êµ¬ì…ì¼: {year}"
                lines = content.split('\n')
                results = []
                count = 0
                
                for line in lines:
                    if pattern in line:
                        count += 1
                        if len(results) < 10:  # ì²˜ìŒ 10ê°œë§Œ
                            results.append(line)
                
                if results:
                    response = f"ğŸ“Š {year}ë…„ êµ¬ì… ì¥ë¹„: ì´ {count}ê°œ\n"
                    response += "â”" * 40 + "\n"
                    response += "\n[ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ)]:\n"
                    for result in results:
                        response += f"â€¢ {result}\n"
                    response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                    # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                    return self._enhance_asset_response(response, query)
                else:
                    return f"âŒ {year}ë…„ì— êµ¬ì…í•œ ì¥ë¹„ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            return "âŒ ì—°ë„ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _search_location_summary(self, txt_path: Path, location: str) -> str:
        """íŠ¹ì • ìœ„ì¹˜ì˜ ì¥ë¹„ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì •ë¦¬"""
        query = f"{location} ì¥ë¹„ í˜„í™©"  # query ë³€ìˆ˜ ìƒì„± for _enhance_asset_response
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            equipment_by_category = {}  # {ì¹´í…Œê³ ë¦¬: [ì¥ë¹„ ì •ë³´]}
            current_item = []
            total_count = 0
            total_amount = 0  # ì´ ê¸ˆì•¡
            
            for line in lines:
                if re.match(r'^\[\d{4}\]', line.strip()):
                    if current_item:
                        item_text = '\n'.join(current_item)
                        # ìœ„ì¹˜ í™•ì¸
                        if self._check_location_in_item(item_text, location):
                            total_count += 1
                            # ì¥ë¹„ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                            equipment_name = current_item[0].split(']')[1].strip() if ']' in current_item[0] else current_item[0]
                            
                            # ì¹´í…Œê³ ë¦¬ ê²°ì •
                            category = self._determine_equipment_category(equipment_name, item_text)
                            
                            if category not in equipment_by_category:
                                equipment_by_category[category] = []
                            
                            # ì •ë³´ ì¶”ì¶œ
                            info = {
                                'name': equipment_name,
                                'model': "N/A",
                                'price': 0,
                                'quantity': 1,
                                'date': "N/A"
                            }
                            
                            for item_line in current_item:
                                # ëª¨ë¸ëª…
                                if "ëª¨ë¸:" in item_line:
                                    model_match = re.search(r'ëª¨ë¸:\s*([^|]+)', item_line)
                                    if model_match:
                                        info['model'] = model_match.group(1).strip()
                                
                                # ê¸ˆì•¡ ì •ë³´
                                if "ê¸ˆì•¡:" in item_line:
                                    amount_match = re.search(r'ê¸ˆì•¡:\s*([\d,]+)ì›', item_line)
                                    if amount_match:
                                        amount_str = amount_match.group(1).replace(',', '')
                                        try:
                                            info['price'] = int(amount_str)
                                            total_amount += info['price']
                                        except:
                                            pass
                                
                                # ìˆ˜ëŸ‰
                                if "ìˆ˜ëŸ‰:" in item_line:
                                    qty_match = re.search(r'ìˆ˜ëŸ‰:\s*(\d+)', item_line)
                                    if qty_match:
                                        info['quantity'] = int(qty_match.group(1))
                                
                                # êµ¬ì…ì¼
                                if "êµ¬ì…ì¼:" in item_line:
                                    date_match = re.search(r'êµ¬ì…ì¼:\s*([^\s|]+)', item_line)
                                    if date_match:
                                        info['date'] = date_match.group(1).strip()
                            
                            equipment_by_category[category].append(info)
                    
                    current_item = [line]
                else:
                    if current_item:
                        current_item.append(line)
            
            # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
            if current_item:
                item_text = '\n'.join(current_item)
                if self._check_location_in_item(item_text, location):
                    total_count += 1
                    equipment_name = current_item[0].split(']')[1].strip() if ']' in current_item[0] else current_item[0]
                    category = self._determine_equipment_category(equipment_name, item_text)
                    if category not in equipment_by_category:
                        equipment_by_category[category] = []
                    
                    info = {
                        'name': equipment_name,
                        'model': "N/A",
                        'price': 0,
                        'quantity': 1,
                        'date': "N/A"
                    }
                    
                    for item_line in current_item:
                        if "ëª¨ë¸:" in item_line:
                            model_match = re.search(r'ëª¨ë¸:\s*([^|]+)', item_line)
                            if model_match:
                                info['model'] = model_match.group(1).strip()
                        if "ê¸ˆì•¡:" in item_line:
                            amount_match = re.search(r'ê¸ˆì•¡:\s*([\d,]+)ì›', item_line)
                            if amount_match:
                                amount_str = amount_match.group(1).replace(',', '')
                                try:
                                    info['price'] = int(amount_str)
                                    total_amount += info['price']
                                except:
                                    pass
                        if "ìˆ˜ëŸ‰:" in item_line:
                            qty_match = re.search(r'ìˆ˜ëŸ‰:\s*(\d+)', item_line)
                            if qty_match:
                                info['quantity'] = int(qty_match.group(1))
                        if "êµ¬ì…ì¼:" in item_line:
                            date_match = re.search(r'êµ¬ì…ì¼:\s*([^\s|]+)', item_line)
                            if date_match:
                                info['date'] = date_match.group(1).strip()
                    
                    equipment_by_category[category].append(info)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            if equipment_by_category:
                response = f"ğŸ“Š **{location} ì¥ë¹„ í˜„í™©**\n"
                response += "=" * 70 + "\n"
                response += f"âœ… ì´ **{total_count}ê°œ** ì¥ë¹„\n"
                if total_amount > 0:
                    # ê¸ˆì•¡ í¬ë§·íŒ… (ì–µ/ì²œë§Œì› ë‹¨ìœ„)
                    if total_amount >= 100000000:  # 1ì–µ ì´ìƒ
                        amount_str = f"{total_amount/100000000:.1f}ì–µì›"
                    elif total_amount >= 10000000:  # 1ì²œë§Œì› ì´ìƒ
                        amount_str = f"{total_amount/10000000:.0f}ì²œë§Œì›"
                    else:
                        amount_str = f"{total_amount:,}ì›"
                    response += f"ğŸ’° ì´ ìì‚°ê°€ì¹˜: **{amount_str}**\n\n"
                else:
                    response += "\n"
                
                # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
                response += "### ğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ í˜„í™©\n"
                response += "-" * 50 + "\n"
                
                # ì¹´í…Œê³ ë¦¬ ì •ë ¬ (ì¥ë¹„ ìˆ˜ ë§ì€ ìˆœ)
                sorted_categories = sorted(equipment_by_category.items(), key=lambda x: len(x[1]), reverse=True)
                
                for category, items in sorted_categories:
                    # ì¹´í…Œê³ ë¦¬ë³„ ì´ì•¡ ê³„ì‚°
                    category_amount = sum(item['price'] for item in items)
                    
                    response += f"\n**{category}** ({len(items)}ê°œ"
                    if category_amount > 0:
                        if category_amount >= 100000000:
                            response += f", {category_amount/100000000:.1f}ì–µì›"
                        elif category_amount >= 10000000:
                            response += f", {category_amount/10000000:.0f}ì²œë§Œì›"
                        else:
                            response += f", {category_amount:,}ì›"
                    response += ")\n"
                    
                    # ê°™ì€ ì¥ë¹„ëª…ë¼ë¦¬ ê·¸ë£¹í™”
                    equipment_summary = {}
                    for item in items:
                        key = f"{item['name']} ({item['model']})" if item['model'] != "N/A" else item['name']
                        if key not in equipment_summary:
                            equipment_summary[key] = {
                                'count': 0,
                                'total_price': 0,
                                'unit_price': item['price'] // item['quantity'] if item['quantity'] > 0 else item['price']
                            }
                        equipment_summary[key]['count'] += item['quantity']
                        equipment_summary[key]['total_price'] += item['price']
                    
                    # ê¸ˆì•¡ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                    sorted_equipment = sorted(equipment_summary.items(), 
                                           key=lambda x: x[1]['total_price'], 
                                           reverse=True)
                    
                    # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                    for i, (equip_name, equip_info) in enumerate(sorted_equipment[:5], 1):
                        line = f"  {i}. {equip_name}"
                        if equip_info['count'] > 1:
                            line += f" - {equip_info['count']}ê°œ"
                        if equip_info['total_price'] > 0:
                            line += f" ({equip_info['total_price']:,}ì›)"
                        response += line + "\n"
                    
                    if len(sorted_equipment) > 5:
                        response += f"  ... ì™¸ {len(sorted_equipment)-5}ì¢…\n"
                
                response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                return self._enhance_asset_response(response, query)
            else:
                return f"âŒ {location}ì—ì„œ ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _determine_equipment_category(self, equipment_name: str, item_text: str) -> str:
        """ì¥ë¹„ëª…ê³¼ í…ìŠ¤íŠ¸ë¡œ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
        name_lower = equipment_name.lower()
        text_lower = item_text.lower()
        
        if any(kw in name_lower or kw in text_lower for kw in ['camera', 'ì¹´ë©”ë¼', 'ccu', 'viewfinder', 'ë·°íŒŒì¸ë”']):
            return "ğŸ“¹ ì¹´ë©”ë¼ ì‹œìŠ¤í…œ"
        elif any(kw in name_lower or kw in text_lower for kw in ['monitor', 'ëª¨ë‹ˆí„°', 'display']):
            return "ğŸ–¥ï¸ ëª¨ë‹ˆí„°"
        elif any(kw in name_lower or kw in text_lower for kw in ['audio', 'ì˜¤ë””ì˜¤', 'mixer', 'ë¯¹ì„œ', 'mic', 'ë§ˆì´í¬']):
            return "ğŸ™ï¸ ì˜¤ë””ì˜¤ ì¥ë¹„"
        elif any(kw in name_lower or kw in text_lower for kw in ['server', 'ì„œë²„', 'storage', 'ìŠ¤í† ë¦¬ì§€']):
            return "ğŸ’¾ ì„œë²„/ìŠ¤í† ë¦¬ì§€"
        elif any(kw in name_lower or kw in text_lower for kw in ['switch', 'ìŠ¤ìœ„ì¹˜', 'router', 'ë¼ìš°í„°', 'matrix']):
            return "ğŸ”Œ ìŠ¤ìœ„ì¹­/ë¼ìš°íŒ…"
        elif any(kw in name_lower or kw in text_lower for kw in ['cable', 'ì¼€ì´ë¸”', 'connector', 'ì»¤ë„¥í„°']):
            return "ğŸ”— ì¼€ì´ë¸”/ì»¤ë„¥í„°"
        elif any(kw in name_lower or kw in text_lower for kw in ['tripod', 'íŠ¸ë¼ì´í¬ë“œ', 'pedestal', 'í˜ë°ìŠ¤íƒˆ']):
            return "ğŸ¬ ì¹´ë©”ë¼ ì§€ì›ì¥ë¹„"
        elif any(kw in name_lower or kw in text_lower for kw in ['intercom', 'ì¸í„°ì»´', 'talkback']):
            return "ğŸ“ ì¸í„°ì»´"
        elif any(kw in name_lower or kw in text_lower for kw in ['converter', 'ì»¨ë²„í„°', 'encoder', 'ì¸ì½”ë”']):
            return "ğŸ”„ ì»¨ë²„í„°/ì¸ì½”ë”"
        else:
            return "ğŸ“¦ ê¸°íƒ€ ì¥ë¹„"

    def _search_location_unified(self, txt_path: Path, query: str) -> str:
        """í†µì¼ëœ ìœ„ì¹˜ë³„ ê²€ìƒ‰ - ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            # ìœ„ì¹˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            location_keywords = {
                'ì¤‘ê³„ì°¨': 'ì¤‘ê³„ì°¨',
                'news van': 'News VAN',
                'mini van': 'Mini VAN',
                'ê´‘í™”ë¬¸': 'ê´‘í™”ë¬¸',
                'ìŠ¤íŠœë””ì˜¤': 'ìŠ¤íŠœë””ì˜¤',
                'ë¶€ì¡°': 'ë¶€ì¡°',
                'í¸ì§‘ì‹¤': 'í¸ì§‘ì‹¤',
                'ë”ë¹™ì‹¤': 'ë”ë¹™ì‹¤'
            }
            
            found_location = None
            query_lower = query.lower()
            for key, value in location_keywords.items():
                if key in query_lower:
                    found_location = value
                    break
            
            # ìœ„ì¹˜ê°€ ëª…í™•í•˜ë©´ ìš”ì•½ í˜•ì‹ìœ¼ë¡œ
            if found_location:
                # "ì „ë¶€", "í˜„í™©", "ì „ì²´" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
                if any(kw in query for kw in ['ì „ë¶€', 'í˜„í™©', 'ì „ì²´', 'ëª¨ë“ ']):
                    return self._search_location_summary(txt_path, found_location)
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ìœ„ì¹˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            location_keyword = None
            
            # ì¤‘ê³„ì°¨ ì²´í¬
            if 'ì¤‘ê³„ì°¨' in query.lower() or 'van' in query.lower():
                location_keyword = 'ì¤‘ê³„ì°¨'
                search_term = 'Van'  # ì‹¤ì œ ë°ì´í„°ì—ì„œëŠ” Vanìœ¼ë¡œ ì €ì¥ë¨
            else:
                # ë‹¤ë¥¸ ìœ„ì¹˜ íŒ¨í„´ ì°¾ê¸°
                location_patterns = [
                    (r'ê´‘í™”ë¬¸\s*ìŠ¤íŠœë””ì˜¤', 'ê´‘í™”ë¬¸'),
                    (r'ëŒ€í˜•\s*ìŠ¤íŠœë””ì˜¤', 'ëŒ€í˜•ìŠ¤íŠœë””ì˜¤'),
                    (r'ë‰´ìŠ¤\s*ë¶€ì¡°', 'ë‰´ìŠ¤ë¶€ì¡°'),
                    (r'ì¢…í¸\s*ë¶€ì¡°', 'ì¢…í¸ë¶€ì¡°'),
                    (r'í¸ì§‘ì‹¤', 'í¸ì§‘ì‹¤'),
                    (r'ë”ë¹™ì‹¤', 'ë”ë¹™ì‹¤'),
                    (r'[ê°€-í£]{2,}(?:ìŠ¤íŠœë””ì˜¤|ë¶€ì¡°ì •ì‹¤|í¸ì§‘ì‹¤|ë”ë¹™ì‹¤|ì‚¬ì˜¥|ì„¼í„°|ì‹¤|ì¸µ|ê´€|ë™|í˜¸)', None)
                ]
                
                for pattern, keyword in location_patterns:
                    match = re.search(pattern, query)
                    if match:
                        location_keyword = keyword if keyword else match.group(0)
                        search_term = location_keyword
                        break
            
            if not location_keyword:
                return "âŒ ìœ„ì¹˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜ˆ: ê´‘í™”ë¬¸ ìŠ¤íŠœë””ì˜¤, ì¤‘ê³„ì°¨, ëŒ€í˜•ìŠ¤íŠœë””ì˜¤)"
            
            # ìœ„ì¹˜ë³„ ì¥ë¹„ ê²€ìƒ‰
            lines = content.split('\n')
            matching_items = []
            current_item = []
            item_count = 0
            
            for line in lines:
                if re.match(r'^\[\d{4}\]', line):
                    # ì´ì „ í•­ëª© ê²€ì‚¬
                    if current_item:
                        item_text = '\n'.join(current_item)
                        # ìœ„ì¹˜ í™•ì¸
                        if search_term in item_text or (location_keyword in item_text):
                            matching_items.append(item_text)
                            item_count += 1
                    
                    current_item = [line]
                elif current_item:
                    current_item.append(line)
            
            # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
            if current_item:
                item_text = '\n'.join(current_item)
                if search_term in item_text or (location_keyword in item_text):
                    matching_items.append(item_text)
                    item_count += 1
            
            if matching_items:
                # í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
                response = f"ğŸ“Š **{location_keyword} ì¥ë¹„ í˜„í™©**\n"
                response += "=" * 70 + "\n"
                response += f"âœ… ì´ **{item_count}ê°œ** ì¥ë¹„\n\n"
                
                # ì¥ë¹„ íƒ€ì…ë³„ ë¶„ë¥˜
                equipment_types = {}
                for item in matching_items:
                    # ë¶€í’ˆëª… ì¶”ì¶œ
                    if 'ë¶€í’ˆëª…:' in item:
                        part_match = re.search(r'ë¶€í’ˆëª…:\s*([^\n]+)', item)
                        if part_match:
                            part_name = part_match.group(1).strip()
                            # íƒ€ì… ë¶„ë¥˜
                            if 'Camera' in part_name or 'ì¹´ë©”ë¼' in part_name:
                                type_name = 'ì¹´ë©”ë¼'
                            elif 'Monitor' in part_name or 'ëª¨ë‹ˆí„°' in part_name:
                                type_name = 'ëª¨ë‹ˆí„°'
                            elif 'Audio' in part_name or 'ì˜¤ë””ì˜¤' in part_name or 'Mic' in part_name:
                                type_name = 'ì˜¤ë””ì˜¤/ë§ˆì´í¬'
                            elif 'Light' in part_name or 'ì¡°ëª…' in part_name:
                                type_name = 'ì¡°ëª…'
                            elif 'Lens' in part_name or 'ë Œì¦ˆ' in part_name:
                                type_name = 'ë Œì¦ˆ'
                            elif 'CCU' in part_name or 'Control Unit' in part_name:
                                type_name = 'CCU/ì»¨íŠ¸ë¡¤'
                            elif 'Server' in part_name or 'ì„œë²„' in part_name:
                                type_name = 'ì„œë²„/ìŠ¤í† ë¦¬ì§€'
                            else:
                                type_name = 'ê¸°íƒ€'
                            
                            equipment_types[type_name] = equipment_types.get(type_name, 0) + 1
                
                if equipment_types:
                    response += "ğŸ“‹ **ì¥ë¹„ íƒ€ì…ë³„ ë¶„ë¥˜**:\n"
                    for type_name, count in sorted(equipment_types.items(), key=lambda x: x[1], reverse=True):
                        response += f"  â€¢ {type_name}: {count}ê°œ\n"
                    response += "\n"
                
                response += "-" * 70 + "\n"
                response += "ğŸ“„ **ìƒì„¸ ì¥ë¹„ ëª©ë¡** (ìµœëŒ€ 15ê°œ):\n\n"
                
                # ìƒì„¸ ëª©ë¡ (ìµœëŒ€ 15ê°œ)
                for i, item in enumerate(matching_items[:15], 1):
                    lines = item.split('\n')
                    
                    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    item_info = {}
                    for line in lines:
                        if re.match(r'^\[\d{4}\]', line):
                            item_info['id'] = line.strip()
                        elif 'ë¶€í’ˆëª…:' in line:
                            item_info['name'] = line.split('ë¶€í’ˆëª…:')[1].strip()
                        elif 'ëª¨ë¸:' in line:
                            item_info['model'] = line.split('ëª¨ë¸:')[1].strip()
                        elif 'ì œì¡°ì‚¬:' in line and 'ëª¨ë¸:' not in line:
                            item_info['manufacturer'] = line.split('ì œì¡°ì‚¬:')[1].strip()
                        elif 'êµ¬ì…ì¼:' in line:
                            item_info['date'] = line.split('êµ¬ì…ì¼:')[1].strip()[:10]
                        elif 'ë‹´ë‹¹ì:' in line:
                            item_info['manager'] = line.split('ë‹´ë‹¹ì:')[1].strip()
                    
                    response += f"[{i}] **{item_info.get('id', '')}**"
                    if 'name' in item_info:
                        response += f" {item_info['name']}\n"
                    else:
                        response += "\n"
                    
                    if 'model' in item_info:
                        response += f"    ğŸ“Œ ëª¨ë¸: {item_info['model']}\n"
                    if 'manufacturer' in item_info:
                        response += f"    ğŸ¢ ì œì¡°ì‚¬: {item_info['manufacturer']}\n"
                    if 'date' in item_info:
                        response += f"    ğŸ“… êµ¬ì…ì¼: {item_info['date']}\n"
                    if 'manager' in item_info:
                        response += f"    ğŸ‘¤ ë‹´ë‹¹ì: {item_info['manager']}\n"
                    response += "\n"
                
                if len(matching_items) > 15:
                    response += f"... ì™¸ {len(matching_items) - 15}ê°œ ì¥ë¹„\n"
                
                response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                return self._enhance_asset_response(response, query)
            else:
                return f"âŒ {location_keyword}ì—ì„œ ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _search_asset_by_location(self, txt_path: Path, query: str) -> str:
        """ìœ„ì¹˜ë³„ ìì‚° ê²€ìƒ‰"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš© (assets í´ë”ì—ì„œ)
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ìœ„ì¹˜ í‚¤ì›Œë“œ ì°¾ê¸°
            # ì¿¼ë¦¬ì—ì„œ ìœ„ì¹˜ í‚¤ì›Œë“œ ë™ì  ì¶”ì¶œ
            # í•œê¸€ 2ê¸€ì ì´ìƒ + ì¼ë°˜ì ì¸ ì¥ì†Œ ì ‘ë¯¸ì‚¬
            location_pattern = r'[ê°€-í£]{2,}(?:ìŠ¤íŠœë””ì˜¤|ë¶€ì¡°|í¸ì§‘ì‹¤|ë”ë¹™ì‹¤|ì‚¬ì˜¥|ì„¼í„°|ì‹¤|ì¸µ|ê´€|ë™|í˜¸)'
            locations = re.findall(location_pattern, query)
            found_location = None
            for loc in locations:
                if loc in query:
                    found_location = loc
                    break
            
            if found_location:
                # ì •í™•í•œ ìœ„ì¹˜ ë§¤ì¹­ìœ¼ë¡œ ê²€ìƒ‰
                lines = content.split('\n')
                results = []
                count = 0
                
                for line in lines:
                    if 'ìœ„ì¹˜:' in line:
                        # ì‹¤ì œ ìœ„ì¹˜ ì¶”ì¶œí•˜ì—¬ ì •í™•íˆ ë§¤ì¹­
                        location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', line)
                        if location_match:
                            actual_location = location_match.group(1).strip()
                            
                            # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™ ì ìš©
                            is_match = False
                            if found_location == actual_location:
                                # ì™„ì „ ì¼ì¹˜
                                is_match = True
                            elif found_location == 'ë¶€ì¡°ì •ì‹¤':
                                # 'ë¶€ì¡°ì •ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*ë¶€ì¡°ì •ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                                is_match = actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
                            elif found_location == 'ìŠ¤íŠœë””ì˜¤':
                                # 'ìŠ¤íŠœë””ì˜¤'ë¡œ ê²€ìƒ‰ì‹œ '*ìŠ¤íŠœë””ì˜¤' íŒ¨í„´ë§Œ ë§¤ì¹­ 
                                is_match = actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
                            elif found_location == 'í¸ì§‘ì‹¤':
                                # 'í¸ì§‘ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*í¸ì§‘ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                                is_match = actual_location.endswith('í¸ì§‘ì‹¤')
                            elif found_location in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
                                # ì¤‘ê³„ì°¨ ê²€ìƒ‰ì‹œ Van ê´€ë ¨ ìœ„ì¹˜ ëª¨ë‘ ë§¤ì¹­
                                is_match = 'Van' in actual_location or 'VAN' in actual_location
                            elif len(found_location) > 3:
                                # 3ê¸€ì ì´ìƒì˜ êµ¬ì²´ì ì¸ ìœ„ì¹˜ëª…ì€ ë¶€ë¶„ ë§¤ì¹­ í—ˆìš©
                                is_match = found_location in actual_location
                            
                            if is_match:
                                count += 1
                                if len(results) < 10:  # ì²˜ìŒ 10ê°œë§Œ
                                    results.append(line)
                
                if results:
                    response = f"ğŸ“Š {found_location} ìœ„ì¹˜ ì¥ë¹„: ì´ {count}ê°œ\n"
                    response += "â”" * 40 + "\n"
                    response += "\n[ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ)]:\n"
                    for result in results:
                        response += f"â€¢ {result}\n"
                    response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                    # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                    return self._enhance_asset_response(response, query)
                else:
                    return f"âŒ {found_location}ì— ìœ„ì¹˜í•œ ì¥ë¹„ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            return "âŒ ìœ„ì¹˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _search_asset_with_llm(self, txt_path: Path, query: str) -> str:
        """LLMì„ í™œìš©í•œ ë™ì  ìì‚° ê²€ìƒ‰ - ì •í™•í•œ ìˆ˜ëŸ‰ ê³„ì‚°"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            query_lower = query.lower()
            lines = content.split('\n')
            
            # ë‹´ë‹¹ìë³„ ê²€ìƒ‰
            if 'ë‹´ë‹¹' in query_lower or 'ê´€ë¦¬' in query_lower:
                # ë‹´ë‹¹ìëª… ì¶”ì¶œ
                # í•œê¸€ ì´ë¦„ íŒ¨í„´ìœ¼ë¡œ ë™ì  ì¶”ì¶œ (ì„±+ì´ë¦„ í˜•íƒœ)
                manager_patterns = re.findall(r'[ê°€-í£]{2,4}(?:\s+[ê°€-í£]{2,3})?', query)
                manager = None
                for name in manager_patterns:
                    if name in query:
                        manager = name
                        break
                
                if manager:
                    result = self._count_by_field(content, "ë‹´ë‹¹ì", manager)
                    response = f"ğŸ“Š {manager} ë‹´ë‹¹ì ê´€ë¦¬ ì¥ë¹„ í˜„í™©\n"
                    response += "=" * 50 + "\n"
                    response += f"âœ… ì´ ê´€ë¦¬ ì¥ë¹„: {result['count']}ê°œ\n\n"
                    
                    if result.get('sample_items'):
                        response += "ğŸ“‹ ëŒ€í‘œ ì¥ë¹„ ëª©ë¡:\n"
                        for i, item in enumerate(result['sample_items'][:5], 1):
                            lines = item.split('\n')
                            for line in lines:
                                if '[' in line and ']' in line:
                                    response += f"\n{i}. {line.strip()}\n"
                                elif 'ë¶€í’ˆëª…' in line or 'ëª¨ë¸:' in line:
                                    response += f"   {line.strip()}\n"
                    
                    response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                    # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                    return self._enhance_asset_response(response, query)
            
            # ìœ„ì¹˜ë³„ ê²€ìƒ‰ - ë™ì  ì²˜ë¦¬
            # ìœ„ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ë™ì ìœ¼ë¡œ ì°¾ê¸°
            location_keywords = re.findall(r'[ê°€-í£]+(?:ìŠ¤íŠœë””ì˜¤|ë¶€ì¡°ì •ì‹¤|ì¥ë¹„ì‹¤|ë°ì´í„°ì„¼í„°|í¸ì§‘ì‹¤|ë”ë¹™ì‹¤|ì‚¬ì˜¥|ì¸µ|ê´€|ë™|í˜¸)', query)
            if location_keywords:
                for location in location_keywords:
                    result = self._count_by_field(content, "ìœ„ì¹˜", location)
                    if result['count'] > 0:
                        # "ëª©ë¡", "ì „ì²´" ë“±ì´ ìˆìœ¼ë©´ ìƒì„¸ ëª©ë¡ í‘œì‹œ
                        if any(keyword in query for keyword in ["ëª©ë¡", "ì „ì²´", "ë³´ì—¬ë‹¬ë¼", "ë³´ì—¬ì¤˜", "ë¦¬ìŠ¤íŠ¸", "ì•Œë ¤ì¤˜"]):
                            return self._generate_detailed_location_list(content, location, result, txt_path)
                        else:
                            # ê°„ë‹¨í•œ í˜„í™©ë§Œ í‘œì‹œ
                            response = f"ğŸ“Š {location} ì¥ë¹„ í˜„í™©\n"
                            response += "=" * 50 + "\n"
                            response += f"âœ… ì´ ë³´ìœ  ì¥ë¹„: {result['count']}ê°œ\n\n"
                            response += f"ğŸ“„ ì¶œì²˜: {txt_path.name}"
                            return response
            
            # ë²¤ë”ì‚¬ë³„ ê²€ìƒ‰ - ë™ì  ì²˜ë¦¬
            # ë²¤ë”ì‚¬ í‚¤ì›Œë“œë¥¼ ë¬¸ì„œì—ì„œ ë™ì ìœ¼ë¡œ ì°¾ê¸°
            vendor_keywords = []
            if 'ë²¤ë”' in query or 'ë‚©í’ˆ' in query or 'ì—…ì²´' in query:
                # ì¿¼ë¦¬ì—ì„œ íšŒì‚¬ëª… íŒ¨í„´ ì¶”ì¶œ (í•œê¸€+ì‹œìŠ¤í…œ/ì½”ë¦¬ì•„/ë””ì§€íƒˆ ë“±)
                vendor_patterns = re.findall(r'[ê°€-í£]+(?:ì‹œìŠ¤í…œ|ì½”ë¦¬ì•„|ë””ì§€íƒˆ|ì „ì|ì •ë³´|í†µì‹ |í…Œí¬|ì†”ë£¨ì…˜)|[A-Z]{2,}', query)
                for vendor in vendor_patterns:
                    result = self._count_by_field(content, "ë²¤ë”ì‚¬", vendor)
                    if result['count'] > 0:
                        response = f"ğŸ“Š {vendor} ë‚©í’ˆ ì¥ë¹„ í˜„í™©\n"
                        response += "=" * 50 + "\n"
                        response += f"âœ… ì´ ë‚©í’ˆ ì¥ë¹„: {result['count']}ê°œ\n\n"
                        response += f"ğŸ“„ ì¶œì²˜: {txt_path.name}"
                        return response
            
            # CCU ê´€ë ¨ ì§ˆë¬¸ íŠ¹ë³„ ì²˜ë¦¬
            elif 'ccu' in query_lower or 'camera control' in query_lower:
                ccu_items = []
                ccu_blocks = []
                ccu_count = 0
                
                # CCU í•­ëª©ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ì°¾ê¸°
                for i, line in enumerate(lines):
                    if 'Camera Control Unit' in line or 'CCU' in line:
                        # ì´ ì¤„ì´ ì œëª©ì¤„ì¸ì§€ í™•ì¸ (ëŒ€ê´„í˜¸ë¡œ ì‹œì‘)
                        if line.strip().startswith('['):
                            # ì „ì²´ ë¸”ë¡ ìˆ˜ì§‘ (í˜„ì¬ ì¤„ + ë‹¤ìŒ 4ì¤„)
                            block_lines = []
                            for j in range(i, min(i+5, len(lines))):
                                if lines[j].strip():
                                    block_lines.append(lines[j].strip())
                            
                            if block_lines:
                                ccu_blocks.append('\n'.join(block_lines))
                                ccu_count += 1
                
                response = f"ğŸ“Š CCU(Camera Control Unit) í˜„í™©\n"
                response += "=" * 50 + "\n"
                response += f"### ğŸ“ˆ ì´ CCU ì¥ë¹„: **{ccu_count}ê°œ**\n\n"
                
                # ì œì¡°ì‚¬ë³„ ë¶„í¬ ë¶„ì„
                manufacturers = {}
                for block in ccu_blocks:
                    mfr_match = re.search(r'ì œì¡°ì‚¬:\s*([^|]+)', block)
                    if mfr_match:
                        mfr = mfr_match.group(1).strip()
                        manufacturers[mfr] = manufacturers.get(mfr, 0) + 1
                
                if manufacturers:
                    response += "### ğŸ“Š ì œì¡°ì‚¬ë³„ ë¶„í¬\n"
                    for mfr, cnt in sorted(manufacturers.items(), key=lambda x: x[1], reverse=True):
                        response += f"â€¢ **{mfr}**: {cnt}ê°œ\n"
                    response += "\n"
                
                if ccu_blocks:
                    # CCUëŠ” ì¤‘ìš” ì¥ë¹„ì´ë¯€ë¡œ 30ê°œ ë¯¸ë§Œì€ ì „ì²´ í‘œì‹œ
                    display_limit = 30 if ccu_count > 30 else ccu_count
                    
                    if ccu_count <= 30:
                        response += "### ğŸ“‹ ì „ì²´ ìƒì„¸ ëª©ë¡\n"
                    else:
                        response += f"### ğŸ“‹ ìƒì„¸ ëª©ë¡ ({display_limit}ê°œ/ì´ {ccu_count}ê°œ)\n"
                    
                    for i, block in enumerate(ccu_blocks[:display_limit], 1):
                        response += f"{block}\n\n"
                    
                    if ccu_count > display_limit:
                        response += f"\nâš ï¸ í‘œì‹œëœ {display_limit}ê°œ ì™¸ì— {ccu_count - display_limit}ê°œê°€ ë” ìˆìŠµë‹ˆë‹¤.\n"
                        response += "ğŸ” ì „ì²´ ëª©ë¡ì´ í•„ìš”í•˜ì‹œë©´ 'ì „ì²´ CCU ëª©ë¡ ë³´ì—¬ì¤˜'ë¼ê³  ìš”ì²­í•˜ì„¸ìš”.\n"
                
                response += f"\n---\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                return response
            
            # ë²”ìš©ì ì¸ ìì‚° ê²€ìƒ‰ ê°œì„  - ë¸”ë¡ ë‹¨ìœ„ë¡œ ì™„ì „í•œ ì •ë³´ ì œê³µ
            # ì¿¼ë¦¬ì—ì„œ ê²€ìƒ‰ì–´ ì¶”ì¶œ
            search_terms = []
            
            # "ì–´ë””" "ìˆëŠ”" ê°™ì€ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ìœ„ì¹˜ ê²€ìƒ‰
            location_search = False
            if any(word in query_lower for word in ['ì–´ë””', 'ìœ„ì¹˜', 'ìˆëŠ”', 'ë³´ê´€']):
                location_search = True
            
            # ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë™ì  ì¶”ì¶œ
            # ì˜ì–´ ë‹¨ì–´, í•œê¸€ ë‹¨ì–´, ìˆ«ì ë“±
            potential_keywords = re.findall(r'[A-Za-z]{2,}[\s\-]?[A-Za-z]*|[ê°€-í£]{2,}|[A-Z]+[-\s]?\d+', query)
            
            if potential_keywords:
                # ë¸”ë¡ ë‹¨ìœ„ë¡œ ì¥ë¹„ ì •ë³´ ìˆ˜ì§‘
                matching_items = []
                current_item = []
                is_matching = False
                item_count = 0
                
                for line in lines:
                    # ìƒˆë¡œìš´ ì¥ë¹„ í•­ëª© ì‹œì‘ ê°ì§€
                    if re.match(r'^\[\d{4}\]', line):
                        # ì´ì „ í•­ëª©ì´ ë§¤ì¹­ë˜ì—ˆìœ¼ë©´ ì €ì¥
                        if is_matching and current_item:
                            matching_items.append('\n'.join(current_item))
                            item_count += 1
                        
                        # ìƒˆ í•­ëª© ì‹œì‘
                        current_item = [line]
                        is_matching = False
                        
                        # ì²« ì¤„ì—ì„œë„ ë§¤ì¹­ ì²´í¬
                        for keyword in potential_keywords:
                            if keyword.lower() in line.lower():
                                is_matching = True
                                break
                    elif current_item:
                        current_item.append(line)
                        # í˜„ì¬ ì¤„ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                        for keyword in potential_keywords:
                            if keyword.lower() in line.lower():
                                is_matching = True
                                break
                
                # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
                if is_matching and current_item:
                    matching_items.append('\n'.join(current_item))
                    item_count += 1
                
                if matching_items:
                    # ê²€ìƒ‰ì–´ í‘œì‹œ
                    search_display = ' + '.join(potential_keywords)
                    response = f"ğŸ“Š '{search_display}' ê²€ìƒ‰ ê²°ê³¼\n"
                    response += "=" * 50 + "\n"
                    response += f"### ğŸ“ˆ ì´ ê²€ìƒ‰ ê²°ê³¼: **{len(matching_items)}ê°œ**\n\n"
                    
                    # ë™ì  ë¶„ë¥˜ (ì¥ë¹„ëª… ê¸°ë°˜)
                    equipment_categories = {}
                    for item in matching_items:
                        first_line = item.split('\n')[0]
                        # ì¥ë¹„ëª… ì¶”ì¶œ
                        equipment_match = re.search(r'\[\d{4}\]\s*(.+?)(?:\s*\||$)', first_line)
                        if equipment_match:
                            equipment_name = equipment_match.group(1).strip()
                            
                            # ë™ì  ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                            category = self._categorize_equipment(equipment_name)
                            
                            if category not in equipment_categories:
                                equipment_categories[category] = []
                            equipment_categories[category].append(equipment_name)
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ëŸ‰ í‘œì‹œ
                    if equipment_categories and len(equipment_categories) > 1:
                        response += "### ğŸ“‹ ì¥ë¹„ íƒ€ì…ë³„ ë¶„ë¥˜\n"
                        for cat_name, equipments in sorted(equipment_categories.items()):
                            response += f"â€¢ **{cat_name}**: {len(equipments)}ê°œ\n"
                        response += "\n"
                    
                    # ìƒì„¸ ì¥ë¹„ ëª©ë¡ (ë¸”ë¡ ë‹¨ìœ„ ì „ì²´ ì •ë³´)
                    response += "### ğŸ“„ ìƒì„¸ ì¥ë¹„ ì •ë³´\n\n"
                    
                    # í‘œì‹œí•  í•­ëª© ìˆ˜ ê²°ì • (ë§ìœ¼ë©´ 30ê°œ, ì ìœ¼ë©´ ì „ì²´)
                    display_count = min(30, len(matching_items))
                    
                    for i, item in enumerate(matching_items[:display_count], 1):
                        lines_of_item = item.split('\n')
                        
                        # ì „ì²´ ì •ë³´ ë¸”ë¡ í‘œì‹œ
                        response += f"[{i}] " + lines_of_item[0][6:] + "\n"  # ë²ˆí˜¸ ì œê±°í•˜ê³  í‘œì‹œ
                        
                        # ì£¼ìš” ì •ë³´ ì¶”ì¶œ ë° í‘œì‹œ
                        for line in lines_of_item[1:]:
                            line = line.strip()
                            if line and not line.startswith('â”'):
                                # ì •ë³´ í•„ë“œ ì •ë¦¬
                                if any(field in line for field in ['ì œì¡°ì‚¬:', 'ëª¨ë¸ëª…:', 'ì‹œë¦¬ì–¼:', 'ìœ„ì¹˜:', 'ë‹´ë‹¹ì:', 'ë²¤ë”ì‚¬:', 'ì·¨ë“ì¼ì:']):
                                    response += f"  {line}\n"
                        response += "\n"
                    
                    if len(matching_items) > 30:
                        response += f"\nâš ï¸ ì´ {len(matching_items)}ê°œ ì¤‘ 30ê°œë§Œ í‘œì‹œ\n"
                        response += "ğŸ” ì „ì²´ ëª©ë¡ì´ í•„ìš”í•˜ì‹œë©´ 'ì „ì²´ ëª©ë¡ ë³´ì—¬ì¤˜'ë¼ê³  ìš”ì²­í•˜ì„¸ìš”.\n"
                    
                    response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                    # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                    return self._enhance_asset_response(response, query)
            
            # ê¸°íƒ€ í‚¤ì›Œë“œ ê²€ìƒ‰
            results = []
            search_keywords = re.findall(r'[ê°€-í£]{2,}|[A-Za-z]{3,}|\d{4,}', query)
            
            for line in lines:
                for keyword in search_keywords:
                    if keyword.lower() in line.lower():
                        results.append(line.strip())
                        if len(results) >= 20:  # ìµœëŒ€ 20ê°œ
                            break
                if len(results) >= 20:
                    break
            
            if results:
                response = f"ğŸ“Š '{query}' ê²€ìƒ‰ ê²°ê³¼:\n"
                response += "â”" * 40 + "\n"
                
                # ê²°ê³¼ ìš”ì•½
                response += f"ì´ {len(results)}ê°œ í•­ëª© ë°œê²¬\n\n"
                
                # 30ê°œê¹Œì§€ í‘œì‹œ
                display_limit = min(30, len(results))
                
                for i, result in enumerate(results[:display_limit], 1):
                    response += f"{i}. {result[:150]}\n"
                
                if len(results) > display_limit:
                    response += f"\nâš ï¸ ì´ {len(results)}ê°œ ì¤‘ {display_limit}ê°œë§Œ í‘œì‹œ\n"
                    response += "ğŸ” ì „ì²´ ëª©ë¡ì´ í•„ìš”í•˜ì‹œë©´ ë‹¤ì‹œ ìš”ì²­í•˜ì„¸ìš”.\n"
                    
                response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                return self._enhance_asset_response(response, query)
            
            # ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë” ë³µì¡í•œ ì§ˆë¬¸ì¸ ê²½ìš° LLM ì‚¬ìš©
            # LLMìœ¼ë¡œ ë” ì§€ëŠ¥ì ì¸ ê²€ìƒ‰ ìˆ˜í–‰
            print("ğŸ¤– LLMì„ ì‚¬ìš©í•œ ê³ ê¸‰ ê²€ìƒ‰ ì‹œì‘...")
            
            # ì „ì²´ ë‚´ìš©ì—ì„œ ê´€ë ¨ ë¶€ë¶„ ì¶”ì¶œ (ìµœëŒ€ 10000ì)
            content_sample = content[:10000] if len(content) > 10000 else content
            
            # LLM ì´ˆê¸°í™”
            if self.llm is None:
                self._preload_llm()
            
            if self.llm:
                prompt = f"""ë‹¤ìŒì€ ì±„ë„A ë°©ì†¡ì¥ë¹„ ìì‚° ë°ì´í„°ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë°ì´í„° ìƒ˜í”Œ:
{content_sample}

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
- ì •í™•í•œ ìˆ˜ëŸ‰ê³¼ ì„¸ë¶€ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
- í‘œë‚˜ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”
- ê´€ë ¨ ì¥ë¹„ì˜ ëª¨ë¸ëª…, ì œì¡°ì‚¬, ìœ„ì¹˜ ë“±ì„ í¬í•¨í•˜ì„¸ìš”
- ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”

ë‹µë³€:"""
                
                try:
                    # QwenLLMì˜ ì˜¬ë°”ë¥¸ ë©”ì„œë“œ ì‚¬ìš©
                    if hasattr(self.llm, 'generate_response'):
                        # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
                        context_chunks = [{
                            'content': content_sample,
                            'source': str(txt_path.name),
                            'score': 1.0
                        }]
                        response_obj = self.llm.generate_response(prompt, context_chunks)
                        if response_obj and hasattr(response_obj, 'answer'):
                            llm_response = response_obj.answer
                        else:
                            llm_response = str(response_obj) if response_obj else ""
                    else:
                        # í´ë°± - ì§ì ‘ __call__ ì‚¬ìš©
                        llm_response = self.llm(prompt, max_tokens=1024, temperature=0.1)['choices'][0]['text']
                    
                    if llm_response and llm_response.strip():
                        return f"ğŸ“Š AI ë¶„ì„ ê²°ê³¼:\n\n{llm_response}\n\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                except Exception as e:
                    print(f"âš ï¸ LLM ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # LLMë„ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€
            return f"âŒ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”:\n- ì œì¡°ì‚¬ëª… (HP, SONY, Harris ë“±)\n- ì¥ë¹„ëª… (CCU, ì¹´ë©”ë¼, ë§ˆì´í¬ ë“±)\n- ì—°ë„ (2015ë…„, 2020ë…„ ë“±)"
            
        except Exception as e:
            return f"âŒ ìì‚° ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _get_asset_summary(self, txt_path: Path) -> str:
        """ìì‚° íŒŒì¼ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            summary = "ğŸ“Š ì±„ë„A ë°©ì†¡ì¥ë¹„ ìì‚° í˜„í™©\n"
            summary += "â”" * 40 + "\n"
            
            # ì²˜ìŒ 100ì¤„ì—ì„œ ì£¼ìš” í†µê³„ ì¶”ì¶œ
            for line in lines[:100]:
                if 'ì´ ë³´ìœ  ì¥ë¹„:' in line or \
                   'â€¢ ' in line and ('ê°œ' in line or 'ëŒ€' in line):
                    summary += line
            
            summary += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
            return summary
            
        except Exception as e:
            return f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}"
    
    def _categorize_equipment(self, equipment_name: str) -> str:
        """ì¥ë¹„ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        name_lower = equipment_name.lower()
        
        # íŒ¨í„´ ê¸°ë°˜ ë™ì  ë¶„ë¥˜
        if any(word in name_lower for word in ['camera', 'ccu', 'ì¹´ë©”ë¼', 'cam']):
            return 'ì¹´ë©”ë¼ ê´€ë ¨'
        elif any(word in name_lower for word in ['monitor', 'ëª¨ë‹ˆí„°', 'display']):
            return 'ëª¨ë‹ˆí„°/ë””ìŠ¤í”Œë ˆì´'
        elif any(word in name_lower for word in ['mic', 'microphone', 'ë§ˆì´í¬', 'audio']):
            return 'ì˜¤ë””ì˜¤/ë§ˆì´í¬'
        elif any(word in name_lower for word in ['switcher', 'router', 'ìŠ¤ìœ„ì²˜', 'ë¼ìš°í„°']):
            return 'ìŠ¤ìœ„ì²˜/ë¼ìš°í„°'
        elif any(word in name_lower for word in ['server', 'ì„œë²„', 'storage', 'nas']):
            return 'ì„œë²„/ìŠ¤í† ë¦¬ì§€'
        elif any(word in name_lower for word in ['converter', 'ì»¨ë²„í„°', 'adapter']):
            return 'ì»¨ë²„í„°/ì–´ëŒ‘í„°'
        elif any(word in name_lower for word in ['lens', 'ë Œì¦ˆ', 'optical']):
            return 'ë Œì¦ˆ/ê´‘í•™'
        elif any(word in name_lower for word in ['tripod', 'ì‚¼ê°ëŒ€', 'stand']):
            return 'ì‚¼ê°ëŒ€/ìŠ¤íƒ ë“œ'
        elif any(word in name_lower for word in ['battery', 'ë°°í„°ë¦¬', 'charger', 'power']):
            return 'ì „ì›/ë°°í„°ë¦¬'
        elif any(word in name_lower for word in ['cable', 'ì¼€ì´ë¸”', 'connector']):
            return 'ì¼€ì´ë¸”/ì»¤ë„¥í„°'
        elif any(word in name_lower for word in ['analyzer', 'test', 'ë¶„ì„', 'í…ŒìŠ¤íŠ¸']):
            return 'ë¶„ì„/í…ŒìŠ¤íŠ¸ ì¥ë¹„'
        elif any(word in name_lower for word in ['transmitter', 'receiver', 'ì†¡ì‹ ', 'ìˆ˜ì‹ ']):
            return 'ì†¡ìˆ˜ì‹  ì¥ë¹„'
        elif any(word in name_lower for word in ['recorder', 'player', 'ë ˆì½”ë”', 'í”Œë ˆì´ì–´']):
            return 'ë…¹í™”/ì¬ìƒ ì¥ë¹„'
        elif any(word in name_lower for word in ['light', 'ì¡°ëª…', 'led', 'lamp']):
            return 'ì¡°ëª… ì¥ë¹„'
        elif 'nexio' in name_lower:
            return 'NEXIO ì‹œìŠ¤í…œ'
        elif 'hp' in name_lower and any(word in name_lower for word in ['z8', 'z6', 'z4', 'workstation']):
            return 'HP ì›Œí¬ìŠ¤í…Œì´ì…˜'
        else:
            # ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
            return 'ê¸°íƒ€ ì¥ë¹„'
    
    def _count_by_field(self, content: str, field_name: str, search_value: str) -> dict:
        """íŠ¹ì • í•„ë“œê°’ìœ¼ë¡œ ì¥ë¹„ ìˆ˜ëŸ‰ ê³„ì‚°"""
        lines = content.split('\n')
        count = 0
        items = []
        current_item = []
        is_matching = False
        
        for line in lines:
            # ìƒˆë¡œìš´ ì¥ë¹„ í•­ëª© ì‹œì‘
            if re.match(r'^\[\d{4}\]', line):
                # ì´ì „ í•­ëª©ì´ ë§¤ì¹­ë˜ì—ˆìœ¼ë©´ ì¹´ìš´íŠ¸
                if is_matching and current_item:
                    count += 1
                    if len(items) < 10:  # ìƒ˜í”Œ 10ê°œë§Œ ì €ì¥
                        items.append('\n'.join(current_item))
                
                # ìƒˆ í•­ëª© ì‹œì‘
                current_item = [line]
                is_matching = False
            elif current_item:
                current_item.append(line)
                # í•„ë“œë³„ ë§¤ì¹­ í™•ì¸
                if field_name == "ë‹´ë‹¹ì" and "ë‹´ë‹¹ì:" in line:
                    if search_value in line:
                        is_matching = True
                elif field_name == "ìœ„ì¹˜" and "ìœ„ì¹˜:" in line:
                    # ì •í™•í•œ ìœ„ì¹˜ ë§¤ì¹­ ë¡œì§ ì ìš©
                    location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', line)
                    if location_match:
                        actual_location = location_match.group(1).strip()
                        
                        # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™
                        if search_value == actual_location:
                            # ì™„ì „ ì¼ì¹˜
                            is_matching = True
                        elif search_value == 'ë¶€ì¡°ì •ì‹¤':
                            # 'ë¶€ì¡°ì •ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*ë¶€ì¡°ì •ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                            is_matching = actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
                        elif search_value == 'ìŠ¤íŠœë””ì˜¤':
                            # 'ìŠ¤íŠœë””ì˜¤'ë¡œ ê²€ìƒ‰ì‹œ '*ìŠ¤íŠœë””ì˜¤' íŒ¨í„´ë§Œ ë§¤ì¹­ 
                            is_matching = actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
                        elif search_value == 'í¸ì§‘ì‹¤':
                            # 'í¸ì§‘ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*í¸ì§‘ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                            is_matching = actual_location.endswith('í¸ì§‘ì‹¤')
                        elif search_value in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
                            # ì¤‘ê³„ì°¨ ê²€ìƒ‰ì‹œ Van ê´€ë ¨ ìœ„ì¹˜ ëª¨ë‘ ë§¤ì¹­
                            is_matching = 'Van' in actual_location or 'VAN' in actual_location
                        elif len(search_value) > 3:
                            # 3ê¸€ì ì´ìƒì˜ êµ¬ì²´ì ì¸ ìœ„ì¹˜ëª…ì€ ë¶€ë¶„ ë§¤ì¹­ í—ˆìš©
                            is_matching = search_value in actual_location
                elif field_name == "ë²¤ë”ì‚¬" and "ë²¤ë”ì‚¬:" in line:
                    if search_value in line:
                        is_matching = True
                elif field_name == "ì œì¡°ì‚¬" and "ì œì¡°ì‚¬:" in line:
                    if search_value.upper() in line.upper():
                        is_matching = True
        
        # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
        if is_matching and current_item:
            count += 1
            if len(items) < 10:
                items.append('\n'.join(current_item))
        
        return {
            'count': count,
            'sample_items': items
        }
    
    def _search_asset_by_manufacturer(self, txt_path: Path, query: str) -> str:
        """ì œì¡°ì‚¬ë³„ ìì‚° ê²€ìƒ‰ - ì •í™•í•œ ìˆ˜ëŸ‰ ê³„ì‚°"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì œì¡°ì‚¬ëª… ì¶”ì¶œ (ë” ìœ ì—°í•œ ì ‘ê·¼)
            # 1. ì•Œë ¤ì§„ ì œì¡°ì‚¬ ëª©ë¡
            known_manufacturers = [
                'SONY', 'HARRIS', 'HP', 'TOSHIBA', 'PANASONIC', 'CANON',
                'DELL', 'APPLE', 'SAMSUNG', 'LG', 'AVID', 'BLACKMAGIC',
                'MICROSOFT', 'ADOBE', 'GRASS VALLEY', 'EVERTZ', 'IMAGINE',
                'ASHIDAVOX', 'ATOMOS', 'ARRI', 'RED', 'JVC', 'IKEGAMI'
            ]
            
            manufacturer = None
            query_upper = query.upper()
            
            # ì•Œë ¤ì§„ ì œì¡°ì‚¬ í™•ì¸
            for mfr in known_manufacturers:
                if mfr in query_upper:
                    manufacturer = mfr
                    break
            
            # ëª» ì°¾ì•˜ìœ¼ë©´ ì˜ë¬¸ ëŒ€ë¬¸ì ë‹¨ì–´ ì¶”ì¶œ
            if not manufacturer:
                words = re.findall(r'\b[A-Z][A-Z0-9]*\b', query_upper)
                for word in words:
                    if len(word) >= 2 and word not in ['THE', 'AND', 'OR', 'FOR']:
                        manufacturer = word
                        break
            
            if manufacturer:
                # ì •í™•í•œ ìˆ˜ëŸ‰ ê³„ì‚°
                result = self._count_by_field(content, "ì œì¡°ì‚¬", manufacturer)
                
                response = f"ğŸ“Š {manufacturer} ì œì¡°ì‚¬ ì¥ë¹„ í˜„í™©\n"
                response += "=" * 50 + "\n"
                response += f"âœ… ì´ ë³´ìœ  ìˆ˜ëŸ‰: {result['count']}ê°œ\n\n"
                
                if result.get('sample_items'):
                    # ëª¨ë¸ë³„ í†µê³„
                    model_counts = {}
                    for item in result['sample_items']:
                        lines = item.split('\n')
                        for line in lines:
                            if 'ëª¨ë¸:' in line:
                                model = line.split('ëª¨ë¸:')[1].split('|')[0].strip()
                                if model:
                                    model_counts[model] = model_counts.get(model, 0) + 1
                    
                    if model_counts:
                        response += "ğŸ“¦ ì£¼ìš” ëª¨ë¸:\n"
                        for model, count in sorted(model_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                            response += f"  â€¢ {model}: {count}ê°œ\n"
                        response += "\n"
                    
                    # ìƒ˜í”Œ ì¥ë¹„ ëª©ë¡
                    response += "ğŸ“‹ ëŒ€í‘œ ì¥ë¹„ (ìƒ˜í”Œ):\n"
                    for i, item in enumerate(result['sample_items'][:5], 1):
                        lines = item.split('\n')
                        for line in lines:
                            if line.startswith('[') and ']' in line:
                                response += f"\n{i}. {line.strip()}\n"
                            elif 'ê¸°ë³¸ì •ë³´:' in line:
                                # ëª¨ë¸ê³¼ ì‹œë¦¬ì–¼ë§Œ í‘œì‹œ
                                parts = line.split('|')
                                for part in parts:
                                    if 'ëª¨ë¸:' in part or 'S/N:' in part:
                                        response += f"   {part.strip()}\n"
                
                response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                return self._enhance_asset_response(response, query)
            
            # ì œì¡°ì‚¬ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
            else:
                # ì œì¡°ì‚¬ë³„ ê²€ìƒ‰
                lines = content.split('\n')
                results = []
                count = 0
                models = {}  # ëª¨ë¸ë³„ ì§‘ê³„
                
                for line in lines:
                    if manufacturer in line.upper() or f"ì œì¡°ì‚¬: {manufacturer}" in line:
                        count += 1
                        if len(results) < 20:  # ì²˜ìŒ 20ê°œ
                            results.append(line.strip())
                        
                        # ëª¨ë¸ëª… ì¶”ì¶œí•˜ì—¬ ì§‘ê³„
                        model_match = re.search(r'ëª¨ë¸:\s*([^\|]+)', line)
                        if model_match:
                            model = model_match.group(1).strip()
                            models[model] = models.get(model, 0) + 1
                
                if results:
                    response = f"ğŸ“Š {manufacturer} ì œì¡°ì‚¬ ì¥ë¹„ í˜„í™©\n"
                    response += "=" * 50 + "\n"
                    response += f"ì´ ë³´ìœ  ìˆ˜ëŸ‰: {count}ê°œ\n\n"
                    
                    # ëª¨ë¸ë³„ í˜„í™© (ìƒìœ„ 10ê°œ)
                    if models:
                        response += "ğŸ“¦ ëª¨ë¸ë³„ ë¶„í¬:\n"
                        for model, cnt in sorted(models.items(), key=lambda x: x[1], reverse=True)[:10]:
                            response += f"  â€¢ {model}: {cnt}ê°œ\n"
                        response += "\n"
                    
                    response += "ğŸ“‹ ìƒì„¸ ëª©ë¡ (ì¼ë¶€):\n"
                    response += "-" * 40 + "\n"
                    # ì œì¡°ì‚¬ë³„ ê²€ìƒ‰ë„ 30ê°œê¹Œì§€ í‘œì‹œ
                    display_limit = min(30, count)
                    for i, result in enumerate(results[:display_limit], 1):
                        response += f"{i}. {result}\n"
                    
                    if count > display_limit:
                        response += f"\nâš ï¸ ì´ {count}ê°œ ì¤‘ {display_limit}ê°œë§Œ í‘œì‹œ\n"
                        response += "ğŸ” ì „ì²´ ëª©ë¡ì´ í•„ìš”í•˜ì‹œë©´ ë‹¤ì‹œ ìš”ì²­í•˜ì„¸ìš”.\n"
                    
                    response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
                    # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
                    return self._enhance_asset_response(response, query)
                else:
                    return f"âŒ {manufacturer} ì œì¡°ì‚¬ ì¥ë¹„ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            return "âŒ ì œì¡°ì‚¬ëª…ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _format_detailed_asset_response(self, asset_data: list, model: str, query: str) -> str:
        """ìì‚° ë°ì´í„°ë¥¼ ìƒì„¸í•˜ê³  ì •í™•í•˜ê²Œ í¬ë§·íŒ…"""
        if not asset_data:
            return f"âŒ {model} ëª¨ë¸ ì¥ë¹„ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        lines.append(f"ğŸ“Š **{model} ì¥ë¹„ í˜„í™©**")
        lines.append("=" * 60)
        lines.append(f"\n### ğŸ“ˆ ì „ì²´ ë³´ìœ  ìˆ˜ëŸ‰: **{len(asset_data)}ê°œ**\n")
        
        # ìœ„ì¹˜ë³„ ë¶„í¬
        locations = {}
        for item in asset_data:
            if 'ìœ„ì¹˜' in item:
                loc = item['ìœ„ì¹˜']
                locations[loc] = locations.get(loc, 0) + 1
        
        if locations:
            lines.append("### ğŸ“ ìœ„ì¹˜ë³„ ë¶„í¬")
            for loc, count in sorted(locations.items(), key=lambda x: x[1], reverse=True)[:5]:
                lines.append(f"â€¢ **{loc}**: {count}ê°œ")
            lines.append("")
        
        # ìƒì„¸ ëª©ë¡ - ë” ìì„¸í•œ ê´€ë¦¬ì •ë³´ ì¤‘ì‹¬ í˜•ì‹ìœ¼ë¡œ ë³€ê²½
        lines.append("### ğŸ“‹ ìƒì„¸ ì¥ë¹„ ì •ë³´")
        lines.append("")
        
        for i, item in enumerate(asset_data[:10], 1):  # ì²˜ìŒ 10ê°œë§Œ
            # [1] í˜•ì‹ìœ¼ë¡œ ë³€ê²½í•˜ê³  í•­ëª©ëª… í¬í•¨
            item_name = f"{item.get('ëª¨ë¸', model)}"
            lines.append(f"[{i}]  {item_name}")
            
            # ê¸°ë³¸ì •ë³´ - ëª¨ë¸ëª… ë³„ë„ í‘œì‹œ
            basic_info = []
            if 'ì œì¡°ì‚¬' in item:
                basic_info.append(f"ì œì¡°ì‚¬: {item['ì œì¡°ì‚¬']}")
            if 'ëª¨ë¸' in item:
                basic_info.append(f"ëª¨ë¸: {item['ëª¨ë¸']}")
            if 'S/N' in item:
                basic_info.append(f"S/N: {item['S/N']}")
            if basic_info:
                lines.append(f"  ê¸°ë³¸ì •ë³´: {' | '.join(basic_info)}")
            
            # ìœ„ì¹˜ì •ë³´
            location_info = []
            if 'ìœ„ì¹˜' in item:
                location_info.append(f"ìœ„ì¹˜: {item['ìœ„ì¹˜']}")
            if 'ì‹œìŠ¤í…œ' in item:
                location_info.append(f"ì‹œìŠ¤í…œ: {item['ì‹œìŠ¤í…œ']}")
            if location_info:
                lines.append(f"  ìœ„ì¹˜ì •ë³´: {' | '.join(location_info)}")
            
            # ê´€ë¦¬ì •ë³´ - ë” ìƒì„¸í•œ ì •ë³´ í¬í•¨
            mgmt_info = []
            if 'ë‹´ë‹¹ì' in item:
                mgmt_info.append(f"ë‹´ë‹¹ì: {item['ë‹´ë‹¹ì']}")
            if 'ì—°ë½ì²˜' in item:
                mgmt_info.append(f"ì—°ë½ì²˜: {item['ì—°ë½ì²˜']}")
            if 'ë²¤ë”ì‚¬' in item:
                mgmt_info.append(f"ë²¤ë”ì‚¬: {item['ë²¤ë”ì‚¬']}")
            if 'ê³„ì•½ì—…ì²´' in item:
                mgmt_info.append(f"ê³„ì•½ì—…ì²´: {item['ê³„ì•½ì—…ì²´']}")
            if mgmt_info:
                lines.append(f"  ê´€ë¦¬ì •ë³´: {' | '.join(mgmt_info)}")
            
            lines.append("")  # ê° í•­ëª© ì‚¬ì´ ê³µë°±
        
        if len(asset_data) > 10:
            lines.append(f"\n... ì™¸ {len(asset_data) - 10}ê°œ ë” ìˆìŒ")
        
        # ì¶”ê°€ í†µê³„
        lines.append("\n### ğŸ“Š ì¶”ê°€ í†µê³„")
        
        # êµ¬ì…ì—°ë„ë³„ ë¶„í¬
        years = {}
        for item in asset_data:
            if 'êµ¬ì…ì¼' in item:
                year = item['êµ¬ì…ì¼'][:4] if len(item['êµ¬ì…ì¼']) >= 4 else None
                if year and year.isdigit():
                    years[year] = years.get(year, 0) + 1
        
        if years:
            lines.append("\n**êµ¬ì…ì—°ë„ë³„ ë¶„í¬:**")
            for year, count in sorted(years.items(), reverse=True)[:3]:
                lines.append(f"â€¢ {year}ë…„: {count}ê°œ")
        
        lines.append("\n---")
        lines.append("ğŸ“ ì¶œì²˜: ì±„ë„A ë°©ì†¡ì¥ë¹„ ìì‚° ë°ì´í„°ë² ì´ìŠ¤")
        
        return '\n'.join(lines)
    
    def _search_asset_by_model(self, txt_path: Path, query: str) -> str:
        """ëª¨ë¸ë³„ ìì‚° ê²€ìƒ‰"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ëª¨ë¸ëª… ì¶”ì¶œ (HP Z840, XDS-PD1000 ë“± - ë” ìœ ì—°í•œ íŒ¨í„´)
            model_patterns = [
                r'(Z\d+)',  # Z840, Z620 ë“±
                r'([A-Z]{2,}[-]?[A-Z]*\d+)',  # XDS-PD1000, DL360 ë“±
                r'([A-Z]+[-]?\d+)',  # PD1000, DL-360 ë“±
                r'(\w+[-]?\w+\d+)',  # ì¼ë°˜ì ì¸ ëª¨ë¸ëª…
            ]
            
            model = None
            for pattern in model_patterns:
                model_match = re.search(pattern, query, re.IGNORECASE)
                if model_match:
                    model = model_match.group(1).upper()
                    break
            
            if model:
                
                # ëª¨ë¸ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
                lines = content.split('\n')
                results = []
                count = 0
                
                for line in lines:
                    if model in line.upper():
                        count += 1
                        if len(results) < 10:  # ì²˜ìŒ 10ê°œë§Œ
                            results.append(line)
                
                if results:
                    # ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì—¬ëŸ¬ ì¤„ ì²˜ë¦¬)
                    asset_data = []
                    
                    # ê° í•­ëª©ì„ ì°¾ê¸° (ëª¨ë¸ëª…ì´ ìˆëŠ” ì¤„ì„ ì°¾ê³  ì „í›„ ì¤„ í¬í•¨)
                    for i, line in enumerate(lines):
                        if model in line.upper() and 'ëª¨ë¸:' in line:
                            # í•­ëª© ë¸”ë¡ êµ¬ì„± (ì´ì „ ì¤„ + í˜„ì¬ ì¤„ + ë‹¤ìŒ ì¤„ë“¤)
                            block_lines = []
                            
                            # ì´ì „ ì¤„ (í•­ëª© ë²ˆí˜¸)
                            if i > 0:
                                block_lines.append(lines[i-1])
                            
                            # í˜„ì¬ ì¤„ë¶€í„° ì‹œì‘í•´ì„œ ë‹¤ìŒ í•­ëª© ì‹œì‘ê¹Œì§€ ë˜ëŠ” ìµœëŒ€ 15ì¤„
                            for j in range(i, min(i+15, len(lines))):
                                block_lines.append(lines[j])
                                # ë‹¤ìŒ í•­ëª©ì´ ì‹œì‘ë˜ë©´ ì¤‘ë‹¨
                                if j > i and lines[j].strip().startswith('[') and lines[j].strip().endswith(']'):
                                    break
                            
                            # ë¸”ë¡ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
                            block = '\n'.join(block_lines)
                            
                            # ì •ë³´ ì¶”ì¶œ
                            item_dict = {'ëª¨ë¸': model}
                            
                            # ê¸°ë³¸ì •ë³´ ì¶”ì¶œ (ë¸”ë¡ì—ì„œ)
                            # ì œì¡°ì‚¬ ì¶”ì¶œ
                            mfr_match = re.search(r'ì œì¡°ì‚¬:\s*([^|]+)', block)
                            if mfr_match:
                                item_dict['ì œì¡°ì‚¬'] = mfr_match.group(1).strip()
                            
                            # S/N ì¶”ì¶œ (ì—¬ëŸ¬ ê°œì˜ S/N ì²˜ë¦¬)
                            serial_match = re.search(r'S/N:\s*([^|\n]+)', block)
                            if serial_match:
                                sn_text = serial_match.group(1).strip()
                                # ì—¬ëŸ¬ ì‹œë¦¬ì–¼ ë²ˆí˜¸ê°€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ìš°
                                if '\n' in block[block.find('S/N:'):]:
                                    # S/N ì´í›„ ë‹¤ìŒ í•­ëª©ê¹Œì§€ ëª¨ë“  ì¤„ ìˆ˜ì§‘
                                    sn_lines = []
                                    in_sn = False
                                    for line in block.split('\n'):
                                        if 'S/N:' in line:
                                            in_sn = True
                                            sn_lines.append(line.split('S/N:')[1].strip())
                                        elif in_sn and 'êµ¬ì…ì •ë³´:' not in line and 'ìœ„ì¹˜ì •ë³´:' not in line and 'ê´€ë¦¬ì •ë³´:' not in line:
                                            # ë‹¤ìŒ ì„¹ì…˜ì´ ì‹œì‘ë˜ê¸° ì „ê¹Œì§€ ìˆ˜ì§‘
                                            if line.strip() and not line.strip().startswith('['):
                                                sn_lines.append(line.strip())
                                        elif 'êµ¬ì…ì •ë³´:' in line or 'ìœ„ì¹˜ì •ë³´:' in line or 'ê´€ë¦¬ì •ë³´:' in line:
                                            break
                                    item_dict['S/N'] = ', '.join(sn_lines) if sn_lines else sn_text
                                else:
                                    item_dict['S/N'] = sn_text
                            
                            # êµ¬ì…ì •ë³´ ì¶”ì¶œ
                            # êµ¬ì…ì¼ ì¶”ì¶œ
                            date_match = re.search(r'êµ¬ì…ì¼:\s*([^|]+)', block)
                            if date_match:
                                date_str = date_match.group(1).strip()
                                item_dict['êµ¬ì…ì¼'] = date_str[:10] if len(date_str) >= 10 else date_str
                            
                            # ë‹¨ê°€ ë° ê¸ˆì•¡ ì¶”ì¶œ
                            price_match = re.search(r'ë‹¨ê°€:\s*([^|\n]+)', block)
                            if price_match:
                                item_dict['ë‹¨ê°€'] = price_match.group(1).strip()
                            
                            amount_match = re.search(r'ê¸ˆì•¡:\s*([^|\n]+)', block)
                            if amount_match:
                                item_dict['ê¸ˆì•¡'] = amount_match.group(1).strip()
                            
                            # ìœ„ì¹˜ì •ë³´ ì¶”ì¶œ  
                            loc_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', block)
                            if loc_match:
                                item_dict['ìœ„ì¹˜'] = loc_match.group(1).strip()
                                
                            system_match = re.search(r'ì‹œìŠ¤í…œ:\s*([^|\n]+)', block)
                            if system_match:
                                item_dict['ì‹œìŠ¤í…œ'] = system_match.group(1).strip()
                            
                            # ê´€ë¦¬ì •ë³´ ì¶”ì¶œ
                            manager_match = re.search(r'ë‹´ë‹¹ì:\s*([^|\n]+)', block)
                            if manager_match:
                                item_dict['ë‹´ë‹¹ì'] = manager_match.group(1).strip()
                            
                            vendor_match = re.search(r'ë²¤ë”ì‚¬:\s*([^|\n]+)', block)
                            if vendor_match:
                                item_dict['ë²¤ë”ì‚¬'] = vendor_match.group(1).strip()
                            
                            # ì—°ë½ì²˜ ì¶”ì¶œ
                            contact_match = re.search(r'ì—°ë½ì²˜:\s*([^|\n]+)', block)
                            if contact_match:
                                item_dict['ì—°ë½ì²˜'] = contact_match.group(1).strip()
                            
                            # ê³„ì•½ì—…ì²´ ì¶”ì¶œ
                            contract_match = re.search(r'ê³„ì•½ì—…ì²´:\s*([^|\n]+)', block)
                            if contract_match:
                                item_dict['ê³„ì•½ì—…ì²´'] = contract_match.group(1).strip()
                            
                            # ê³µë°± ë¼ì¸ ì œì™¸
                            if item_dict.get('S/N'):
                                asset_data.append(item_dict)
                    
                    # ì§ì ‘ ìƒì„¸í•œ í¬ë§·ìœ¼ë¡œ ì¶œë ¥
                    return self._format_detailed_asset_response(asset_data, model, query)
                    
                    # ìœ„ì¹˜ë³„ ë¶„í¬ ë¶„ì„
                    locations = {}
                    for line in lines:
                        if model in line.upper():
                            loc_match = re.search(r'ìœ„ì¹˜:\s*([^|]+)', line)
                            if loc_match:
                                loc = loc_match.group(1).strip()
                                locations[loc] = locations.get(loc, 0) + 1
                    
                    if locations:
                        response += "### ğŸ“ ìœ„ì¹˜ë³„ ë¶„í¬\n"
                        for loc, cnt in sorted(locations.items(), key=lambda x: x[1], reverse=True)[:5]:
                            response += f"â€¢ **{loc}**: {cnt}ê°œ\n"
                        response += "\n"
                    
                    response += "### ğŸ“‹ ìƒì„¸ ëª©ë¡ (ì¼ë¶€)\n"
                    response += "```\n"
                    for i, result in enumerate(results, 1):
                        # ê° í•­ëª©ì„ ë” ì½ê¸° ì‰½ê²Œ í¬ë§·
                        serial_match = re.search(r'S/N:\s*([^|]+)', result)
                        loc_match = re.search(r'ìœ„ì¹˜:\s*([^|]+)', result)
                        
                        formatted_line = f"[{i:02d}]"
                        if serial_match:
                            formatted_line += f" S/N: {serial_match.group(1).strip()}"
                        if loc_match:
                            formatted_line += f" | ìœ„ì¹˜: {loc_match.group(1).strip()}"
                        
                        response += f"{formatted_line}\n"
                    response += "```\n"
                    
                    if count > 10:
                        response += f"\n... ì™¸ {count - 10}ê°œ ë” ìˆìŒ\n"
                    
                    response += f"\n---\nğŸ“ ì¶œì²˜: ì±„ë„A ë°©ì†¡ì¥ë¹„ ìì‚° ë°ì´í„°ë² ì´ìŠ¤"
                    return response
                else:
                    return f"âŒ {model} ëª¨ë¸ ì¥ë¹„ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            return "âŒ ëª¨ë¸ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _search_multiple_documents(self, query: str) -> str:
        """ì—¬ëŸ¬ ë¬¸ì„œ ê²€ìƒ‰ ë° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        try:
            # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            query_lower = query.lower()
            
            # ë§¤ì¹­ëœ ë¬¸ì„œë“¤ ìˆ˜ì§‘
            matched_docs = []
            
            for cache_key, metadata in self.metadata_cache.items():
                # TXT íŒŒì¼ì€ ì œì™¸ (PDFë§Œ ì²˜ë¦¬)
                if metadata.get('is_txt', False):
                    continue

                filename = metadata.get('filename', cache_key)
                score = 0
                filename_lower = filename.lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                keywords_in_query = []

                # ë™ì  í‚¤ì›Œë“œ ì¶”ì¶œ (í•˜ë“œì½”ë”© ì œê±°)
                query_words = re.findall(r'[ê°€-í£]+|[A-Za-z]+', query_lower)
                file_words = re.findall(r'[ê°€-í£]+|[A-Za-z]+', filename_lower)

                # ë¶ˆìš©ì–´ ì œì™¸
                stopwords = ['ì˜', 'ë°', 'ê±´', 'ê²€í† ì„œ', 'ê´€ë ¨', 'ë¬¸ì„œ', 'ì°¾ì•„', 'ì¤˜', 'ìˆì–´', 'ì–´ë–¤', 'ê¸°ì•ˆì„œ']

                # ì¥ë¹„ëª… íŠ¹ë³„ ê°€ì¤‘ì¹˜ (DVR, CCU ë“±)
                equipment_names = ['dvr', 'ccu', 'ì¹´ë©”ë¼', 'ë Œì¦ˆ', 'ëª¨ë‹ˆí„°', 'ìŠ¤ìœ„ì²˜', 'ë§ˆì´í¬', 'ë¯¹ì„œ', 'ì‚¼ê°ëŒ€', 'ì¤‘ê³„ì°¨']
                for equipment in equipment_names:
                    if equipment in query_lower:
                        # ì¿¼ë¦¬ì— ì¥ë¹„ëª…ì´ ìˆê³  íŒŒì¼ëª…ì—ë„ ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜
                        if equipment in filename_lower:
                            score += 15  # ì¥ë¹„ëª… ì™„ì „ ë§¤ì¹­ì‹œ ë†’ì€ ì ìˆ˜
                        # íŒŒì¼ëª…ì— ì—†ë”ë¼ë„ ë©”íƒ€ë°ì´í„° í‚¤ì›Œë“œì— ìˆìœ¼ë©´ ì ìˆ˜ ë¶€ì—¬
                        elif any(equipment in kw.lower() for kw in metadata.get('keywords', [])):
                            score += 8  # í‚¤ì›Œë“œì— ìˆìœ¼ë©´ ì¤‘ê°„ ì ìˆ˜
                        # í…ìŠ¤íŠ¸ ê²€ìƒ‰ì€ í•„ìš”ì‹œì—ë§Œ (ì„±ëŠ¥ ìµœì í™”)
                        elif score < 3 and metadata.get('text'):
                            # ì ìˆ˜ê°€ ë‚®ì„ ë•Œë§Œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
                            if equipment in metadata.get('text', '').lower()[:500]:
                                score += 3
                
                for word in query_words:
                    if len(word) >= 2 and word not in stopwords:
                        keywords_in_query.append(word)
                        # íŒŒì¼ëª…ì— í•´ë‹¹ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì ìˆ˜ ë¶€ì—¬
                        if word in file_words:
                            # ë‹¨ì–´ ê¸¸ì´ì— ë¹„ë¡€í•œ ì ìˆ˜
                            score += len(word) * 2
                        # ë¶€ë¶„ ë§¤ì¹­ (3ê¸€ì ì´ìƒ)
                        elif len(word) >= 3:
                            for f_word in file_words:
                                if word in f_word or f_word in word:
                                    score += len(word)
                
                # ë©”íƒ€ë°ì´í„° í‚¤ì›Œë“œ ë§¤ì¹­
                for keyword in metadata['keywords']:
                    if keyword.lower() in query_lower:
                        score += 3
                
                # ì—°ë„ì™€ ì›” ë§¤ì¹­
                year_match = re.search(r'(20\d{2})', query)
                month_match = re.search(r'(\d{1,2})\s*ì›”', query)
                
                if year_match:
                    query_year = year_match.group(1)
                    if metadata['year'] == query_year:
                        score += 5
                        
                        # ì›”ë„ ì§€ì •ëœ ê²½ìš°
                        if month_match:
                            query_month = int(month_match.group(1))
                            file_month_match = re.search(r'\d{4}-(\d{2})-\d{2}', filename)
                            if file_month_match:
                                file_month = int(file_month_match.group(1))
                                if file_month == query_month:
                                    score += 10  # ì›”ê¹Œì§€ ì¼ì¹˜í•˜ë©´ ë†’ì€ ì ìˆ˜
                                else:
                                    score = 0  # ì›”ì´ ë‹¤ë¥´ë©´ ì œì™¸
                                    continue
                    else:
                        # ì—°ë„ê°€ ë‹¤ë¥´ë©´ ì œì™¸
                        continue  # ì—°ë„ê°€ ë‹¤ë¥´ë©´ ë¬´ì¡°ê±´ ì œì™¸
                
                # ìµœì†Œ ì ìˆ˜ ê¸°ì¤€ ì„¤ì • (ë„ˆë¬´ ë§ì€ ë¬¸ì„œ ë°©ì§€)
                # ê¸°ì•ˆìë‚˜ íŠ¹ì • ë…„ë„ ê²€ìƒ‰ì´ ì•„ë‹ˆë©´ ê¸°ë³¸ ê¸°ì¤€
                if 'ê¸°ì•ˆì' in query_lower or re.search(r'20\d{2}ë…„', query):
                    MIN_SCORE = 2  # ê¸°ì•ˆì/ë…„ë„ ê²€ìƒ‰ì€ ë‚®ì€ ê¸°ì¤€
                else:
                    MIN_SCORE = 3  # ê¸°ë³¸ ìµœì†Œ 3ì  ì´ìƒë§Œ í¬í•¨

                # ì¥ë¹„ëª… ê²€ìƒ‰ì‹œ ì ì ˆí•œ ê¸°ì¤€ ì ìš©
                has_equipment = False
                for equipment in equipment_names:
                    if equipment in query_lower:
                        has_equipment = True
                        # ì¥ë¹„ëª…ì´ íŒŒì¼ëª…ì— ìˆìœ¼ë©´ ë¬´ì¡°ê±´ í¬í•¨
                        if equipment in filename_lower:
                            MIN_SCORE = 0  # íŒŒì¼ëª…ì— ìˆìœ¼ë©´ ë¬´ì¡°ê±´ í¬í•¨
                        else:
                            MIN_SCORE = max(3, MIN_SCORE)  # ì¥ë¹„ ê²€ìƒ‰ì‹œ ìµœì†Œ 3ì 
                        break
                
                if score >= MIN_SCORE:
                    # metadataì˜ path ì‚¬ìš©
                    pdf_path = metadata.get('path')
                    if isinstance(pdf_path, str):
                        pdf_path = Path(pdf_path)
                    if not pdf_path:
                        pdf_path = self.docs_dir / filename

                    info = self._extract_pdf_info(pdf_path)
                    matched_docs.append({
                        'filename': filename,
                        'score': score,
                        'info': info,
                        'year': metadata['year'],
                        'cache_key': cache_key  # íŒŒì¼ ê²½ë¡œ ì •ë³´ ì¶”ê°€
                    })
            
            # ì¤‘ë³µ ì œê±° ë° ìµœì í™”
            unique_docs = {}
            for doc in matched_docs:
                filename = doc['filename']
                if filename not in unique_docs:
                    unique_docs[filename] = doc
                else:
                    # ë” ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ë¬¸ì„œë¥¼ ìœ ì§€
                    if doc['score'] > unique_docs[filename]['score']:
                        unique_docs[filename] = doc
                    # ê°™ì€ ì ìˆ˜ë©´ year_ í´ë” ìš°ì„ 
                    elif doc['score'] == unique_docs[filename]['score'] and 'year_' in doc.get('cache_key', ''):
                        unique_docs[filename] = doc

            matched_docs = list(unique_docs.values())

            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            matched_docs.sort(key=lambda x: x['score'], reverse=True)

            # ê²°ê³¼ ì œí•œ (ì„±ëŠ¥ ìµœì í™”)
            # ì¥ë¹„ ê²€ìƒ‰ì€ 20ê°œê¹Œì§€, ì¼ë°˜ ê²€ìƒ‰ì€ 15ê°œê¹Œì§€
            max_results = 20 if has_equipment else 15
            if len(matched_docs) > max_results:
                matched_docs = matched_docs[:max_results]
            
            if not matched_docs:
                return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê²°ê³¼ í¬ë§·íŒ… (í†µí•©í˜• UI)
            report = []
            report.append(f"## ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼")
            report.append(f"**ì´ {len(matched_docs)}ê°œ ë¬¸ì„œ ë°œê²¬**\n")
            report.append("---\n")
            
            # ì—°ë„ë³„ë¡œ ê·¸ë£¹í™” (ì¤‘ë³µ ì œê±°ëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì™„ë£Œ)
            docs_by_year = {}

            for doc in matched_docs:
                year = doc['year']
                if year not in docs_by_year:
                    docs_by_year[year] = []
                docs_by_year[year].append(doc)
            
            # ì—°ë„ë³„ë¡œ í‘œì‹œ
            for year in sorted(docs_by_year.keys(), reverse=True):
                report.append(f"### ğŸ“… {year}ë…„ ({len(docs_by_year[year])}ê°œ)\n")
                
                for doc in docs_by_year[year]:
                    info = doc['info']
                    filename = doc['filename']
                    relative_path = doc.get('cache_key', filename)  # ìºì‹œ í‚¤ê°€ ìƒëŒ€ ê²½ë¡œ
                    
                    # ì¹´í…Œê³ ë¦¬ íŒë‹¨ ë° ì´ëª¨ì§€
                    if 'êµ¬ë§¤' in filename:
                        category = "êµ¬ë§¤ìš”ì²­"
                        emoji = "ğŸ›’"
                    elif 'ìˆ˜ë¦¬' in filename or 'ë³´ìˆ˜' in filename:
                        category = "ìˆ˜ë¦¬/ë³´ìˆ˜"
                        emoji = "ğŸ”§"
                    elif 'íê¸°' in filename:
                        category = "íê¸°ì²˜ë¦¬"
                        emoji = "ğŸ—‘ï¸"
                    elif 'ê²€í† ' in filename:
                        category = "ê²€í† ë³´ê³ ì„œ"
                        emoji = "ğŸ“‹"
                    else:
                        category = "ê¸°íƒ€"
                        emoji = "ğŸ“„"
                    
                    # ì œëª© ì¶”ì¶œ (ë‚ ì§œ ì œì™¸)
                    title_parts = filename.replace('.pdf', '').split('_', 1)
                    title = title_parts[1] if len(title_parts) > 1 else title_parts[0]
                    
                    # ê¸°ë³¸ ì •ë³´
                    drafter = info.get('ê¸°ì•ˆì', '')
                    date = info.get('ë‚ ì§œ', '')
                    amount = info.get('ê¸ˆì•¡', '')
                    
                    # ê°œìš” ìƒì„± - metadata_cacheì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ í™œìš©
                    summary = ""

                    # metadataì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                    cached_metadata = None
                    for ck, md in self.metadata_cache.items():
                        if md.get('filename') == filename:
                            cached_metadata = md
                            break

                    if cached_metadata and cached_metadata.get('text'):
                        # ìºì‹œëœ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
                        text = cached_metadata['text'][:500]  # ì²˜ìŒ 500ì

                        # ì£¼ìš” ì •ë³´ ì¶”ì¶œ
                        if 'ëª©ì ' in text:
                            purpose_match = re.search(r'ëª©ì [:\s]+([^\n]+)', text)
                            if purpose_match:
                                summary = purpose_match.group(1).strip()
                        elif 'ë‚´ìš©' in text:
                            content_match = re.search(r'ë‚´ìš©[:\s]+([^\n]+)', text)
                            if content_match:
                                summary = content_match.group(1).strip()
                        elif 'ì‚¬ìœ ' in text:
                            reason_match = re.search(r'ì‚¬ìœ [:\s]+([^\n]+)', text)
                            if reason_match:
                                summary = reason_match.group(1).strip()

                    # í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ì¶”ì¶œ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
                    if not summary:
                        if 'êµ¬ë§¤' in filename:
                            summary = "ì¥ë¹„ êµ¬ë§¤ ìš”ì²­"
                        elif 'ìˆ˜ë¦¬' in filename or 'ë³´ìˆ˜' in filename:
                            summary = "ì¥ë¹„ ìˆ˜ë¦¬/ë³´ìˆ˜ ê±´"
                        elif 'íê¸°' in filename:
                            summary = "ë…¸í›„ ì¥ë¹„ íê¸° ì²˜ë¦¬"
                        elif 'êµì²´' in filename:
                            summary = "ë…¸í›„ ì¥ë¹„ êµì²´ ê²€í† "
                        elif 'ê²€í† ' in filename:
                            summary = "ê¸°ìˆ  ê²€í†  ë³´ê³ ì„œ"
                        else:
                            summary = "ê¸°ìˆ ê´€ë¦¬íŒ€ ì—…ë¬´ ë¬¸ì„œ"
                    
                    # ì¹´ë“œ UI í˜•íƒœë¡œ ì¶œë ¥
                    report.append(f"#### {emoji} **{title}**")
                    report.append(f"**[{category}]** | {date if date else 'ë‚ ì§œ ë¯¸ìƒ'}")
                    
                    # ìƒì„¸ ì •ë³´
                    if drafter:
                        report.append(f"- **ê¸°ì•ˆì**: {drafter}")
                    if amount:
                        report.append(f"- **ê¸ˆì•¡**: {amount}")
                    if summary:
                        report.append(f"- **ê°œìš”**: {summary}")
                    
                    # íŒŒì¼ ê²½ë¡œ ì •ë³´ í¬í•¨ (web_interfaceì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡)
                    # íŠ¹ë³„í•œ ë§ˆì»¤ ì‚¬ìš©: @@PDF_PREVIEW@@
                    file_path_str = str(relative_path) if relative_path else filename
                    report.append(f"- **íŒŒì¼**: [{file_path_str}] @@PDF_PREVIEW@@{file_path_str}@@")
                    report.append("")  # ê°„ê²©
                
                report.append("---\n")
            
            return "\n".join(report)
            
        except Exception as e:
            return f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"

    def _search_and_analyze_by_content(self, query: str) -> str:
        """íŠ¹ì • ë‚´ìš©ì´ ì–¸ê¸‰ëœ ê²½ìš° ê´€ë ¨ ë¬¸ì„œë“¤ì„ ëª¨ë‘ ì°¾ì•„ì„œ ë¶„ì„

        ì˜ˆì‹œ:
        - "DVR êµì²´ ê²€í†  ë‚´ìš©" â†’ DVR ê´€ë ¨ ëª¨ë“  ë¬¸ì„œ ì°¾ê³  êµì²´ ê²€í†  ë‚´ìš© ì •ë¦¬
        - "ì‚¼ê°ëŒ€ êµ¬ë§¤ ê±´" â†’ ì‚¼ê°ëŒ€ ê´€ë ¨ ëª¨ë“  êµ¬ë§¤ ë¬¸ì„œ ì°¾ê¸°
        """
        try:
            # 1. í•µì‹¬ í‚¤ì›Œë“œì™€ ì‘ì—… íƒ€ì… ë¶„ë¦¬
            query_lower = query.lower()

            # ì¥ë¹„/ì‹œìŠ¤í…œ í‚¤ì›Œë“œ
            equipment_keywords = []
            equipment_terms = ['DVR', 'ì¤‘ê³„ì°¨', 'ì¹´ë©”ë¼', 'ì‚¼ê°ëŒ€', 'ëª¨ë‹ˆí„°', 'CCU', 'ì˜¤ë””ì˜¤', 'ì„œë²„', 'ë§ˆì´í¬', 'ìŠ¤ìœ„ì¹˜']
            for term in equipment_terms:
                if term.lower() in query_lower:
                    equipment_keywords.append(term)

            # ì‘ì—… íƒ€ì… í‚¤ì›Œë“œ
            action_keywords = []
            action_terms = ['êµì²´', 'ê²€í† ', 'êµ¬ë§¤', 'ìˆ˜ë¦¬', 'ë³´ìˆ˜', 'íê¸°', 'ë„ì…', 'ì—…ê·¸ë ˆì´ë“œ', 'ì„¤ì¹˜']
            for term in action_terms:
                if term in query_lower:
                    action_keywords.append(term)

            if not equipment_keywords:
                # ë¬¸ì¥ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ
                nouns = re.findall(r'[\uac00-\ud7a3]{2,}', query)
                equipment_keywords = [n for n in nouns if n not in ['ê´€ë ¨', 'ë¬¸ì„œ', 'ë‚´ìš©', 'ì •ë¦¬', 'ë¶„ì„']]

            print(f"ğŸ” ì¥ë¹„ í‚¤ì›Œë“œ: {equipment_keywords}, ì‘ì—… í‚¤ì›Œë“œ: {action_keywords}")

            # 2. ë‹¨ê³„ë³„ ë¬¸ì„œ ê²€ìƒ‰
            # ë‹¨ê³„ 1: íŒŒì¼ëª…ì— í‚¤ì›Œë“œê°€ ìˆëŠ” ë¬¸ì„œ
            primary_files = []
            # ë‹¨ê³„ 2: ì‘ì—… íƒ€ì…ë§Œ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ
            secondary_files = []
            # ë‹¨ê³„ 3: ë‚´ìš©ì— í‚¤ì›Œë“œê°€ ìˆëŠ” ë¬¸ì„œ (ëŠë¦¼, í•„ìš”ì‹œë§Œ)
            content_match_files = []

            for cache_key, metadata in self.metadata_cache.items():
                if metadata.get('is_pdf'):
                    # ì‹¤ì œ íŒŒì¼ëª… ì‚¬ìš© (cache_keyê°€ ì•„ë‹Œ metadata['filename'])
                    filename_lower = metadata.get('filename', cache_key).lower()
                    path = metadata['path']

                    # íŒŒì¼ëª…ì— ì¥ë¹„ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                    has_equipment_keyword = any(kw.lower() in filename_lower for kw in equipment_keywords)
                    has_action_keyword = any(kw.lower() in filename_lower for kw in action_keywords)

                    if has_equipment_keyword:
                        primary_files.append(path)
                    elif has_action_keyword:
                        secondary_files.append(path)

            # 3. ê²°ê³¼ ë³‘í•© (ìµœëŒ€ 15ê°œ)
            relevant_files = primary_files[:10] + secondary_files[:5]

            if not relevant_files:
                # í‚¤ì›Œë“œê°€ ë„ˆë¬´ ì—†ìœ¼ë©´ ë‚´ìš© ê²€ìƒ‰ ì‹œë„ (ì‹œê°„ ì†Œìš”)
                if len(equipment_keywords) > 0:
                    print("ğŸ” íŒŒì¼ëª…ì—ì„œ ì°¾ì§€ ëª»í•¨, ë‚´ìš© ê²€ìƒ‰ ì‹œì‘...")
                    for file_path, metadata in list(self.metadata_cache.items())[:30]:  # ìµœëŒ€ 30ê°œë§Œ
                        if metadata.get('is_pdf'):
                            try:
                                info = self._extract_pdf_info(metadata['path'])
                                if info and 'text' in info:
                                    content = info['text'][:2000]  # ì²˜ìŒ 2000ìë§Œ
                                    if any(kw.lower() in content.lower() for kw in equipment_keywords):
                                        content_match_files.append(metadata['path'])
                                        if len(content_match_files) >= 5:  # ìµœëŒ€ 5ê°œ
                                            break
                            except:
                                continue
                    relevant_files.extend(content_match_files)

            if not relevant_files:
                return f"âŒ '{', '.join(equipment_keywords + action_keywords)}' ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            print(f"ğŸ“„ {len(relevant_files)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")

            # ì„±ëŠ¥ ìµœì í™”: ìƒìœ„ 5ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬
            max_docs_to_process = 5
            files_to_process = relevant_files[:max_docs_to_process]
            if len(relevant_files) > max_docs_to_process:
                print(f"âš¡ ì„±ëŠ¥ ìµœì í™”: ìƒìœ„ {max_docs_to_process}ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬ (ì „ì²´ {len(relevant_files)}ê°œ ì¤‘)")

            # 4. ê° ë¬¸ì„œì˜ ë‚´ìš© ì¶”ì¶œ ë° ë¶„ì„
            document_analyses = []
            all_contents = []

            for file_path in files_to_process:
                try:
                    info = self._extract_pdf_info(file_path)
                    if info:
                        # ì‘ì—… í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë¶€ë¶„ ì¶”ì¶œ
                        relevant_content = []
                        if 'text' in info:
                            lines = info['text'].split('\n')
                            for i, line in enumerate(lines):
                                line_lower = line.lower()
                                # ì‘ì—… í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ê³¼ ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ
                                if any(kw in line_lower for kw in action_keywords + equipment_keywords):
                                    # ì „í›„ 2ì¤„ì”© í¬í•¨
                                    start = max(0, i-2)
                                    end = min(len(lines), i+3)
                                    context = ' '.join(lines[start:end])
                                    relevant_content.append(context)

                        doc_analysis = {
                            'filename': file_path.name,
                            'title': info.get('ì œëª©', file_path.stem),
                            'date': info.get('ë‚ ì§œ', ''),
                            'drafter': info.get('ê¸°ì•ˆì', ''),
                            'amount': info.get('ê¸ˆì•¡', ''),
                            'relevant_content': relevant_content[:3],  # ìµœëŒ€ 3ê°œ ê´€ë ¨ ë¶€ë¶„
                            'full_text': info.get('text', '')[:2000]  # ì „ì²´ í…ìŠ¤íŠ¸ ì¼ë¶€
                        }
                        document_analyses.append(doc_analysis)
                        all_contents.append(f"[{file_path.name}]\n" + '\n'.join(relevant_content[:3]))
                except Exception as e:
                    print(f"âš ï¸ {file_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue

            if not document_analyses:
                return "âŒ ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # 5. LLMì„ ì‚¬ìš©í•˜ì—¬ ì¢…í•© ë¶„ì„
            if self.llm is None:
                if not LLMSingleton.is_loaded():
                    print("ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.llm = LLMSingleton.get_instance(model_path=self.model_path)

            combined_text = '\n\n'.join(all_contents)

            prompt = f"""ë‹¤ìŒì€ '{', '.join(equipment_keywords)}' ê´€ë ¨ '{', '.join(action_keywords)}' ë¬¸ì„œë“¤ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë¬¸ì„œ ë‚´ìš©:
{combined_text[:6000]}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.
í¬í•¨í•´ì•¼ í•  ë‚´ìš©:
1. ê° ë¬¸ì„œì—ì„œ ì°¾ì€ í•µì‹¬ ì •ë³´
2. ì—°ë„ë³„/ì‹œê¸°ë³„ ë³€í™” (ìˆë‹¤ë©´)
3. ê¸°ìˆ ì  ì‚¬ì–‘ì´ë‚˜ ëª¨ë¸ ì •ë³´
4. ë¹„ìš© ì •ë³´
5. ê²°ë¡  ë° ì¶”ì²œì‚¬í•­

ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""

            context_chunks = [{
                'content': combined_text[:6000],
                'metadata': {'source': 'multiple_documents'},
                'score': 1.0
            }]

            response_obj = self.llm.generate_response(prompt, context_chunks)
            llm_response = response_obj.answer if hasattr(response_obj, 'answer') else str(response_obj)

            # 6. ê²°ê³¼ êµ¬ì„±
            result = []
            result.append(f"ğŸ“Š **'{', '.join(equipment_keywords)}' ê´€ë ¨ {len(document_analyses)}ê°œ ë¬¸ì„œ ë¶„ì„**\n")
            result.append("="*50 + "\n\n")

            # LLM ë¶„ì„ ê²°ê³¼
            result.append(llm_response)
            result.append("\n" + "="*50 + "\n")

            # ë¶„ì„ëœ ë¬¸ì„œ ëª©ë¡
            result.append("\nğŸ“„ **ë¶„ì„ëœ ë¬¸ì„œ:**\n")
            for doc in document_analyses:
                result.append(f"\nâ€¢ **{doc['title']}**")
                if doc['date']:
                    result.append(f"  - ë‚ ì§œ: {doc['date']}")
                if doc['drafter']:
                    result.append(f"  - ê¸°ì•ˆì: {doc['drafter']}")
                if doc['amount']:
                    result.append(f"  - ê¸ˆì•¡: {doc['amount']}")
                if doc['relevant_content']:
                    result.append(f"  - í•µì‹¬ ë‚´ìš©: {len(doc['relevant_content'])}ê°œ ë¶€ë¶„ ë°œê²¬")

            return '\n'.join(result)

        except Exception as e:
            return f"âŒ ë‚´ìš© ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨: {e}"

    def _read_and_summarize_documents(self, query: str) -> str:
        """ê´€ë ¨ ë¬¸ì„œë“¤ì„ ì‹¤ì œë¡œ ì½ê³  ì¢…í•© ì •ë¦¬í•˜ëŠ” ë©”ì„œë“œ

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸ (ì˜ˆ: "DVRê´€ë ¨ ë¬¸ì„œ ë‹¤ ì½ê³  ì •ë¦¬í•´ì¤˜")

        Returns:
            ì¢…í•© ì •ë¦¬ëœ ë‚´ìš©
        """
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            query_lower = query.lower()
            keywords = []

            # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            important_keywords = ['DVR', 'ì¤‘ê³„ì°¨', 'ì¹´ë©”ë¼', 'ì‚¼ê°ëŒ€', 'ëª¨ë‹ˆí„°', 'ì˜¤ë””ì˜¤', 'ì„œë²„', 'ìŠ¤ìœ„ì¹˜']
            for kw in important_keywords:
                if kw.lower() in query_lower:
                    keywords.append(kw)

            # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê¸°ì¤€ ì‚¬ìš©
            if not keywords:
                # ë¬¸ì¥ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ
                nouns = re.findall(r'[\uac00-\ud7a3]{2,}', query)
                keywords = [n for n in nouns if n not in ['ê´€ë ¨', 'ë¬¸ì„œ', 'ì½ê³ ', 'ì •ë¦¬', 'ë‚´ìš©', 'ëª¨ë‘', 'ì „ë¶€']]

            if not keywords:
                return "âŒ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”."

            print(f"ğŸ” í‚¤ì›Œë“œë¡œ ë¬¸ì„œ ê²€ìƒ‰: {keywords}")

            # ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
            relevant_files = []
            for file_path, metadata in self.metadata_cache.items():
                if metadata.get('is_pdf'):
                    # íŒŒì¼ëª…ì´ë‚˜ ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    filename_lower = file_path.lower()
                    for kw in keywords:
                        if kw.lower() in filename_lower:
                            relevant_files.append(metadata['path'])
                            break

            if not relevant_files:
                return f"âŒ '{', '.join(keywords)}' ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            print(f"ğŸ“„ {len(relevant_files)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")

            # ê° ë¬¸ì„œì˜ ë‚´ìš© ì¶”ì¶œ
            all_contents = []
            document_summaries = []

            for file_path in relevant_files[:10]:  # ìµœëŒ€ 10ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬
                try:
                    # PDF ë‚´ìš© ì¶”ì¶œ
                    info = self._extract_pdf_info(file_path)
                    if info:
                        doc_summary = {
                            'filename': file_path.name,
                            'title': info.get('ì œëª©', file_path.stem),
                            'date': info.get('ë‚ ì§œ', 'ë‚ ì§œ ë¯¸ìƒ'),
                            'drafter': info.get('ê¸°ì•ˆì', 'ë¯¸ìƒ'),
                            'amount': info.get('ê¸ˆì•¡', ''),
                            'content': info.get('text', '')[:3000]  # ì²˜ìŒ 3000ì
                        }

                        # ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
                        if 'text' in info:
                            # ì¤‘ìš”í•œ ë¬¸ì¥ ì¶”ì¶œ
                            important_sentences = []
                            lines = info['text'].split('\n')
                            for line in lines:
                                line = line.strip()
                                if line and len(line) > 20:
                                    # ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥
                                    if any(kw in line for kw in ['ê°œìš”', 'ëª©ì ', 'ë‚´ìš©', 'ê²°ê³¼', 'ê²°ë¡ ', 'ì¶”ì²œ', 'í•„ìš”', 'ì˜ˆì‚°', 'ê¸ˆì•¡']):
                                        important_sentences.append(line[:200])

                            doc_summary['key_points'] = important_sentences[:5]

                        document_summaries.append(doc_summary)
                        all_contents.append(f"\n[{file_path.name}]\n{info.get('text', '')[:3000]}")

                except Exception as e:
                    print(f"âš ï¸ {file_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue

            if not document_summaries:
                return "âŒ ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # LLMì„ ì‚¬ìš©í•˜ì—¬ ì¢…í•© ì •ë¦¬
            if self.llm is None:
                if not LLMSingleton.is_loaded():
                    print("ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.llm = LLMSingleton.get_instance(model_path=self.model_path)

            # ì¢…í•© ì •ë¦¬ í”„ë¡¬í”„íŠ¸
            combined_text = '\n\n'.join(all_contents)

            prompt = f"""ë‹¤ìŒì€ '{', '.join(keywords)}' ê´€ë ¨ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.

ì´ ë¬¸ì„œë“¤ì„ ì½ê³  ì¢…í•©ì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

í¬í•¨í•´ì•¼ í•  ë‚´ìš©:
1. ì£¼ìš” ë‚´ìš© ìš”ì•½
2. ì—°ë„ë³„/ì‹œê¸°ë³„ ì£¼ìš” ì‚¬í•­
3. ê¸°ìˆ ì  ì‚¬ì–‘ì´ë‚˜ ëª¨ë¸ ì •ë³´ (ìˆë‹¤ë©´)
4. ë¹„ìš© ì •ë³´ (ìˆë‹¤ë©´)
5. ê²€í†  ê²°ê³¼ë‚˜ ì¶”ì²œì‚¬í•­
6. ê³µí†µì ê³¼ ì°¨ì´ì 

ë¬¸ì„œ ë‚´ìš©:
{combined_text[:8000]}

ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”. í…Œí”Œë¦¿ í˜•ì‹ì´ ì•„ë‹Œ ëŒ€í™”í˜• í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

            # LLM í˜¸ì¶œ
            context_chunks = [{
                'content': combined_text[:8000],
                'metadata': {'source': 'multiple_documents'},
                'score': 1.0
            }]

            response_obj = self.llm.generate_response(prompt, context_chunks)
            llm_response = response_obj.answer if hasattr(response_obj, 'answer') else str(response_obj)

            # ê²°ê³¼ êµ¬ì„±
            result = []
            result.append(f"ğŸ“„ **{len(document_summaries)}ê°œ {', '.join(keywords)} ê´€ë ¨ ë¬¸ì„œ ì¢…í•© ë¶„ì„**\n")
            result.append("="*50 + "\n")

            # LLM ì‘ë‹µ ì¶”ê°€
            result.append(llm_response)
            result.append("\n" + "="*50 + "\n")

            # ê° ë¬¸ì„œ ê°„ë‹¨ ì •ë³´
            result.append("\nğŸ“Š **ë¶„ì„ëœ ë¬¸ì„œ ëª©ë¡:**\n")
            for doc in document_summaries:
                result.append(f"\nâ€¢ **{doc['title']}**")
                result.append(f"  - ë‚ ì§œ: {doc['date']}")
                result.append(f"  - ê¸°ì•ˆì: {doc['drafter']}")
                if doc['amount']:
                    result.append(f"  - ê¸ˆì•¡: {doc['amount']}")

            return '\n'.join(result)

        except Exception as e:
            return f"âŒ ë¬¸ì„œ ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {e}"

    def _generate_detailed_location_list(self, content: str, location: str, result: dict, txt_path: Path) -> str:
        """íŠ¹ì • ìœ„ì¹˜ì˜ ì¥ë¹„ ëª©ë¡ì„ LLMìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ í‘œì‹œ"""
        try:
            # LLM ë¡œë“œ (í•„ìš”ì‹œ)
            if self.llm is None:
                if not LLMSingleton.is_loaded():
                    print("ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.llm = LLMSingleton.get_instance(model_path=self.model_path)
            
            # í•´ë‹¹ ìœ„ì¹˜ì˜ ì¥ë¹„ë“¤ ìˆ˜ì§‘
            lines = content.split('\n')
            location_items = []
            current_item = []
            
            for line in lines:
                if line.strip().startswith('[') and line.strip().endswith(']'):
                    # ìƒˆë¡œìš´ ì¥ë¹„ í•­ëª© ì‹œì‘
                    if current_item and self._is_location_match(current_item, location):
                        location_items.append('\n'.join(current_item))
                    current_item = [line]
                elif current_item:
                    current_item.append(line)
            
            # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
            if current_item and self._is_location_match(current_item, location):
                location_items.append('\n'.join(current_item))
            
            if not location_items:
                return f"âŒ {location}ì—ì„œ ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # LLMìœ¼ë¡œ ì •ë¦¬í•  ë°ì´í„° ì¤€ë¹„ (ìµœëŒ€ 50ê°œ)
            sample_items = location_items[:50]
            sample_text = '\n\n'.join(sample_items)
            
            # LLM í”„ë¡¬í”„íŠ¸ - ê¸°ìˆ ê´€ë¦¬íŒ€ í•„ìˆ˜ ì •ë³´ í¬í•¨
            prompt = f"""
ë‹¤ìŒì€ "{location}"ì— ì„¤ì¹˜ëœ ë°©ì†¡ì¥ë¹„ ëª©ë¡ì…ë‹ˆë‹¤. 
ê¸°ìˆ ê´€ë¦¬íŒ€ì´ ìì£¼ í™•ì¸í•˜ëŠ” ì¤‘ìš” ì •ë³´ë“¤ì„ í¬í•¨í•˜ì—¬ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ì¥ë¹„ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜ (ì¹´ë©”ë¼, ì˜¤ë””ì˜¤, ëª¨ë‹ˆí„°, ì„œë²„, ë„¤íŠ¸ì›Œí¬ ë“±)
2. ê° ì¥ë¹„ë§ˆë‹¤ ë‹¤ìŒ ì •ë³´ í¬í•¨: êµ¬ë¶„3/êµ¬ë¶„4/êµ¬ë¶„5, ì œì¡°ì‚¬, ëª¨ë¸ëª…, S/N, ë‹¨ê°€, ê¸ˆì•¡, êµ¬ì…ì¼, ë‹´ë‹¹ì, ë²¤ë”ì‚¬
3. ì´ ì¥ë¹„ ìˆ˜: {len(location_items)}ê°œ

ì¥ë¹„ ë°ì´í„°:
{sample_text[:8000]}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

## ğŸ“Š {location} ì‹œìŠ¤í…œ ì¥ë¹„ í˜„í™©

### ğŸ“ˆ ì´ ì¥ë¹„ ìˆ˜: {len(location_items)}ê°œ

### ğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ì¥ë¹„ ë¶„ë¥˜

**ğŸ¥ ì¹´ë©”ë¼ ê´€ë ¨**
- ì¥ë¹„ëª… (ì œì¡°ì‚¬/ëª¨ë¸/S/N) | êµ¬ë¶„: êµ¬ë¶„3>êµ¬ë¶„4>êµ¬ë¶„5 | ğŸ’° ë‹¨ê°€/ê¸ˆì•¡ | ğŸ“… êµ¬ì…ì¼ | ğŸ‘¤ ë‹´ë‹¹ì | ğŸ¢ ë²¤ë”ì‚¬

**ğŸ”Š ì˜¤ë””ì˜¤ ê´€ë ¨** 
- ì¥ë¹„ëª… (ì œì¡°ì‚¬/ëª¨ë¸/S/N) | êµ¬ë¶„: êµ¬ë¶„3>êµ¬ë¶„4>êµ¬ë¶„5 | ğŸ’° ë‹¨ê°€/ê¸ˆì•¡ | ğŸ“… êµ¬ì…ì¼ | ğŸ‘¤ ë‹´ë‹¹ì | ğŸ¢ ë²¤ë”ì‚¬

**ğŸ’» ì„œë²„/ì»´í“¨í„°**
- ì¥ë¹„ëª… (ì œì¡°ì‚¬/ëª¨ë¸/S/N) | êµ¬ë¶„: êµ¬ë¶„3>êµ¬ë¶„4>êµ¬ë¶„5 | ğŸ’° ë‹¨ê°€/ê¸ˆì•¡ | ğŸ“… êµ¬ì…ì¼ | ğŸ‘¤ ë‹´ë‹¹ì | ğŸ¢ ë²¤ë”ì‚¬

**ğŸ“º ëª¨ë‹ˆí„°/ë””ìŠ¤í”Œë ˆì´**
- ì¥ë¹„ëª… (ì œì¡°ì‚¬/ëª¨ë¸/S/N) | êµ¬ë¶„: êµ¬ë¶„3>êµ¬ë¶„4>êµ¬ë¶„5 | ğŸ’° ë‹¨ê°€/ê¸ˆì•¡ | ğŸ“… êµ¬ì…ì¼ | ğŸ‘¤ ë‹´ë‹¹ì | ğŸ¢ ë²¤ë”ì‚¬

**ğŸŒ ë„¤íŠ¸ì›Œí¬/í†µì‹ **
- ì¥ë¹„ëª… (ì œì¡°ì‚¬/ëª¨ë¸/S/N) | êµ¬ë¶„: êµ¬ë¶„3>êµ¬ë¶„4>êµ¬ë¶„5 | ğŸ’° ë‹¨ê°€/ê¸ˆì•¡ | ğŸ“… êµ¬ì…ì¼ | ğŸ‘¤ ë‹´ë‹¹ì | ğŸ¢ ë²¤ë”ì‚¬

**ğŸ”§ ê¸°íƒ€**
- ì¥ë¹„ëª… (ì œì¡°ì‚¬/ëª¨ë¸/S/N) | êµ¬ë¶„: êµ¬ë¶„3>êµ¬ë¶„4>êµ¬ë¶„5 | ğŸ’° ë‹¨ê°€/ê¸ˆì•¡ | ğŸ“… êµ¬ì…ì¼ | ğŸ‘¤ ë‹´ë‹¹ì | ğŸ¢ ë²¤ë”ì‚¬

ì¤‘ìš”: êµ¬ë¶„ì •ë³´, êµ¬ì…ì •ë³´(ë‹¨ê°€/ê¸ˆì•¡/êµ¬ì…ì¼), ê´€ë¦¬ì •ë³´(ë‹´ë‹¹ì/ë²¤ë”ì‚¬) ëª¨ë‘ í¬í•¨í•´ì£¼ì„¸ìš”.
"""

            # LLM í˜¸ì¶œ
            context = [{'content': sample_text[:8000], 'metadata': {'source': txt_path.name}, 'score': 1.0}]
            response = self.llm.generate_response(prompt, context)
            
            answer = response.answer if hasattr(response, 'answer') else str(response)
            
            if len(location_items) > 50:
                answer += f"\n\nâš ï¸ ì´ {len(location_items)}ê°œ ì¤‘ ìƒìœ„ 50ê°œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤."
            
            answer += f"\n\nğŸ“„ ì¶œì²˜: {txt_path.name}"
            return answer
            
        except Exception as e:
            # LLM ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í¬ë§·ìœ¼ë¡œ í‘œì‹œ
            response = f"ğŸ“Š {location} ì¥ë¹„ ëª©ë¡\n"
            response += "=" * 50 + "\n"
            response += f"âœ… ì´ ì¥ë¹„ ìˆ˜: {result['count']}ê°œ\n\n"
            
            if result.get('sample_items'):
                response += "ğŸ“‹ ì£¼ìš” ì¥ë¹„ ëª©ë¡:\n"
                for i, item in enumerate(result['sample_items'][:10], 1):
                    lines = item.split('\n')
                    for line in lines:
                        if '[' in line and ']' in line:
                            response += f"{i}. {line.strip()}\n"
                        elif 'ëª¨ë¸:' in line or 'ì œì¡°ì‚¬:' in line:
                            response += f"   {line.strip()}\n"
                    response += "\n"
            
            response += f"ğŸ“„ ì¶œì²˜: {txt_path.name}"
            return response

    def _is_location_match(self, item_lines: list, location: str) -> bool:
        """ìœ„ì¹˜ ë§¤ì¹­ ë¡œì§ ê°œì„  - ì •í™•í•œ ìœ„ì¹˜ ë§¤ì¹­"""
        item_text = '\n'.join(item_lines)
        
        # ìœ„ì¹˜ ì •ë³´ê°€ ìˆëŠ” ë¼ì¸ ì°¾ê¸°
        for line in item_lines:
            if 'ìœ„ì¹˜:' in line or 'ìœ„ì¹˜ì •ë³´:' in line:
                # ì‹¤ì œ ìœ„ì¹˜ ì¶”ì¶œ
                location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', line)
                if location_match:
                    actual_location = location_match.group(1).strip()
                    
                    # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™
                    if location == actual_location:
                        # ì™„ì „ ì¼ì¹˜ (ê°€ì¥ ìš°ì„ )
                        return True
                    elif location == 'ë¶€ì¡°ì •ì‹¤':
                        # 'ë¶€ì¡°ì •ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*ë¶€ì¡°ì •ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                        return actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
                    elif location == 'ìŠ¤íŠœë””ì˜¤':
                        # 'ìŠ¤íŠœë””ì˜¤'ë¡œ ê²€ìƒ‰ì‹œ '*ìŠ¤íŠœë””ì˜¤' íŒ¨í„´ë§Œ ë§¤ì¹­ 
                        return actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
                    elif location == 'í¸ì§‘ì‹¤':
                        # 'í¸ì§‘ì‹¤'ë¡œ ê²€ìƒ‰ì‹œ '*í¸ì§‘ì‹¤' íŒ¨í„´ë§Œ ë§¤ì¹­
                        return actual_location.endswith('í¸ì§‘ì‹¤')
                    elif location in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
                        # ì¤‘ê³„ì°¨ ê²€ìƒ‰ì‹œ Van ê´€ë ¨ ìœ„ì¹˜ ëª¨ë‘ ë§¤ì¹­
                        return 'Van' in actual_location or 'VAN' in actual_location
                    elif location == "ê´‘í™”ë¬¸ë¶€ì¡°ì •ì‹¤":
                        # "ê´‘í™”ë¬¸ ë¶€ì¡°ì •ì‹¤" ê°™ì€ ë³µí•© ìœ„ì¹˜ëª… ì²˜ë¦¬
                        return "ê´‘í™”ë¬¸" in actual_location and "ë¶€ì¡°ì •ì‹¤" in actual_location
                    elif len(location) > 3:
                        # 3ê¸€ì ì´ìƒì˜ êµ¬ì²´ì ì¸ ìœ„ì¹˜ëª…ì€ ë¶€ë¶„ ë§¤ì¹­ í—ˆìš©
                        return location in actual_location
        
        return False

    def _search_location_equipment_combo(self, txt_path: Path, query: str) -> str:
        """ìœ„ì¹˜ + ì¥ë¹„ëª… ë³µí•© ê²€ìƒ‰ (ì˜ˆ: ì¤‘ê³„ì°¨ CCUí˜„í™©)"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì¥ë¹„ í‚¤ì›Œë“œ ì¶”ì¶œ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
            equipment_keywords = ["CCU", "ì¹´ë©”ë¼", "ëª¨ë‹ˆí„°", "ì˜¤ë””ì˜¤", "ë¹„ë””ì˜¤", "ì„œë²„", "ìŠ¤ìœ„ì¹˜", "ë¼ìš°í„°"]
            found_equipment = None
            query_upper = query.upper()
            for eq in equipment_keywords:
                if eq.upper() in query_upper:
                    found_equipment = eq
                    break
            
            if not found_equipment:
                # ì¥ë¹„ëª…ì´ ì—†ì–´ë„ ì¿¼ë¦¬ ì „ì²´ì—ì„œ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²€ìƒ‰
                found_equipment = query  # ì „ì²´ ì¿¼ë¦¬ë¥¼ ì¥ë¹„ëª…ìœ¼ë¡œ ì‚¬ìš©
            
            # "ìœ„ì¹˜ë³„ë¡œ" ê°™ì€ ì „ì²´ ìœ„ì¹˜ ìš”ì²­ì¸ì§€ í™•ì¸
            is_all_locations = "ìœ„ì¹˜ë³„" in query or "í˜„í™©" in query
            
            # ìœ„ì¹˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            location_pattern = r'[ê°€-í£]{2,}(?:ìŠ¤íŠœë””ì˜¤|ë¶€ì¡°ì •ì‹¤|í¸ì§‘ì‹¤|ë”ë¹™ì‹¤|ì‚¬ì˜¥|ì„¼í„°|ì‹¤|ì¸µ|ê´€|ë™|í˜¸)|ì¤‘ê³„ì°¨|van|Van|VAN'
            locations = re.findall(location_pattern, query, re.IGNORECASE)
            
            # ëª¨ë“  ìœ„ì¹˜ë³„ë¡œ ê²€ìƒ‰í•˜ëŠ” ê²½ìš°
            if is_all_locations and not locations:
                # ëª¨ë“  ìœ„ì¹˜ë³„ CCU í˜„í™© ì •ë¦¬
                return self._search_equipment_all_locations(txt_path, found_equipment)
            
            if not locations and not is_all_locations:
                return "âŒ ìœ„ì¹˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            found_location = locations[0] if locations else None
            
            # í•­ëª©ë³„ë¡œ ê²€ìƒ‰ - ì˜¬ë°”ë¥¸ [NNNN] í˜•ì‹ ì‚¬ìš©
            lines = content.split('\n')
            matching_items = []
            current_item = []
            
            for line in lines:
                # [NNNN] í˜•ì‹ì˜ ì‹œì‘ ë¼ì¸ì„ ì°¾ê¸°
                if re.match(r'^\[\d{4}\]', line.strip()):
                    # ì´ì „ í•­ëª© ê²€ì‚¬
                    if current_item:
                        item_text = '\n'.join(current_item)
                        # ìœ„ì¹˜ ì¡°ê±´ í™•ì¸
                        location_match = self._check_location_in_item(item_text, found_location)
                        # ì¥ë¹„ëª… ì¡°ê±´ í™•ì¸ - CCUì˜ ê²½ìš° "Camera Control Unit"ë„ í¬í•¨
                        if found_equipment.upper() == "CCU":
                            equipment_match = "CCU" in item_text.upper() or "Camera Control Unit" in item_text
                        else:
                            equipment_match = found_equipment.upper() in item_text.upper()
                        
                        if location_match and equipment_match:
                            matching_items.append(item_text)
                    
                    current_item = [line]
                else:
                    if current_item:  # í˜„ì¬ í•­ëª©ì´ ì‹œì‘ëœ í›„ì—ë§Œ ì¶”ê°€
                        current_item.append(line)
            
            # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
            if current_item:
                item_text = '\n'.join(current_item)
                location_match = self._check_location_in_item(item_text, found_location)
                # ì¥ë¹„ëª… ì¡°ê±´ í™•ì¸ - CCUì˜ ê²½ìš° "Camera Control Unit"ë„ í¬í•¨
                if found_equipment == "CCU":
                    equipment_match = "CCU" in item_text or "Camera Control Unit" in item_text
                else:
                    equipment_match = found_equipment in item_text
                
                if location_match and equipment_match:
                    matching_items.append(item_text)
            
            if not matching_items:
                return f"âŒ {found_location}ì—ì„œ {found_equipment} ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê²°ê³¼ í¬ë§·íŒ…
            response = f"ğŸ“Š {found_location} {found_equipment} í˜„í™©\n"
            response += "=" * 60 + "\n"
            response += f"âœ… ì´ {len(matching_items)}ê°œ\n\n"
            
            # ìƒì„¸ ëª©ë¡
            for i, item in enumerate(matching_items, 1):
                response += f"[{i}] "
                lines = item.split('\n')
                
                # ì œëª© ì¶”ì¶œ
                title_line = lines[0] if lines else ""
                response += title_line.replace('[', '').replace(']', '').strip() + "\n"
                
                # ì£¼ìš” ì •ë³´ ì¶”ì¶œ
                for line in lines[1:]:
                    if 'ê¸°ë³¸ì •ë³´:' in line or 'ìœ„ì¹˜ì •ë³´:' in line or 'ê´€ë¦¬ì •ë³´:' in line:
                        response += f"  {line.strip()}\n"
                
                response += "\n"
            
            response += f"ğŸ“„ ì¶œì²˜: {txt_path.name}"
            return response
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _search_location_all_equipment(self, txt_path: Path, query: str) -> str:
        """íŠ¹ì • ìœ„ì¹˜ì˜ ëª¨ë“  ì¥ë¹„ë¥¼ ìƒì„¸í•˜ê²Œ í‘œì‹œ"""
        try:
            # ì™„ì „íŒ íŒŒì¼ ìš°ì„  ì‚¬ìš©
            complete_path = self.docs_dir / "assets" / "ì±„ë„A_ë°©ì†¡ì¥ë¹„_ìì‚°_ì „ì²´_7904ê°œ_ì™„ì „íŒ.txt"
            if complete_path.exists():
                txt_path = complete_path
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ìœ„ì¹˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            location_keyword = None
            if 'ì¤‘ê³„ì°¨' in query:
                location_keyword = 'ì¤‘ê³„ì°¨'
            else:
                # ë‹¤ë¥¸ ìœ„ì¹˜ íŒ¨í„´ ì°¾ê¸°
                location_pattern = r'[ê°€-í£]{2,}(?:ìŠ¤íŠœë””ì˜¤|ë¶€ì¡°ì •ì‹¤|í¸ì§‘ì‹¤|ë”ë¹™ì‹¤|ì‚¬ì˜¥|ì„¼í„°|ì‹¤|ì¸µ|ê´€|ë™|í˜¸)'
                locations = re.findall(location_pattern, query)
                if locations:
                    location_keyword = locations[0]
            
            if not location_keyword:
                return "âŒ ìœ„ì¹˜ë¥¼ ëª…í™•íˆ ì§€ì •í•´ì£¼ì„¸ìš”."
            
            # í•­ëª©ë³„ë¡œ ê²€ìƒ‰
            lines = content.split('\n')
            matching_items = []
            current_item = []
            
            for line in lines:
                # [NNNN] í˜•ì‹ì˜ ì‹œì‘ ë¼ì¸
                if re.match(r'^\[\d{4}\]', line.strip()):
                    # ì´ì „ í•­ëª© ê²€ì‚¬
                    if current_item:
                        item_text = '\n'.join(current_item)
                        # ìœ„ì¹˜ ì²´í¬
                        if self._check_location_in_item(item_text, location_keyword):
                            matching_items.append(item_text)
                    
                    current_item = [line]
                else:
                    if current_item:
                        current_item.append(line)
            
            # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
            if current_item:
                item_text = '\n'.join(current_item)
                if self._check_location_in_item(item_text, location_keyword):
                    matching_items.append(item_text)
            
            if not matching_items:
                return f"âŒ {location_keyword}ì— ì¥ë¹„ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            # ê²°ê³¼ í¬ë§·íŒ… - ìƒì„¸ ì •ë³´ í¬í•¨
            response = f"ğŸ“Š {location_keyword} ì¥ë¹„ í˜„í™©\n"
            response += "=" * 70 + "\n"
            response += f"âœ… ì´ {len(matching_items)}ê°œ ì¥ë¹„\n\n"
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
            categories = {}
            for item in matching_items:
                # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ê¸°ë³¸ì •ë³´ì—ì„œ)
                cat_match = re.search(r'ê¸°ë³¸ì •ë³´:\s*([^|]+)', item)
                if cat_match:
                    cat = cat_match.group(1).strip().split()[0] if cat_match.group(1).strip() else "ê¸°íƒ€"
                else:
                    cat = "ê¸°íƒ€"
                
                if cat not in categories:
                    categories[cat] = []
                categories[cat].append(item)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
            response += "ğŸ“‹ ì¥ë¹„ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜:\n"
            for cat, items in sorted(categories.items()):
                response += f"  â€¢ {cat}: {len(items)}ê°œ\n"
            response += "\n" + "-" * 70 + "\n\n"
            
            # ê¸ˆì•¡ ì´ê³„ ê³„ì‚°
            total_value = 0
            for item in matching_items:
                amount_match = re.search(r'ê¸ˆì•¡:\s*([\d,]+)ì›', item)
                if amount_match:
                    amount_str = amount_match.group(1).replace(',', '')
                    try:
                        total_value += int(amount_str)
                    except:
                        pass
            
            if total_value > 0:
                # ì–µì› ë‹¨ìœ„ë¡œ ë³€í™˜
                value_in_billions = total_value / 100000000
                response += f"ğŸ’° **ì´ ìì‚°ê°€ì¹˜**: {value_in_billions:.1f}ì–µì›\n\n"
            
            # ìƒì„¸ ëª©ë¡ (30ê°œë¡œ ì¦ê°€)
            response += "ğŸ“„ **ìƒì„¸ ì¥ë¹„ ëª©ë¡** (ìµœëŒ€ 30ê°œ):\n\n"
            
            # ì „ì²´ í‘œì‹œ ì—¬ë¶€ ê²°ì • (30ê°œë¡œ ì¦ê°€)
            display_items = matching_items[:30] if len(matching_items) > 30 else matching_items
            
            for i, item in enumerate(display_items, 1):
                lines = item.split('\n')
                
                # ì œëª© ì¶”ì¶œ
                title_match = re.match(r'^\[(\d{4})\]\s*(.+)', lines[0])
                if title_match:
                    item_no = title_match.group(1)
                    item_name = title_match.group(2)
                    response += f"[{i}] [{item_no}] {item_name}\n"
                
                # ëª¨ë“  ì •ë³´ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
                item_info = {}
                
                # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                for line in lines[1:]:
                    if 'ê¸°ë³¸ì •ë³´:' in line:
                        info_match = re.search(r'ê¸°ë³¸ì •ë³´:\s*(.+?)(?:\||$)', line)
                        if info_match:
                            basic_info = info_match.group(1).strip()
                            # ëª¨ë¸ëª…ê³¼ ì œì¡°ì‚¬ ì¶”ì¶œ
                            parts = basic_info.split()
                            if len(parts) >= 2:
                                item_info['model'] = parts[0]
                                item_info['manufacturer'] = parts[1] if len(parts) > 1 else ''
                            response += f"    ğŸ“Œ ëª¨ë¸: {basic_info}\n"
                    elif 'ìœ„ì¹˜:' in line:
                        loc_match = re.search(r'ìœ„ì¹˜:\s*([^|]+)', line)
                        if loc_match:
                            item_info['location'] = loc_match.group(1).strip()
                            response += f"    ğŸ“ ìœ„ì¹˜: {item_info['location']}\n"
                    elif 'ê´€ë¦¬ì •ë³´:' in line:
                        # ì „ì²´ ê´€ë¦¬ì •ë³´ ë¼ì¸ íŒŒì‹±
                        mgmt_full = line.split('ê´€ë¦¬ì •ë³´:')[1] if 'ê´€ë¦¬ì •ë³´:' in line else ''
                        mgmt_parts = mgmt_full.split('|')
                        
                        for part in mgmt_parts:
                            part = part.strip()
                            if 'ë‹´ë‹¹ì:' in part:
                                manager = part.replace('ë‹´ë‹¹ì:', '').strip()
                                if manager:
                                    response += f"    ğŸ‘¤ ë‹´ë‹¹ì: {manager}\n"
                            elif 'ìì‚°ë²ˆí˜¸:' in part:
                                asset_no = part.replace('ìì‚°ë²ˆí˜¸:', '').strip()
                                if asset_no:
                                    response += f"    ğŸ”¢ ìì‚°ë²ˆí˜¸: {asset_no}\n"
                            elif 'ì‹œë¦¬ì–¼:' in part:
                                serial = part.replace('ì‹œë¦¬ì–¼:', '').strip()
                                if serial and serial != 'N/A':
                                    response += f"    ğŸ”¤ ì‹œë¦¬ì–¼: {serial}\n"
                    elif 'êµ¬ì…ì •ë³´:' in line:
                        # ì „ì²´ êµ¬ì…ì •ë³´ ë¼ì¸ íŒŒì‹±
                        purchase_full = line.split('êµ¬ì…ì •ë³´:')[1] if 'êµ¬ì…ì •ë³´:' in line else ''
                        purchase_parts = purchase_full.split('|')
                        
                        for part in purchase_parts:
                            part = part.strip()
                            if 'êµ¬ì…ì¼:' in part:
                                purchase_date = part.replace('êµ¬ì…ì¼:', '').strip()
                                if purchase_date:
                                    response += f"    ğŸ“… êµ¬ì…ì¼: {purchase_date}\n"
                            elif 'ê¸ˆì•¡:' in part:
                                amount = part.replace('ê¸ˆì•¡:', '').strip()
                                if amount and amount != '0ì›':
                                    response += f"    ğŸ’° ê¸ˆì•¡: {amount}\n"
                            elif 'ì›' in part and 'ê¸ˆì•¡' not in part:
                                # ê¸ˆì•¡ì´ ë”°ë¡œ í‘œì‹œëœ ê²½ìš°
                                if part.strip() and part.strip() != '0ì›':
                                    response += f"    ğŸ’° ê¸ˆì•¡: {part.strip()}\n"
                
                response += "\n"
            
            if len(matching_items) > len(display_items):
                response += f"\n... ì™¸ {len(matching_items) - len(display_items)}ê°œ ì¥ë¹„ ë” ìˆìŒ\n"
            
            response += f"\nğŸ“„ ì¶œì²˜: {txt_path.name}"
            
            # LLMìœ¼ë¡œ ë‹µë³€ ê°œì„ 
            if self.llm and len(matching_items) > 0:
                try:
                    # Asset LLM Enhancer ë¡œë“œ
                    if not self.asset_enhancer:
                        from asset_llm_enhancer import AssetLLMEnhancer
                        self.asset_enhancer = AssetLLMEnhancer(self.llm)
                    
                    # ë‹µë³€ ê°œì„ 
                    enhanced_response = self.asset_enhancer.enhance_asset_response(
                        raw_data=response,
                        query=query,
                        llm=self.llm
                    )
                    
                    if enhanced_response and len(enhanced_response) > len(response):
                        return enhanced_response
                except Exception as e:
                    print(f"Asset LLM ê°œì„  ì‹¤íŒ¨: {e}")
                    # ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
            
            return response
            
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _search_equipment_all_locations(self, txt_path: Path, equipment: str) -> str:
        """ëª¨ë“  ìœ„ì¹˜ë³„ ì¥ë¹„ í˜„í™© ì •ë¦¬"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # í•­ëª©ë³„ë¡œ ê²€ìƒ‰
            lines = content.split('\n')
            location_equipment = {}  # {ìœ„ì¹˜: [ì¥ë¹„ ëª©ë¡]}
            current_item = []
            
            for line in lines:
                # [NNNN] í˜•ì‹ì˜ ì‹œì‘ ë¼ì¸ì„ ì°¾ê¸°
                if re.match(r'^\[\d{4}\]', line.strip()):
                    # ì´ì „ í•­ëª© ê²€ì‚¬
                    if current_item:
                        item_text = '\n'.join(current_item)
                        # ì¥ë¹„ëª… ì¡°ê±´ í™•ì¸
                        if equipment.upper() == "CCU":
                            equipment_match = "CCU" in item_text.upper() or "Camera Control Unit" in item_text
                        else:
                            equipment_match = equipment.upper() in item_text.upper()
                        
                        if equipment_match:
                            # ìœ„ì¹˜ ì¶”ì¶œ
                            location_info = None
                            for item_line in current_item:
                                if "ìœ„ì¹˜:" in item_line:
                                    # ìœ„ì¹˜: ë’¤ì˜ ê°’ ì¶”ì¶œ
                                    match = re.search(r'ìœ„ì¹˜:\s*([^|]+)', item_line)
                                    if match:
                                        location_info = match.group(1).strip()
                                        break
                            
                            if location_info:
                                if location_info not in location_equipment:
                                    location_equipment[location_info] = []
                                # ì¥ë¹„ ì •ë³´ ì¶”ì¶œ (ì²« ì¤„ë§Œ)
                                location_equipment[location_info].append(current_item[0])
                    
                    current_item = [line]
                else:
                    if current_item:
                        current_item.append(line)
            
            # ë§ˆì§€ë§‰ í•­ëª© ì²˜ë¦¬
            if current_item:
                item_text = '\n'.join(current_item)
                if equipment.upper() == "CCU":
                    equipment_match = "CCU" in item_text.upper() or "Camera Control Unit" in item_text
                else:
                    equipment_match = equipment.upper() in item_text.upper()
                
                if equipment_match:
                    location_info = None
                    for item_line in current_item:
                        if "ìœ„ì¹˜:" in item_line:
                            match = re.search(r'ìœ„ì¹˜:\s*([^|]+)', item_line)
                            if match:
                                location_info = match.group(1).strip()
                                break
                    
                    if location_info:
                        if location_info not in location_equipment:
                            location_equipment[location_info] = []
                        location_equipment[location_info].append(current_item[0])
            
            # ê²°ê³¼ í¬ë§·íŒ…
            if location_equipment:
                total_count = sum(len(items) for items in location_equipment.values())
                response = f"ğŸ“Š **{equipment.upper()} ìœ„ì¹˜ë³„ í˜„í™©**\n"
                response += "=" * 70 + "\n"
                response += f"âœ… ì´ {total_count}ê°œ ì¥ë¹„ê°€ {len(location_equipment)}ê°œ ìœ„ì¹˜ì— ë¶„í¬\n\n"
                
                # ìœ„ì¹˜ë³„ ì •ë ¬ (ë§ì€ ìˆœ)
                sorted_locations = sorted(location_equipment.items(), key=lambda x: len(x[1]), reverse=True)
                
                for location, items in sorted_locations:
                    response += f"ğŸ“ **{location}**: {len(items)}ê°œ\n"
                    # ìƒ˜í”Œ 3ê°œë§Œ í‘œì‹œ
                    for i, item in enumerate(items[:3], 1):
                        response += f"   {i}. {item}\n"
                    if len(items) > 3:
                        response += f"   ... ì™¸ {len(items)-3}ê°œ\n"
                    response += "\n"
                
                response += f"ğŸ“„ ì¶œì²˜: {txt_path.name}"
                return response
            else:
                return f"âŒ {equipment.upper()} ì¥ë¹„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"

    def _check_location_in_item(self, item_text: str, search_location: str) -> bool:
        """í•­ëª©ì—ì„œ ìœ„ì¹˜ ì¡°ê±´ í™•ì¸"""
        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
        location_match = re.search(r'ìœ„ì¹˜:\s*([^|\n]+)', item_text)
        if not location_match:
            return False
            
        actual_location = location_match.group(1).strip()
        
        # ì •í™•í•œ ë§¤ì¹­ ê·œì¹™ ì ìš©
        if search_location == actual_location:
            return True
        elif search_location == 'ë¶€ì¡°ì •ì‹¤':
            return actual_location.endswith('ë¶€ì¡°ì •ì‹¤')
        elif search_location == 'ìŠ¤íŠœë””ì˜¤':
            return actual_location.endswith('ìŠ¤íŠœë””ì˜¤')
        elif search_location == 'í¸ì§‘ì‹¤':
            return actual_location.endswith('í¸ì§‘ì‹¤')
        elif search_location in ['ì¤‘ê³„ì°¨', 'van', 'Van', 'VAN']:
            return 'Van' in actual_location or 'VAN' in actual_location or 'ì¤‘ê³„ì°¨' in actual_location
        elif len(search_location) > 3:
            return search_location in actual_location
        
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸš€ Perfect RAG - ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = PerfectRAG()
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_queries = [
        "2024 ì±„ë„ì—ì´ ì¤‘ê³„ì°¨ ë…¸í›„ ë³´ìˆ˜ê±´ ê¸°ì•ˆì ëˆ„êµ¬?",
        "ë·°íŒŒì¸ë” ì†Œëª¨í’ˆ ì¼€ì´ë¸” êµ¬ë§¤ ë‚ ì§œ ì–¸ì œ?",
        "í‹°ë¹„ë¡œì§ ì›”ëª¨ë‹ˆí„° ê³ ì¥ ìˆ˜ë¦¬ ê¸ˆì•¡ ì–¼ë§ˆ?",
        "2021ë…„ ì§ë²Œì¹´ë©”ë¼ êµ¬ë§¤ ê¸°ì•ˆì ëˆ„êµ¬?",
        "ìŠ¤íŠœë””ì˜¤ ëª¨ë‹ˆí„° êµì²´ ê²€í† ì„œ ë‚´ìš© ìš”ì•½",
    ]
    
    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nì§ˆë¬¸ {i}: {query}")
        print("-" * 40)
        answer = rag.answer(query)
        print(answer)
        print("-" * 40)
        
        # ìë™ ì§„í–‰ (input ì œê±°)
    
    print("\n" + "=" * 60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)



if __name__ == "__main__":
    main()