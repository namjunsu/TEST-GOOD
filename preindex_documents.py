#!/usr/bin/env python3
"""
ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì‚¬ì „ ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸
ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰ ì „ì— ì‹¤í–‰í•˜ì—¬ ì´ˆê¸° ë¡œë”© ì†ë„ ê°œì„ 
"""

import sys
from pathlib import Path
from datetime import datetime
import time
import re
import html

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from document_metadata_cache import DocumentMetadataCache
import config

def preindex_all_documents():
    """ëª¨ë“  ë¬¸ì„œë¥¼ ì‚¬ì „ ì¸ë±ì‹±"""
    print("ğŸ“š ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì‚¬ì „ ì¸ë±ì‹± ì‹œì‘...")
    start_time = time.time()

    docs_path = Path(config.DOCS_DIR)
    cache = DocumentMetadataCache()

    # ëª¨ë“  PDF íŒŒì¼ ëª©ë¡
    pdf_files = list(docs_path.glob("*.pdf"))
    total_files = len(pdf_files)

    print(f"ğŸ“„ ì´ {total_files}ê°œ PDF íŒŒì¼ ë°œê²¬")

    indexed_count = 0
    skipped_count = 0
    error_count = 0

    for i, pdf_file in enumerate(pdf_files, 1):
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        if i % 10 == 0:
            print(f"  ì§„í–‰ì¤‘... {i}/{total_files} ({i*100//total_files}%)")

        # ì´ë¯¸ ìºì‹œë˜ì–´ ìˆê³  ìµœì‹ ì´ë©´ ìŠ¤í‚µ
        if cache.is_cached(pdf_file):
            skipped_count += 1
            continue

        try:
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            stat = pdf_file.stat()
            name_parts = pdf_file.stem.split('_', 1)
            doc_date = name_parts[0] if len(name_parts) > 0 else ""
            doc_title = name_parts[1] if len(name_parts) > 1 else pdf_file.stem
            doc_title = html.unescape(doc_title)

            # ì—°ë„ ì¶”ì¶œ
            year = doc_date[:4] if len(doc_date) >= 4 else "ì—°ë„ì—†ìŒ"
            month = doc_date[5:7] if len(doc_date) >= 7 else ""

            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            category = "ê¸°íƒ€"
            if "êµ¬ë§¤" in pdf_file.name:
                category = "êµ¬ë§¤"
            elif "íê¸°" in pdf_file.name:
                category = "íê¸°"
            elif "ìˆ˜ë¦¬" in pdf_file.name or "ë³´ìˆ˜" in pdf_file.name:
                category = "ìˆ˜ë¦¬"
            elif "ì†Œëª¨í’ˆ" in pdf_file.name:
                category = "ì†Œëª¨í’ˆ"

            # ê¸°ì•ˆì ì¶”ì¶œ (ë¹„ìš©ì´ í° ì‘ì—…)
            drafter = "ë¯¸ìƒ"
            try:
                import pdfplumber
                with pdfplumber.open(pdf_file) as pdf:
                    if pdf.pages:
                        first_page_text = pdf.pages[0].extract_text() or ""

                        drafter_patterns = [
                            r'ê¸°ì•ˆì[\s:ï¼š]*([ê°€-í£]{2,4})',
                            r'ì‘ì„±ì[\s:ï¼š]*([ê°€-í£]{2,4})',
                            r'ë‹´ë‹¹[\s:ï¼š]*([ê°€-í£]{2,4})',
                            r'ê¸°ì•ˆ[\s:ï¼š]*([ê°€-í£]{2,4})',
                            r'ë‹´ë‹¹ì[\s:ï¼š]*([ê°€-í£]{2,4})',
                            r'ì„±ëª…[\s:ï¼š]*([ê°€-í£]{2,4})',
                        ]

                        for pattern in drafter_patterns:
                            match = re.search(pattern, first_page_text)
                            if match:
                                drafter = match.group(1).strip()
                                if 2 <= len(drafter) <= 4:
                                    break
                                else:
                                    drafter = "ë¯¸ìƒ"
            except Exception as e:
                print(f"  âš ï¸ {pdf_file.name}: ê¸°ì•ˆì ì¶”ì¶œ ì‹¤íŒ¨ - {str(e)[:50]}")

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                'title': doc_title,
                'filename': pdf_file.name,
                'path': str(pdf_file),
                'category': category,
                'date': doc_date,
                'year': year,
                'month': month,
                'drafter': drafter,
                'modified': datetime.fromtimestamp(stat.st_mtime)
            }

            # ìºì‹œì— ì €ì¥
            cache.save_metadata(pdf_file, metadata)
            indexed_count += 1

        except Exception as e:
            print(f"  âŒ {pdf_file.name}: ì¸ë±ì‹± ì‹¤íŒ¨ - {str(e)}")
            error_count += 1

    # ì™„ë£Œ í†µê³„
    elapsed_time = time.time() - start_time
    stats = cache.get_stats()

    print("\n" + "="*50)
    print("âœ… ì‚¬ì „ ì¸ë±ì‹± ì™„ë£Œ!")
    print(f"  - ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    print(f"  - ì‹ ê·œ ì¸ë±ì‹±: {indexed_count}ê°œ")
    print(f"  - ìŠ¤í‚µ (ì´ë¯¸ ìºì‹œë¨): {skipped_count}ê°œ")
    print(f"  - ì˜¤ë¥˜: {error_count}ê°œ")
    print(f"  - ì „ì²´ ìºì‹œëœ ë¬¸ì„œ: {stats['total_cached']}ê°œ")
    print("\nì¹´í…Œê³ ë¦¬ë³„:")
    for category, count in stats['by_category'].items():
        print(f"  - {category}: {count}ê°œ")

    return indexed_count > 0 or skipped_count > 0

if __name__ == "__main__":
    success = preindex_all_documents()
    sys.exit(0 if success else 1)