#!/usr/bin/env python3
"""
ê°œì„ ëœ ë¹ ë¥¸ ê²€ìƒ‰ RAG - LLM ìš”ì•½ + ì¶œì²˜ ì¸ìš© ê°•ì œ + L2 ë¦¬ë­í‚¹
"""

# --- IMPORT TRACE (toggle) ---
import os, atexit, json, builtins
if os.getenv("IMPORT_TRACE") == "1":
    _orig_import = builtins.__import__
    loaded = set()
    def _trace_import(name, *a, **k):
        m = _orig_import(name, *a, **k)
        f = getattr(m, "__file__", None)
        if f: loaded.add(f)
        return m
    builtins.__import__ = _trace_import
    import pathlib
    pathlib.Path("logs").mkdir(exist_ok=True)
    atexit.register(lambda: open("logs/import_trace.json","w",encoding="utf-8")
        .write(json.dumps(sorted(loaded), ensure_ascii=False, indent=2)))
# --- /IMPORT TRACE ---

from modules.search_module_hybrid import SearchModuleHybrid
from modules.reranker import RuleBasedReranker
import time
import re
import sqlite3

from app.core.logging import get_logger

logger = get_logger(__name__)

class QuickFixRAG:
    """ë¹ ë¥¸ ê²€ìƒ‰ + LLM ìš”ì•½ - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ"""

    def __init__(self, use_hybrid: bool = True):
        """
        Args:
            use_hybrid: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ëª¨ë“ˆ ì‚¬ìš© ì‹œë„
            self.search_module = SearchModuleHybrid(use_hybrid=use_hybrid)
            logger.info(f"âœ… SearchModuleHybrid ì´ˆê¸°í™” ì„±ê³µ (hybrid={use_hybrid})")
        except Exception as e:
            logger.warning(f"âš ï¸ SearchModuleHybrid ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ SearchModule ì‚¬ìš©: {e}")
            from modules.search_module import SearchModule
            self.search_module = SearchModule()

        # L2 ë¦¬ë­ì»¤ ì´ˆê¸°í™”
        try:
            self.reranker = RuleBasedReranker()
            logger.info("âœ… RuleBasedReranker ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ RuleBasedReranker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.reranker = None

        # LLM (ì§€ì—° ë¡œë”©)
        self.llm = None
        self.llm_loaded = False

    def answer(self, query: str, use_llm_summary: bool = True) -> str:
        """
        ê²€ìƒ‰ + LLM ìš”ì•½ ë°˜í™˜

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            use_llm_summary: LLM ìš”ì•½ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)
        """

        # ë©”íŠ¸ë¦­ ì¸¡ì • ì‹œì‘
        start_time = time.time()
        metrics = {
            'retrieval_ms': 0,
            'rerank_ms': 0,
            'llm_ms': 0,
            'total_ms': 0,
            'retrieved_k': 0,
            'reranked_k': 0,
            'context_tokens': 0,
            'fallback_reason': None
        }

        try:
            # ğŸ”¥ P0: íŒŒì¼ëª… ì§ì ‘ ë§¤ì¹­ (ìµœìš°ì„  ìˆœìœ„)
            filename_pattern = r'(\S+\.pdf)'
            filename_match = re.search(filename_pattern, query, re.IGNORECASE)

            if filename_match:
                filename = filename_match.group(1).strip()
                logger.info(f"ğŸ¯ P0: íŒŒì¼ëª… ì§ì ‘ ë§¤ì¹­ ì‹œë„ - {filename}")

                # DBì—ì„œ íŒŒì¼ ì§ì ‘ ì¡°íšŒ (3ë‹¨ê³„ ë§¤ì¹­)
                file_result, match_stage, candidates = self._search_by_exact_filename(filename)

                # ë¡œê·¸ ë©”íŠ¸ë¦­ì— ê¸°ë¡
                metrics['filename_match'] = match_stage

                if file_result:
                    logger.info(f"âœ… íŒŒì¼ëª… ë§¤ì¹­ ì„±ê³µ: {filename} (stage={match_stage})")
                    metrics['retrieval_ms'] = int((time.time() - start_time) * 1000)
                    metrics['retrieved_k'] = 1
                    metrics['router_reason'] = f'filename_{match_stage}'
                    metrics['total_ms'] = int((time.time() - start_time) * 1000)
                    self._log_metrics(metrics)
                    return self._format_file_result(filename, file_result)

                elif match_stage == 'like_multiple':
                    # ë‹¤ì¤‘ í›„ë³´ ë°œê²¬ - ì‚¬ìš©ì ì„ íƒ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                    logger.info(f"ğŸ“‹ ë‹¤ì¤‘ íŒŒì¼ í›„ë³´ ì œê³µ: {len(candidates)}ê±´")
                    metrics['retrieval_ms'] = int((time.time() - start_time) * 1000)
                    metrics['retrieved_k'] = len(candidates)
                    metrics['router_reason'] = 'filename_ambiguous'
                    metrics['total_ms'] = int((time.time() - start_time) * 1000)
                    self._log_metrics(metrics)
                    return self._format_candidate_list(filename, candidates)

            # 1. ê¸°ì•ˆì ë° ì—°ë„ íŒ¨í„´ ì¶”ì¶œ
            drafter_name = self._extract_author_name(query)
            year_match = re.search(r'(\d{4})\s*ë…„', query)
            year = year_match.group(1) if year_match else None

            # 2. ì¡°í•© ê²€ìƒ‰: ì—°ë„ + ê¸°ì•ˆì (ìš°ì„ ìˆœìœ„ ìµœìƒìœ„)
            if year and drafter_name:
                retrieval_start = time.time()
                logger.info(f"âœ… ì¡°í•© ê²€ìƒ‰ ëª¨ë“œ: {year}ë…„ + ê¸°ì•ˆì={drafter_name}")
                search_results = self._search_by_year_and_drafter(year, drafter_name)
                metrics['retrieval_ms'] = int((time.time() - retrieval_start) * 1000)
                metrics['retrieved_k'] = len(search_results)

                # L2 ë¦¬ë­í‚¹ ì ìš©
                if search_results and self.reranker:
                    rerank_start = time.time()
                    search_results = self.reranker.rerank(query, search_results, top_k=20)
                    metrics['rerank_ms'] = int((time.time() - rerank_start) * 1000)
                    metrics['reranked_k'] = len(search_results)
                    logger.info(f"ğŸ”„ ë¦¬ë­í‚¹ ì™„ë£Œ: {len(search_results)}ê±´")

                metrics['total_ms'] = int((time.time() - start_time) * 1000)
                self._log_metrics(metrics)

                if search_results:
                    logger.info(f"âœ… {year}ë…„ {drafter_name} ë¬¸ì„œ {len(search_results)}ê°œ ë°œê²¬")
                    return self._format_drafter_results(query, f"{year}ë…„ {drafter_name}", search_results)
                else:
                    metrics['fallback_reason'] = 'no_results'
                    logger.warning(f"âš ï¸  {year}ë…„ {drafter_name} ë¬¸ì„œ ì—†ìŒ")
                    return f"âŒ {year}ë…„ì— {drafter_name}ì´(ê°€) ì‘ì„±í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # 3. ê¸°ì•ˆìë§Œ ê²€ìƒ‰
            if drafter_name:
                retrieval_start = time.time()
                logger.info(f"âœ… ê¸°ì•ˆì ê²€ìƒ‰ ëª¨ë“œ: {drafter_name}")
                search_results = self.search_module.search_by_drafter(drafter_name, top_k=200)
                metrics['retrieval_ms'] = int((time.time() - retrieval_start) * 1000)
                metrics['retrieved_k'] = len(search_results)

                # L2 ë¦¬ë­í‚¹ ì ìš©
                if search_results and self.reranker:
                    rerank_start = time.time()
                    search_results = self.reranker.rerank(query, search_results, top_k=20)
                    metrics['rerank_ms'] = int((time.time() - rerank_start) * 1000)
                    metrics['reranked_k'] = len(search_results)
                    logger.info(f"ğŸ”„ ë¦¬ë­í‚¹ ì™„ë£Œ: {len(search_results)}ê±´")

                metrics['total_ms'] = int((time.time() - start_time) * 1000)
                self._log_metrics(metrics)

                if search_results:
                    logger.info(f"âœ… ê¸°ì•ˆì '{drafter_name}' ë¬¸ì„œ {len(search_results)}ê°œ ë°œê²¬")
                    return self._format_drafter_results(query, drafter_name, search_results)
                else:
                    logger.warning(f"âš ï¸  ê¸°ì•ˆì '{drafter_name}' ë¬¸ì„œ ì—†ìŒ")

            # 4. ì—°ë„ë§Œ ê²€ìƒ‰
            if year:
                retrieval_start = time.time()
                logger.info(f"âœ… ì—°ë„ ê²€ìƒ‰ ëª¨ë“œ: {year}ë…„")
                search_results = self._search_by_year(year)
                metrics['retrieval_ms'] = int((time.time() - retrieval_start) * 1000)
                metrics['retrieved_k'] = len(search_results)

                # L2 ë¦¬ë­í‚¹ ì ìš©
                if search_results and self.reranker:
                    rerank_start = time.time()
                    search_results = self.reranker.rerank(query, search_results, top_k=20)
                    metrics['rerank_ms'] = int((time.time() - rerank_start) * 1000)
                    metrics['reranked_k'] = len(search_results)
                    logger.info(f"ğŸ”„ ë¦¬ë­í‚¹ ì™„ë£Œ: {len(search_results)}ê±´")

                metrics['total_ms'] = int((time.time() - start_time) * 1000)
                self._log_metrics(metrics)

                if search_results:
                    logger.info(f"âœ… {year}ë…„ ë¬¸ì„œ {len(search_results)}ê°œ ë°œê²¬")
                    return self._format_search_results(f"{year}ë…„ ë¬¸ì„œ", search_results)

            # 5. ì¼ë°˜ ê²€ìƒ‰
            retrieval_start = time.time()
            search_results = self.search_module.search_by_content(query, top_k=20)
            metrics['retrieval_ms'] = int((time.time() - retrieval_start) * 1000)
            metrics['retrieved_k'] = len(search_results)

            # L2 ë¦¬ë­í‚¹ ì ìš©
            if search_results and self.reranker:
                rerank_start = time.time()
                search_results = self.reranker.rerank(query, search_results, top_k=5)
                metrics['rerank_ms'] = int((time.time() - rerank_start) * 1000)
                metrics['reranked_k'] = len(search_results)
                logger.info(f"ğŸ”„ ë¦¬ë­í‚¹ ì™„ë£Œ: {len(search_results)}ê±´")

            if not search_results:
                metrics['fallback_reason'] = 'no_results'
                metrics['total_ms'] = int((time.time() - start_time) * 1000)
                self._log_metrics(metrics)
                return self._format_no_results_message(query)

            # 6. LLM ìš”ì•½ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
            if use_llm_summary and self._ensure_llm_loaded():
                llm_start = time.time()
                result = self._answer_with_llm_summary(query, search_results)
                metrics['llm_ms'] = int((time.time() - llm_start) * 1000)
                metrics['context_tokens'] = sum(len(str(doc.get('content', '')).split()) for doc in search_results[:3])
                metrics['total_ms'] = int((time.time() - start_time) * 1000)
                self._log_metrics(metrics)
                return result
            else:
                # LLM ì—†ì´ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜ (ì¶œì²˜ í¬í•¨)
                metrics['fallback_reason'] = 'llm_disabled'
                metrics['total_ms'] = int((time.time() - start_time) * 1000)
                self._log_metrics(metrics)
                fallback_msg = "ğŸ’¡ **LLM ë¹„í™œì„±**: ê²€ìƒ‰ ê²°ê³¼ë§Œ í‘œì‹œí•©ë‹ˆë‹¤ (ìš”ì•½ ë¯¸ì œê³µ)\n\n"
                return fallback_msg + self._format_search_results(query, search_results)

        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return f"âŒ ì˜¤ë¥˜: {str(e)}"

    def _answer_with_llm_summary(self, query: str, search_results: list) -> str:
        """LLMìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ (í•µì‹¬ë§Œ ì¶”ì¶œ)"""

        try:
            # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
            top_docs = search_results[:3]

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ê¸ˆì•¡/í’ˆëª© ì •ë³´ í¬í•¨í•˜ë„ë¡ ì¶©ë¶„í•œ ê¸¸ì´)
            context_chunks = []
            for doc in top_docs:
                context_chunks.append({
                    'source': doc['filename'],
                    'content': doc.get('content', '')[:3000],  # 3000ìë¡œ í™•ì¥ (ê¸ˆì•¡/í’ˆëª© ì •ë³´ í¬í•¨)
                    'score': doc.get('score', 0.8),
                    'metadata': {
                        'ë‚ ì§œ': doc.get('date', ''),
                        'ì¹´í…Œê³ ë¦¬': doc.get('category', ''),
                        'ê¸°ì•ˆì': doc.get('department', '')
                    }
                })

            # LLMì—ê²Œ í•µì‹¬ë§Œ ìš”ì•½ ìš”ì²­
            response = self.llm.generate_response(query, context_chunks, max_retries=1)

            # ë‹µë³€ ì¶”ì¶œ
            if hasattr(response, 'answer'):
                summary = response.answer
            else:
                summary = str(response)

            # ì¶œì²˜ ê°•ì œ ì¶”ê°€ (LLMì´ ì¸ìš© ì•ˆí–ˆì„ ê²½ìš°)
            if '[' not in summary or '.pdf]' not in summary:
                # LLMì´ ì¶œì²˜ë¥¼ ì•ˆ ë‹¬ì•˜ìœ¼ë©´ ê°•ì œë¡œ ì¶”ê°€
                sources = [f"[{doc['filename']}]" for doc in top_docs[:2]]
                summary += f"\n\nì¶œì²˜: {', '.join(sources)}"

            return summary

        except Exception as e:
            logger.error(f"âŒ LLM ìš”ì•½ ì‹¤íŒ¨: {e}, ê²€ìƒ‰ ê²°ê³¼ë¡œ ëŒ€ì²´")
            fallback_msg = "âš ï¸ **LLM ìš”ì•½ ì‹¤íŒ¨**: ê²€ìƒ‰ ê²°ê³¼ë§Œ í‘œì‹œí•©ë‹ˆë‹¤\n\n"
            return fallback_msg + self._format_search_results(query, search_results)

    def _format_no_results_message(self, query: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ë©”ì‹œì§€ (ì‚¬ìš©ì ì¹œí™”ì )

        Args:
            query: ê²€ìƒ‰ ì§ˆì˜

        Returns:
            ì‚¬ìš©ì ì¹œí™”ì  ì•ˆë‚´ ë©”ì‹œì§€
        """
        return f"""ğŸ” **ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ**

**ì§ˆì˜:** {query}

**ì•ˆë‚´:**
- ì…ë ¥í•˜ì‹  ì§ˆì˜ì™€ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤
- ë‹¤ìŒ ë°©ë²•ì„ ì‹œë„í•´ë³´ì„¸ìš”:
  1. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (ì˜ˆ: "2025ë…„ ë¬¸ì„œ", "ë°©ì†¡ ì¥ë¹„")
  2. íŒŒì¼ëª… ì§ì ‘ ì§€ì • (ì˜ˆ: "2025-03-04_ë°©ì†¡_ì˜ìƒ_ë³´ì¡´ìš©_DVR_êµì²´_ê²€í† ì˜_ê±´.pdf")
  3. ê¸°ì•ˆì ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰ (ì˜ˆ: "ë‚¨ì¤€ìˆ˜ ë¬¸ì„œ", "ìµœìƒˆë¦„ì´ ì‘ì„±í•œ ë¬¸ì„œ")
  4. ì—°ë„ë¡œ ê²€ìƒ‰ (ì˜ˆ: "2024ë…„ ë¬¸ì„œ")

ğŸ’¡ **ê²€ìƒ‰ íŒ:**
- êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì‚¬ìš©
- ì—¬ëŸ¬ í‚¤ì›Œë“œ ì¡°í•© (ì˜ˆ: "2025ë…„ ì¹´ë©”ë¼ êµ¬ë§¤")
- íŒŒì¼ëª…ì´ë‚˜ ê¸°ì•ˆìëª… ì •í™•íˆ ì§€ì •
"""

    def _is_valid_drafter_name(self, drafter: str) -> bool:
        """ê¸°ì•ˆì ì´ë¦„ ìœ íš¨ì„± ê²€ì¦ (ì˜ëª»ëœ í‚¤ì›Œë“œ í•„í„°ë§)

        Args:
            drafter: ê¸°ì•ˆì ì´ë¦„

        Returns:
            ìœ íš¨í•œ ì´ë¦„ì´ë©´ True, ì˜ëª»ëœ í‚¤ì›Œë“œë©´ False
        """
        if not drafter or drafter == 'ë¯¸ìƒ':
            return False

        # ğŸ”¥ ì˜ëª»ëœ í‚¤ì›Œë“œ í•„í„° (íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œëœ ë‹¨ì–´ë“¤)
        invalid_keywords = [
            'DVR', 'dvr', 'CAMERA', 'camera', 'ì¹´ë©”ë¼', 'TV', 'tv',
            'ìŠ¤íŠœë””ì˜¤', 'studio', 'STUDIO', 'ë°©ì†¡', 'ì¥ë¹„', 'ëª¨ë‹ˆí„°',
            'ì›Œí¬ìŠ¤í…Œì´ì…˜', 'ì»´í“¨í„°', 'PC', 'pc', 'ìˆ˜ë¦¬', 'êµ¬ë§¤', 'êµì²´'
        ]

        return drafter not in invalid_keywords

    def _format_search_results(self, query: str, search_results: list) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ í¬ë§¤íŒ… (ì¶œì²˜ ê°•ì œ í¬í•¨)"""

        # ğŸ”§ Hotfix Gate 3: ëª©ë¡ í’ˆì§ˆ í›„ì²˜ë¦¬ (ì¤‘ë³µ ì œê±° + ìŠ¤ë‹ˆí« í´ë¦°)
        from app.rag.render.list_postprocess import dedup_and_clean
        before_count = len(search_results)
        search_results = dedup_and_clean(search_results)
        after_count = len(search_results)
        if before_count != after_count:
            logger.info(f"ğŸ”§ ì¤‘ë³µ ì œê±°: {before_count}ê±´ â†’ {after_count}ê±´")

        total_count = len(search_results)

        answer = f"**{query}** ê²€ìƒ‰ ê²°ê³¼\n\n"
        answer += f"ğŸ“Š **ì´ {total_count}ê°œ ë¬¸ì„œ** ë°œê²¬\n\n"

        for i, doc in enumerate(search_results, 1):
            answer += f"**{i}. {doc['filename']}**\n"
            if doc.get('date'):
                answer += f"   - ë‚ ì§œ: {doc['date']}\n"
            if doc.get('category'):
                answer += f"   - ì¹´í…Œê³ ë¦¬: {doc['category']}\n"

            # ê¸°ì•ˆì ì •ë³´ ìš°ì„  í‘œì‹œ (ìœ íš¨í•œ ì´ë¦„ë§Œ)
            drafter = doc.get('department', '')
            if self._is_valid_drafter_name(drafter):
                answer += f"   - ê¸°ì•ˆì: {drafter}\n"

            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (í´ë¦°ëœ ìŠ¤ë‹ˆí« ì‚¬ìš©)
            snippet = doc.get('snippet', '')
            snippet_preview = snippet[:150]
            if len(snippet) > 150:
                snippet_preview += "..."
            answer += f"   - ë‚´ìš©: {snippet_preview}\n"

            # âœ… ì¶œì²˜ ê°•ì œ ì¶”ê°€
            answer += f"   - ğŸ“ ì¶œì²˜: [{doc['filename']}]\n\n"

        return answer

    def _format_drafter_results(self, query: str, drafter_name: str, search_results: list) -> str:
        """ê¸°ì•ˆì ê²€ìƒ‰ ê²°ê³¼ í¬ë§¤íŒ… (ì¶œì²˜ í¬í•¨)"""

        # ğŸ”§ Hotfix Gate 3: ëª©ë¡ í’ˆì§ˆ í›„ì²˜ë¦¬
        from app.rag.render.list_postprocess import dedup_and_clean
        before_count = len(search_results)
        search_results = dedup_and_clean(search_results)
        after_count = len(search_results)
        if before_count != after_count:
            logger.info(f"ğŸ”§ ì¤‘ë³µ ì œê±°: {before_count}ê±´ â†’ {after_count}ê±´")

        total_count = len(search_results)
        display_count = min(100, total_count)  # ìµœëŒ€ 100ê°œ í‘œì‹œ

        answer = f"**ê¸°ì•ˆì: {drafter_name}** ê²€ìƒ‰ ê²°ê³¼\n\n"
        answer += f"ğŸ“Š **ì´ {total_count}ê°œ ë¬¸ì„œ** ë°œê²¬ ({display_count}ê°œ í‘œì‹œ)\n\n"

        # ë‚ ì§œë³„ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        sorted_results = sorted(search_results,
                                key=lambda x: x.get('date', ''),
                                reverse=True)

        for i, doc in enumerate(sorted_results[:display_count], 1):
            answer += f"**{i}. {doc['filename']}**\n"
            if doc.get('date'):
                answer += f"   - ë‚ ì§œ: {doc['date']}\n"
            if doc.get('category'):
                answer += f"   - ì¹´í…Œê³ ë¦¬: {doc['category']}\n"

            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (í´ë¦°ëœ ìŠ¤ë‹ˆí« ì‚¬ìš©)
            snippet = doc.get('snippet', '')
            snippet_preview = snippet[:150]
            if len(snippet) > 150:
                snippet_preview += "..."
            answer += f"   - ë‚´ìš©: {snippet_preview}\n"

            # âœ… ì¶œì²˜ ê°•ì œ ì¶”ê°€
            answer += f"   - ğŸ“ ì¶œì²˜: [{doc['filename']}]\n\n"

        # ë‚¨ì€ ë¬¸ì„œ ì•ˆë‚´
        remaining = total_count - display_count
        if remaining > 0:
            answer += f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            answer += f"ğŸ“Œ **{remaining}ê°œ ë¬¸ì„œ ë” ìˆìŠµë‹ˆë‹¤**\n"
            answer += f"ğŸ’¡ ì¹´í…Œê³ ë¦¬ë‚˜ ê¸°ê°„ìœ¼ë¡œ ì¢í˜€ë³´ì„¸ìš”\n"

        return answer

    def _search_by_year_and_drafter(self, year: str, drafter: str) -> list:
        """ì—°ë„ + ê¸°ì•ˆì ì¡°í•© ê²€ìƒ‰ (metadata.db ì‚¬ìš©, ì¤‘ë³µ ì œê±°)

        Args:
            year: ì—°ë„ (ì˜ˆ: "2025")
            drafter: ê¸°ì•ˆì ì´ë¦„ (ì˜ˆ: "ìµœìƒˆë¦„")

        Returns:
            í•´ë‹¹ ì—°ë„ + ê¸°ì•ˆìì˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°ë¨)
        """
        try:
            import sqlite3
            import pdfplumber
            from pathlib import Path

            conn = sqlite3.connect('metadata.db')
            cursor = conn.cursor()

            # year í•„ë“œì™€ drafter í•„ë“œë¡œ í•„í„° (metadata.db)
            cursor.execute("""
                SELECT path, filename, date, category, drafter, text_preview
                FROM documents
                WHERE year = ? AND drafter LIKE ?
                ORDER BY date DESC
                LIMIT 200
            """, (year, f'%{drafter}%'))

            results = []
            seen_filenames = set()  # ì¤‘ë³µ ì œê±°ìš©

            for path, filename, date, category, drafter_val, text_preview in cursor.fetchall():
                # ì¤‘ë³µ ì œê±°: íŒŒì¼ëª… ê¸°ì¤€
                if filename in seen_filenames:
                    continue
                seen_filenames.add(filename)

                result = {
                    'filename': filename,
                    'path': path,
                    'date': date or '',
                    'category': category or '',
                    'department': drafter_val or '',
                    'content': text_preview or '',
                    'score': 1.5
                }

                # PDF ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë“œ (ì²˜ìŒ 5ê°œë§Œ)
                if len(results) < 5 and path:
                    try:
                        pdf_path = Path(path)
                        if pdf_path.exists() and pdf_path.suffix.lower() == '.pdf':
                            with pdfplumber.open(pdf_path) as pdf:
                                full_text = ""
                                for page in pdf.pages[:3]:  # ìµœëŒ€ 3í˜ì´ì§€
                                    page_text = page.extract_text() or ""
                                    full_text += page_text + "\n\n"
                                    if len(full_text) > 5000:
                                        break
                                result['content'] = full_text
                    except Exception as e:
                        logger.warning(f"PDF ì½ê¸° ì‹¤íŒ¨ ({filename}): {e}")

                results.append(result)

            conn.close()
            logger.info(f"âœ… ì¡°í•© ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê±´ (ì¤‘ë³µ ì œê±° í›„)")
            return results

        except Exception as e:
            logger.error(f"âŒ ì¡°í•© ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _search_by_year(self, year: str) -> list:
        """ì—°ë„ë³„ ë¬¸ì„œ ê²€ìƒ‰ (metadata.db ì‚¬ìš©)

        Args:
            year: ì—°ë„ (ì˜ˆ: "2025")

        Returns:
            í•´ë‹¹ ì—°ë„ì˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            import sqlite3
            import pdfplumber
            from pathlib import Path

            conn = sqlite3.connect('metadata.db')
            cursor = conn.cursor()

            # year í•„ë“œì—ì„œ ê²€ìƒ‰ (metadata.db)
            cursor.execute("""
                SELECT path, filename, date, category, drafter, text_preview
                FROM documents
                WHERE year = ?
                ORDER BY date DESC
                LIMIT 200
            """, (year,))

            results = []
            for path, filename, date, category, drafter, text_preview in cursor.fetchall():
                result = {
                    'filename': filename,
                    'path': path,
                    'date': date or '',
                    'category': category or '',
                    'department': drafter or '',
                    'content': text_preview or '',
                    'score': 1.5
                }

                # PDF ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë“œ (ì²˜ìŒ 5ê°œë§Œ)
                if len(results) < 5 and path:
                    try:
                        pdf_path = Path(path)
                        if pdf_path.exists() and pdf_path.suffix.lower() == '.pdf':
                            with pdfplumber.open(pdf_path) as pdf:
                                full_text = ""
                                for page in pdf.pages[:3]:  # ìµœëŒ€ 3í˜ì´ì§€
                                    page_text = page.extract_text() or ""
                                    full_text += page_text + "\n\n"
                                    if len(full_text) > 5000:
                                        break
                                result['content'] = full_text
                    except Exception as e:
                        logger.warning(f"PDF ì½ê¸° ì‹¤íŒ¨ ({filename}): {e}")

                results.append(result)

            conn.close()
            return results

        except Exception as e:
            logger.error(f"âŒ ì—°ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _extract_author_name(self, query: str) -> str:
        """ì§ˆë¬¸ì—ì„œ ê¸°ì•ˆì/ì‘ì„±ì ì´ë¦„ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´ ì§€ì›)

        ì§€ì› íŒ¨í„´:
        - "ë‚¨ì¤€ìˆ˜ ë¬¸ì„œ"
        - "ë‚¨ì¤€ìˆ˜ê°€ ì‘ì„±í•œ"
        - "ë‚¨ì¤€ìˆ˜ê°€ ì‘ì„±ì•ˆ" (ì˜¤íƒ€)
        - "ê¸°ì•ˆì ë‚¨ì¤€ìˆ˜"
        - "ì‘ì„±ì: ë‚¨ì¤€ìˆ˜"
        - "ë‚¨ì¤€ìˆ˜ ê¸°ì•ˆì„œ"
        """
        # íŒ¨í„´ 1: "ê¸°ì•ˆì XXX", "ì‘ì„±ì XXX"
        match = re.search(r'(ê¸°ì•ˆì|ì‘ì„±ì|ì œì•ˆì)[:\s]+([ê°€-í£]{2,4})', query)
        if match:
            return match.group(2)

        # íŒ¨í„´ 2: "XXXê°€ ì‘ì„±í•œ", "XXXê°€ ì‘ì„±ì•ˆ"
        match = re.search(r'([ê°€-í£]{2,4})ê°€?\s*(ì‘ì„±í•œ|ì‘ì„±ì•ˆ|ê¸°ì•ˆí•œ|ì“´|ë§Œë“ )', query)
        if match:
            return match.group(1)

        # íŒ¨í„´ 3: "XXX ë¬¸ì„œ", "XXX ê¸°ì•ˆì„œ" (2-4ê¸€ì í•œê¸€ ì´ë¦„)
        match = re.search(r'([ê°€-í£]{2,4})\s*(ë¬¸ì„œ|ê¸°ì•ˆì„œ|ê¸°ì•ˆ|ê²€í† ì„œ)', query)
        if match:
            name = match.group(1)
            # ì¼ë°˜ ëª…ì‚¬ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ (ì˜ˆ: "êµ¬ë§¤ ë¬¸ì„œ"ëŠ” ì œì™¸)
            if name not in ['êµ¬ë§¤', 'ìˆ˜ë¦¬', 'ì¥ë¹„', 'ì¹´ë©”ë¼', 'ìµœê·¼', 'ìµœì‹ ', 'ì „ì²´']:
                return name

        return None

    def _log_metrics(self, metrics: dict) -> None:
        """ë‹¨ê³„ë³„ ë©”íŠ¸ë¦­ ë¡œê¹… (1í–‰ ìš”ì•½)

        Args:
            metrics: ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        # 1í–‰ ìš”ì•½ ë¡œê·¸
        log_parts = []
        log_parts.append(f"total={metrics['total_ms']}ms")

        # ë¼ìš°íŒ… ì •ë³´
        if metrics.get('router_reason'):
            log_parts.append(f"route={metrics['router_reason']}")

        if metrics.get('filename_match'):
            log_parts.append(f"file_match={metrics['filename_match']}")

        if metrics['retrieval_ms'] > 0:
            log_parts.append(f"retrieval={metrics['retrieval_ms']}ms")

        if metrics['rerank_ms'] > 0:
            log_parts.append(f"rerank={metrics['rerank_ms']}ms")

        if metrics['llm_ms'] > 0:
            log_parts.append(f"llm={metrics['llm_ms']}ms")

        log_parts.append(f"retrieved={metrics['retrieved_k']}")

        if metrics['reranked_k'] > 0:
            log_parts.append(f"reranked={metrics['reranked_k']}")

        if metrics['context_tokens'] > 0:
            log_parts.append(f"tokens={metrics['context_tokens']}")

        if metrics['fallback_reason']:
            log_parts.append(f"fallback={metrics['fallback_reason']}")

        logger.info(f"ğŸ“Š ë©”íŠ¸ë¦­: {' | '.join(log_parts)}")

    def _ensure_llm_loaded(self) -> bool:
        """LLM ë¡œë”© (ì§€ì—° ë¡œë”©)"""
        if self.llm_loaded:
            return True

        try:
            from rag_system.qwen_llm import QwenLLM
            from config import QWEN_MODEL_PATH

            logger.info("ğŸ¤– LLM ë¡œë”© ì¤‘ (ë¹ ë¥¸ ê²€ìƒ‰ ìš”ì•½ìš©)...")
            self.llm = QwenLLM(model_path=QWEN_MODEL_PATH)
            self.llm_loaded = True
            logger.info("âœ… LLM ë¡œë“œ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸ LLM ë¡œë“œ ì‹¤íŒ¨ (ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜): {e}")
            return False

    def _normalize_filename(self, name: str) -> str:
        """íŒŒì¼ëª… ì •ê·œí™”

        Args:
            name: ì›ë³¸ íŒŒì¼ëª…

        Returns:
            ì •ê·œí™”ëœ íŒŒì¼ëª…
        """
        import unicodedata
        import urllib.parse

        n = name.strip()
        n = urllib.parse.unquote(n)            # %20 ë“± í•´ì œ
        n = unicodedata.normalize("NFKC", n)   # ì „ê°/í˜¸í™˜ë¬¸ì í†µì¼
        n = n.replace(" ", "_")                # ê³µë°±â†’ì–¸ë”ìŠ¤ì½”ì–´
        n = re.sub(r'\((\d+)\)(?=\.pdf$)', '', n, flags=re.I)  # (1).pdf â†’ .pdf
        n = re.sub(r'_(\d+)(?=\.pdf$)', '', n, flags=re.I)     # _1.pdf â†’ .pdf
        n = re.sub(r'__+', '_', n)             # ë‹¤ì¤‘ ì–¸ë”ìŠ¤ì½”ì–´ ì¶•ì•½
        n = n.lower()
        return n

    def _search_by_exact_filename(self, filename: str) -> tuple:
        """íŒŒì¼ëª…ìœ¼ë¡œ ì •í™•íˆ ê²€ìƒ‰ (3ë‹¨ê³„ ë§¤ì¹­)

        Args:
            filename: íŒŒì¼ëª… (ì˜ˆ: "2025-03-20_ì±„ë„ì—ì´_ì¤‘ê³„ì°¨_ì¹´ë©”ë¼_ë…¸í›„í™”_ì¥ì• _ê¸´ê¸‰_ë³´ìˆ˜ê±´.pdf")

        Returns:
            (result, match_stage, candidates)
            - result: íŒŒì¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (ì—†ìœ¼ë©´ None)
            - match_stage: 'eq' | 'norm' | 'like' | 'none'
            - candidates: LIKE ë‹¨ê³„ì—ì„œ ë‹¤ì¤‘ ë§¤ì¹­ëœ í›„ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            conn = sqlite3.connect('metadata.db', uri=True, check_same_thread=False, timeout=3.0)
            cursor = conn.cursor()

            # ì½ê¸° ì „ìš© ìµœì í™”
            cursor.execute("PRAGMA query_only=ON")

            # 1ë‹¨ê³„: ì •í™• ì¼ì¹˜ (COLLATE NOCASE)
            cursor.execute("""
                SELECT path, filename, drafter, date, category, text_preview, doctype, display_date, claimed_total, sum_match
                FROM documents
                WHERE filename = ? COLLATE NOCASE
                LIMIT 1
            """, (filename,))

            result = cursor.fetchone()
            if result:
                conn.close()
                logger.info(f"âœ… íŒŒì¼ëª… ë§¤ì¹­: eq (ì •í™• ì¼ì¹˜)")
                return self._build_file_result(result), 'eq', []

            # 2ë‹¨ê³„: ì •ê·œí™” ì¼ì¹˜
            normalized = self._normalize_filename(filename)
            cursor.execute("""
                SELECT path, filename, drafter, date, category, text_preview, doctype, display_date, claimed_total, sum_match
                FROM documents
                WHERE normalized_filename = ?
                LIMIT 1
            """, (normalized,))

            result = cursor.fetchone()
            if result:
                conn.close()
                logger.info(f"âœ… íŒŒì¼ëª… ë§¤ì¹­: norm (ì •ê·œí™” ì¼ì¹˜)")
                return self._build_file_result(result), 'norm', []

            # 3ë‹¨ê³„: ë¶€ë¶„ ì¼ì¹˜ (LIKE) - ìµœëŒ€ 5ê±´
            cursor.execute("""
                SELECT path, filename, drafter, date, category, text_preview, doctype, display_date, claimed_total, sum_match
                FROM documents
                WHERE filename LIKE ? COLLATE NOCASE
                LIMIT 5
            """, (f'%{filename}%',))

            results = cursor.fetchall()
            conn.close()

            if not results:
                logger.warning(f"âš ï¸ íŒŒì¼ëª… ë§¤ì¹­ ì‹¤íŒ¨: {filename}")
                return None, 'none', []

            # ë‹¨ì¼ ë§¤ì¹­
            if len(results) == 1:
                logger.info(f"âœ… íŒŒì¼ëª… ë§¤ì¹­: like (ë¶€ë¶„ ì¼ì¹˜, ë‹¨ì¼)")
                return self._build_file_result(results[0]), 'like', []

            # ë‹¤ì¤‘ ë§¤ì¹­ - í›„ë³´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            logger.warning(f"âš ï¸ íŒŒì¼ëª… ë‹¤ì¤‘ ë§¤ì¹­: {len(results)}ê±´")
            candidates = [self._build_file_result(r) for r in results[:3]]
            return None, 'like_multiple', candidates

        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ëª… ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None, 'error', []

    def _build_file_result(self, row: tuple) -> dict:
        """DB ê²°ê³¼ë¥¼ íŒŒì¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜

        Args:
            row: (path, filename, drafter, date, category, text_preview, doctype, display_date, claimed_total, sum_match)

        Returns:
            íŒŒì¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        path, fname, drafter, date, category, text_preview, doctype, display_date, claimed_total, sum_match = row
        return {
            'path': path,
            'filename': fname,
            'drafter': drafter or 'ì •ë³´ ì—†ìŒ',
            'date': date or 'ì •ë³´ ì—†ìŒ',
            'category': category or 'ë¯¸ë¶„ë¥˜',
            'content': text_preview or '',
            'doctype': doctype or 'proposal',
            'display_date': display_date or date or 'ì •ë³´ ì—†ìŒ',
            'claimed_total': claimed_total,
            'sum_match': sum_match
        }

    def _format_file_result(self, filename: str, file_result: dict) -> str:
        """íŒŒì¼ ê²€ìƒ‰ ê²°ê³¼ í¬ë§¤íŒ… (doctype ê¸°ë°˜ í…œí”Œë¦¿ + ë…¸ì´ì¦ˆ ì œê±°)

        Args:
            filename: ìš”ì²­í•œ íŒŒì¼ëª…
            file_result: íŒŒì¼ ì •ë³´ (doctype, display_date, claimed_total, sum_match í¬í•¨)

        Returns:
            í¬ë§¤íŒ…ëœ ë¬¸ìì—´
        """
        # doctype ì •ë³´ ì¶”ì¶œ
        doctype = file_result.get('doctype', 'proposal')
        doctype_names = {
            'proposal': 'ê¸°ì•ˆì„œ',
            'report': 'ë³´ê³ ì„œ',
            'review': 'ê²€í† ì„œ',
            'minutes': 'íšŒì˜ë¡',
            'unknown': 'ë¯¸ë¶„ë¥˜'
        }
        doctype_label = doctype_names.get(doctype, 'ë¬¸ì„œ')

        answer = f"**ğŸ“„ ë¬¸ì„œ:** {file_result['filename']}\n"
        answer += f"**ğŸ·ï¸ ìœ í˜•:** {doctype_label}\n\n"

        # ë©”íƒ€ë°ì´í„°
        answer += "**ğŸ“‹ ë¬¸ì„œ ì •ë³´**\n"
        answer += f"- **ê¸°ì•ˆì:** {file_result['drafter']}\n"
        answer += f"- **ë‚ ì§œ:** {file_result.get('display_date', file_result['date'])}\n"
        answer += f"- **ì¹´í…Œê³ ë¦¬:** {file_result['category']}\n"

        # ë¹„ìš© ì •ë³´ (ìˆëŠ” ê²½ìš°)
        if file_result.get('claimed_total'):
            answer += f"- **ë¹„ìš© í•©ê³„:** â‚©{file_result['claimed_total']:,}"
            if file_result.get('sum_match') is False:
                answer += " âš ï¸ (ê²€ì¦ í•„ìš”)"
            answer += "\n"

        answer += "\n"

        # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ë…¸ì´ì¦ˆ ì œê±° + ì²˜ìŒ 1000ì)
        content = file_result.get('content', '')
        if content:
            # í…ìŠ¤íŠ¸ í´ë¦¬ë„ˆ ì ìš©
            try:
                from app.rag.preprocess.clean_text import TextCleaner
                cleaner = TextCleaner()
                cleaned_content, noise_counts = cleaner.clean(content)

                if sum(noise_counts.values()) > 0:
                    logger.debug(f"ğŸ§¹ ë…¸ì´ì¦ˆ ì œê±°: {sum(noise_counts.values())}ê°œ")

                content = cleaned_content
            except Exception as e:
                logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ í´ë¦¬ë‹ ì‹¤íŒ¨ (ì›ë¬¸ ì‚¬ìš©): {e}")

            if content.strip():
                answer += "**ğŸ“ ì£¼ìš” ë‚´ìš©**\n"
                content_preview = content[:1000].strip()
                answer += content_preview

                if len(content) > 1000:
                    answer += "...\n\n*(ì „ì²´ ë¬¸ì„œëŠ” ë” ê¸´ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤)*"
            else:
                # í´ë°±: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ 1~2ì¤„ ìš”ì•½
                answer += "**ğŸ“ ìš”ì•½**\n"
                answer += f"ê¸°ì•ˆì {file_result['drafter']}ê°€ {file_result['date']}ì— ì‘ì„±í•œ "
                answer += f"{file_result['category']} ê´€ë ¨ ë¬¸ì„œì…ë‹ˆë‹¤."
        else:
            # í´ë°±: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ 1~2ì¤„ ìš”ì•½
            answer += "**ğŸ“ ìš”ì•½**\n"
            answer += f"ê¸°ì•ˆì {file_result['drafter']}ê°€ {file_result['date']}ì— ì‘ì„±í•œ "
            answer += f"{file_result['category']} ê´€ë ¨ ë¬¸ì„œì…ë‹ˆë‹¤."

        answer += f"\n\n**ğŸ“ ì¶œì²˜:** [{file_result['filename']}]"

        return answer

    def _format_candidate_list(self, query_filename: str, candidates: list) -> str:
        """ë‹¤ì¤‘ íŒŒì¼ í›„ë³´ ë¦¬ìŠ¤íŠ¸ í¬ë§¤íŒ…

        Args:
            query_filename: ì‚¬ìš©ìê°€ ìš”ì²­í•œ íŒŒì¼ëª…
            candidates: í›„ë³´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            í¬ë§¤íŒ…ëœ ë¬¸ìì—´
        """
        answer = f"**âš ï¸ íŒŒì¼ëª…ì´ ëª¨í˜¸í•©ë‹ˆë‹¤:** `{query_filename}`\n\n"
        answer += f"**{len(candidates)}ê°œì˜ ìœ ì‚¬í•œ íŒŒì¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•í•œ íŒŒì¼ëª…ì„ ì„ íƒí•´ì£¼ì„¸ìš”:**\n\n"

        for i, candidate in enumerate(candidates, 1):
            answer += f"**{i}. {candidate['filename']}**\n"
            answer += f"   - ê¸°ì•ˆì: {candidate['drafter']}\n"
            answer += f"   - ë‚ ì§œ: {candidate['date']}\n"
            answer += f"   - ì¹´í…Œê³ ë¦¬: {candidate['category']}\n\n"

        answer += "ğŸ’¡ **ì •í™•í•œ íŒŒì¼ëª…ì„ ë³µì‚¬í•˜ì—¬ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.**"

        return answer


if __name__ == "__main__":
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    print("ğŸš€ QuickFixRAG v3 (LLM ìš”ì•½ + ì¶œì²˜ ê°•ì œ)")
    print("=" * 60)

    start = time.time()
    rag = QuickFixRAG()
    init_time = time.time() - start

    print(f"â±ï¸  ì´ˆê¸°í™” ì‹œê°„: {init_time:.4f}ì´ˆ")
    print()

    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "ì¹´ë©”ë¼ ìˆ˜ë¦¬",
        "HP Z8 ì›Œí¬ìŠ¤í…Œì´ì…˜ ì–¼ë§ˆ"
    ]

    for query in test_queries:
        print(f"\nğŸ“ ì§ˆë¬¸: {query}")
        print("-" * 60)

        start = time.time()
        answer = rag.answer(query)
        elapsed = time.time() - start

        print(answer[:500])
        if len(answer) > 500:
            print(f"... (ì´ {len(answer)}ì)")
        print(f"\nâ±ï¸  ì‘ë‹µ ì‹œê°„: {elapsed:.4f}ì´ˆ")
        print("=" * 60)
