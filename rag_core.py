"""
Professional RAG Core Engine
ì§„ì§œ RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ì—”ì§„
"""

import os
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import json
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import numpy as np

@dataclass
class Document:
    """ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    content: str
    metadata: Dict[str, Any]
    chunks: List['Chunk'] = None
    embeddings: np.ndarray = None

@dataclass
class Chunk:
    """ë¬¸ì„œ ì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    doc_id: str
    content: str
    metadata: Dict[str, Any]
    embedding: np.ndarray = None
    start_pos: int = 0
    end_pos: int = 0

class RAGCore:
    """í”„ë¡œë•ì…˜ê¸‰ RAG ì½”ì–´ ì—”ì§„"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.documents = {}
        self.chunks = {}
        self.index = None
        self.vector_store = None
        self.llm = None

        # ì´ˆê¸°í™”
        self._initialize_components()

    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        from document_processor import DocumentProcessor
        from vector_store import VectorStore
        from search_engine import SearchEngine
        from llm_handler import LLMHandler

        self.doc_processor = DocumentProcessor(self.config)
        self.vector_store = VectorStore(self.config)
        self.search_engine = SearchEngine(self.config, self.vector_store)
        self.llm_handler = LLMHandler(self.config)

        print("âœ… RAG Core ì´ˆê¸°í™” ì™„ë£Œ")

    def index_documents(self, doc_dir: str) -> Dict[str, Any]:
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        print(f"ğŸ“š ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘: {doc_dir}")

        start_time = time.time()
        results = {
            'processed': 0,
            'failed': 0,
            'chunks_created': 0
        }

        # ë¬¸ì„œ ë¡œë“œ ë° ì²˜ë¦¬
        doc_paths = list(Path(doc_dir).rglob("*.pdf"))
        doc_paths.extend(list(Path(doc_dir).rglob("*.txt")))

        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for doc_path in doc_paths:
                future = executor.submit(self._process_document, doc_path)
                futures.append(future)

            for future in futures:
                try:
                    doc = future.result()
                    if doc:
                        self.documents[doc.id] = doc
                        results['processed'] += 1
                        results['chunks_created'] += len(doc.chunks) if doc.chunks else 0
                except Exception as e:
                    results['failed'] += 1
                    print(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        # ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
        if self.documents:
            self._build_vector_index()

        elapsed = time.time() - start_time
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {results['processed']}ê°œ ë¬¸ì„œ, "
              f"{results['chunks_created']}ê°œ ì²­í¬ ({elapsed:.1f}ì´ˆ)")

        return results

    def _process_document(self, doc_path: Path) -> Optional[Document]:
        """ê°œë³„ ë¬¸ì„œ ì²˜ë¦¬"""
        try:
            # ë¬¸ì„œ ë¡œë“œ
            content = self.doc_processor.load_document(doc_path)
            if not content:
                return None

            # ë¬¸ì„œ ID ìƒì„±
            doc_id = hashlib.md5(str(doc_path).encode()).hexdigest()

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self.doc_processor.extract_metadata(doc_path, content)

            # ë¬¸ì„œ ê°ì²´ ìƒì„±
            doc = Document(
                id=doc_id,
                content=content,
                metadata=metadata
            )

            # ì²­í‚¹
            chunks = self.doc_processor.chunk_document(doc)
            doc.chunks = chunks

            # ì²­í¬ ì €ì¥
            for chunk in chunks:
                self.chunks[chunk.id] = chunk

            return doc

        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜ ({doc_path}): {e}")
            return None

    def _build_vector_index(self):
        """ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ğŸ”§ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

        # ëª¨ë“  ì²­í¬ì˜ ì„ë² ë”© ìƒì„±
        chunk_list = list(self.chunks.values())
        texts = [chunk.content for chunk in chunk_list]

        # ë°°ì¹˜ ì„ë² ë”©
        embeddings = self.vector_store.create_embeddings(texts)

        # ì²­í¬ì— ì„ë² ë”© í• ë‹¹
        for chunk, embedding in zip(chunk_list, embeddings):
            chunk.embedding = embedding

        # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
        self.vector_store.add_documents(chunk_list)

        print(f"âœ… ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(chunk_list)}ê°œ ì²­í¬")

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        # ë²¡í„° ê²€ìƒ‰ + BM25 + ì¬ìˆœìœ„
        results = self.search_engine.hybrid_search(
            query=query,
            documents=self.documents,
            chunks=self.chunks,
            top_k=top_k
        )

        return results

    def generate_answer(self, query: str, context: List[Dict[str, Any]]) -> str:
        """ë‹µë³€ ìƒì„±"""
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_text = self._build_context(context)

        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        answer = self.llm_handler.generate(
            query=query,
            context=context_text
        )

        return answer

    def _build_context(self, search_results: List[Dict[str, Any]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []

        for i, result in enumerate(search_results, 1):
            chunk = self.chunks.get(result['chunk_id'])
            if chunk:
                doc = self.documents.get(chunk.doc_id)
                source = doc.metadata.get('filename', 'ì•Œ ìˆ˜ ì—†ìŒ') if doc else 'ì•Œ ìˆ˜ ì—†ìŒ'
                context_parts.append(f"[{i}. {source}]\n{chunk.content}\n")

        return "\n".join(context_parts)

    def query(self, question: str) -> Dict[str, Any]:
        """ì§ˆì˜ ì‘ë‹µ ë©”ì¸ í•¨ìˆ˜"""
        start_time = time.time()

        # ê²€ìƒ‰
        search_results = self.search(question)

        # ë‹µë³€ ìƒì„±
        answer = self.generate_answer(question, search_results)

        # ê²°ê³¼ êµ¬ì„±
        result = {
            'answer': answer,
            'sources': [r['metadata'] for r in search_results],
            'time': time.time() - start_time
        }

        return result

def create_rag_engine(config_path: str = "config.py") -> RAGCore:
    """RAG ì—”ì§„ ìƒì„± íŒ©í† ë¦¬ í•¨ìˆ˜"""
    import config

    rag_config = {
        'chunk_size': 500,
        'chunk_overlap': 100,
        'embedding_model': 'ko-sroberta-multitask',
        'vector_db': 'chromadb',
        'llm_model': 'qwen2.5-7b',
        'search_type': 'hybrid'
    }

    return RAGCore(rag_config)