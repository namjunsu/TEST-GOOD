"""
í•œêµ­ì–´ BM25 ê²€ìƒ‰ êµ¬í˜„
kiwipiepy ê¸°ë°˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©
"""

from app.core.logging import get_logger
import os
import json
import pickle
import re
import time
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from collections import defaultdict
from functools import lru_cache
import math

# kiwipiepy í† í¬ë‚˜ì´ì € (í•œêµ­ì–´ íŠ¹í™”)
# AVX-VNNI ë¬¸ì œë¡œ ì¸í•´ ë¹„í™œì„±í™”
KIWIPIEPY_AVAILABLE = False
print("âš ï¸  kiwipiepy disabled due to AVX-VNNI issue, using basic tokenization")

class KoreanTokenizer:
    """í•œêµ­ì–´ í† í¬ë‚˜ì´ì €"""
    
    # í† í¬ë‚˜ì´ì € ìƒìˆ˜
    MIN_TOKEN_LENGTH = 1
    VALID_POS_TAGS = ['N', 'V', 'A', 'M']  # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ìˆ˜ì‹ì–¸
    TOKEN_PATTERN = r'[^\w\sê°€-í£]'  # í•œê¸€, ì˜ë¬¸, ìˆ«ì ì™¸ ì œê±°
    CACHE_SIZE = 2048  # í† í° ìºì‹œ í¬ê¸°

    def __init__(self):
        self.logger = get_logger(__name__)

        # íŒ¨í„´ ì»´íŒŒì¼
        self._compiled_token_pattern = re.compile(self.TOKEN_PATTERN)

        # ì„±ëŠ¥ í†µê³„
        self.tokenize_count = 0
        self.cache_hits = 0

        if KIWIPIEPY_AVAILABLE:
            try:
                # AVX-VNNI ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ num_workers=0ìœ¼ë¡œ ì„¤ì •
                self.kiwi = Kiwi(num_workers=0)
                self.use_kiwi = True
                self.logger.info("Kiwi í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©: {e}")
                self.use_kiwi = False
        else:
            self.use_kiwi = False
    
    @lru_cache(maxsize=CACHE_SIZE)
    def tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í•  (ìºì‹œë¨)"""
        self.tokenize_count += 1
        if text in self.tokenize.__wrapped__.__dict__.get('cache', {}):
            self.cache_hits += 1

        if not text or not text.strip():
            return []
        
        try:
            if self.use_kiwi:
                # Kiwië¡œ í˜•íƒœì†Œ ë¶„ì„
                result = self.kiwi.analyze(text)
                tokens = []
                for token, pos, _, _ in result[0][0]:  # ì²« ë²ˆì§¸ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©
                    # ì˜ë¯¸ìˆëŠ” í’ˆì‚¬ë§Œ ì„ íƒ
                    if pos[0] in self.VALID_POS_TAGS:
                        tokens.append(token.lower())
                return tokens
            else:
                # ê¸°ë³¸ í† í¬ë‚˜ì´ì € (ê³µë°± + íŠ¹ìˆ˜ë¬¸ì ê¸°ì¤€)
                text = self._compiled_token_pattern.sub(' ', text)  # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€
                tokens = [t.lower() for t in text.split() if len(t) > self.MIN_TOKEN_LENGTH]
                return tokens

        except Exception as e:
            self.logger.error(f"í† í°í™” ì‹¤íŒ¨: {e}")
            # fallback
            text = self._compiled_token_pattern.sub(' ', text)
            return [t.lower() for t in text.split() if len(t) > self.MIN_TOKEN_LENGTH]

class BM25Store:
    """BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ êµ¬í˜„"""
    
    # BM25 íŒŒë¼ë¯¸í„° ê¸°ë³¸ê°’
    DEFAULT_K1 = 1.2  # ìš©ì–´ ë¹ˆë„ í¬í™” ë§¤ê°œë³€ìˆ˜
    DEFAULT_B = 0.75  # ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™” ë§¤ê°œë³€ìˆ˜
    DEFAULT_INDEX_PATH = "rag_system/db/bm25_index.pkl"
    
    def __init__(self, index_path: str = None, k1: float = None, b: float = None):
        self.index_path = Path(index_path) if index_path else Path(self.DEFAULT_INDEX_PATH)
        self.k1 = k1 if k1 is not None else self.DEFAULT_K1
        self.b = b if b is not None else self.DEFAULT_B
        
        self.logger = get_logger(__name__)
        self.tokenizer = KoreanTokenizer()

        # BM25 ì¸ë±ìŠ¤
        self.documents = []  # ì›ë³¸ ë¬¸ì„œë“¤
        self.metadata = []   # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        self.term_freqs = []  # ê° ë¬¸ì„œë³„ ìš©ì–´ ë¹ˆë„
        self.doc_freqs = defaultdict(int)  # ìš©ì–´ë³„ ë¬¸ì„œ ë¹ˆë„
        self.doc_lens = []   # ê° ë¬¸ì„œì˜ ê¸¸ì´
        self.avg_doc_len = 0.0  # í‰ê·  ë¬¸ì„œ ê¸¸ì´
        self.vocab = set()   # ì „ì²´ ì–´íœ˜

        # ì„±ëŠ¥ í†µê³„
        self.search_count = 0
        self.total_search_time = 0.0
        self.index_time = 0.0
        
        self._load_or_create_index()
    
    def _load_or_create_index(self):
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒˆ ì¸ë±ìŠ¤ ìƒì„±"""
        if self.index_path.exists():
            try:
                self.load_index()
                self.logger.info(f"BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            except Exception as e:
                self.logger.error(f"BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self._create_new_index()
        else:
            self._create_new_index()
    
    def _create_new_index(self):
        """ìƒˆ ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        self.documents = []
        self.metadata = []
        self.term_freqs = []
        self.doc_freqs = defaultdict(int)
        self.doc_lens = []
        self.avg_doc_len = 0.0
        self.vocab = set()
        self.logger.info("ìƒˆ BM25 ì¸ë±ìŠ¤ ìƒì„±")
    
    def save_index(self):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            self.index_path.parent.mkdir(parents=True, exist_ok=True)
            
            index_data = {
                'documents': self.documents,
                'metadata': self.metadata,
                'term_freqs': self.term_freqs,
                'doc_freqs': dict(self.doc_freqs),
                'doc_lens': self.doc_lens,
                'avg_doc_len': self.avg_doc_len,
                'vocab': list(self.vocab),
                'k1': self.k1,
                'b': self.b
            }
            
            with open(self.index_path, 'wb') as f:
                pickle.dump(index_data, f)
            
            self.logger.info(f"BM25 ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.index_path}")
            
        except Exception as e:
            self.logger.error(f"BM25 ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            with open(self.index_path, 'rb') as f:
                index_data = pickle.load(f)

            self.documents = index_data['documents']
            self.metadata = index_data['metadata']
            self.term_freqs = index_data['term_freqs']
            self.doc_freqs = defaultdict(int, index_data['doc_freqs'])
            self.doc_lens = index_data['doc_lens']
            self.avg_doc_len = index_data['avg_doc_len']
            self.vocab = set(index_data['vocab'])
            self.k1 = index_data.get('k1', 1.2)
            self.b = index_data.get('b', 0.75)

            # ë©”íƒ€ë°ì´í„° ê¸¸ì´ ë³´ì • (IndexError ë°©ì§€)
            N = len(self.documents)
            if len(self.metadata) < N:
                self.metadata += [{} for _ in range(N - len(self.metadata))]
                self.logger.warning(f"ë©”íƒ€ë°ì´í„° ë¶€ì¡±ë¶„ íŒ¨ë”©: {N - len(self.metadata)}ê°œ")
            elif len(self.metadata) > N:
                self.metadata = self.metadata[:N]
                self.logger.warning(f"ë©”íƒ€ë°ì´í„° ì´ˆê³¼ë¶„ ì ˆë‹¨: {len(self.metadata) - N}ê°œ")

        except Exception as e:
            self.logger.error(f"BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def add_documents(self, texts: List[str], metadatas: List[Dict[str, Any]], batch_size: int = 100) -> None:
        """ë¬¸ì„œë“¤ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)"""
        if len(texts) != len(metadatas):
            raise ValueError("í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

        start_time = time.time()
        total_docs = len(texts)

        try:
            # ë°°ì¹˜ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            for batch_start in range(0, total_docs, batch_size):
                batch_end = min(batch_start + batch_size, total_docs)
                batch_texts = texts[batch_start:batch_end]
                batch_metadatas = metadatas[batch_start:batch_end]

                for text, metadata in zip(batch_texts, batch_metadatas):
                    # í† í°í™”
                    tokens = self.tokenizer.tokenize(text)

                    # ë¬¸ì„œ ì¶”ê°€
                    self.documents.append(text)
                    self.metadata.append(metadata)

                    # ìš©ì–´ ë¹ˆë„ ê³„ì‚°
                    term_freq = defaultdict(int)
                    for token in tokens:
                        term_freq[token] += 1
                        self.vocab.add(token)

                    self.term_freqs.append(dict(term_freq))
                    self.doc_lens.append(len(tokens))

                    # ë¬¸ì„œ ë¹ˆë„ ì—…ë°ì´íŠ¸
                    for token in set(tokens):
                        self.doc_freqs[token] += 1

                # ë°°ì¹˜ ë¡œê¹…
                if (batch_end - batch_start) >= 10:
                    self.logger.debug(f"BM25 ì¸ë±ì‹± ì§„í–‰: {batch_end}/{total_docs}")

            # í‰ê·  ë¬¸ì„œ ê¸¸ì´ ì¬ê³„ì‚°
            if self.doc_lens:
                self.avg_doc_len = sum(self.doc_lens) / len(self.doc_lens)

            # ì¸ë±ì‹± ì‹œê°„ ê¸°ë¡
            self.index_time += time.time() - start_time

            self.logger.info(f"{total_docs}ê°œ ë¬¸ì„œ BM25 ì¸ë±ì‹± ì™„ë£Œ (ì´ {len(self.documents)}ê°œ, {time.time() - start_time:.2f}ì´ˆ)")
            
        except Exception as e:
            self.logger.error(f"BM25 ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise

    
    def search(self, query: str, top_k: int = 5, snippet_max: int = 5000, **kwargs) -> List[Dict[str, Any]]:
        """BM25 ìŠ¤ì½”ì–´ë¡œ ë¬¸ì„œ ê²€ìƒ‰ (ì„±ëŠ¥ ì¶”ì  í¬í•¨)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            snippet_max: ìŠ¤ë‹ˆí« ìµœëŒ€ ê¸¸ì´ (ê¸°ë³¸: 5000ì)
            **kwargs: ì¶”ê°€ ì˜µì…˜
        """
        if not self.documents:
            return []

        start_time = time.time()
        self.search_count += 1

        try:
            # ì¿¼ë¦¬ í† í°í™” (ë¹ˆ ì¿¼ë¦¬ ë°©ì–´)
            query_tokens = self.tokenizer.tokenize(query)
            if not query_tokens:
                return []
            
            # ê° ë¬¸ì„œë³„ BM25 ìŠ¤ì½”ì–´ ê³„ì‚°
            scores = []
            N = len(self.documents)  # ì´ ë¬¸ì„œ ìˆ˜
            
            for doc_idx in range(N):
                score = 0.0
                doc_len = self.doc_lens[doc_idx]
                
                for token in query_tokens:
                    if token in self.term_freqs[doc_idx]:
                        # ìš©ì–´ ë¹ˆë„ (TF)
                        tf = self.term_freqs[doc_idx][token]
                        
                        # ë¬¸ì„œ ë¹ˆë„ (DF)
                        df = self.doc_freqs.get(token, 0)
                        if df == 0:
                            continue
                        
                        # IDF ê³„ì‚°
                        idf = math.log((N - df + 0.5) / (df + 0.5))
                        
                        # BM25 ìŠ¤ì½”ì–´ ê³„ì‚°
                        numerator = tf * (self.k1 + 1)
                        denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avg_doc_len))
                        
                        score += idf * (numerator / denominator)
                
                scores.append((score, doc_idx))
            
            # ìŠ¤ì½”ì–´ ê¸°ì¤€ ì •ë ¬
            scores.sort(key=lambda x: x[0], reverse=True)
            
            # ê²°ê³¼ êµ¬ì„±
            results = []
            for i, (score, doc_idx) in enumerate(scores[:top_k]):
                if score > 0:  # ì–‘ì˜ ìŠ¤ì½”ì–´ë§Œ
                    # ë©”íƒ€ë°ì´í„° ì•ˆì „ ì ‘ê·¼ (IndexError ë°©ì§€)
                    metadata = self.metadata[doc_idx] if doc_idx < len(self.metadata) else {}
                    # ìŠ¤ë‹ˆí« ê¸¸ì´ ì œí•œ ì ìš©
                    content = self.documents[doc_idx]
                    snippet = content[:snippet_max] if snippet_max > 0 else content

                    result = {
                        'rank': i + 1,
                        'score': float(score),
                        'content': snippet,
                        'query_tokens': query_tokens,
                        **metadata
                    }
                    results.append(result)
            
            # ê²€ìƒ‰ ì‹œê°„ ê¸°ë¡
            search_time = time.time() - start_time
            self.total_search_time += search_time

            if self.search_count % 100 == 0:
                self.logger.info(f"BM25 ê²€ìƒ‰ í†µê³„: {self.search_count}íšŒ, í‰ê·  {self.total_search_time/self.search_count:.3f}ì´ˆ")

            return results

        except Exception as e:
            self.logger.error(f"BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
        finally:
            self.total_search_time += time.time() - start_time
    
    def get_stats(self) -> Dict[str, Any]:
        """BM25 ì¸ë±ìŠ¤ í†µê³„ (í™•ì¥ëœ ë©”íŠ¸ë¦­)"""
        tokenizer_stats = {
            'type': 'kiwipiepy' if self.tokenizer.use_kiwi else 'basic',
            'tokenize_count': self.tokenizer.tokenize_count,
            'cache_hits': self.tokenizer.cache_hits,
            'cache_hit_rate': self.tokenizer.cache_hits / self.tokenizer.tokenize_count * 100 if self.tokenizer.tokenize_count > 0 else 0
        }

        return {
            'total_documents': len(self.documents),
            'vocab_size': len(self.vocab),
            'avg_doc_length': self.avg_doc_len,
            'avgdl': self.avg_doc_len,  # í˜¸í™˜ì„± alias
            'index_path': str(self.index_path),
            'bm25_index_path': str(self.index_path),  # í˜¸í™˜ì„± alias
            'bm25_index_docs': len(self.documents),  # í˜¸í™˜ì„± alias
            'has_tf': bool(self.term_freqs),
            'has_df': bool(self.doc_freqs),
            'parameters': {
                'k1': self.k1,
                'b': self.b
            },
            'tokenizer': tokenizer_stats,
            'performance': {
                'total_index_time': self.index_time,
                'search_count': self.search_count,
                'total_search_time': self.total_search_time,
                'avg_search_time': self.total_search_time / self.search_count if self.search_count > 0 else 0
            }
        }

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_bm25_store():
    """BM25 ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” BM25 ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "í•€ë§ˆì´í¬ ëª¨ë¸ ECM-77BCë¥¼ êµ¬ë§¤ ê²€í† í•©ë‹ˆë‹¤. ê°€ê²©ì€ 336,000ì›ì…ë‹ˆë‹¤.",
        "ì˜ìƒí¸ì§‘íŒ€ ì›Œí¬ìŠ¤í…Œì´ì…˜ êµì²´ ë¹„ìš©ì€ 179,300,000ì›ì…ë‹ˆë‹¤. HP Z8 ëª¨ë¸ì…ë‹ˆë‹¤.",
        "ê´‘í™”ë¬¸ ìŠ¤íŠœë””ì˜¤ ëª¨ë‹ˆí„° êµì²´ ì´ì•¡ì€ 9,760,000ì›ì…ë‹ˆë‹¤. LG ëª¨ë‹ˆí„° 3ëŒ€ì…ë‹ˆë‹¤.",
        "ì¹´ë©”ë¼ ì¥ë¹„ êµ¬ë§¤ë¥¼ ìœ„í•œ ì˜ˆì‚° ê²€í† ì„œì…ë‹ˆë‹¤. ì´ ì˜ˆì‚°ì€ 50,000,000ì›ì…ë‹ˆë‹¤.",
        "ìŠ¤íŠœë””ì˜¤ ì¡°ëª… ì‹œì„¤ êµì²´ì— ëŒ€í•œ ê¸°ì•ˆì„œì…ë‹ˆë‹¤. í•„ë¦½ìŠ¤ LED ì¡°ëª… 20ëŒ€ë¥¼ êµ¬ë§¤í•©ë‹ˆë‹¤."
    ]
    
    test_metadatas = [
        {
            'doc_id': 'doc1',
            'chunk_id': 'chunk1_1',
            'filename': '2021-05-13_í•€ë§ˆì´í¬_êµ¬ë§¤ê²€í† .pdf',
            'content': test_texts[0],
            'metadata': {'doc_type': 'ê¸°ì•ˆì„œ', 'amount': 336000}
        },
        {
            'doc_id': 'doc2',
            'chunk_id': 'chunk2_1',
            'filename': '2022-02-03_ì›Œí¬ìŠ¤í…Œì´ì…˜_êµì²´ê²€í† .pdf',
            'content': test_texts[1],
            'metadata': {'doc_type': 'ê²€í† ì„œ', 'amount': 179300000}
        },
        {
            'doc_id': 'doc3',
            'chunk_id': 'chunk3_1',
            'filename': '2025-01-09_ëª¨ë‹ˆí„°_êµì²´ê²€í† .pdf',
            'content': test_texts[2],
            'metadata': {'doc_type': 'ê¸°ì•ˆì„œ', 'amount': 9760000}
        },
        {
            'doc_id': 'doc4',
            'chunk_id': 'chunk4_1',
            'filename': '2023-07-15_ì¹´ë©”ë¼ì¥ë¹„_ì˜ˆì‚°ê²€í† .pdf',
            'content': test_texts[3],
            'metadata': {'doc_type': 'ì˜ˆì‚°ì„œ', 'amount': 50000000}
        },
        {
            'doc_id': 'doc5',
            'chunk_id': 'chunk5_1',
            'filename': '2024-03-20_ì¡°ëª…ì‹œì„¤_êµì²´ê¸°ì•ˆ.pdf',
            'content': test_texts[4],
            'metadata': {'doc_type': 'ê¸°ì•ˆì„œ', 'amount': 0}  # ê¸ˆì•¡ ë¯¸ìƒ
        }
    ]
    
    try:
        # BM25 ìŠ¤í† ì–´ ìƒì„±
        bm25 = BM25Store(index_path="rag_system/db/test_bm25_index.pkl")
        
        # ë¬¸ì„œ ì¶”ê°€
        print("ğŸ“ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€ ì¤‘...")
        bm25.add_documents(test_texts, test_metadatas)
        
        # ì¸ë±ìŠ¤ ì €ì¥
        bm25.save_index()
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_queries = [
            "í•€ë§ˆì´í¬ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?",
            "ì›Œí¬ìŠ¤í…Œì´ì…˜ ëª¨ë¸ëª…ì€?",
            "ëª¨ë‹ˆí„° ê°œìˆ˜ëŠ” ëª‡ ê°œì¸ê°€ìš”?",
            "ì¹´ë©”ë¼ ì˜ˆì‚°",
            "ì¡°ëª… êµì²´"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” BM25 ê²€ìƒ‰: '{query}'")
            results = bm25.search(query, top_k=3)
            
            for result in results:
                print(f"  ìˆœìœ„ {result['rank']}: {result['filename']}")
                print(f"    BM25 ìŠ¤ì½”ì–´: {result['score']:.3f}")
                print(f"    ì¿¼ë¦¬ í† í°: {result['query_tokens']}")
                print(f"    ë‚´ìš©: {result['content'][:50]}...")
        
        # í†µê³„ ì¶œë ¥
        stats = bm25.get_stats()
        print(f"\nğŸ“Š BM25 ì¸ë±ìŠ¤ í†µê³„:")
        print(f"  ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
        print(f"  ì–´íœ˜ í¬ê¸°: {stats['vocab_size']}")
        print(f"  í‰ê·  ë¬¸ì„œ ê¸¸ì´: {stats['avg_doc_length']:.1f}")
        print(f"  í† í¬ë‚˜ì´ì €: {stats['tokenizer']['type']}")
        print(f"  í† í° ìºì‹œ íˆíŠ¸ìœ¨: {stats['tokenizer']['cache_hit_rate']:.1f}%")
        print(f"  íŒŒë¼ë¯¸í„°: k1={stats['parameters']['k1']}, b={stats['parameters']['b']}")
        print(f"  í‰ê·  ê²€ìƒ‰ ì‹œê°„: {stats['performance']['avg_search_time']:.3f}ì´ˆ")
        
        print("âœ… BM25 ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ BM25 ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)
    test_bm25_store()