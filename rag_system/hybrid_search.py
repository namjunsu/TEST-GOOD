"""
í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
ë²¡í„° ê²€ìƒ‰ + BM25 ê²€ìƒ‰ì„ RRFë¡œ ê²°í•©
"""

import os
from typing import List, Dict, Any, Optional, Tuple, Set
from pathlib import Path
import time
import re
from functools import lru_cache
import hashlib

from app.core.logging import get_logger

try:
    import pdfplumber
except ImportError:
    pdfplumber = None

from .korean_vector_store import KoreanVectorStore
from .bm25_store import BM25Store
from .query_optimizer import QueryOptimizer
from .korean_reranker import KoreanReranker
from .query_expansion import QueryExpansion
from .document_compression import DocumentCompression
from .multilevel_filter import MultilevelFilter

# ê²€ìƒ‰ ì„¤ì • ìƒìˆ˜ (.envì—ì„œ ì½ê¸°)
DEFAULT_VECTOR_WEIGHT = float(os.getenv('SEARCH_VECTOR_WEIGHT', '0.1'))
DEFAULT_BM25_WEIGHT = float(os.getenv('SEARCH_BM25_WEIGHT', '0.9'))
DEFAULT_TOP_K = int(os.getenv('SEARCH_TOP_K', '5'))
DEFAULT_RRF_K = 20  # Reciprocal Rank Fusion íŒŒë¼ë¯¸í„°
DEFAULT_FUSION_METHOD = "weighted_sum"  # "rrf" ë˜ëŠ” "weighted_sum"

# ì¸ë±ìŠ¤ ê²½ë¡œ ìƒìˆ˜
DEFAULT_VECTOR_INDEX_PATH = "rag_system/db/korean_vector_index.faiss"
DEFAULT_BM25_INDEX_PATH = "rag_system/db/bm25_index.pkl"

# ê²°ê³¼ ì œí•œ ìƒìˆ˜
MAX_SEARCH_RESULTS = 100

class HybridSearch:
    """ë²¡í„° + BM25 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
    
    def __init__(
        self,
        vector_index_path: str = None,
        bm25_index_path: str = None,
        vector_weight: float = DEFAULT_VECTOR_WEIGHT,
        bm25_weight: float = DEFAULT_BM25_WEIGHT,
        rrf_k: int = DEFAULT_RRF_K,
        use_reranker: bool = True,
        use_query_expansion: bool = True,
        use_document_compression: bool = True,
        use_multilevel_filter: bool = True,
        single_document_mode: bool = False,
        fusion_method: str = DEFAULT_FUSION_METHOD
    ):
        self.vector_weight = vector_weight
        self.bm25_weight = bm25_weight
        self.rrf_k = rrf_k  # RRF íŒŒë¼ë¯¸í„°
        self.use_reranker = use_reranker
        self.use_query_expansion = use_query_expansion
        self.use_document_compression = use_document_compression
        self.use_multilevel_filter = use_multilevel_filter
        self.single_document_mode = single_document_mode
        self.fusion_method = fusion_method

        self.logger = get_logger(__name__)
        self.query_optimizer = QueryOptimizer()

        # ê²€ìƒ‰ ìºì‹œ (ì¿¼ë¦¬ í•´ì‹œ -> ê²°ê³¼)
        self.search_cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        # Advanced RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ë”¥ì…”ë„ˆë¦¬ë¡œ ê´€ë¦¬)
        self.optional_components = {
            'reranker': (use_reranker, KoreanReranker, "í•œêµ­ì–´ Reranker"),
            'query_expander': (use_query_expansion, QueryExpansion, "Query Expansion"),
            'doc_compressor': (use_document_compression, DocumentCompression, "Document Compression"),
            'multilevel_filter': (use_multilevel_filter, MultilevelFilter, "ë‹¤ë‹¨ê³„ í•„í„°ë§ ì‹œìŠ¤í…œ")
        }

        for attr_name, (use_flag, component_class, component_name) in self.optional_components.items():
            self._init_optional_component(attr_name, use_flag, component_class, component_name)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        try:
            vector_path = vector_index_path or DEFAULT_VECTOR_INDEX_PATH
            bm25_path = bm25_index_path or DEFAULT_BM25_INDEX_PATH
            
            self.vector_store = KoreanVectorStore(index_path=vector_path)
            self.bm25_store = BM25Store(index_path=bm25_path)
            
            self.logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _init_optional_component(self, attr_name: str, use_flag: bool, component_class, component_name: str):
        """ì„ íƒì  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” í—¬í¼ ë©”ì„œë“œ"""
        setattr(self, attr_name, None)
        if use_flag:
            try:
                setattr(self, attr_name, component_class())
                self.logger.info(f"{component_name} ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"{component_name} ì´ˆê¸°í™” ì‹¤íŒ¨, ë¹„í™œì„±í™”: {e}")
                setattr(self, f"use_{attr_name.replace('_', '')}", False)
    
    def add_documents(self, texts: List[str], metadatas: List[Dict[str, Any]]):
        """ë‘ ì¸ë±ìŠ¤ì— ë™ì‹œì— ë¬¸ì„œ ì¶”ê°€"""
        try:
            self.vector_store.add_documents(texts, metadatas)
            self.bm25_store.add_documents(texts, metadatas)
            self.logger.info(f"{len(texts)}ê°œ ë¬¸ì„œ í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ì‹± ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def save_indexes(self):
        """ë‘ ì¸ë±ìŠ¤ ëª¨ë‘ ì €ì¥"""
        try:
            self.vector_store.save_index()
            self.bm25_store.save_index()
            self.logger.info("í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def _get_doc_id(self, result: Dict[str, Any]) -> str:
        """ë¬¸ì„œ ID ì¶”ì¶œ í—¬í¼ ë©”ì„œë“œ"""
        return result.get('chunk_id', result.get('doc_id', result.get('source', '')))

    def _remove_duplicates(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±° í—¬í¼ ë©”ì„œë“œ"""
        seen_ids: Set[str] = set()
        unique_results = []

        for result in results:
            doc_id = self._get_doc_id(result)
            if doc_id and doc_id not in seen_ids:
                seen_ids.add(doc_id)
                unique_results.append(result)

        return unique_results

    def _normalize_scores(self, results: List[Dict[str, Any]], score_key: str = 'score') -> List[Dict[str, Any]]:
        """ìŠ¤ì½”ì–´ ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ)"""
        if not results:
            return results

        scores = [r[score_key] for r in results]
        min_score = min(scores)
        max_score = max(scores)

        # ì •ê·œí™”
        if max_score > min_score:
            for result in results:
                normalized = (result[score_key] - min_score) / (max_score - min_score)
                result[f'normalized_{score_key}'] = normalized
        else:
            for result in results:
                result[f'normalized_{score_key}'] = 1.0

        return results
    
    def _reciprocal_rank_fusion(
        self, 
        vector_results: List[Dict[str, Any]], 
        bm25_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """RRFë¡œ ê²°ê³¼ ìœµí•©"""
        
        # ë¬¸ì„œë³„ RRF ìŠ¤ì½”ì–´ ê³„ì‚°
        doc_scores = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì˜ RRF ìŠ¤ì½”ì–´
        for rank, result in enumerate(vector_results):
            doc_id = result.get('chunk_id', result.get('doc_id', str(rank)))
            rrf_score = 1.0 / (self.rrf_k + rank + 1)
            
            if doc_id not in doc_scores:
                doc_scores[doc_id] = {
                    'vector_rrf': 0.0,
                    'bm25_rrf': 0.0,
                    'vector_result': None,
                    'bm25_result': None
                }
            
            doc_scores[doc_id]['vector_rrf'] = rrf_score
            doc_scores[doc_id]['vector_result'] = result
        
        # BM25 ê²€ìƒ‰ ê²°ê³¼ì˜ RRF ìŠ¤ì½”ì–´
        for rank, result in enumerate(bm25_results):
            doc_id = result.get('chunk_id', result.get('doc_id', str(rank)))
            rrf_score = 1.0 / (self.rrf_k + rank + 1)
            
            if doc_id not in doc_scores:
                doc_scores[doc_id] = {
                    'vector_rrf': 0.0,
                    'bm25_rrf': 0.0,
                    'vector_result': None,
                    'bm25_result': None
                }
            
            doc_scores[doc_id]['bm25_rrf'] = rrf_score
            doc_scores[doc_id]['bm25_result'] = result
        
        # ìµœì¢… RRF ìŠ¤ì½”ì–´ ê³„ì‚° ë° ì •ë ¬
        final_results = []
        
        for doc_id, scores in doc_scores.items():
            # ê°€ì¤‘ RRF ìŠ¤ì½”ì–´
            final_rrf = (
                self.vector_weight * scores['vector_rrf'] + 
                self.bm25_weight * scores['bm25_rrf']
            )
            
            # ê²°ê³¼ êµ¬ì„± (ë²¡í„° ê²°ê³¼ ìš°ì„ , ì—†ìœ¼ë©´ BM25 ê²°ê³¼)
            base_result = scores['vector_result'] or scores['bm25_result']
            if base_result:
                final_result = {
                    **base_result,
                    'hybrid_score': final_rrf,
                    'vector_score': scores['vector_result']['score'] if scores['vector_result'] else 0.0,
                    'bm25_score': scores['bm25_result']['score'] if scores['bm25_result'] else 0.0,
                    'vector_rrf': scores['vector_rrf'],
                    'bm25_rrf': scores['bm25_rrf'],
                    'fusion_method': 'RRF'
                }
                final_results.append(final_result)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë¡œ ì •ë ¬
        final_results.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        # ìˆœìœ„ ì¬ë¶€ì—¬
        for i, result in enumerate(final_results):
            result['rank'] = i + 1
        
        return final_results
    
    def _fuse_with_weighted_sum(self, vector_results: List[Dict], 
                               bm25_results: List[Dict]) -> List[Dict[str, Any]]:
        """ë‹¨ìˆœ ê°€ì¤‘í•©ì„ ì‚¬ìš©í•œ ìœµí•© (RRF ëŒ€ì‹ )"""
        
        # ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€ê°’ ê³„ì‚°
        vector_max_score = max([r.get('score', 0) for r in vector_results]) if vector_results else 1.0
        bm25_max_score = max([r.get('score', 0) for r in bm25_results]) if bm25_results else 1.0
        
        # ë¬¸ì„œë³„ ì ìˆ˜ ìˆ˜ì§‘
        doc_scores = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        for result in vector_results:
            doc_id = result.get('chunk_id', result.get('doc_id', result.get('filename', 'unknown')))
            normalized_score = result.get('score', 0) / vector_max_score
            
            if doc_id not in doc_scores:
                doc_scores[doc_id] = {
                    'vector_score': 0.0,
                    'bm25_score': 0.0,
                    'vector_result': None,
                    'bm25_result': None
                }
            
            doc_scores[doc_id]['vector_score'] = normalized_score
            doc_scores[doc_id]['vector_result'] = result
        
        # BM25 ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        for result in bm25_results:
            doc_id = result.get('chunk_id', result.get('doc_id', result.get('filename', 'unknown')))
            normalized_score = result.get('score', 0) / bm25_max_score
            
            if doc_id not in doc_scores:
                doc_scores[doc_id] = {
                    'vector_score': 0.0,
                    'bm25_score': 0.0,
                    'vector_result': None,
                    'bm25_result': None
                }
            
            doc_scores[doc_id]['bm25_score'] = normalized_score
            doc_scores[doc_id]['bm25_result'] = result
        
        # ê°€ì¤‘í•© ê³„ì‚° ë° ìµœì¢… ê²°ê³¼ ìƒì„±
        final_results = []
        
        for doc_id, scores in doc_scores.items():
            # ê°€ì¤‘í•© ìŠ¤ì½”ì–´ ê³„ì‚°
            weighted_score = (
                self.vector_weight * scores['vector_score'] + 
                self.bm25_weight * scores['bm25_score']
            )
            
            # ê²°ê³¼ êµ¬ì„± (BM25 ê²°ê³¼ ìš°ì„ , ì—†ìœ¼ë©´ ë²¡í„° ê²°ê³¼)
            base_result = scores['bm25_result'] or scores['vector_result']
            if base_result and weighted_score > 0:
                final_result = {
                    **base_result,
                    'hybrid_score': weighted_score,
                    'vector_score': scores['vector_score'],
                    'bm25_score': scores['bm25_score'],
                    'fusion_method': 'weighted_sum'
                }
                final_results.append(final_result)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë¡œ ì •ë ¬
        final_results.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        # ìˆœìœ„ ì¬ë¶€ì—¬
        for i, result in enumerate(final_results):
            result['rank'] = i + 1
        
        return final_results
    
    def _extract_year_from_query(self, query: str) -> Optional[str]:
        """ì¿¼ë¦¬ì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ"""
        # 2024ë…„, 2024, 24ë…„ ë“± íŒ¨í„´ ê°ì§€
        year_patterns = [
            r'(20\d{2})ë…„',  # 2024ë…„
            r'(20\d{2})',    # 2024
            r'(\d{2})ë…„'     # 24ë…„
        ]
        
        for pattern in year_patterns:
            match = re.search(pattern, query)
            if match:
                year_str = match.group(1)
                # 2ìë¦¬ ë…„ë„ë¥¼ 4ìë¦¬ë¡œ ë³€í™˜ (24 â†’ 2024)
                if len(year_str) == 2:
                    year_str = "20" + year_str
                return year_str
        return None
    
    def _apply_date_filtering(self, results: List[Dict[str, Any]], target_year: str) -> List[Dict[str, Any]]:
        """ë‚ ì§œ ê¸°ë°˜ í•„í„°ë§ìœ¼ë¡œ ê²°ê³¼ ìˆœìœ„ ì¡°ì •"""
        if not target_year:
            return results
        
        boosted_results = []
        other_results = []
        
        for result in results:
            filename = result.get('filename', '')
            source = result.get('source', '')
            
            # íŒŒì¼ëª…ì´ë‚˜ ì†ŒìŠ¤ì—ì„œ ë…„ë„ ê²€ì‚¬
            if target_year in filename or target_year in source:
                # ì ìˆ˜ ë¶€ìŠ¤íŠ¸ (1.5ë°°)
                if 'hybrid_score' in result:
                    result['hybrid_score'] *= 1.5
                    result['date_boosted'] = True
                boosted_results.append(result)
            else:
                other_results.append(result)
        
        # ë¶€ìŠ¤íŠ¸ëœ ê²°ê³¼ë¥¼ ì•ì— ë°°ì¹˜í•˜ê³  ì ìˆ˜ìˆœ ì¬ì •ë ¬
        all_results = boosted_results + other_results
        all_results.sort(key=lambda x: x.get('hybrid_score', 0), reverse=True)
        
        # ìˆœìœ„ ì¬ë¶€ì—¬
        for i, result in enumerate(all_results):
            result['rank'] = i + 1
        
        return all_results
    
    def _extract_keywords_from_query(self, query: str) -> Dict[str, List[str]]:
        """ì¿¼ë¦¬ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = {
            'amounts': [],
            'proper_nouns': [],
            'model_codes': []
        }
        
        # ê¸ˆì•¡ íŒ¨í„´ (2,370,000ì›, 237ë§Œì› ë“±)
        amount_patterns = [
            r'[\d,]+ì›',       # 2,370,000ì›
            r'\d+ë§Œì›',        # 237ë§Œì›  
            r'\d+ì–µì›',        # 23ì–µì›
            r'[\d,]+',         # 2,370,000 (ì› ì—†ì´ë„)
            r'\d{1,3}(?:,\d{3})*'  # ì»´ë§ˆ êµ¬ë¶„ ìˆ«ì
        ]
        for pattern in amount_patterns:
            matches = re.findall(pattern, query)
            keywords['amounts'].extend(matches)
        
        # ëª¨ë¸ ì½”ë“œ (ECM-77BC, HP Z8 ë“±)
        model_patterns = [
            r'[A-Z]+-[0-9A-Z]+',  # ECM-77BC
            r'[A-Z]+\s+[A-Z0-9]+' # HP Z8
        ]
        for pattern in model_patterns:
            matches = re.findall(pattern, query)
            keywords['model_codes'].extend(matches)
        
        # í•œêµ­ì–´ ê³ ìœ ëª…ì‚¬ (ê°„ë‹¨í•œ íŒ¨í„´)
        # 2-4ê¸€ì í•œê¸€ ë‹¨ì–´ ì¤‘ ã…‡ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš° (ì˜ˆ: ë‚¨ì¤€ìˆ˜, ë…¸ê·œë¯¼)
        korean_name_pattern = r'[ê°€-í£]{2,4}'
        potential_names = re.findall(korean_name_pattern, query)
        for name in potential_names:
            if len(name) >= 3:  # 3ê¸€ì ì´ìƒë§Œ ê³ ìœ ëª…ì‚¬ë¡œ ê°„ì£¼
                keywords['proper_nouns'].append(name)
        
        return keywords
    
    def _apply_keyword_boosting(self, results: List[Dict[str, Any]], keywords: Dict[str, List[str]]) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­ ê¸°ë°˜ ì ìˆ˜ ë¶€ìŠ¤íŠ¸"""
        if not any(keywords.values()):
            return results
        
        for result in results:
            content = result.get('content', '')
            filename = result.get('filename', '')
            boost_factor = 1.0
            boost_reasons = []
            
            # ê¸ˆì•¡ ì •í™• ë§¤ì¹­ (ê°€ì¥ ì¤‘ìš”)
            for amount in keywords['amounts']:
                # ì •í™• ë§¤ì¹­
                if amount in content or amount in filename:
                    boost_factor *= 2.0  # 2ë°° ë¶€ìŠ¤íŠ¸
                    boost_reasons.append(f"ê¸ˆì•¡_ì •í™•ë§¤ì¹­_{amount}")
                else:
                    # ìˆ«ìë§Œ ë§¤ì¹­ (ì½¤ë§ˆ ì œê±° í›„)
                    clean_amount = amount.replace(',', '').replace('ì›', '')
                    if clean_amount.isdigit() and len(clean_amount) >= 4:
                        if clean_amount in content.replace(',', '') or clean_amount in filename.replace(',', ''):
                            boost_factor *= 1.8  # 1.8ë°° ë¶€ìŠ¤íŠ¸
                            boost_reasons.append(f"ê¸ˆì•¡_ìˆ«ìë§¤ì¹­_{clean_amount}")
            
            # ëª¨ë¸ ì½”ë“œ ë§¤ì¹­
            for model in keywords['model_codes']:
                if model in content or model in filename:
                    boost_factor *= 1.7  # 1.7ë°° ë¶€ìŠ¤íŠ¸
                    boost_reasons.append(f"ëª¨ë¸_ë§¤ì¹­_{model}")
            
            # ê³ ìœ ëª…ì‚¬ ë§¤ì¹­
            for name in keywords['proper_nouns']:
                if name in content or name in filename:
                    boost_factor *= 1.5  # 1.5ë°° ë¶€ìŠ¤íŠ¸
                    boost_reasons.append(f"ì´ë¦„_ë§¤ì¹­_{name}")
            
            # ë¶€ìŠ¤íŠ¸ ì ìš©
            if boost_factor > 1.0:
                if 'hybrid_score' in result:
                    result['hybrid_score'] *= boost_factor
                result['keyword_boosted'] = True
                result['boost_factor'] = boost_factor
                result['boost_reasons'] = boost_reasons
        
        # ì ìˆ˜ìˆœ ì¬ì •ë ¬
        results.sort(key=lambda x: x.get('hybrid_score', 0), reverse=True)
        
        # ìˆœìœ„ ì¬ë¶€ì—¬
        for i, result in enumerate(results):
            result['rank'] = i + 1
        
        return results
    
    def search(self, query: str, top_k: int = 5, include_debug: bool = False) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ë¬¸ì„œë³„ ê²€ìƒ‰ ë³´ì¥ í¬í•¨)"""
        start_time = time.time()
        
        try:
            # 0. íŠ¹ì • ë¬¸ì„œ ì§€ì • í™•ì¸ (ìµœìš°ì„ )
            target_document = self._extract_document_name_from_query(query)
            if target_document:
                self.logger.info(f"ë¬¸ì„œë³„ ê²€ìƒ‰ ëª¨ë“œ: {target_document}")
                return self._search_specific_document(query, target_document, top_k, include_debug)
            
            # 1. ì¿¼ë¦¬ì—ì„œ ë…„ë„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
            target_year = self._extract_year_from_query(query)
            keywords = self._extract_keywords_from_query(query)
            # 1. Query Expansion (ì„ íƒì )
            expansion_time = 0.0
            expanded_queries = [query]  # ì›ë³¸ ì¿¼ë¦¬ëŠ” í•­ìƒ í¬í•¨
            expansion_info = None
            
            if self.use_query_expansion and self.query_expander:
                expansion_start = time.time()
                expansion_result = self.query_expander.expand_query(query)
                expansion_time = time.time() - expansion_start
                
                # í™•ì¥ëœ ì¿¼ë¦¬ ì¶”ê°€ (ìƒìœ„ ëª‡ ê°œë§Œ)
                expanded_queries.extend(expansion_result['expanded_queries'][:2])  # ìµœëŒ€ 2ê°œ ì¶”ê°€
                expansion_info = {
                    'total_expansions': len(expansion_result['expanded_queries']),
                    'used_expansions': min(2, len(expansion_result['expanded_queries'])),
                    'expansion_time': expansion_time
                }
                self.logger.info(f"Query expansion: {len(expanded_queries)}ê°œ ì¿¼ë¦¬ ìƒì„± (ì‹œê°„: {expansion_time:.3f}ì´ˆ)")
            
            # 2. ì¿¼ë¦¬ ì „ì²˜ë¦¬: í•œêµ­ì–´ ì¡°ì‚¬ ì œê±° (ëª¨ë“  ì¿¼ë¦¬ì— ì ìš©)
            cleaned_queries = []
            for q in expanded_queries:
                cleaned = self.query_optimizer.clean_query_for_search(q)
                cleaned_queries.append(cleaned)
            
            self.logger.info(f"Query cleaning: {len(cleaned_queries)}ê°œ ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì™„ë£Œ")
            
            # 3. ê°œë³„ ê²€ìƒ‰ ìˆ˜í–‰ (í™•ì¥ëœ ì¿¼ë¦¬ë“¤ë¡œ)
            all_vector_results = []
            all_bm25_results = []
            
            vector_start = time.time()
            for cleaned_query in cleaned_queries:
                vector_results = self.vector_store.search(cleaned_query, top_k=top_k * 2)
                all_vector_results.extend(vector_results)
            vector_time = time.time() - vector_start
            
            bm25_start = time.time()
            for cleaned_query in cleaned_queries:
                bm25_results = self.bm25_store.search(cleaned_query, top_k=top_k * 2)
                all_bm25_results.extend(bm25_results)
            bm25_time = time.time() - bm25_start
            
            # ì¤‘ë³µ ì œê±° (chunk_id ê¸°ì¤€)
            seen_vector = set()
            unique_vector_results = []
            for result in all_vector_results:
                chunk_id = result.get('chunk_id', result.get('doc_id', ''))
                if chunk_id not in seen_vector:
                    seen_vector.add(chunk_id)
                    unique_vector_results.append(result)
            
            seen_bm25 = set()
            unique_bm25_results = []
            for result in all_bm25_results:
                chunk_id = result.get('chunk_id', result.get('doc_id', ''))
                if chunk_id not in seen_bm25:
                    seen_bm25.add(chunk_id)
                    unique_bm25_results.append(result)
            
            # 4. ë‹¤ë‹¨ê³„ í•„í„°ë§ ì ìš© (ì‹ ê·œ)
            multilevel_stats = None
            if self.use_multilevel_filter and self.multilevel_filter:
                # ë” ë§ì€ í›„ë³´ í™•ë³´ (ê¸°ì¡´ì˜ 3ë°° â†’ ëŒ€ìš©ëŸ‰ í•„í„°ë§ìš©ìœ¼ë¡œ 10ë°°)
                vector_results_large = unique_vector_results[:top_k * 10]
                bm25_results_large = unique_bm25_results[:top_k * 10]

                # ìŠ¤í‚¤ë§ˆ ì •ê·œí™” (multilevel_filter ì‹¤í–‰ ì „ì— í•„ìˆ˜!)
                normalized_vector = []
                for result in vector_results_large:
                    result_id = result.get('chunk_id') or result.get('doc_id') or result.get('id') or result.get('filename', 'unknown')
                    normalized_vector.append({
                        'chunk_id': result_id,
                        'doc_id': result_id,
                        **result
                    })

                normalized_bm25 = []
                for result in bm25_results_large:
                    result_id = result.get('chunk_id') or result.get('doc_id') or result.get('id') or result.get('filename', 'unknown')
                    normalized_bm25.append({
                        'chunk_id': result_id,
                        'doc_id': result_id,
                        **result
                    })

                # ìŠ¤ì½”ì–´ ì •ê·œí™”
                vector_results_large = self._normalize_scores(normalized_vector, 'similarity')
                bm25_results_large = self._normalize_scores(normalized_bm25, 'score')
                
                # ë‹¤ë‹¨ê³„ í•„í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                multilevel_start = time.time()
                filtered_results, multilevel_stats = self.multilevel_filter.process_full_pipeline(
                    vector_results=vector_results_large,
                    bm25_results=bm25_results_large,
                    query=query,
                    reranker=self.reranker if self.use_reranker else None
                )
                multilevel_time = time.time() - multilevel_start
                
                # FilterResultë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                fused_results = []
                for filter_result in filtered_results:
                    # ì›ë³¸ vector/bm25 ê²°ê³¼ì—ì„œ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    original_result = None
                    for vr in vector_results_large:
                        if vr.get('chunk_id') == filter_result.chunk_id:
                            original_result = vr
                            break
                    if not original_result:
                        for br in bm25_results_large:
                            if br.get('chunk_id') == filter_result.chunk_id:
                                original_result = br
                                break
                    
                    result_dict = {
                        'chunk_id': filter_result.chunk_id,
                        'content': filter_result.content,
                        'hybrid_score': filter_result.score,
                        'multilevel_phase': filter_result.phase,
                        'filter_reasoning': filter_result.reasoning,
                        'metadata': filter_result.metadata
                    }
                    
                    # ì›ë³¸ ê²°ê³¼ì˜ ì¶”ê°€ í•„ë“œë“¤ ë³‘í•©
                    if original_result:
                        result_dict.update({
                            k: v for k, v in original_result.items() 
                            if k not in result_dict
                        })
                    
                    fused_results.append(result_dict)
                
                # ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸
                multilevel_stats['multilevel_time'] = multilevel_time
                fusion_time = multilevel_stats.get('total_processing_time', multilevel_time)
                rerank_time = 0.0  # ë‹¤ë‹¨ê³„ í•„í„°ë§ì— í¬í•¨ë¨
                
                self.logger.info(f"ë‹¤ë‹¨ê³„ í•„í„°ë§ ì™„ë£Œ: {len(vector_results_large)} â†’ {len(fused_results)} (ì‹œê°„: {multilevel_time:.3f}ì´ˆ)")
            
            else:
                # ê¸°ì¡´ ë°©ì‹ (ë‹¤ë‹¨ê³„ í•„í„°ë§ ë¹„í™œì„±í™” ì‹œ)
                vector_results = unique_vector_results[:top_k * 3]
                bm25_results = unique_bm25_results[:top_k * 3]
                
                # ìŠ¤ì½”ì–´ ì •ê·œí™”
                vector_results = self._normalize_scores(vector_results, 'similarity')
                bm25_results = self._normalize_scores(bm25_results, 'score')
                
                # ìœµí•© ë°©ë²•ì— ë”°ë¼ ê²°ê³¼ ìœµí•©
                fusion_start = time.time()
                if self.fusion_method == "weighted_sum":
                    fused_results = self._fuse_with_weighted_sum(vector_results, bm25_results)
                else:
                    fused_results = self._reciprocal_rank_fusion(vector_results, bm25_results)
                fusion_time = time.time() - fusion_start
                
                # Reranker ì ìš© (ì„ íƒì )
                rerank_time = 0.0
                if self.use_reranker and self.reranker and len(fused_results) > 1:
                    rerank_start = time.time()
                    fused_results = self.reranker.rerank_documents(query, fused_results[:top_k * 2], top_k)
                    rerank_time = time.time() - rerank_start
                    self.logger.info(f"Reranker ì ìš© ì™„ë£Œ (ì‹œê°„: {rerank_time:.3f}ì´ˆ)")
                else:
                    fused_results = fused_results[:top_k]
            
            # 5. Document Compression ì ìš© (ì„ íƒì )
            compression_time = 0.0
            compression_info = None
            
            if self.use_document_compression and self.doc_compressor and fused_results:
                compression_start = time.time()
                
                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document Compressionì´ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                documents_for_compression = []
                for result in fused_results:
                    doc = {
                        'content': result.get('content', ''),
                        'filename': result.get('filename', ''),
                        'score': result.get('hybrid_score', result.get('score', 0.0)),
                        # ê¸°íƒ€ ë©”íƒ€ë°ì´í„° ìœ ì§€
                        'doc_id': result.get('doc_id', ''),
                        'chunk_id': result.get('chunk_id', ''),
                        'metadata': result.get('metadata', {})
                    }
                    documents_for_compression.append(doc)
                
                # ë¬¸ì„œ ì••ì¶• ìˆ˜í–‰
                compression_result = self.doc_compressor.compress_documents(
                    documents=documents_for_compression,
                    query=query,
                    target_length=500,  # ë¬¸ì„œë‹¹ ëª©í‘œ ê¸¸ì´
                    compression_ratio=0.7  # 70% ìœ ì§€
                )
                
                compression_time = time.time() - compression_start
                
                # ì••ì¶•ëœ ê²°ê³¼ë¥¼ ì›ë˜ í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜
                if compression_result['compressed_documents']:
                    compressed_final_results = []
                    for i, compressed_doc in enumerate(compression_result['compressed_documents']):
                        # ì›ë³¸ ê²°ê³¼ì—ì„œ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                        original_result = fused_results[i] if i < len(fused_results) else {}
                        
                        compressed_result = {
                            **original_result,  # ì›ë³¸ í•„ë“œë“¤ ìœ ì§€
                            'content': compressed_doc['content'],  # ì••ì¶•ëœ ë‚´ìš©ìœ¼ë¡œ ëŒ€ì²´
                            'original_content_length': compressed_doc.get('original_length', 0),
                            'compressed_content_length': compressed_doc.get('compressed_length', 0),
                            'compression_ratio': compressed_doc.get('compression_ratio', 1.0),
                            'sentences_kept': compressed_doc.get('sentences_kept', 0),
                            'sentences_total': compressed_doc.get('sentences_total', 0)
                        }
                        compressed_final_results.append(compressed_result)
                    
                    final_results = compressed_final_results
                    compression_info = {
                        'applied': True,
                        'original_total_chars': compression_result['compression_stats']['original_total_chars'],
                        'compressed_total_chars': compression_result['compression_stats']['compressed_total_chars'],
                        'compression_ratio': compression_result['compression_stats']['compression_ratio'],
                        'processing_time': compression_time
                    }
                    self.logger.info(f"Document compression ì ìš© ì™„ë£Œ (ì••ì¶•ë¥ : {compression_info['compression_ratio']:.2f}, ì‹œê°„: {compression_time:.3f}ì´ˆ)")
                else:
                    final_results = fused_results
                    compression_info = {'applied': False, 'reason': 'compression_failed'}
            else:
                final_results = fused_results
                compression_info = {'applied': False, 'reason': 'disabled_or_no_results'}
            
            # 6. ë‚ ì§œ ê¸°ë°˜ í•„í„°ë§ ì ìš© (ìƒˆë¡œ ì¶”ê°€)
            if target_year:
                final_results = self._apply_date_filtering(final_results, target_year)
                self.logger.info(f"ë‚ ì§œ í•„í„°ë§ ì ìš©: {target_year}ë…„ ë¬¸ì„œ ìš°ì„ ìˆœìœ„ ë¶€ìŠ¤íŠ¸")
            
            # 7. í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­ ë¶€ìŠ¤íŠ¸ ì ìš© (ìƒˆë¡œ ì¶”ê°€)
            if any(keywords.values()):
                final_results = self._apply_keyword_boosting(final_results, keywords)
                self.logger.info(f"í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… ì ìš©: {sum(len(v) for v in keywords.values())}ê°œ í‚¤ì›Œë“œ")
            
            # 8. ë‹¨ì¼ ë¬¸ì„œ ëª¨ë“œ ì ìš© (í™˜ê° ë°©ì§€ ê°•í™”)
            if self.single_document_mode and final_results:
                # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë¬¸ì„œë§Œ ì„ íƒ
                best_document_source = final_results[0].get('source', '')
                best_document_score = final_results[0].get('score', 0.0)
                single_doc_results = []

                # ì ìˆ˜ ì„ê³„ê°’ ì„¤ì • (ë„ˆë¬´ ë‚®ì€ ì ìˆ˜ ë¬¸ì„œëŠ” ì œì™¸)
                score_threshold = max(0.1, best_document_score * 0.7)

                for result in final_results:
                    if (result.get('source', '') == best_document_source and
                        result.get('score', 0.0) >= score_threshold):
                        single_doc_results.append(result)

                    # ìµœëŒ€ 3ê°œ ì²­í¬ë§Œ (ê°™ì€ ë¬¸ì„œì—ì„œ ê³ í’ˆì§ˆë§Œ)
                    if len(single_doc_results) >= 3:
                        break

                # ë‹¨ì¼ ë¬¸ì„œì—ì„œ ì¶©ë¶„í•œ ì²­í¬ê°€ ìˆì„ ë•Œë§Œ ì ìš©
                if len(single_doc_results) >= 2:
                    final_results = single_doc_results
                    self.logger.info(f"ë‹¨ì¼ ë¬¸ì„œ ëª¨ë“œ ì ìš©: {best_document_source} - {len(final_results)}ê°œ ì²­í¬ (ì ìˆ˜ ì„ê³„ê°’: {score_threshold:.3f})")
                else:
                    self.logger.info(f"ë‹¨ì¼ ë¬¸ì„œ ëª¨ë“œ ì·¨ì†Œ: ì¶©ë¶„í•œ ì²­í¬ ì—†ìŒ ({len(single_doc_results)}ê°œ < 2ê°œ)")
                    # ì›ë˜ ê²°ê³¼ ìœ ì§€í•˜ë˜ ìƒìœ„ 3ê°œë¡œ ì œí•œ
                    final_results = final_results[:3]

            # 9. ìŠ¤í‚¤ë§ˆ ì •ê·œí™”: chunk_id, doc_id, snippet, page, meta ë³´ì¥
            normalized_results = []
            for result in final_results:
                # ID ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: chunk_id > doc_id > id > filename)
                result_id = result.get('chunk_id') or result.get('doc_id') or result.get('id') or result.get('filename', 'unknown')

                normalized = {
                    'chunk_id': result_id,
                    'doc_id': result_id,
                    'snippet': result.get('snippet') or result.get('content', ''),
                    'page': result.get('page', 0),
                    'meta': result.get('meta') or result.get('metadata', {}),
                    # ê¸°ì¡´ í•„ë“œ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
                    **{k: v for k, v in result.items() if k not in ['chunk_id', 'doc_id', 'snippet', 'page', 'meta']}
                }
                normalized_results.append(normalized)

            total_time = time.time() - start_time

            search_result = {
                'fused_results': normalized_results,
                'search_time': total_time,
                'timing': {
                    'expansion_time': expansion_time,
                    'vector_time': vector_time,
                    'bm25_time': bm25_time,
                    'fusion_time': fusion_time,
                    'rerank_time': rerank_time,
                    'compression_time': compression_time,
                    'total_time': total_time
                }
            }
            
            # ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€
            if include_debug:
                debug_info = {
                    'vector_results': vector_results[:top_k],
                    'bm25_results': bm25_results[:top_k],
                    'fusion_method': 'RRF',
                    'weights': {
                        'vector': self.vector_weight,
                        'bm25': self.bm25_weight
                    },
                    'parameters': {
                        'rrf_k': self.rrf_k
                    },
                    'reranker_enabled': self.use_reranker,
                    'reranker_applied': rerank_time > 0.0,
                    'query_expansion_enabled': self.use_query_expansion,
                    'document_compression_enabled': self.use_document_compression,
                    'multilevel_filter_enabled': self.use_multilevel_filter
                }
                
                # Query Expansion ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€
                if expansion_info:
                    debug_info['query_expansion'] = expansion_info
                    debug_info['expanded_queries'] = expanded_queries
                
                # Document Compression ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€  
                if compression_info:
                    debug_info['document_compression'] = compression_info
                
                # Multilevel Filter ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€
                if multilevel_stats:
                    debug_info['multilevel_filter'] = multilevel_stats
                
                search_result.update(debug_info)
            
            return search_result
            
        except Exception as e:
            self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {
                'fused_results': [],
                'search_time': time.time() - start_time,
                'error': str(e)
            }
    
    def should_use_full_document_mode(self, query: str) -> Tuple[bool, str]:
        """ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ ì „ì²´ ë¬¸ì„œ ëª¨ë“œë¥¼ ì‚¬ìš©í• ì§€ ê²°ì •"""
        
        # ì „ì²´ ë¬¸ì„œ ëª¨ë“œ íŠ¸ë¦¬ê±° í‚¤ì›Œë“œ
        full_document_keywords = [
            'ìì„¸íˆ', 'ìƒì„¸íˆ', 'ì„¸ë¶€ì‚¬í•­', 'ëª¨ë“ ', 'ì „ì²´', 'ëª¨ë‘', 
            'ìš”ì•½', 'ì •ë¦¬', 'ë‚´ìš©', 'ë¹ ì§ì—†ì´', 'ì™„ì „íˆ', 'ìƒì„¸í•˜ê²Œ',
            'ë””í…Œì¼', 'êµ¬ì²´ì ìœ¼ë¡œ', 'í¬ê´„ì ìœ¼ë¡œ', 'ì´ì •ë¦¬', 'ì „ë°˜ì ìœ¼ë¡œ',
            'ì „ë¬¸', 'ì™„ì „í•œ', 'ìƒì„¸í•œ', 'ìì„¸í•œ', 'ì¢…í•©ì ìœ¼ë¡œ'
        ]
        
        # ë¬¸ì„œ ìš”ì•½ í‚¤ì›Œë“œ
        summary_keywords = [
            'ìš”ì•½', 'ì •ë¦¬', 'ê°œìš”', 'ì´ì •ë¦¬', 'ì¢…í•©', 'ì •ë¦¬í•´ì„œ', 'ìš”ì•½í•´ì„œ',
            'ìš”ì•½í•´ì¤˜', 'ì •ë¦¬í•´ì¤˜', 'ì¢…í•©í•´ì„œ'
        ]
        
        # 1. ì „ì²´ ë¬¸ì„œ í‚¤ì›Œë“œ ì²´í¬
        for keyword in full_document_keywords:
            if keyword in query:
                return True, f"í‚¤ì›Œë“œ '{keyword}' ê°ì§€"
        
        # 2. ë¬¸ì„œ ìš”ì•½ ìš”ì²­ ì²´í¬
        for keyword in summary_keywords:
            if keyword in query:
                return True, f"ìš”ì•½ ìš”ì²­ '{keyword}' ê°ì§€"
        
        # 3. ì§ˆë¬¸ ê¸¸ì´ ì²´í¬ (ê¸´ ì§ˆë¬¸ì€ ë³µì¡í•œ ì •ë³´ ìš”êµ¬)
        if len(query) > 50:
            return True, "ë³µì¡í•œ ì§ˆë¬¸ (50ì ì´ìƒ)"
        
        # 4. ë‹¤ì¤‘ ì¡°ê±´ ì²´í¬ ('ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ê°ê°' ë“±)
        multi_conditions = ['ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ê°ê°', 'ëª¨ë‘', 'ì „ë¶€', 'ëª¨ë“ ']
        for cond in multi_conditions:
            if cond in query:
                return True, f"ë‹¤ì¤‘ ì¡°ê±´ '{cond}' ê°ì§€"
                
        return False, "ê°„ë‹¨í•œ ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸"
    
    def load_full_document(self, file_path: str) -> Optional[str]:
        """ì „ì²´ PDF ë¬¸ì„œ ë‚´ìš© ë¡œë“œ"""
        if pdfplumber is None:
            self.logger.error("pdfplumberê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install pdfplumber' ì‹¤í–‰ í•„ìš”")
            return None
        
        try:
            full_text = ""
            
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    if page_text:
                        full_text += f"{page_text}\n"
            
            return full_text.strip() if full_text.strip() else None
        except Exception as e:
            self.logger.error(f"PDF ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")
            return None
    
    def search_with_smart_mode(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """ìŠ¤ë§ˆíŠ¸ ëª¨ë“œ: ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì²­í¬ ê²€ìƒ‰ vs ì „ì²´ ë¬¸ì„œ ì„ íƒ"""
        
        # ëª¨ë“œ ê²°ì •
        use_full_mode, reason = self.should_use_full_document_mode(query)
        
        self.logger.info(f"ìŠ¤ë§ˆíŠ¸ ëª¨ë“œ ì„ íƒ: {'ì „ì²´ ë¬¸ì„œ' if use_full_mode else 'ì²­í¬ ê²€ìƒ‰'} ({reason})")
        
        if use_full_mode:
            return self._search_with_full_document(query)
        else:
            # ê¸°ì¡´ ì²­í¬ ê²€ìƒ‰
            search_results = self.search(query, top_k=top_k)
            return {
                'mode': 'chunk_search',
                'reason': reason,
                'search_results': search_results,
                'success': len(search_results.get('fused_results', [])) > 0
            }
    
    def _search_with_full_document(self, query: str) -> Dict[str, Any]:
        """ì „ì²´ ë¬¸ì„œ ëª¨ë“œë¡œ ê²€ìƒ‰"""
        
        # ë¨¼ì € ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° (ë‹¨ì¼ ë¬¸ì„œ ëª¨ë“œ)
        search_results = self.search(query, top_k=1)
        context_chunks = search_results.get('fused_results', [])
        
        if not context_chunks:
            return {
                'mode': 'full_document',
                'reason': 'ì „ì²´ ë¬¸ì„œ ëª¨ë“œ',
                'success': False,
                'error': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }
        
        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš© ë¡œë“œ
        best_chunk = context_chunks[0]
        file_path = best_chunk.get('file_path', best_chunk.get('source', ''))
        
        if not file_path or not os.path.exists(file_path):
            return {
                'mode': 'full_document',
                'reason': 'ì „ì²´ ë¬¸ì„œ ëª¨ë“œ',
                'success': False,
                'error': 'ë¬¸ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }
        
        # ì „ì²´ ë¬¸ì„œ ë¡œë“œ
        full_document_text = self.load_full_document(file_path)
        
        if not full_document_text:
            return {
                'mode': 'full_document',
                'reason': 'ì „ì²´ ë¬¸ì„œ ëª¨ë“œ',
                'success': False,
                'error': 'ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            }
        
        self.logger.info(f"ì „ì²´ ë¬¸ì„œ ë¡œë“œ: {Path(file_path).name} ({len(full_document_text):,}ì)")
        
        # ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í° ì²­í¬ë¡œ êµ¬ì„±
        full_doc_chunk = {
            'source': file_path,
            'content': full_document_text,
            'score': 1.0,  # ìµœê³  ì ìˆ˜
            'metadata': best_chunk.get('metadata', {}),
            'rank': 1
        }
        
        return {
            'mode': 'full_document',
            'reason': 'ì „ì²´ ë¬¸ì„œ ëª¨ë“œ',
            'success': True,
            'document_path': file_path,
            'document_length': len(full_document_text),
            'full_document_chunk': full_doc_chunk,
            'search_results': {
                'fused_results': [full_doc_chunk],
                'total_results': 1,
                'timing': {'total_time': 0.0}
            }
        }
    
    def _extract_document_name_from_query(self, query: str) -> Optional[str]:
        """ì¿¼ë¦¬ì—ì„œ íŠ¹ì • PDF íŒŒì¼ëª… ì¶”ì¶œ"""
        # PDF íŒŒì¼ëª… íŒ¨í„´ ì¶”ì¶œ (í™•ì¥ì í¬í•¨/ë¯¸í¬í•¨ ëª¨ë‘)
        pdf_patterns = [
            r'(\d{4}[-_]\d{2}[-_]\d{2}[^.]*\.pdf)',  # ì™„ì „í•œ PDF íŒŒì¼ëª…
            r'(\d{4}[-_]\d{2}[-_]\d{2}[^.\s]*)',     # í™•ì¥ì ì—†ëŠ” íŒŒì¼ëª…
        ]
        
        for pattern in pdf_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                filename = matches[0]
                if not filename.endswith('.pdf'):
                    filename += '.pdf'
                return filename
        
        return None
    
    def _search_specific_document(self, query: str, target_document: str, top_k: int, include_debug: bool) -> Dict[str, Any]:
        """íŠ¹ì • ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰ ìˆ˜í–‰ (ì¼ê´€ì„± ë³´ì¥)"""
        start_time = time.time()
        
        try:
            # ë²¡í„° ê²€ìƒ‰ì—ì„œ íŠ¹ì • ë¬¸ì„œ í•„í„°ë§
            vector_results = []
            if self.vector_store and self.vector_store.index.ntotal > 0:
                all_vector_results = self.vector_store.search(query, top_k=50)  # ë„‰ë„‰íˆ ê²€ìƒ‰
                for result in all_vector_results:
                    if result.get('filename', '').lower() == target_document.lower():
                        vector_results.append(result)
                vector_results = vector_results[:top_k]  # ìƒìœ„ Kê°œë§Œ
            
            # BM25 ê²€ìƒ‰ì—ì„œ íŠ¹ì • ë¬¸ì„œ í•„í„°ë§
            bm25_results = []
            if self.bm25_store:
                all_bm25_results = self.bm25_store.search(query, top_k=50)
                for result in all_bm25_results:
                    if result.get('filename', '').lower() == target_document.lower():
                        bm25_results.append(result)
                bm25_results = bm25_results[:top_k]
            
            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°˜í™˜
            if not vector_results and not bm25_results:
                self.logger.error(f"ì§€ì •ëœ ë¬¸ì„œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {target_document}")
                return {
                    'fused_results': [],
                    'vector_results': [],
                    'bm25_results': [],
                    'processing_time': time.time() - start_time,
                    'error': f"ë¬¸ì„œ '{target_document}'ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    'target_document': target_document
                }
            
            # ì ìˆ˜ ìœµí•© (íŠ¹ì • ë¬¸ì„œ ë‚´ì—ì„œë§Œ)
            if self.fusion_method == "weighted_sum":
                fused_results = self._fuse_with_weighted_sum(vector_results, bm25_results)
            else:
                fused_results = self._reciprocal_rank_fusion(vector_results, bm25_results)
            
            # top_kë¡œ ì œí•œ
            fused_results = fused_results[:top_k]
            
            # ëª¨ë“  ê²°ê³¼ê°€ ì§€ì •ëœ ë¬¸ì„œì—ì„œ ë‚˜ì™”ëŠ”ì§€ ê²€ì¦
            for result in fused_results:
                if result.get('filename', '').lower() != target_document.lower():
                    self.logger.warning(f"ë¬¸ì„œ í•„í„°ë§ ì‹¤íŒ¨: {result.get('filename')} != {target_document}")

            # ìŠ¤í‚¤ë§ˆ ì •ê·œí™” ì ìš©
            normalized_results = []
            for result in fused_results:
                # ID ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: chunk_id > doc_id > id > filename)
                result_id = result.get('chunk_id') or result.get('doc_id') or result.get('id') or result.get('filename', 'unknown')

                normalized = {
                    'chunk_id': result_id,
                    'doc_id': result_id,
                    'snippet': result.get('snippet') or result.get('content', ''),
                    'page': result.get('page', 0),
                    'meta': result.get('meta') or result.get('metadata', {}),
                    **{k: v for k, v in result.items() if k not in ['chunk_id', 'doc_id', 'snippet', 'page', 'meta']}
                }
                normalized_results.append(normalized)

            return {
                'fused_results': normalized_results,
                'vector_results': vector_results,
                'bm25_results': bm25_results,
                'processing_time': time.time() - start_time,
                'target_document': target_document,
                'document_specific_search': True
            }
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œë³„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {
                'fused_results': [],
                'vector_results': [],
                'bm25_results': [],
                'processing_time': time.time() - start_time,
                'error': str(e),
                'target_document': target_document
            }
    
    def get_stats(self) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í†µê³„"""
        try:
            vector_stats = self.vector_store.get_stats()
            bm25_stats = self.bm25_store.get_stats()
            
            return {
                'hybrid_config': {
                    'vector_weight': self.vector_weight,
                    'bm25_weight': self.bm25_weight,
                    'rrf_k': self.rrf_k
                },
                'vector_store': vector_stats,
                'bm25_store': bm25_stats
            }
        except Exception as e:
            self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def update_weights(self, vector_weight: float, bm25_weight: float):
        """ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        if vector_weight + bm25_weight != 1.0:
            self.logger.warning(f"ê°€ì¤‘ì¹˜ í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤: {vector_weight + bm25_weight}")
        
        self.vector_weight = vector_weight
        self.bm25_weight = bm25_weight
        self.logger.info(f"ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸: ë²¡í„°={vector_weight}, BM25={bm25_weight}")

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_hybrid_search():
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "í•€ë§ˆì´í¬ ëª¨ë¸ ECM-77BCë¥¼ êµ¬ë§¤ ê²€í† í•©ë‹ˆë‹¤. ê°€ê²©ì€ 336,000ì›ì…ë‹ˆë‹¤.",
        "ì˜ìƒí¸ì§‘íŒ€ ì›Œí¬ìŠ¤í…Œì´ì…˜ êµì²´ ë¹„ìš©ì€ 179,300,000ì›ì…ë‹ˆë‹¤. HP Z8 ëª¨ë¸ì…ë‹ˆë‹¤.",
        "ê´‘í™”ë¬¸ ìŠ¤íŠœë””ì˜¤ ëª¨ë‹ˆí„° êµì²´ ì´ì•¡ì€ 9,760,000ì›ì…ë‹ˆë‹¤. LG ëª¨ë‹ˆí„° 3ëŒ€ì…ë‹ˆë‹¤.",
        "ì¹´ë©”ë¼ ì¥ë¹„ êµ¬ë§¤ë¥¼ ìœ„í•œ ì˜ˆì‚° ê²€í† ì„œì…ë‹ˆë‹¤. ì´ ì˜ˆì‚°ì€ 50,000,000ì›ì…ë‹ˆë‹¤.",
        "ìŠ¤íŠœë””ì˜¤ ì¡°ëª… ì‹œì„¤ êµì²´ì— ëŒ€í•œ ê¸°ì•ˆì„œì…ë‹ˆë‹¤. í•„ë¦½ìŠ¤ LED ì¡°ëª… 20ëŒ€ë¥¼ êµ¬ë§¤í•©ë‹ˆë‹¤."
    ]
    
    test_metadatas = [
        {
            'doc_id': 'doc1',
            'chunk_id': 'chunk1_1',
            'filename': '2021-05-13_í•€ë§ˆì´í¬_êµ¬ë§¤ê²€í† .pdf',
            'content': test_texts[0],
            'metadata': {'doc_type': 'ê¸°ì•ˆì„œ', 'amount': 336000}
        },
        {
            'doc_id': 'doc2',
            'chunk_id': 'chunk2_1',
            'filename': '2022-02-03_ì›Œí¬ìŠ¤í…Œì´ì…˜_êµì²´ê²€í† .pdf',
            'content': test_texts[1],
            'metadata': {'doc_type': 'ê²€í† ì„œ', 'amount': 179300000}
        },
        {
            'doc_id': 'doc3',
            'chunk_id': 'chunk3_1',
            'filename': '2025-01-09_ëª¨ë‹ˆí„°_êµì²´ê²€í† .pdf',
            'content': test_texts[2],
            'metadata': {'doc_type': 'ê¸°ì•ˆì„œ', 'amount': 9760000}
        },
        {
            'doc_id': 'doc4',
            'chunk_id': 'chunk4_1',
            'filename': '2023-07-15_ì¹´ë©”ë¼ì¥ë¹„_ì˜ˆì‚°ê²€í† .pdf',
            'content': test_texts[3],
            'metadata': {'doc_type': 'ì˜ˆì‚°ì„œ', 'amount': 50000000}
        },
        {
            'doc_id': 'doc5',
            'chunk_id': 'chunk5_1',
            'filename': '2024-03-20_ì¡°ëª…ì‹œì„¤_êµì²´ê¸°ì•ˆ.pdf',
            'content': test_texts[4],
            'metadata': {'doc_type': 'ê¸°ì•ˆì„œ', 'amount': 0}
        }
    ]
    
    try:
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ìƒì„± (.env ì„¤ì • ì‚¬ìš©)
        hybrid = HybridSearch(
            vector_index_path="rag_system/db/test_hybrid_vector.faiss",
            bm25_index_path="rag_system/db/test_hybrid_bm25.pkl",
            vector_weight=DEFAULT_VECTOR_WEIGHT,
            bm25_weight=DEFAULT_BM25_WEIGHT
        )
        
        # ë¬¸ì„œ ì¶”ê°€
        print("ğŸ“ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€ ì¤‘...")
        hybrid.add_documents(test_texts, test_metadatas)
        
        # ì¸ë±ìŠ¤ ì €ì¥
        hybrid.save_indexes()
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_queries = [
            "í•€ë§ˆì´í¬ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?",
            "ì›Œí¬ìŠ¤í…Œì´ì…˜ HP Z8",
            "ëª¨ë‹ˆí„° 3ëŒ€",
            "ì¹´ë©”ë¼ ì˜ˆì‚° 5000ë§Œì›",
            "LED ì¡°ëª… êµì²´"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: '{query}'")
            results = hybrid.search(query, top_k=3, include_debug=True)
            
            print(f"  ê²€ìƒ‰ ì‹œê°„: {results['timing']['total_time']:.3f}ì´ˆ")
            print(f"    - ë²¡í„°: {results['timing']['vector_time']:.3f}ì´ˆ")
            print(f"    - BM25: {results['timing']['bm25_time']:.3f}ì´ˆ") 
            print(f"    - ìœµí•©: {results['timing']['fusion_time']:.3f}ì´ˆ")
            
            for result in results['fused_results']:
                print(f"  ìˆœìœ„ {result['rank']}: {result['filename']}")
                print(f"    í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´: {result['hybrid_score']:.4f}")
                print(f"    ë²¡í„° ìŠ¤ì½”ì–´: {result['vector_score']:.3f}")
                print(f"    BM25 ìŠ¤ì½”ì–´: {result['bm25_score']:.3f}")
                print(f"    ë‚´ìš©: {result['content'][:40]}...")
        
        # í†µê³„ ì¶œë ¥
        stats = hybrid.get_stats()
        print(f"\nğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í†µê³„:")
        print(f"  ê°€ì¤‘ì¹˜: ë²¡í„°={stats['hybrid_config']['vector_weight']}, BM25={stats['hybrid_config']['bm25_weight']}")
        print(f"  RRF íŒŒë¼ë¯¸í„°: k={stats['hybrid_config']['rrf_k']}")
        print(f"  ë²¡í„° ë¬¸ì„œ ìˆ˜: {stats['vector_store']['total_vectors']}")
        print(f"  BM25 ë¬¸ì„œ ìˆ˜: {stats['bm25_store']['total_documents']}")
        
        print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)
    test_hybrid_search()