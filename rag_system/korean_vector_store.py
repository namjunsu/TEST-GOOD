"""
í•œêµ­ì–´ íŠ¹í™” ë²¡í„° ì„ë² ë”© ë° FAISS ë²¡í„° ìŠ¤í† ì–´ êµ¬í˜„
jhgan/ko-sroberta-multitask ëª¨ë¸ ì‚¬ìš©
"""

import os
import json
import logging
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import pickle
import hashlib
from functools import lru_cache

import torch
import faiss
from sentence_transformers import SentenceTransformer

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
except ImportError:
    TfidfVectorizer = None

# ì„¤ì • ìƒìˆ˜
DEFAULT_MODEL_NAME = "jhgan/ko-sroberta-multitask"
DEFAULT_EMBEDDING_DIM = 768  # ko-sroberta-multitask ê¸°ë³¸ ì°¨ì›
DEFAULT_INDEX_PATH = "rag_system/db/korean_vector_index.faiss"
DEFAULT_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"  # ìë™ GPU ê°ì§€
MAX_BATCH_SIZE = 1024 if torch.cuda.is_available() else 512  # GPUì‹œ ë” í° ë°°ì¹˜

# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (í•œ ë²ˆë§Œ)
if "TRANSFORMERS_OFFLINE" not in os.environ:
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    os.environ["HF_HUB_OFFLINE"] = "1"

class KoreanVectorStore:
    """í•œêµ­ì–´ íŠ¹í™” FAISS ê¸°ë°˜ ë²¡í„° ìŠ¤í† ì–´"""
    
    def __init__(self, model_name: str = DEFAULT_MODEL_NAME,
                 index_path: str = None,
                 device: str = DEFAULT_DEVICE,
                 batch_size: int = MAX_BATCH_SIZE):
        self.model_name = model_name
        self.index_path = Path(index_path) if index_path else Path(DEFAULT_INDEX_PATH)
        self.device = device
        self.batch_size = batch_size
        self.metadata_path = self.index_path.with_suffix('.metadata.pkl')
        
        self.logger = logging.getLogger(__name__)
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.embedding_model = None
        self.embedding_dim = DEFAULT_EMBEDDING_DIM
        self._cache_folder = None  # ìºì‹œ í´ë” ì €ì¥
        
        # FAISS ì¸ë±ìŠ¤
        self.index = None
        self.metadata = []  # ê° ë²¡í„°ì— ëŒ€ì‘í•˜ëŠ” ë©”íƒ€ë°ì´í„°
        
        self._initialize()
    
    @lru_cache(maxsize=1)
    def _get_cache_folder(self) -> str:
        """ìºì‹œ í´ë” ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (ìºì‹œë¨)"""
        try:
            from config import SENTENCE_TRANSFORMERS_CACHE
            cache_folder = SENTENCE_TRANSFORMERS_CACHE
        except ImportError:
            cache_folder = "./models/sentence_transformers"

        os.environ["TRANSFORMERS_CACHE"] = cache_folder
        return cache_folder

    def _initialize(self):
        """ì„ë² ë”© ëª¨ë¸ ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ì™„ì „ ì˜¤í”„ë¼ì¸ ëª¨ë“œ)"""
        try:
            self.logger.info(f"í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")

            # ìºì‹œ í´ë” ì„¤ì • (í•œ ë²ˆë§Œ)
            self._cache_folder = self._get_cache_folder()
            
            try:
                # ë¡œì»¬ ê²½ë¡œì—ì„œ ì§ì ‘ ë¡œë“œ ì‹œë„
                local_model_path = f"{self._cache_folder}/{self.model_name.replace('/', '--')}"
                
                if Path(local_model_path).exists():
                    self.logger.info(f"ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©: {local_model_path}")
                    self.embedding_model = SentenceTransformer(
                        local_model_path,
                        device=self.device
                    )
                else:
                    # ë¡œì»¬ ìºì‹œ í´ë”ì—ì„œ ë¡œë“œ ì‹œë„
                    self.logger.info(f"ìºì‹œ í´ë”ì—ì„œ ë¡œë“œ ì‹œë„: {self._cache_folder}")
                    
                    self.embedding_model = SentenceTransformer(
                        self.model_name,
                        device=self.device,
                        cache_folder=self._cache_folder
                    )
                
                # ì‹¤ì œ ì„ë² ë”© ì°¨ì› í™•ì¸
                self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
                
                self.logger.info(f"í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {self.embedding_dim})")
                
            except Exception as model_error:
                self.logger.error(f"í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_error}")
                self.logger.info("í´ë°±: ë”ë¯¸ ì„ë² ë”© ëª¨ë¸ ìƒì„±")
                
                # í´ë°±: ë”ë¯¸ ëª¨ë¸ ìƒì„±
                self._create_fallback_embedder()
                self.logger.warning("ë”ë¯¸ ì„ë² ë”© ëª¨ë¸ë¡œ ë™ì‘ - ê²€ìƒ‰ í’ˆì§ˆ ì œí•œë¨")
            
            # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë˜ëŠ” ë¡œë“œ
            if self.index_path.exists() and self.metadata_path.exists():
                self.load_index()
            else:
                self.create_new_index()
                
        except Exception as e:
            self.logger.error(f"í•œêµ­ì–´ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _create_fallback_embedder(self):
        """í´ë°± ì„ë² ë”© í•¨ìˆ˜ ìƒì„± (TF-IDF ê¸°ë°˜)"""
        
        class FallbackEmbedder:
            def __init__(self, dim=DEFAULT_EMBEDDING_DIM):
                self.dim = dim
                if TfidfVectorizer:
                    self.vectorizer = TfidfVectorizer(max_features=self.dim, stop_words=None)
                else:
                    self.vectorizer = None
                self.is_fitted = False
                
            def encode(self, texts, **kwargs):
                # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© (ì¼ê´€ì„± ë³´ì¥)
                embeddings = []
                for text in texts:
                    # í…ìŠ¤íŠ¸ë¥¼ í•´ì‹œí•˜ì—¬ ë²¡í„° ìƒì„±
                    hash_obj = hashlib.sha256(str(text).encode())
                    hash_bytes = hash_obj.digest()
                    
                    # ì§€ì •ëœ ì°¨ì› ë²¡í„°ë¡œ í™•ì¥ (í•´ì‹œ ë°˜ë³µ)
                    vector = []
                    for i in range(self.dim):
                        vector.append((hash_bytes[i % len(hash_bytes)] / 255.0) - 0.5)
                    
                    embeddings.append(vector)
                
                return np.array(embeddings, dtype='float32')
            
            def get_sentence_embedding_dimension(self):
                return self.dim
        
        self.embedding_model = FallbackEmbedder()
        self.embedding_dim = DEFAULT_EMBEDDING_DIM
    
    def create_new_index(self):
        """ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¸ë±ìŠ¤ (í•œêµ­ì–´ í…ìŠ¤íŠ¸ì— ë” ì í•©)
        self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        self.metadata = []
        self.logger.info("ìƒˆ í•œêµ­ì–´ FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def save_index(self):
        """ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            # ë””ë ‰í„°ë¦¬ ìƒì„±
            self.index_path.parent.mkdir(parents=True, exist_ok=True)
            
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.index, str(self.index_path))
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            with open(self.metadata_path, 'wb') as f:
                pickle.dump(self.metadata, f)
            
            self.logger.info(f"í•œêµ­ì–´ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.index_path}")
            
        except Exception as e:
            self.logger.error(f"í•œêµ­ì–´ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            self.index = faiss.read_index(str(self.index_path))
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(self.metadata_path, 'rb') as f:
                self.metadata = pickle.load(f)
            
            self.logger.info(f"í•œêµ­ì–´ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.metadata)}ê°œ ë¬¸ì„œ")
            
        except Exception as e:
            self.logger.error(f"í•œêµ­ì–´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ì‹œ ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
            self.create_new_index()
    
    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜ (L2 ì •ê·œí™” í¬í•¨, ë°°ì¹˜ ì²˜ë¦¬)"""
        try:
            # ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
            if len(texts) > self.batch_size:
                all_embeddings = []
                for i in range(0, len(texts), self.batch_size):
                    batch = texts[i:i + self.batch_size]
                    batch_embeddings = self.embedding_model.encode(
                        batch,
                        convert_to_numpy=True,
                        show_progress_bar=False,  # ë°°ì¹˜ë³„ë¡œëŠ” í‘œì‹œ ì•ˆí•¨
                        normalize_embeddings=True,
                        batch_size=self.batch_size
                    )
                    all_embeddings.append(batch_embeddings)
                embeddings = np.vstack(all_embeddings)
            else:
                embeddings = self.embedding_model.encode(
                    texts,
                    convert_to_numpy=True,
                    show_progress_bar=True,
                    normalize_embeddings=True,  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•œ ì •ê·œí™”
                    batch_size=self.batch_size
                )
            return embeddings.astype('float32')
        except Exception as e:
            self.logger.error(f"í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            raise
    
    def add_documents(self, texts: List[str], metadatas: List[Dict[str, Any]]):
        """ë¬¸ì„œë“¤ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€"""
        if len(texts) != len(metadatas):
            raise ValueError("í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        try:
            # í…ìŠ¤íŠ¸ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
            embeddings = self.encode_texts(texts)
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(embeddings)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            self.metadata.extend(metadatas)
            
            self.logger.info(f"{len(texts)}ê°œ í•œêµ­ì–´ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ (ì´ {len(self.metadata)}ê°œ)")
            
        except Exception as e:
            self.logger.error(f"í•œêµ­ì–´ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)"""
        if self.index.ntotal == 0:
            return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© (ì •ê·œí™” í¬í•¨)
            query_embedding = self.encode_texts([query])
            
            # FAISS ê²€ìƒ‰ (ë‚´ì  ê¸°ë°˜ - ì •ê·œí™”ëœ ë²¡í„°ì—ì„œëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ì¼)
            scores, indices = self.index.search(query_embedding, top_k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.metadata) and idx != -1:  # ìœ íš¨í•œ ì¸ë±ìŠ¤ í™•ì¸
                    result = {
                        'rank': i + 1,
                        'score': float(score),  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
                        'similarity': float(score),  # ì •ê·œí™”ëœ ë²¡í„°ì—ì„œ ë‚´ì  = ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                        **self.metadata[idx]
                    }
                    results.append(result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"í•œêµ­ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´"""
        return {
            'total_vectors': self.index.ntotal if self.index else 0,
            'embedding_dim': self.embedding_dim,
            'model_name': self.model_name,
            'index_path': str(self.index_path),
            'metadata_count': len(self.metadata),
            'model_type': 'Korean Specialized (jhgan/ko-sroberta-multitask)',
            'similarity_metric': 'Cosine Similarity'
        }
    
    def rebuild_from_existing_data(self, old_vector_store_path: str = None):
        """ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë°ì´í„°ë¥¼ ìƒˆ ëª¨ë¸ë¡œ ì¬êµ¬ì¶•"""
        try:
            old_path = Path(old_vector_store_path) if old_vector_store_path else Path("rag_system/db/vector_index.faiss")
            old_metadata_path = old_path.with_suffix('.metadata.pkl')
            
            if not (old_path.exists() and old_metadata_path.exists()):
                self.logger.warning("ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤")
                return
                
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ë§Œ ë¡œë“œ (í…ìŠ¤íŠ¸ ì¶”ì¶œìš©)
            with open(old_metadata_path, 'rb') as f:
                old_metadata = pickle.load(f)
            
            if not old_metadata:
                self.logger.warning("ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return
            
            self.logger.info(f"ê¸°ì¡´ {len(old_metadata)}ê°œ ë¬¸ì„œë¥¼ ìƒˆ í•œêµ­ì–´ ëª¨ë¸ë¡œ ì¬êµ¬ì¶• ì¤‘...")
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = [item.get('content', '') for item in old_metadata if item.get('content')]
            valid_metadata = [item for item in old_metadata if item.get('content')]
            
            if texts:
                # ìƒˆ ëª¨ë¸ë¡œ ì¬ì¸ë±ì‹±
                self.create_new_index()
                self.add_documents(texts, valid_metadata)
                self.save_index()
                
                self.logger.info(f"í•œêµ­ì–´ ëª¨ë¸ ì¬êµ¬ì¶• ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
            else:
                self.logger.warning("ì¬êµ¬ì¶•í•  ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            self.logger.error(f"í•œêµ­ì–´ ëª¨ë¸ ì¬êµ¬ì¶• ì‹¤íŒ¨: {e}")
            raise

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_korean_vector_store():
    """í•œêµ­ì–´ ë²¡í„° ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” í•œêµ­ì–´ ë²¡í„° ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (í•œêµ­ì–´ ë°©ì†¡ ê¸°ìˆ  ê´€ë ¨)
    test_texts = [
        "í•€ë§ˆì´í¬ ëª¨ë¸ ECM-77BCë¥¼ êµ¬ë§¤ ê²€í† í•©ë‹ˆë‹¤. ê°€ê²©ì€ 336,000ì›ì…ë‹ˆë‹¤.",
        "ì˜ìƒí¸ì§‘íŒ€ ì›Œí¬ìŠ¤í…Œì´ì…˜ êµì²´ ë¹„ìš©ì€ 179,300,000ì›ì…ë‹ˆë‹¤. HP Z8 ëª¨ë¸ì…ë‹ˆë‹¤.",
        "ê´‘í™”ë¬¸ ìŠ¤íŠœë””ì˜¤ ëª¨ë‹ˆí„° êµì²´ ì´ì•¡ì€ 9,760,000ì›ì…ë‹ˆë‹¤. LG ëª¨ë‹ˆí„° 3ëŒ€ì…ë‹ˆë‹¤.",
        "ë°©ì†¡ ì¥ë¹„ ì¤‘ ì¹´ë©”ë¼ ë Œì¦ˆ Canon EF 70-200mmë¥¼ ì‹ ê·œ ë„ì…í•©ë‹ˆë‹¤.",
        "ìŒí–¥ ì¥ë¹„ ì—…ê·¸ë ˆì´ë“œë¥¼ ìœ„í•´ ë¯¹ì‹± ì½˜ì†”ì„ êµì²´ ê²€í† ì¤‘ì…ë‹ˆë‹¤."
    ]
    
    test_metadatas = [
        {
            'doc_id': 'doc1',
            'chunk_id': 'chunk1_1',
            'filename': '2021-05-13_í•€ë§ˆì´í¬_êµ¬ë§¤ê²€í† .pdf',
            'content': test_texts[0],
            'metadata': {'doc_type': 'ê¸°ì•ˆì„œ', 'amount': 336000}
        },
        {
            'doc_id': 'doc2', 
            'chunk_id': 'chunk2_1',
            'filename': '2022-02-03_ì›Œí¬ìŠ¤í…Œì´ì…˜_êµì²´ê²€í† .pdf',
            'content': test_texts[1],
            'metadata': {'doc_type': 'ê²€í† ì„œ', 'amount': 179300000}
        },
        {
            'doc_id': 'doc3',
            'chunk_id': 'chunk3_1', 
            'filename': '2025-01-09_ëª¨ë‹ˆí„°_êµì²´ê²€í† .pdf',
            'content': test_texts[2],
            'metadata': {'doc_type': 'ê¸°ì•ˆì„œ', 'amount': 9760000}
        },
        {
            'doc_id': 'doc4',
            'chunk_id': 'chunk4_1',
            'filename': '2023-08-15_ì¹´ë©”ë¼ë Œì¦ˆ_ë„ì….pdf',
            'content': test_texts[3],
            'metadata': {'doc_type': 'ë„ì…ì„œ', 'amount': 5000000}
        },
        {
            'doc_id': 'doc5',
            'chunk_id': 'chunk5_1',
            'filename': '2024-03-20_ìŒí–¥ì¥ë¹„_ì—…ê·¸ë ˆì´ë“œ.pdf',
            'content': test_texts[4],
            'metadata': {'doc_type': 'ê²€í† ì„œ', 'amount': 15000000}
        }
    ]
    
    try:
        # í•œêµ­ì–´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        kvs = KoreanVectorStore(index_path="rag_system/db/test_korean_vector_index.faiss")
        
        # ë¬¸ì„œ ì¶”ê°€
        print("ğŸ“ í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€ ì¤‘...")
        kvs.add_documents(test_texts, test_metadatas)
        
        # ì¸ë±ìŠ¤ ì €ì¥
        kvs.save_index()
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (í•œêµ­ì–´ ì¿¼ë¦¬)
        test_queries = [
            "í•€ë§ˆì´í¬ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?",
            "ì›Œí¬ìŠ¤í…Œì´ì…˜ ëª¨ë¸ëª…ì„ ì•Œë ¤ì£¼ì„¸ìš”", 
            "ëª¨ë‹ˆí„° ëª‡ ëŒ€ë¥¼ êµì²´í•˜ë‚˜ìš”?",
            "ì¹´ë©”ë¼ ì¥ë¹„ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤",
            "ìŒí–¥ ì¥ë¹„ ì˜ˆì‚°ì€ ì–´ëŠ ì •ë„ì¸ê°€ìš”?",
            "ê°€ì¥ ë¹„ì‹¼ ì¥ë¹„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” ê²€ìƒ‰: '{query}'")
            results = kvs.search(query, top_k=3)
            
            for result in results:
                print(f"  ìˆœìœ„ {result['rank']}: {result['filename']}")
                print(f"    ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {result['similarity']:.4f}")
                print(f"    ë‚´ìš©: {result['content'][:60]}...")
        
        # í†µê³„ ì¶œë ¥
        stats = kvs.get_stats()
        print(f"\nğŸ“Š í•œêµ­ì–´ ì¸ë±ìŠ¤ í†µê³„:")
        print(f"  ì´ ë²¡í„° ìˆ˜: {stats['total_vectors']}")
        print(f"  ì„ë² ë”© ì°¨ì›: {stats['embedding_dim']}")
        print(f"  ëª¨ë¸: {stats['model_name']}")
        print(f"  ìœ ì‚¬ë„ ë©”íŠ¸ë¦­: {stats['similarity_metric']}")
        
        print("âœ… í•œêµ­ì–´ ë²¡í„° ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ í•œêµ­ì–´ ë²¡í„° ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)
    test_korean_vector_store()