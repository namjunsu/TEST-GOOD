"""
Multi-LLM Support System 
Qwen GGUF + Llama Safetensors ëª¨ë¸ ì§€ì›
í•œêµ­ì–´ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìµœì í™”
"""

import logging
import re
import time
import gc
import yaml
import os
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
from functools import lru_cache
import weakref


# Generation ì„¤ì • ìƒìˆ˜
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 1200
DEFAULT_TOP_P = 0.9
DEFAULT_TOP_K = 40
DEFAULT_REPEAT_PENALTY = 1.1
MAX_LLM_RETRY = int(os.getenv('MAX_LLM_RETRY', '1'))  # .envì—ì„œ ì½ê¸°

# ì ì‘í˜• ê¸¸ì´ ì„¤ì • ìƒìˆ˜
ADAPTIVE_LENGTH_ENABLED = True
LENGTH_PREFERENCE_DEFAULT = "balanced"
LENGTH_PREFERENCES = ["concise", "balanced", "detailed"]

@dataclass
class GenerationConfig:
    """ìƒì„± ì„¤ì •"""
    temperature: float = DEFAULT_TEMPERATURE
    max_tokens: int = DEFAULT_MAX_TOKENS
    top_p: float = DEFAULT_TOP_P
    top_k: int = DEFAULT_TOP_K
    repeat_penalty: float = DEFAULT_REPEAT_PENALTY

    # ì ì‘í˜• ê¸¸ì´ ì¡°ì • ì„¤ì •
    enable_adaptive_length: bool = ADAPTIVE_LENGTH_ENABLED
    length_preference: str = LENGTH_PREFERENCE_DEFAULT
    min_length_override: Optional[int] = None
    max_length_override: Optional[int] = None

@dataclass 
class RAGResponse:
    """RAG ì‘ë‹µ êµ¬ì¡°"""
    answer: str
    sources_cited: List[str]
    confidence: float
    generation_time: float
    has_proper_citation: bool
    retry_count: int = 0
    
    # ì ì‘í˜• ê¸¸ì´ ì¡°ì • ê´€ë ¨ ì •ë³´
    length_recommendation: Optional[Any] = None
    original_length: Optional[int] = None
    length_adjustments: List[str] = None
    adaptive_length_used: bool = False

class QwenLLM:

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒìˆ˜
    SYSTEM_ROLE = "í•œêµ­ ë°©ì†¡ì‚¬ì˜ ì „ë¬¸ ì§€ì‹ ê²€ìƒ‰ ë„ìš°ë¯¸"
    ANSWER_LANGUAGE = "í•œêµ­ì–´"
    CITATION_FORMAT = "[íŒŒì¼ëª….pdf]"

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    IMPROVED_SYSTEM_PROMPT = f"""ë‹¹ì‹ ì€ {SYSTEM_ROLE}ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ì¤‘ìš” ì§€ì¹¨:
1. ë¬¸ì„œì—ì„œ ì§ì ‘ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
2. ìˆ«ì, ê¸ˆì•¡, ë‚ ì§œëŠ” ì •í™•í•˜ê²Œ ì¸ìš©í•˜ì„¸ìš”
3. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” "ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŒ"ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
4. ë‹µë³€ì€ êµ¬ì²´ì ì´ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
5. ë°˜ë“œì‹œ {ANSWER_LANGUAGE}ë¡œ ë‹µë³€í•˜ì„¸ìš”"""

    IMPROVED_QUERY_TEMPLATE = """ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.
- ê´€ë ¨ ì •ë³´ê°€ ìˆë‹¤ë©´ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ í¬í•¨í•˜ì„¸ìš”
- ê¸ˆì•¡ì´ ìˆë‹¤ë©´ ì •í™•í•œ ìˆ«ìë¥¼ ì œì‹œí•˜ì„¸ìš”
- ë‚ ì§œê°€ ìˆë‹¤ë©´ ëª…ì‹œí•˜ì„¸ìš”
- ì¶œì²˜ëŠ” {CITATION_FORMAT} í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”

ë‹µë³€:""".replace("{CITATION_FORMAT}", CITATION_FORMAT)

    """Qwen ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, config: GenerationConfig = None):
        self.model_path = Path(model_path)
        self.config = config or GenerationConfig()
        self.logger = logging.getLogger(__name__)

        # LLM ìµœì í™” ì„¤ì • ë¡œë“œ
        self._load_optimization_config()

        # Qwen ì „ìš© ì„¤ì •
        self.chat_format = "qwen"  # qwen2 ëŒ€ì‹  qwen ì‚¬ìš©
        self.stop_tokens = ["</s>", "<|im_end|>", "<|endoftext|>"]
        
        self.llm = None
        self._load_model()
        
        
        # ì¸ìš© íŒ¨í„´ ì»´íŒŒì¼ (ì„±ëŠ¥ í–¥ìƒ)
        self.citation_patterns = [
            re.compile(r'\[([^\]]+\.pdf[^\]]*)\]'),  # [íŒŒì¼ëª….pdf] í˜•ì‹
            re.compile(r'ã€Œ([^ã€]+\.pdf[^ã€]*)ã€'),    # ã€ŒíŒŒì¼ëª….pdfã€ í˜•ì‹
            re.compile(r'ì¶œì²˜:\s*([^\n]+\.pdf[^\n]*)'), # ì¶œì²˜: íŒŒì¼ëª….pdf í˜•ì‹
            re.compile(r'ê·¼ê±°:\s*([^\n]+\.pdf[^\n]*)'), # ê·¼ê±°: íŒŒì¼ëª….pdf í˜•ì‹
            re.compile(r'\[([^\]]*\d{4}-\d{2}-\d{2}[^\]]*\.pdf[^\]]*)\]'), # ë‚ ì§œ í¬í•¨ íŒŒì¼ëª…
            re.compile(r'(\d{4}-\d{2}-\d{2}_[^\s\]]+\.pdf)'), # ë‚ ì§œë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ëª…
            re.compile(r'([A-Za-z0-9ê°€-í£_\-\s]+\.pdf)'), # ì¼ë°˜ì ì¸ PDF íŒŒì¼ëª… íŒ¨í„´
        ]

    def _load_optimization_config(self):
        """LLM ìµœì í™” ì„¤ì • ë¡œë“œ"""
        self.use_optimized_prompts = False
        self.max_context_tokens = 4000
        self.max_response_tokens = 1200

        config_path = Path(__file__).parent.parent / 'config' / 'llm_optimization.yaml'

        # í™˜ê²½ ë³€ìˆ˜ë¡œ ìµœì í™” ê°•ì œ í™œì„±í™”
        if os.environ.get('USE_OPTIMIZED_PROMPTS', 'false').lower() == 'true':
            self.use_optimized_prompts = True
            self.max_context_tokens = int(os.environ.get('MAX_CONTEXT_TOKENS', '2000'))
            self.logger.info("í™˜ê²½ë³€ìˆ˜ë¡œ ìµœì í™” í™œì„±í™”")

        # YAML ì„¤ì • íŒŒì¼ ë¡œë“œ
        elif config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    opt_config = yaml.safe_load(f)
                    if opt_config and 'prompts' in opt_config:
                        self.use_optimized_prompts = opt_config['prompts'].get('use_optimized', False)
                        self.max_context_tokens = opt_config['prompts'].get('max_context_tokens', 4000)
                        self.max_response_tokens = opt_config['prompts'].get('max_response_tokens', 1200)
                        self.logger.info(f"ìµœì í™” ì„¤ì • ë¡œë“œ: {config_path}")
            except Exception as e:
                self.logger.warning(f"ìµœì í™” ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            from llama_cpp import Llama
            
            # config.pyì—ì„œ GPU ìµœì í™” ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            try:
                from config import N_THREADS, N_CTX, N_BATCH, USE_MLOCK, USE_MMAP, N_GPU_LAYERS, F16_KV
            except ImportError:
                # config.py ì—†ì„ ë•Œ ê¸°ë³¸ê°’ (GPU ìµœì í™”)
                N_THREADS, N_CTX, N_BATCH = 8, 8192, 512
                USE_MLOCK, USE_MMAP, N_GPU_LAYERS, F16_KV = False, True, -1, True
            
            # GPU ì„¤ì •: ì˜ëª»ëœ íŒŒë¼ë¯¸í„° ì œê±° (offload_kqv, mul_mat_q ë“±ì´ GPU ì‚¬ìš© ë°©í•´)
            # ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©í•˜ì—¬ GPU ì˜¤í”„ë¡œë“œê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ë„ë¡ í•¨

            self.llm = Llama(
                model_path=str(self.model_path),
                chat_format=self.chat_format,
                n_ctx=N_CTX,           # config: 16384 (í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸)
                n_threads=N_THREADS,   # config: 4 (GPU ì‚¬ìš©ì‹œ CPU ìŠ¤ë ˆë“œ ìµœì†Œí™”)
                n_gpu_layers=N_GPU_LAYERS,  # config: -1 (ëª¨ë“  ë ˆì´ì–´ GPU ì‚¬ìš©!)
                f16_kv=F16_KV,        # config: True (GPU ë©”ëª¨ë¦¬ ìµœì í™”)
                use_mlock=USE_MLOCK,  # config: False (GPU ì‚¬ìš©ì‹œ ë¹„í™œì„±í™”)
                use_mmap=USE_MMAP,    # config: True (ë©”ëª¨ë¦¬ ë§¤í•‘)
                verbose=True,         # GPU ë¡œë”© ìƒíƒœ í™•ì¸
                n_batch=N_BATCH       # config: 1024 (ë°°ì¹˜ í¬ê¸° ì¦ê°€)
            )
            
            self.logger.info(f"Qwen ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            self.logger.info(f"ìµœì í™” ëª¨ë“œ: {'í™œì„±í™”' if self.use_optimized_prompts else 'ë¹„í™œì„±í™”'}")

        except ImportError:
            self.logger.error("llama-cpp-python íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    @lru_cache(maxsize=32)
    def create_system_prompt(self) -> str:
        """ìµœì í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìºì‹œë¨)"""
        if self.use_optimized_prompts:
            # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ - 73.9% í† í° ê°ì†Œ
            return """ë°©ì†¡ê¸°ìˆ  ì „ë¬¸ê°€. ì œê³µë¬¸ì„œ ê¸°ë°˜ ì •í™•ë‹µë³€. ì¶œì²˜ëª…ì‹œ."""
        else:
            # ì›ë³¸ í”„ë¡¬í”„íŠ¸
            return """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë°©ì†¡ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

í•„ìˆ˜ ê·œì¹™:
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´ë‚˜ ì˜ì–´ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
2. ë‹µë³€ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ [ì¶œì²˜: íŒŒì¼ëª….pdf] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.

ì—­í• : ì œê³µëœ ë¬¸ì„œë¥¼ ì² ì €íˆ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì™„ì „í•˜ê³  ì •í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€

ì›ì¹™:
1. ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ë‹µë³€
2. ë¬¸ì„œì— ìˆëŠ” ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ì¢…í•©
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë‚ ì§œ, ëª¨ë¸ëª… ë“±ì„ ì •í™•íˆ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš©
4. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€
5. ë°˜ë“œì‹œ ì¶œì²˜ ë¬¸ì„œë¥¼ [íŒŒì¼ëª….pdf] í˜•ì‹ìœ¼ë¡œ ì¸ìš©

ë‹µë³€ ë°©ì‹: ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ + ì¶œì²˜ ì¸ìš©"""

    def create_user_prompt(self, question: str, context_chunks: List[Dict[str, Any]]) -> str:
        """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„± (ìµœì í™” ëª¨ë“œ ì§€ì›)"""

        if self.use_optimized_prompts:
            return self._create_optimized_user_prompt(question, context_chunks)

        # ìš”ì•½ ìš”ì²­ ì—¬ë¶€ í™•ì¸
        is_summary_request = any(keyword in question.lower() for keyword in ['ìš”ì•½', 'ê°œìš”', 'ë‚´ìš©'])

        # ì „ì²´ ë¦¬ìŠ¤íŠ¸ ìš”ì²­ ì—¬ë¶€ í™•ì¸
        is_list_request = any(keyword in question.lower() for keyword in ['ì „ë¶€', 'ëª¨ë“ ', 'ëª¨ë‘', 'ë¦¬ìŠ¤íŠ¸', 'ëª©ë¡', 'í’ˆëª©'])

        # íŠ¹ë³„ ì²˜ë¦¬ê°€ í•„ìš”í•œ ìš”ì²­
        is_special_request = is_summary_request or is_list_request

        context_text = ""
        
        for i, chunk in enumerate(context_chunks, 1):
            filename = Path(chunk.get('source', '')).name
            # ğŸ”¥ CRITICAL: Support both 'content' and 'snippet' fields
            content = chunk.get('content') or chunk.get('snippet', '')
            score = chunk.get('score', 0.0)
            
            context_text += f"\n--- ë¬¸ì„œ {i}: {filename} (ê´€ë ¨ë„: {score:.3f}) ---\n"
            
            # íŠ¹ë³„ ìš”ì²­ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ë©”íƒ€ë°ì´í„°ê°€ ë‹µë³€ì„ ë°©í•´í•˜ë¯€ë¡œ)
            if not is_special_request:
                metadata = chunk.get('metadata', {})
                author = metadata.get('ê¸°ì•ˆì', metadata.get('author', ''))
                doc_date = metadata.get('ê¸°ì•ˆì¼ì', metadata.get('date', ''))
                doc_type = metadata.get('ì‹ ì²­êµ¬ë¶„', metadata.get('doc_type', ''))
                
                if author or doc_date or doc_type:
                    meta_info = []
                    if author: meta_info.append(f"ê¸°ì•ˆì: {author}")
                    if doc_date: meta_info.append(f"ë‚ ì§œ: {doc_date}")
                    if doc_type: meta_info.append(f"ë¬¸ì„œìœ í˜•: {doc_type}")
                    context_text += f"[ë©”íƒ€ë°ì´í„°: {', '.join(meta_info)}]\n"
            
            context_text += content + "\n"
        
        # ìš”ì²­ ìœ í˜•ë³„ íŠ¹ë³„í•œ ì§€ì‹œë¬¸
        if is_summary_request:
            instruction = """ğŸ¯ ë¬¸ì„œ ìš”ì•½ ì§€ì¹¨:
ìœ„ ë¬¸ì„œë¥¼ ì² ì €íˆ ì½ê³  ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•œ ì™„ì „í•œ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”:

1. **ë¬¸ì„œ ê¸°ë³¸ ì •ë³´**: ë‚ ì§œ, ê¸°ì•ˆì, ë¬¸ì„œ ì¢…ë¥˜
2. **ì£¼ìš” ëª©ì **: ë¬´ì—‡ì„ ìœ„í•œ ë¬¸ì„œì¸ì§€
3. **í•µì‹¬ ë‚´ìš©**: êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ê¸ˆì•¡, ëª¨ë¸ëª…, ìˆ˜ëŸ‰ ë“± ëª¨ë“  ì„¸ë¶€ì‚¬í•­
4. **ê²°ë¡  ë° ê³„íš**: ìŠ¹ì¸ ì‚¬í•­, í–¥í›„ ê³„íš ë“±

ğŸ’¡ ì¤‘ìš”: ë¬¸ì„œì— ê¸°ë¡ëœ ëª¨ë“  ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ì™„ì „í•˜ê³  ìœ ìš©í•œ ìš”ì•½ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""
        
        elif is_list_request:
            instruction = """ğŸ¯ ì „ì²´ ëª©ë¡ ì¶”ì¶œ ì§€ì¹¨:
ìœ„ ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ ëª¨ë“  í•­ëª©ë“¤ì„ ì™„ì „í•˜ê²Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

1. **í’ˆëª©ëª…/ëª¨ë¸ëª…**: ì •í™•í•œ ì œí’ˆëª…ê³¼ ëª¨ë¸ë²ˆí˜¸
2. **ìˆ˜ëŸ‰ ë° ê°€ê²©**: ìˆ˜ì¹˜ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨
3. **ì¶”ê°€ ì •ë³´**: ë¸Œëœë“œ, ìš©ë„, íŠ¹ì§• ë“±
4. **êµ¬ì¡°í™”**: ë²ˆí˜¸ë¥¼ ë§¤ê²¨ì„œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬

ğŸ’¡ ë¬¸ì„œ ì „ì²´ë¥¼ ê¼¼ê¼¼íˆ ê²€í† í•˜ì—¬ ëˆ„ë½ë˜ëŠ” ì •ë³´ ì—†ì´ ì™„ì „í•œ ëª©ë¡ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""
        
        else:
            instruction = """ğŸ¯ ì§ˆë¬¸ ë‹µë³€ ì§€ì¹¨:
ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì™„ì „í•˜ê³  ì •í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ê³ , ë°˜ë“œì‹œ [íŒŒì¼ëª….pdf] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ ì¸ìš©í•´ì£¼ì„¸ìš”.

ğŸ’¡ ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” í•œêµ­ì–´ ë‹µë³€ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""
        
        return f"""í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ì°¸ê³  ë¬¸ì„œ:
{context_text}

{instruction}

ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´ë‚˜ ì˜ì–´ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”."""

    def _create_optimized_user_prompt(self, question: str, context_chunks: List[Dict[str, Any]]) -> str:
        """ìµœì í™”ëœ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„± - ê¸ˆì•¡/í’ˆëª© ì •ë³´ ìš°ì„ """
        context_text = ""
        total_tokens = 0

        # í’ˆëª©/ê¸ˆì•¡ ì§ˆë¬¸ì¸ ê²½ìš° ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
        is_items_query = any(kw in question for kw in ['í’ˆëª©', 'êµ¬ë§¤', 'ì†Œëª¨í’ˆ', 'ì¥ë¹„', 'ë¬¼í’ˆ', 'ê¸ˆì•¡', 'ê°€ê²©', 'ì–¼ë§ˆ'])

        for i, chunk in enumerate(context_chunks, 1):
            if total_tokens >= self.max_context_tokens:
                break

            filename = Path(chunk.get('source', '')).name
            # ğŸ”¥ CRITICAL: Support both 'content' and 'snippet' fields
            content = chunk.get('content') or chunk.get('snippet', '')

            context_text += f"\n[{filename}]\n"

            if is_items_query:
                # í’ˆëª©/ê¸ˆì•¡ ì§ˆë¬¸: 3000ìê¹Œì§€ ì „ë¶€ ì‚¬ìš© (í•„í„°ë§ X)
                context_text += content[:3000]
            else:
                # ì¼ë°˜ ì§ˆë¬¸: ì¤‘ìš” í‚¤ì›Œë“œ í•„í„°ë§
                important_keywords = ['ë‚ ì§œ', 'ê¸ˆì•¡', 'êµ¬ë§¤', 'ëª©ì ', 'ì œëª©', 'ìˆ˜ëŸ‰', 'ëª¨ë¸', 'ê¸°ì•ˆ']
                important_lines = []
                for line in content.split('\n'):
                    if any(kw in line for kw in important_keywords):
                        important_lines.append(line.strip())

                if important_lines:
                    context_text += '\n'.join(important_lines[:20])  # ìµœëŒ€ 20ë¼ì¸ìœ¼ë¡œ í™•ì¥

            context_text += '\n'
            total_tokens += len(context_text.split())

        # í’ˆëª©/ê¸ˆì•¡ ì§ˆë¬¸ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸
        if is_items_query:
            return f"""ë¬¸ì„œ:
{context_text}

ì§ˆë¬¸: {question}

**ë‹µë³€ ì‹œ í•„ìˆ˜ í¬í•¨ ì‚¬í•­:**
1. í’ˆëª©ëª… (ì •í™•í•œ ì´ë¦„)
2. ìˆ˜ëŸ‰
3. ê¸ˆì•¡ (ìˆëŠ” ê²½ìš° ë°˜ë“œì‹œ í¬í•¨)
4. ì¶œì²˜: [íŒŒì¼ëª….pdf]

ë‹µë³€:"""
        else:
            # ì¼ë°˜ ì§ˆë¬¸ìš© í”„ë¡¬í”„íŠ¸
            return f"""ë¬¸ì„œ:
{context_text}

Q: {question}
A:"""

    def create_full_document_prompt(self, question: str, document_text: str, file_path: str) -> str:
        """ì „ì²´ ë¬¸ì„œ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        filename = Path(file_path).name
        
        return f"""ì§ˆë¬¸: {question}

ì „ì²´ ë¬¸ì„œ ë‚´ìš© ({filename}):
{document_text}

ğŸ¯ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ìƒì„± (ë³´ìˆ˜ì  ë‹µë³€ ê¸ˆì§€):
1. **ë¬¸ì„œì— ìˆëŠ” ì •ë³´ëŠ” ë°˜ë“œì‹œ í™œìš©**: ì œê³µëœ ë¬¸ì„œì˜ ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ ì ê·¹ í™œìš©
2. **êµ¬ì²´ì  ì •ë³´ ìš°ì„  ì¶”ì¶œ**: ê¸ˆì•¡, ë‚ ì§œ, ê¸°ì•ˆì, í’ˆëª©ëª…, ìˆ˜ëŸ‰ ë“± ëª¨ë“  êµ¬ì²´ì  ì •ë³´
3. **ì™„ì „í•œ ìš”ì•½ ì œê³µ**: ë¬¸ì„œì˜ ëª©ì , ì£¼ìš” ë‚´ìš©, ê¸ˆì•¡, í’ˆëª©ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬  
4. **ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€**: "í™•ì¸ë˜ì§€ ì•ŠìŒ"ì´ ì•„ë‹Œ ì‹¤ì œ ì •ë³´ ì œê³µ
5. **ì¶œì²˜ ì¸ìš©**: [{filename}] í˜•ì‹ìœ¼ë¡œ ë°˜ë“œì‹œ ì¸ìš©

ğŸ’ª ì ê·¹ì  ì •ë³´ í™œìš© ì›ì¹™:
- ê¸°ì•ˆì„œì—ì„œ ê¸°ì•ˆì, ë‚ ì§œ, ê¸ˆì•¡ ì •ë³´ ì¶”ì¶œ í•„ìˆ˜
- í’ˆëª© ëª©ë¡ì´ ìˆìœ¼ë©´ ì£¼ìš” í’ˆëª©ë“¤ ë‚˜ì—´
- ì´ ê¸ˆì•¡ê³¼ ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ë³„ ê¸ˆì•¡ ì œì‹œ  
- ë¬¸ì„œì˜ ë°°ê²½ê³¼ ëª©ì  ì„¤ëª…
- íŒŒì¼ëª…ì˜ ë‚ ì§œì™€ ì œëª© ì •ë³´ë„ í™œìš©

âš ï¸ ì ˆëŒ€ ê¸ˆì§€: "êµ¬ì²´ì ì¸ ì •ë³´ëŠ” ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ë‹¤" ë“±ì˜ ì†Œê·¹ì  ë‹µë³€

ë‹µë³€ ëª©í‘œ: ì‚¬ìš©ìê°€ ë¬¸ì„œ ë‚´ìš©ì„ ì™„ì „íˆ ì´í•´í•  ìˆ˜ ìˆëŠ” ìœ ìš©í•œ ìš”ì•½ + [{filename}]"""

    def generate_response(self, question: str, context_chunks: List[Dict[str, Any]], 
                         max_retries: int = 2, enable_complex_processing: bool = True) -> RAGResponse:
        """RAG ì‘ë‹µ ìƒì„± (ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬ ë° ì ì‘í˜• ê¸¸ì´ ì¡°ì • í†µí•©)"""
        
        # 0ë‹¨ê³„: ê°™ì€ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ìš°ì„  ì„ íƒ (ì¤‘ê°„ ë‹¨ê³„ ì ‘ê·¼ë²•)
        context_chunks = self._prioritize_same_document_chunks(context_chunks, max_chunks=10)
        
        # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ì„
        question_analysis = None
        length_recommendation = None
        
        # 4ë‹¨ê³„: ê¸°ë³¸ ì²˜ë¦¬ ëª¨ë“œ (ì ì‘í˜• ê¸¸ì´ ì ìš©)
        if self.config.enable_adaptive_length and length_recommendation:
            system_prompt = self._create_adaptive_system_prompt(length_recommendation)
        else:
            system_prompt = self.create_system_prompt()
            
        user_prompt = self.create_user_prompt(question, context_chunks)
        
        retry_count = 0
        start_time = time.time()
        best_answer = None  # ì¸ìš©ì´ ì—†ì–´ë„ ê°€ì¥ ì¢‹ì€ ë‹µë³€ ì €ì¥
        
        for attempt in range(max_retries + 1):
            try:
                # ëŒ€í™” ë©”ì‹œì§€ êµ¬ì„±
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
                
                # ì ì‘í˜• max_tokens ê³„ì‚°
                if self.config.enable_adaptive_length and length_recommendation:
                    adaptive_max_tokens = self._calculate_adaptive_max_tokens(length_recommendation)
                    self.logger.debug(f"ì ì‘í˜• í† í°: {adaptive_max_tokens} (ê¸°ë³¸: {self.config.max_tokens})")
                else:
                    adaptive_max_tokens = self.config.max_tokens
                
                # ìƒì„±
                response = self.llm.create_chat_completion(
                    messages=messages,
                    temperature=self.config.temperature,
                    max_tokens=adaptive_max_tokens,
                    top_p=self.config.top_p,
                    top_k=self.config.top_k,
                    repeat_penalty=self.config.repeat_penalty,
                    stop=self.stop_tokens
                )
                
                answer = response['choices'][0]['message']['content'].strip()
                # ì™¸êµ­ì–´ í…ìŠ¤íŠ¸ í•„í„°ë§
                answer = self._remove_foreign_text(answer)
                generation_time = time.time() - start_time
                
                # ì²« ë²ˆì§¸ ë‹µë³€ì„ ìµœì„ ì˜ ë‹µë³€ìœ¼ë¡œ ì €ì¥ (ì¸ìš© ì—†ì–´ë„)
                if best_answer is None and len(answer) > 10:  # ë„ˆë¬´ ì§§ì§€ ì•Šì€ ë‹µë³€ë§Œ
                    best_answer = {
                        'answer': answer,
                        'generation_time': generation_time,
                        'retry_count': retry_count
                    }
                
                # ì¸ìš© ê²€ì¦
                citation_check = self._validate_citations(answer, context_chunks)
                
                if citation_check['has_citations']:
                    # ì ì‘í˜• ê¸¸ì´ ì¡°ì • ì ìš©
                    original_length = len(answer)
                    length_adjustments = []
                    adjusted_answer = answer
                    
                    if self.config.enable_adaptive_length and length_recommendation:
                        adjusted_answer, length_adjustments = self.length_analyzer.validate_and_adjust_answer(
                            answer, length_recommendation)
                    
                    # ì¸ìš©ì´ ìˆëŠ” ë‹µë³€ - ì¦‰ì‹œ ë°˜í™˜ (ìµœìš°ì„ )
                    return RAGResponse(
                        answer=adjusted_answer,
                        sources_cited=citation_check['cited_files'],
                        confidence=self._calculate_confidence(adjusted_answer, context_chunks),
                        generation_time=generation_time,
                        has_proper_citation=True,
                        retry_count=retry_count,
                        length_recommendation=length_recommendation,
                        original_length=original_length,
                        length_adjustments=length_adjustments,
                        adaptive_length_used=self.config.enable_adaptive_length
                    )
                else:
                    # ì¸ìš©ì´ ì—†ì§€ë§Œ ë‹µë³€ í’ˆì§ˆ ì²´í¬
                    if self._is_meaningful_answer(answer):
                        # ì˜ë¯¸ìˆëŠ” ë‹µë³€ì´ë©´ ì €ì¥í•˜ê³  ì¬ì‹œë„ ê³„ì†
                        best_answer = {
                            'answer': answer,
                            'generation_time': generation_time,
                            'retry_count': retry_count
                        }
                        self.logger.info(f"ì¸ìš© ì—†ì§€ë§Œ ì˜ë¯¸ìˆëŠ” ë‹µë³€ ì €ì¥ (ì‹œë„ {attempt + 1})")
                    
                    # ì¬ì‹œë„ ì¡°ê±´
                    retry_count += 1
                    if attempt < max_retries:
                        self.logger.warning(f"ì¸ìš© ì—†ëŠ” ì‘ë‹µ, ì¬ì‹œë„ {attempt + 1}/{max_retries}")
                        # REMOVED: ì‹¤ì œ filename ìˆì„ ë•Œë§Œ ì¸ìš©í•˜ë¯€ë¡œ placeholder ë¶ˆí•„ìš”
                        continue
                
            except Exception as e:
                self.logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                retry_count += 1
                
                if attempt < max_retries:
                    continue
        
        # ëª¨ë“  ì¬ì‹œë„ ì™„ë£Œ - ìµœì„ ì˜ ë‹µë³€ ë°˜í™˜ + ì¶œì²˜ ê°•ì œ ì¶”ê°€
        if best_answer and len(best_answer['answer']) > 10:
            self.logger.info("ì¸ìš© ì—†ëŠ” ë‹µë³€ â†’ ì¶œì²˜ ê°•ì œ ì¶”ê°€")

            # ì‚¬ìš©ëœ ë¬¸ì„œ ì¶œì²˜ ê°•ì œ ì¶”ê°€
            answer_with_sources = best_answer['answer']

            # ì´ë¯¸ ì¸ìš©ì´ ìˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸
            has_any_citation = '[' in answer_with_sources and '.pdf]' in answer_with_sources

            if not has_any_citation:
                # ìƒìœ„ 2ê°œ ë¬¸ì„œ ì¶œì²˜ ê°•ì œ ì¶”ê°€
                top_sources = [chunk.get('source', '') for chunk in context_chunks[:2] if chunk.get('source')]
                if top_sources:
                    sources_text = ', '.join([f"[{src}]" for src in top_sources])
                    answer_with_sources += f"\n\nì¶œì²˜: {sources_text}"
                    self.logger.info(f"ì¶œì²˜ ê°•ì œ ì¶”ê°€ ì™„ë£Œ: {len(top_sources)}ê°œ")

            return RAGResponse(
                answer=answer_with_sources,
                sources_cited=[chunk.get('source', '') for chunk in context_chunks[:2]],  # ìƒìœ„ 2ê°œ ë¬¸ì„œ
                confidence=self._calculate_confidence(answer_with_sources, context_chunks) * 0.8,  # ì‹ ë¢°ë„ ì•½ê°„ ê°ì†Œ
                generation_time=best_answer['generation_time'],
                has_proper_citation=True,  # ê°•ì œ ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ True
                retry_count=best_answer['retry_count']
            )
        
        # ì™„ì „ ì‹¤íŒ¨ - í•˜ì§€ë§Œ context_chunksê°€ ìˆìœ¼ë©´ ê¸°ë³¸ ìš”ì•½ ì œê³µ
        generation_time = time.time() - start_time

        # ğŸ”¥ CRITICAL: ê²€ìƒ‰ ê²°ê³¼(context_chunks)ê°€ ìˆìœ¼ë©´ "ì—†ìŒ" ë©”ì‹œì§€ ê¸ˆì§€
        if context_chunks and len(context_chunks) > 0:
            self.logger.warning(f"LLM ìƒì„± ì‹¤íŒ¨í–ˆì§€ë§Œ context_chunks={len(context_chunks)}ê°œ ìˆìŒ â†’ ê¸°ë³¸ ìš”ì•½ ì œê³µ")

            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
            summary_parts = []
            for i, chunk in enumerate(context_chunks[:3], 1):
                filename = chunk.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                # ğŸ”¥ CRITICAL: Support both 'content' and 'snippet' fields
                content_preview = ((chunk.get('content') or chunk.get('snippet', ''))[:200] or '(ë‚´ìš© ì—†ìŒ)')
                summary_parts.append(f"{i}. {filename}\n{content_preview}...")

            basic_summary = f"ë‹¤ìŒ {len(context_chunks[:3])}ê°œ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n" + "\n\n".join(summary_parts)

            # ì¶œì²˜ ì¶”ê°€
            top_sources = [chunk.get('source', '') for chunk in context_chunks[:2] if chunk.get('source')]
            if top_sources:
                sources_text = ', '.join([f"[{src}]" for src in top_sources])
                basic_summary += f"\n\nì¶œì²˜: {sources_text}"

            return RAGResponse(
                answer=basic_summary,
                sources_cited=top_sources,
                confidence=0.3,  # ë‚®ì€ ì‹ ë¢°ë„ì§€ë§Œ ê²€ìƒ‰ ê²°ê³¼ëŠ” ìˆìŒ
                generation_time=generation_time,
                has_proper_citation=bool(top_sources),
                retry_count=retry_count
            )

        # ì •ë§ë¡œ context_chunksê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ "ì—†ìŒ" ë©”ì‹œì§€
        return RAGResponse(
            answer="ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.",
            sources_cited=[],
            confidence=0.0,
            generation_time=generation_time,
            has_proper_citation=False,
            retry_count=retry_count
        )
    
    def _generate_structured_response(self, question_analysis: Dict[str, Any], 
                                    context_chunks: List[Dict[str, Any]], 
                                    max_retries: int = 2) -> RAGResponse:
        """êµ¬ì¡°í™”ëœ ë³µí•© ì§ˆë¬¸ ì‘ë‹µ ìƒì„±"""
        
        start_time = time.time()
        retry_count = 0
        
        # ë³µí•© ì§ˆë¬¸ì˜ íŠ¹ë³„ ì²˜ë¦¬ (í’ˆì§ˆ ìš°ì„ ìœ¼ë¡œ ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€)
        quality_retries = max_retries + 1  # í’ˆì§ˆì„ ìœ„í•´ ì¬ì‹œë„ 1íšŒ ì¶”ê°€
        
        if question_analysis.question_type == QuestionType.COMPARISON:
            return self._handle_comparison_question(question_analysis, context_chunks, quality_retries)
        elif question_analysis.question_type == QuestionType.ANALYSIS:
            return self._handle_analysis_question(question_analysis, context_chunks, quality_retries)
        elif question_analysis.question_type == QuestionType.COMPLEX_MULTI:
            return self._handle_complex_multi_question(question_analysis, context_chunks, quality_retries)
        
        # ì¼ë°˜ì ì¸ êµ¬ì¡°í™”ëœ ì²˜ë¦¬
        structured_prompt = self.answer_templates.generate_structured_prompt(question_analysis, context_chunks)
        template = self.answer_templates.get_template(question_analysis.question_type)
        
        for attempt in range(max_retries + 1):
            try:
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— êµ¬ì¡°í™”ëœ ì§€ì¹¨ ì¶”ê°€
                enhanced_system_prompt = self.create_system_prompt() + f"""

ğŸ¯ íŠ¹ë³„ ì§€ì¹¨ (ì§ˆë¬¸ ìœ í˜•: {question_analysis.question_type.value}):
- ìµœëŒ€ {template.max_length}ì ì´ë‚´
- í…œí”Œë¦¿ êµ¬ì¡°: {template.structure}
- í•„ìˆ˜ ìš”ì†Œ: {', '.join(template.required_fields)}

ì˜ˆì‹œ ë‹µë³€: {template.example}"""
                
                messages = [
                    {"role": "system", "content": enhanced_system_prompt},
                    {"role": "user", "content": structured_prompt}
                ]
                
                response = self.llm.create_chat_completion(
                    messages=messages,
                    temperature=self.config.temperature,
                    max_tokens=min(self.config.max_tokens, template.max_length + 50),
                    top_p=self.config.top_p,
                    top_k=self.config.top_k,
                    repeat_penalty=self.config.repeat_penalty,
                    stop=self.stop_tokens
                )
                
                raw_answer = response['choices'][0]['message']['content'].strip()
                # ì™¸êµ­ì–´ í…ìŠ¤íŠ¸ í•„í„°ë§
                raw_answer = self._remove_foreign_text(raw_answer)
                generation_time = time.time() - start_time
                
                # ë‹µë³€ í›„ì²˜ë¦¬ ë° ê²€ì¦
                processed_answer = self.answer_templates.post_process_answer(raw_answer, question_analysis)
                validation = self.answer_templates.validate_answer_structure(processed_answer, template)
                
                # ì¸ìš© ê²€ì¦
                citation_check = self._validate_citations(processed_answer, context_chunks)
                
                # êµ¬ì¡°í™”ëœ ë‹µë³€ ë°˜í™˜
                confidence = self._calculate_confidence(processed_answer, context_chunks)
                
                # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì‹ ë¢°ë„ ê°ì†Œ
                if not validation['is_valid']:
                    confidence *= 0.8
                    self.logger.warning(f"êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨: {validation['issues']}")
                
                return RAGResponse(
                    answer=processed_answer,
                    sources_cited=citation_check['cited_files'],
                    confidence=confidence,
                    generation_time=generation_time,
                    has_proper_citation=citation_check['has_citations'],
                    retry_count=retry_count
                )
                
            except Exception as e:
                self.logger.error(f"êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                retry_count += 1
                
                if attempt < max_retries:
                    continue
        
        # êµ¬ì¡°í™”ëœ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±
        self.logger.warning("êµ¬ì¡°í™”ëœ ì²˜ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±")
        return self.generate_response(question_analysis.original_question, context_chunks,
                                    max_retries=MAX_LLM_RETRY, enable_complex_processing=False)
    
    def _handle_comparison_question(self, question_analysis: Dict[str, Any],
                                  context_chunks: List[Dict[str, Any]],
                                  max_retries: int = None) -> RAGResponse:
        """ë¹„êµ ì§ˆë¬¸ íŠ¹ë³„ ì²˜ë¦¬"""

        # .envì—ì„œ ì½ì€ ê°’ ì‚¬ìš©
        if max_retries is None:
            max_retries = MAX_LLM_RETRY

        # ë‹¨ì¼ ë¬¸ì„œ ëª¨ë“œ ê°•í™” - ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë§Œ ì‚¬ìš©
        if context_chunks:
            best_chunk = context_chunks[0]  # ìµœê³  ì ìˆ˜ ë¬¸ì„œ
            filtered_chunks = [chunk for chunk in context_chunks 
                             if chunk.get('source', '') == best_chunk.get('source', '')]
            
            self.logger.info(f"ë¹„êµ ì§ˆë¬¸: ë‹¨ì¼ ë¬¸ì„œ ëª¨ë“œ ì ìš© - {best_chunk.get('source', '').split('/')[-1]}")
            context_chunks = filtered_chunks[:3]  # ìµœëŒ€ 3ê°œ ì²­í¬ë§Œ ì‚¬ìš©
        
        enhanced_prompt = f"""ì§ˆë¬¸: {question_analysis.original_question}

ğŸ¯ ë¹„êµ ì§ˆë¬¸ ì „ìš© ì§€ì¹¨ (í’ˆì§ˆ ìµœìš°ì„ ):
1. ë¹„êµ ëŒ€ìƒ: {', '.join(question_analysis.comparison_targets) if question_analysis.comparison_targets else 'ë¬¸ì„œì—ì„œ ì°¾ì•„ì„œ ë¹„êµ'}
2. ğŸš¨ ì ˆëŒ€ ì¤‘ìš”: ë¬¸ì„œì— ì •í™•íˆ ëª…ì‹œëœ ìˆ«ìë§Œ ì‚¬ìš©í•˜ì„¸ìš” - ì¶”ì •/ë³€í˜• ì ˆëŒ€ ê¸ˆì§€
3. ê° í•­ëª©ì˜ êµ¬ì²´ì  ìˆ˜ì¹˜ë¥¼ ë¬¸ì„œì—ì„œ ì •í™•íˆ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©
4. "AëŠ” X, BëŠ” Yì…ë‹ˆë‹¤" í˜•ì‹ìœ¼ë¡œ ëª…í™•í•œ ëŒ€ì¡° êµ¬ì¡°
5. ë‹¨ì¼ ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œë§Œ ë¹„êµ (ì •ë³´ í˜¼í•© ì ˆëŒ€ ê¸ˆì§€)
6. ìˆ«ìê°€ ë¶ˆë¶„ëª…í•˜ë©´ ë¹„êµí•˜ì§€ ë§ê³  ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ ì œê³µ

âš ï¸ í™˜ê° ë°©ì§€ ê·œì¹™:
- ë¬¸ì„œì— ì—†ëŠ” ìˆ«ì ì ˆëŒ€ ìƒì„± ê¸ˆì§€
- ëŒ€ëµì ì¸ ê³„ì‚°ì´ë‚˜ ì¶”ì • ê¸ˆì§€  
- ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ "ë¬¸ì„œì—ì„œ í™•ì¸ëœ ì •ë³´ë§Œ ì œê³µ"

ì°¸ê³  ë¬¸ì„œ (ë‹¨ì¼ ë¬¸ì„œ ëª¨ë“œ):"""
        
        for i, chunk in enumerate(context_chunks[:2], 1):  # ìµœëŒ€ 2ê°œë§Œ ì‚¬ìš©
            filename = chunk.get('source', '').split('/')[-1]
            content = chunk.get('content', '')[:200]
            score = chunk.get('score', 0.0)
            
            enhanced_prompt += f"\n--- ì²­í¬ {i}: {filename} (ì ìˆ˜: {score:.3f}) ---\n{content}..."
        
        enhanced_prompt += f"""

ğŸ¯ ë¹„êµ ë‹µë³€ í…œí”Œë¦¿ (ì •í™•ì„± ìµœìš°ì„ ):
ë¬¸ì„œì—ì„œ ì°¾ì€ ì •í™•í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¹„êµí•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. [íŒŒì¼ëª….pdf]

ğŸ” ë‹µë³€ ì‘ì„± ë‹¨ê³„:
1. ë¬¸ì„œì—ì„œ ì²« ë²ˆì§¸ í•­ëª©ì˜ ì •í™•í•œ ìˆ«ì ì°¾ê¸°
2. ë¬¸ì„œì—ì„œ ë‘ ë²ˆì§¸ í•­ëª©ì˜ ì •í™•í•œ ìˆ«ì ì°¾ê¸°  
3. ì°¾ì€ ìˆ«ìë¥¼ ì •í™•íˆ ë³µì‚¬í•˜ì—¬ ë¹„êµë¬¸ ì‘ì„±
4. ìˆ«ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ í•´ë‹¹ í•­ëª©ì€ "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ" ëª…ì‹œ

ğŸš¨ ì ˆëŒ€ ê¸ˆì§€: ì¶”ì¸¡, ê³„ì‚°, ë°˜ì˜¬ë¦¼, ê·¼ì‚¬ì¹˜ ì‚¬ìš© ê¸ˆì§€
âœ… í—ˆìš©: ë¬¸ì„œì— ëª…ì‹œëœ ì •í™•í•œ ìˆ«ìë§Œ ì‚¬ìš©"""
        
        return self._generate_with_enhanced_prompt(enhanced_prompt, context_chunks, max_retries, "ë¹„êµ")
    
    def _handle_analysis_question(self, question_analysis: Dict[str, Any], 
                                context_chunks: List[Dict[str, Any]], 
                                max_retries: int = 2) -> RAGResponse:
        """ë¶„ì„ ì§ˆë¬¸ íŠ¹ë³„ ì²˜ë¦¬"""
        
        enhanced_prompt = f"""ì§ˆë¬¸: {question_analysis.original_question}

ğŸ¯ ë¶„ì„ ì§ˆë¬¸ ì „ìš© ì§€ì¹¨ (í’ˆì§ˆ ìµœìš°ì„ ):
1. ğŸ” ëª¨ë“  ê´€ë ¨ ë°ì´í„°ë¥¼ ì² ì €íˆ ê²€í† í•˜ì—¬ ìµœëŒ€ê°’/ìµœì†Œê°’/í‰ê· ê°’ ì •í™•íˆ ì°¾ê¸°
2. ğŸš¨ ë¬¸ì„œì—ì„œ ì •í™•íˆ í™•ì¸ëœ ìˆ«ìë§Œ ì‚¬ìš© - ì¶”ì • ì ˆëŒ€ ê¸ˆì§€
3. "Xê°€ ê°€ì¥ [ê¸°ì¤€]ì…ë‹ˆë‹¤" í˜•ì‹ìœ¼ë¡œ ê²°ë¡  ëª…í™•íˆ ì œì‹œ
4. êµ¬ì²´ì  ê·¼ê±° ë°ì´í„°ì™€ ìˆ˜ì¹˜ ë°˜ë“œì‹œ í¬í•¨
5. ë¹„êµ ëŒ€ìƒì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ëª¨ë“  í•­ëª© ê²€í†  í›„ íŒë‹¨

âš ï¸ ë¶„ì„ ì •í™•ì„± ë³´ì¥:
- ëª¨ë“  í›„ë³´ë¥¼ ë‚˜ì—´í•˜ê³  ë¹„êµ
- ê°€ì¥ ë†’ì€/ë‚®ì€ ìˆ˜ì¹˜ë¥¼ ì •í™•íˆ ì‹ë³„
- ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ (ê°€ê²©ì€ ê°€ê²©ë¼ë¦¬, ìˆ˜ëŸ‰ì€ ìˆ˜ëŸ‰ë¼ë¦¬)
- í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ê²€í†  ê°€ëŠ¥í•œ ë²”ìœ„ì—ì„œ" ëª…ì‹œ

ì°¸ê³  ë¬¸ì„œ:"""
        
        for i, chunk in enumerate(context_chunks[:3], 1):
            filename = chunk.get('source', '').split('/')[-1]
            content = chunk.get('content', '')[:300]
            score = chunk.get('score', 0.0)
            
            enhanced_prompt += f"\n--- ë¬¸ì„œ {i}: {filename} (ì ìˆ˜: {score:.3f}) ---\n{content}..."
        
        enhanced_prompt += f"""

ğŸ¯ ë¶„ì„ ë‹µë³€ í…œí”Œë¦¿:
"[ê²°ê³¼]ê°€ ê°€ì¥ [ê¸°ì¤€]ì…ë‹ˆë‹¤. [êµ¬ì²´ì ê·¼ê±°] [íŒŒì¼ëª….pdf]"

ì¤‘ìš”: ëª¨ë“  ë°ì´í„°ë¥¼ ë¹„êµ ê²€í† í•œ í›„ ì •í™•í•œ ê²°ë¡ ì„ ë„ì¶œí•˜ì„¸ìš”."""
        
        return self._generate_with_enhanced_prompt(enhanced_prompt, context_chunks, max_retries, "ë¶„ì„")
    
    def _handle_complex_multi_question(self, question_analysis: Dict[str, Any], 
                                     context_chunks: List[Dict[str, Any]], 
                                     max_retries: int = 2) -> RAGResponse:
        """ë³µí•© ë‹¤ì¤‘ ì§ˆë¬¸ íŠ¹ë³„ ì²˜ë¦¬"""
        
        enhanced_prompt = f"""ì§ˆë¬¸: {question_analysis.original_question}

ğŸ¯ ë³µí•© ì§ˆë¬¸ ì „ìš© ì§€ì¹¨:
1. ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•˜ì—¬ ì²˜ë¦¬
2. ê° ë‹¨ê³„ë³„ ë‹µë³€ì„ í†µí•©
3. ì •ë³´ê°€ ë¶€ì¡±í•œ ë¶€ë¶„ì€ ëª…ì‹œ
4. ìš”ì•½ + ì„¸ë¶€ì‚¬í•­ êµ¬ì¡°

í•„ìš” ì •ë³´ ìœ í˜•: {', '.join(question_analysis.required_info_types)}

ì°¸ê³  ë¬¸ì„œ:"""
        
        for i, chunk in enumerate(context_chunks[:4], 1):  # ë³µí•© ì§ˆë¬¸ì€ ìµœëŒ€ 4ê°œ ë¬¸ì„œ
            filename = chunk.get('source', '').split('/')[-1]
            content = chunk.get('content', '')[:250]
            score = chunk.get('score', 0.0)
            
            enhanced_prompt += f"\n--- ë¬¸ì„œ {i}: {filename} (ì ìˆ˜: {score:.3f}) ---\n{content}..."
        
        enhanced_prompt += f"""

ğŸ¯ ë³µí•© ë‹µë³€ í…œí”Œë¦¿:
"[ìš”ì•½ë‹µë³€] [ì„¸ë¶€ì‚¬í•­1] [ì„¸ë¶€ì‚¬í•­2] [íŒŒì¼ëª….pdf]"

ì¤‘ìš”: ê° ìš”êµ¬ì‚¬í•­ì„ ì²´ê³„ì ìœ¼ë¡œ í™•ì¸í•˜ê³  í†µí•©ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
        
        return self._generate_with_enhanced_prompt(enhanced_prompt, context_chunks, max_retries, "ë³µí•©")
    
    def _generate_with_enhanced_prompt(self, enhanced_prompt: str, context_chunks: List[Dict[str, Any]], 
                                     max_retries: int, question_type_name: str) -> RAGResponse:
        """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µ ìƒì„± (í’ˆì§ˆ ìµœìš°ì„  ë§¤ê°œë³€ìˆ˜)"""
        
        start_time = time.time()
        
        # í’ˆì§ˆ ì¤‘ì‹¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°•í™”
        quality_focused_system_prompt = self.create_system_prompt() + """

ğŸ¯ **ë¬¸ì„œ í™œìš© ìµœìš°ì„  ì›ì¹™**:
1. ì œê³µëœ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ë‹µë³€
2. ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ í™œìš©
3. êµ¬ì²´ì ì¸ ìˆ«ì, ë‚ ì§œ, ëª¨ë¸ëª…ì„ ë¬¸ì„œì—ì„œ ì •í™•íˆ ì¶”ì¶œ
4. ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì™„ì „í•˜ê³  ìœ ìš©í•œ ë‹µë³€ ì œê³µ
5. ì¶œì²˜ë¥¼ ì •í™•í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë°˜ë“œì‹œ ì¸ìš©

ğŸ’¡ ì‚¬ìš©ì ë„ì›€ ìµœìš°ì„ : ë¬¸ì„œê°€ ì œê³µë˜ë©´ ê·¸ ë‚´ìš©ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ ì œê³µ"""
        
        for attempt in range(max_retries + 1):
            try:
                messages = [
                    {"role": "system", "content": quality_focused_system_prompt},
                    {"role": "user", "content": enhanced_prompt}
                ]
                
                response = self.llm.create_chat_completion(
                    messages=messages,
                    temperature=self.config.temperature,  # ê¸°ë³¸ ì„¤ì •ê°’ ì‚¬ìš© (0.3)
                    max_tokens=self.config.max_tokens,     # ê¸°ë³¸ ì„¤ì •ê°’ ì‚¬ìš© (800)
                    top_p=self.config.top_p,               # ê¸°ë³¸ ì„¤ì •ê°’ ì‚¬ìš© (0.9)
                    top_k=self.config.top_k,               # ê¸°ë³¸ ì„¤ì •ê°’ ì‚¬ìš© (40)
                    repeat_penalty=self.config.repeat_penalty,
                    stop=self.stop_tokens
                )
                
                answer = response['choices'][0]['message']['content'].strip()
                # ì™¸êµ­ì–´ í…ìŠ¤íŠ¸ í•„í„°ë§
                answer = self._remove_foreign_text(answer)
                generation_time = time.time() - start_time
                
                # ì¸ìš© ê²€ì¦
                citation_check = self._validate_citations(answer, context_chunks)
                
                self.logger.info(f"{question_type_name} ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ: {len(answer)}ì, ì¸ìš©: {citation_check['has_citations']}")
                
                return RAGResponse(
                    answer=answer,
                    sources_cited=citation_check['cited_files'],
                    confidence=self._calculate_confidence(answer, context_chunks),
                    generation_time=generation_time,
                    has_proper_citation=citation_check['has_citations'],
                    retry_count=attempt
                )
                
            except Exception as e:
                self.logger.error(f"{question_type_name} ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                if attempt < max_retries:
                    continue
        
        # ì™„ì „ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì˜¤ë¥˜ ì‘ë‹µ
        return RAGResponse(
            answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. {question_type_name} ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            sources_cited=[],
            confidence=0.0,
            generation_time=time.time() - start_time,
            has_proper_citation=False,
            retry_count=max_retries
        )
    
    def _prioritize_same_document_chunks(self, context_chunks: List[Dict[str, Any]], max_chunks: int = 10) -> List[Dict[str, Any]]:
        """ê°™ì€ ë¬¸ì„œì˜ ì²­í¬ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒí•˜ì—¬ í¬ê´„ì  ì •ë³´ ì œê³µ"""
        if not context_chunks:
            return context_chunks
            
        # ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ë¬¸ì„œ ì°¾ê¸°
        best_chunk = context_chunks[0] 
        best_source = best_chunk.get('source', '')
        
        self.logger.info(f"ìš°ì„  ë¬¸ì„œ: {Path(best_source).name}")
        
        # ê°™ì€ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ì™€ ë‹¤ë¥¸ ë¬¸ì„œì˜ ì²­í¬ ë¶„ë¦¬
        same_doc_chunks = []
        other_doc_chunks = []
        
        for chunk in context_chunks:
            chunk_source = chunk.get('source', '')
            if chunk_source == best_source:
                same_doc_chunks.append(chunk)
            else:
                other_doc_chunks.append(chunk)
        
        # ê°™ì€ ë¬¸ì„œ ì²­í¬ ìš°ì„  + ë‹¤ë¥¸ ë¬¸ì„œ ì²­í¬ë¡œ ìµœëŒ€ max_chunksê°œê¹Œì§€ êµ¬ì„±
        prioritized_chunks = same_doc_chunks[:max_chunks]  # ê°™ì€ ë¬¸ì„œ ì²­í¬ ë¨¼ì €
        
        # ë‚¨ì€ ê³µê°„ì— ë‹¤ë¥¸ ë¬¸ì„œ ì²­í¬ ì¶”ê°€
        remaining_slots = max_chunks - len(prioritized_chunks)
        if remaining_slots > 0:
            prioritized_chunks.extend(other_doc_chunks[:remaining_slots])
        
        self.logger.info(f"ì²­í¬ ì„ íƒ: ê°™ì€ ë¬¸ì„œ {len(same_doc_chunks)}ê°œ, ë‹¤ë¥¸ ë¬¸ì„œ {len(other_doc_chunks)}ê°œ â†’ ì´ {len(prioritized_chunks)}ê°œ ì‚¬ìš©")
        
        return prioritized_chunks
    
    def _remove_foreign_text(self, text: str) -> str:
        """ì™¸êµ­ì–´ í…ìŠ¤íŠ¸ ì œê±° í—¬í¼ ë©”ì„œë“œ"""
        import re
        
        # 1. ì¤‘êµ­ì–´ ë¬¸ì ë²”ìœ„ (CJK í†µí•© í•œì + í™•ì¥)
        chinese_pattern = r'[\u4e00-\u9fff\u3400-\u4dbf\uf900-\ufaff]'
        # 2. ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜/ì¹´íƒ€ì¹´ë‚˜
        japanese_pattern = r'[\u3040-\u309f\u30a0-\u30ff]'
        # 3. ì¤‘êµ­ì–´ êµ¬ë‘ì  ë° íŠ¹ìˆ˜ë¬¸ì
        chinese_punctuation = r'[ã€‚ï¼Œã€ï¼›ï¼šï¼Ÿï¼â€¦â€”Â·ã€Œã€ã€ã€ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€ˆã€‰]'
        
        # ë¼ì¸ë³„ í•„í„°ë§
        lines = text.split('\n')
        filtered_lines = []
        
        for line in lines:
            # ì™¸êµ­ì–´ê°€ ì—†ëŠ” ë¼ì¸ë§Œ ìœ ì§€
            if not (re.search(chinese_pattern, line) or 
                   re.search(japanese_pattern, line) or
                   re.search(chinese_punctuation, line)):
                filtered_lines.append(line)
        
        result = '\n'.join(filtered_lines)
        
        # ë¹ˆ ì¤„ ì •ë¦¬
        result = re.sub(r'\n\n+', '\n\n', result)
        
        return result.strip()
    
    def _validate_citations(self, answer: str, context_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì¸ìš© ìœ íš¨ì„± ê²€ì¦"""
        cited_files = []
        available_files = set()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ëª… ìˆ˜ì§‘
        for chunk in context_chunks:
            filename = Path(chunk.get('source', '')).name
            if filename:
                available_files.add(filename)
        
        # ë‹µë³€ì—ì„œ ì¸ìš© ì¶”ì¶œ (ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)
        for pattern in self.citation_patterns:
            matches = pattern.findall(answer)
            for match in matches:
                # íŒŒì¼ëª… ì •ë¦¬
                filename = match.strip()
                if filename.endswith('.pdf') and filename not in cited_files:
                    cited_files.append(filename)
        
        # ì¸ìš© ê²€ì¦
        valid_citations = []
        for cited_file in cited_files:
            # ì •í™•í•œ íŒŒì¼ëª… ë§¤ì¹­ ë˜ëŠ” ë¶€ë¶„ ë§¤ì¹­
            is_valid = any(
                cited_file == available_file or 
                cited_file in available_file or 
                available_file in cited_file
                for available_file in available_files
            )
            
            if is_valid:
                valid_citations.append(cited_file)
        
        return {
            'has_citations': len(valid_citations) > 0,
            'cited_files': valid_citations,
            'invalid_citations': [f for f in cited_files if f not in valid_citations],
            'citation_count': len(cited_files)
        }
    
    def _is_meaningful_answer(self, answer: str) -> bool:
        """ì˜ë¯¸ìˆëŠ” ë‹µë³€ì¸ì§€ íŒë‹¨ (ë¬¸ì„œ í™œìš© ì¤‘ì‹¬)"""
        
        # ë„ˆë¬´ ì§§ì€ ë‹µë³€ ì œì™¸
        if len(answer.strip()) < 15:
            return False
        
        # ì™„ì „ ê±°ë¶€ í‘œí˜„ë“¤ë§Œ ì œì™¸ (ë¶€ë¶„ì  ì •ë³´ë¼ë„ ì˜ë¯¸ ìˆìŒ)
        rejection_phrases = [
            "ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì „í˜€ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì™„ì „íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        ]
        
        answer_lower = answer.lower()
        for phrase in rejection_phrases:
            if phrase in answer_lower:
                return False
        
        # êµ¬ì²´ì  ì •ë³´ë‚˜ ë¬¸ì„œ í™œìš© ì‹ í˜¸ ì²´í¬
        meaningful_patterns = [
            r'\d{4}ë…„',  # ì—°ë„
            r'\d{1,3}(?:,\d{3})*ì›',  # ê¸ˆì•¡
            r'\d{1,3}(?:,\d{3})*ë§Œì›',  # ë§Œì›
            r'\d+ë§Œ',  # ê°„ë‹¨í•œ ë§Œ ë‹¨ìœ„
            r'\d+ì›',   # ê°„ë‹¨í•œ ì› ë‹¨ìœ„
            r'HP\s*Z8',  # ì œí’ˆëª…
            r'ì›Œí¬ìŠ¤í…Œì´ì…˜',  # ì¥ë¹„ëª…
            r'ì¹´ë©”ë¼',  # ì¥ë¹„ëª…
            r'ëª¨ë‹ˆí„°',  # ì¥ë¹„ëª…
            r'ê´‘í™”ë¬¸',  # ì¥ì†Œëª…
            r'ìŠ¤íŠœë””ì˜¤',  # ì¥ì†Œëª…
            r'ECM-77BC',  # ë§ˆì´í¬ ëª¨ë¸
            r'í‹°ë¹„ë¡œì§',  # ë¸Œëœë“œëª…
            r'ë·°íŒŒì¸ë”',  # ì¥ë¹„ëª…
            r'ì†Œëª¨í’ˆ',  # ì¹´í…Œê³ ë¦¬
            r'ì¼€ì´ë¸”',  # í’ˆëª©
            r'ê¸°ì•ˆì',  # ë¬¸ì„œ ìš”ì†Œ
            r'ê²€í† ',    # ë¬¸ì„œ ìœ í˜•
            r'ìŠ¹ì¸',    # ë¬¸ì„œ ìƒíƒœ
            r'êµ¬ë§¤',    # ì—…ë¬´ ìœ í˜•
            r'êµì²´',    # ì—…ë¬´ ìœ í˜•
        ]
        
        meaningful_count = 0
        for pattern in meaningful_patterns:
            if re.search(pattern, answer):
                meaningful_count += 1
        
        # êµ¬ì²´ì  ì •ë³´ê°€ 1ê°œ ì´ìƒ ìˆìœ¼ë©´ ì˜ë¯¸ìˆëŠ” ë‹µë³€ìœ¼ë¡œ ê°„ì£¼ (ë” ê´€ëŒ€í•˜ê²Œ)
        return meaningful_count >= 1
    
    def _apply_length_preference_adjustments(self, recommendation: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸¸ì´ ì„ í˜¸ë„ì— ë”°ë¥¸ ì¡°ì • ì ìš©"""
        
        # ê¸¸ì´ ì„ í˜¸ë„ë³„ ì¡°ì • ë¹„ìœ¨
        length_multipliers = {
            "concise": 0.7,     # 30% ì§§ê²Œ
            "balanced": 1.0,    # ê¸°ë³¸ê°’
            "detailed": 1.4     # 40% ê¸¸ê²Œ
        }
        
        multiplier = length_multipliers.get(self.config.length_preference, 1.0)
        
        # ì¡°ì •ëœ ê¸¸ì´ ê³„ì‚°
        adjusted_optimal = int(recommendation.optimal_length * multiplier)
        adjusted_min = int(recommendation.min_length * multiplier)
        adjusted_max = int(recommendation.max_length * multiplier)
        
        # ì‚¬ìš©ì ê°•ì œ ì„¤ì • ì ìš©
        if self.config.min_length_override:
            adjusted_min = max(adjusted_min, self.config.min_length_override)
        if self.config.max_length_override:
            adjusted_max = min(adjusted_max, self.config.max_length_override)
            
        # ìµœì†Œ-ìµœëŒ€ ë²”ìœ„ ë³´ì •
        adjusted_optimal = max(adjusted_min, min(adjusted_optimal, adjusted_max))
        
        return dict(
            optimal_length=adjusted_optimal,
            min_length=adjusted_min,
            max_length=adjusted_max,
            reasoning=recommendation.reasoning + f" | ì„ í˜¸ë„ ì¡°ì •: {self.config.length_preference} (x{multiplier})",
            content_density=recommendation.content_density,
            adjustment_factors=recommendation.adjustment_factors + [f"ê¸¸ì´ì„ í˜¸ë„_{self.config.length_preference}"]
        )
    
    def _calculate_adaptive_max_tokens(self, recommendation: Dict[str, Any]) -> int:
        """ì ì‘í˜• ê¸¸ì´ ì¶”ì²œì— ë”°ë¥¸ max_tokens ê³„ì‚°"""
        # í•œêµ­ì–´ ê¸°ì¤€: ëŒ€ëµ 1.5í† í°/ê¸€ì (ì—¬ìœ ë¥¼ ìœ„í•´ 2.0 ì‚¬ìš©)
        tokens_per_char = 2.0
        
        # ëª©í‘œ ê¸¸ì´ì—ì„œ í† í° ìˆ˜ ê³„ì‚° (ì—¬ìœ ë¶„ 20% ì¶”ê°€)
        adaptive_tokens = int(recommendation.max_length * tokens_per_char * 1.2)
        
        # ìµœì†Œ/ìµœëŒ€ í† í° ìˆ˜ ì œí•œ
        min_tokens = 50
        max_tokens = 800
        
        return max(min_tokens, min(adaptive_tokens, max_tokens))
    
    def _create_adaptive_system_prompt(self, recommendation: Dict[str, Any]) -> str:
        """ì ì‘í˜• ê¸¸ì´ ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_prompt = self.create_system_prompt()
        
        adaptive_instructions = f"""

ğŸ¯ **ì ì‘í˜• ë‹µë³€ ê¸¸ì´ ê°€ì´ë“œë¼ì¸**:
- ëª©í‘œ ê¸¸ì´: {recommendation.optimal_length}ì
- í—ˆìš© ë²”ìœ„: {recommendation.min_length}-{recommendation.max_length}ì
- ì½˜í…ì¸  ë°€ë„: {recommendation.content_density:.2f}
- ì¡°ì • ì´ìœ : {recommendation.reasoning}

ğŸ“ **ê¸¸ì´ ì¡°ì • ì›ì¹™**:
1. ì •í™•ì„±ê³¼ ì™„ì „ì„±ì„ ìš°ì„ ìœ¼ë¡œ í•˜ë˜, ê°€ëŠ¥í•œ ëª©í‘œ ê¸¸ì´ì— ë§ì¶”ì„¸ìš”
2. í•„ìˆ˜ ì •ë³´ (ê°€ê²©, ë‚ ì§œ, ëª¨ë¸ëª… ë“±)ëŠ” ê¸¸ì´ì™€ ìƒê´€ì—†ì´ í¬í•¨í•˜ì„¸ìš”
3. ë¶€ê°€ ì„¤ëª…ì€ ëª©í‘œ ê¸¸ì´ì— ë”°ë¼ ì¡°ì ˆí•˜ì„¸ìš”
4. ì¶œì²˜ ì¸ìš©ì€ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš” (ê¸¸ì´ì— í¬í•¨ë˜ì§€ ì•ŠìŒ)

âš¡ **íš¨ìœ¨ì  ë‹µë³€ êµ¬ì„±**:
- ì§§ì€ ë‹µë³€({recommendation.min_length}ì ë¯¸ë§Œ): í•µì‹¬ ì‚¬ì‹¤ë§Œ
- ì ë‹¹í•œ ë‹µë³€({recommendation.min_length}-{recommendation.optimal_length}ì): í•µì‹¬ + ê°„ë‹¨í•œ ì„¤ëª…
- ê¸´ ë‹µë³€({recommendation.optimal_length}ì ì´ìƒ): ìƒì„¸ ì„¤ëª… + ë°°ê²½ ì •ë³´"""

        return base_prompt + adaptive_instructions
    
    def _generate_structured_response_with_adaptive_length(self, question_analysis: Dict[str, Any],
                                                         context_chunks: List[Dict[str, Any]], 
                                                         max_retries: int,
                                                         length_recommendation: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì ì‘í˜• ê¸¸ì´ ì¡°ì •ì„ í¬í•¨í•œ êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±"""
        
        # ê¸°ì¡´ êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„± (ê¸¸ì´ ì¡°ì • ì—†ì´)
        base_response = self._generate_structured_response(question_analysis, context_chunks, max_retries)
        
        # ì ì‘í˜• ê¸¸ì´ ì¡°ì •ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not self.config.enable_adaptive_length or not length_recommendation:
            return base_response
        
        # ê¸¸ì´ ì¡°ì • ì ìš©
        original_length = len(base_response.answer)
        adjusted_answer, length_adjustments = self.length_analyzer.validate_and_adjust_answer(
            base_response.answer, length_recommendation)
        
        # ì ì‘í˜• ì •ë³´ê°€ ì¶”ê°€ëœ ìƒˆë¡œìš´ RAGResponse ìƒì„±
        return RAGResponse(
            answer=adjusted_answer,
            sources_cited=base_response.sources_cited,
            confidence=base_response.confidence,
            generation_time=base_response.generation_time,
            has_proper_citation=base_response.has_proper_citation,
            retry_count=base_response.retry_count,
            length_recommendation=length_recommendation,
            original_length=original_length,
            length_adjustments=length_adjustments,
            adaptive_length_used=True
        )
    
    def _calculate_confidence(self, answer: str, context_chunks: List[Dict[str, Any]]) -> float:
        """ë‹µë³€ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not context_chunks:
            return 0.0
        
        # ê¸°ë³¸ ì‹ ë¢°ë„ëŠ” ìµœê³  ìŠ¤ì½”ì–´ ì²­í¬ì˜ ì ìˆ˜
        base_confidence = max(chunk.get('score', 0.0) for chunk in context_chunks)
        
        # ì¸ìš© ë³´ë„ˆìŠ¤
        citation_check = self._validate_citations(answer, context_chunks)
        citation_bonus = min(0.2, len(citation_check['cited_files']) * 0.1)
        
        # ë‹µë³€ ê¸¸ì´ ê³ ë ¤ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë‹µë³€ì€ ì‹ ë¢°ë„ ê°ì†Œ)
        length_penalty = 0.0
        answer_length = len(answer)
        if answer_length < 50:
            length_penalty = 0.1
        elif answer_length > 1000:
            length_penalty = 0.05
        
        # ë¶€ì • í‘œí˜„ ê°ì§€ ("ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ë“±)
        negative_phrases = ["ì°¾ì„ ìˆ˜ ì—†", "í™•ì¸í•  ìˆ˜ ì—†", "ëª…ì‹œë˜ì§€ ì•Š", "ë¶ˆë¶„ëª…"]
        negative_penalty = 0.0
        for phrase in negative_phrases:
            if phrase in answer:
                negative_penalty = 0.2
                break
        
        confidence = base_confidence + citation_bonus - length_penalty - negative_penalty
        return max(0.0, min(1.0, confidence))
    
    def generate_conversational_response(self, question: str, context_chunks: List[Dict[str, Any]], 
                                        max_retries: int = 2) -> RAGResponse:
        """ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± (ChatGPT/Claude ìŠ¤íƒ€ì¼)"""
        
        start_time = time.time()
        
        # ëŒ€í™”í˜• ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - í•œêµ­ì–´ ì „ìš© ê°•í™”
        conversational_system = """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ChatGPTë‚˜ Claudeì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”.

ğŸš« ì ˆëŒ€ ê·œì¹™: ì˜¤ì§ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´(ä¸­æ–‡), ì˜ì–´(English), ì¼ë³¸ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

ì¤‘ìš” ì›ì¹™:
1. 100% í•œêµ­ì–´ë¡œë§Œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
2. í…œí”Œë¦¿ì´ë‚˜ ì •í˜•í™”ëœ í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
4. í•„ìš”í•˜ë©´ ì¶”ê°€ ì •ë³´ë‚˜ ëŒ€ì•ˆì„ ìì—°ìŠ¤ëŸ½ê²Œ ì œì•ˆí•˜ì„¸ìš”
5. ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”
6. ì¶œì²˜ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ë¬¸ì¥ ì†ì— ë…¹ì—¬ì„œ ì–¸ê¸‰í•˜ì„¸ìš”

ë‹µë³€ ìŠ¤íƒ€ì¼:
- ëŒ€í™”í•˜ë“¯ ìì—°ìŠ¤ëŸ½ê²Œ
- í•µì‹¬ì„ ë¨¼ì €, ì„¸ë¶€ì‚¬í•­ì€ ì´ì–´ì„œ
- ì‹¤ìš©ì ì´ê³  actionableí•œ ì •ë³´ ì œê³µ
- í•„ìš”ì‹œ ì¥ë‹¨ì ì´ë‚˜ ê³ ë ¤ì‚¬í•­ ì–¸ê¸‰

âš ï¸ ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°: ì ˆëŒ€ë¡œ ì¤‘êµ­ì–´ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."""
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”í˜• ë‹µë³€ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸
        context_parts = []
        for chunk in context_chunks[:5]:
            content = chunk.get('content', '').strip()
            source = chunk.get('source', 'unknown')
            
            if content:
                context_parts.append(f"[{source}]\n{content}")
        
        context = "\n\n".join(context_parts)
        
        conversational_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:

{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹µë³€ ë°©ì‹:
- ChatGPTë‚˜ Claudeì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ë“¯ ë‹µë³€
- í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ê°„ë‹¨íˆ ì„¤ëª…í•œ í›„ ì„¸ë¶€ì‚¬í•­ ì œê³µ
- ë„ì›€ì´ ë  ë§Œí•œ ì¶”ê°€ ì •ë³´ë‚˜ ê³ ë ¤ì‚¬í•­ë„ ì–¸ê¸‰
- ì¶œì²˜ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ë¬¸ì¥ ì†ì— í¬í•¨ (ì˜ˆ: 'êµ¬ë§¤ê¸°ì•ˆì„œì— ë”°ë¥´ë©´...')
- ë”±ë”±í•œ ë¦¬ìŠ¤íŠ¸ë‚˜ í…œí”Œë¦¿ í˜•ì‹ ì‚¬ìš© ê¸ˆì§€

âš ï¸ ì¤‘ìš” ì§€ì¹¨:
- ì œê³µëœ ë¬¸ì„œ ì •ë³´ì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ê¸ˆì•¡ì€ ë¬¸ì„œì— ëª…ì‹œëœ ì •í™•í•œ ê¸ˆì•¡ë§Œ ì–¸ê¸‰í•˜ì„¸ìš”
- ì¶”ì¸¡í•˜ê±°ë‚˜ ì—†ëŠ” ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”
- í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”

ğŸš¨ í•„ìˆ˜: ì˜¤ì§ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´(ä¸­æ–‡/æ±‰å­—), ì˜ì–´, ì¼ë³¸ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€!
ë‹µë³€ ì–¸ì–´: í•œêµ­ì–´ (Korean Only)"""
        
        retry_count = 0
        best_answer = None
        
        for attempt in range(max_retries + 1):
            try:
                messages = [
                    {"role": "system", "content": conversational_system},
                    {"role": "user", "content": conversational_prompt}
                ]
                
                response = self.llm.create_chat_completion(
                    messages=messages,
                    temperature=0.7,  # ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ
                    max_tokens=1500,
                    top_p=0.9,
                    top_k=40,
                    repeat_penalty=1.1,
                    stop=self.stop_tokens
                )
                
                answer = response['choices'][0]['message']['content']
                
                # ì™¸êµ­ì–´ í…ìŠ¤íŠ¸ í•„í„°ë§ (í—¬í¼ ë©”ì„œë“œ ì‚¬ìš©)
                answer = self._remove_foreign_text(answer)
                
                best_answer = answer
                
                # ì¶œì²˜ ì¶”ì¶œ (ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ëœ ê²½ìš°ë„ ì²˜ë¦¬)
                sources_cited = []
                for pattern in self.citation_patterns:
                    matches = re.findall(pattern, answer)
                    sources_cited.extend(matches)
                
                sources_cited = list(set(sources_cited))  # ì¤‘ë³µ ì œê±°
                
                generation_time = time.time() - start_time
                
                return RAGResponse(
                    answer=answer,
                    sources_cited=sources_cited,
                    confidence=0.85,
                    generation_time=generation_time,
                    has_proper_citation=len(sources_cited) > 0,
                    retry_count=retry_count
                )
                
            except Exception as e:
                self.logger.error(f"ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                retry_count += 1
                if attempt == max_retries:
                    break
        
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        if best_answer:
            return RAGResponse(
                answer=best_answer,
                sources_cited=[],
                confidence=0.5,
                generation_time=time.time() - start_time,
                has_proper_citation=False,
                retry_count=retry_count
            )
        
        return RAGResponse(
            answer="ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            sources_cited=[],
            confidence=0.0,
            generation_time=time.time() - start_time,
            has_proper_citation=False,
            retry_count=retry_count
        )
    
    def generate_smart_response(self, question: str, search_result: Dict[str, Any], max_retries: int = 2) -> RAGResponse:
        """ìŠ¤ë§ˆíŠ¸ ëª¨ë“œ ì‘ë‹µ ìƒì„±: ê²€ìƒ‰ ê²°ê³¼ì— ë”°ë¼ ì ì ˆí•œ ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        
        if not search_result.get('success', False):
            # ê²€ìƒ‰ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì˜¤ë¥˜ ì‘ë‹µ
            return RAGResponse(
                answer=search_result.get('error', 'ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'),
                sources_cited=[],
                confidence=0.0,
                generation_time=0.0,
                has_proper_citation=False,
                retry_count=0
            )
        
        mode = search_result.get('mode', 'chunk_search')
        start_time = time.time()
        
        if mode == 'full_document':
            # ì „ì²´ ë¬¸ì„œ ëª¨ë“œ
            return self._generate_full_document_response(question, search_result, max_retries, start_time)
        else:
            # ì²­í¬ ê²€ìƒ‰ ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹)
            search_results = search_result.get('search_results', {})
            context_chunks = search_results.get('fused_results', [])
            return self.generate_response(question, context_chunks, max_retries)
    
    def _generate_full_document_response(self, question: str, search_result: Dict[str, Any], 
                                       max_retries: int, start_time: float) -> RAGResponse:
        """ì „ì²´ ë¬¸ì„œ ëª¨ë“œ ì‘ë‹µ ìƒì„±"""
        
        full_doc_chunk = search_result.get('full_document_chunk', {})
        document_path = search_result.get('document_path', '')
        document_text = full_doc_chunk.get('content', '')
        
        if not document_text:
            generation_time = time.time() - start_time
            return RAGResponse(
                answer="ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                sources_cited=[],
                confidence=0.0,
                generation_time=generation_time,
                has_proper_citation=False,
                retry_count=0
            )
        
        # ì „ì²´ ë¬¸ì„œ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
        full_doc_prompt = self.create_full_document_prompt(question, document_text, document_path)
        
        retry_count = 0
        best_answer = None
        
        for attempt in range(max_retries + 1):
            try:
                # ì „ì²´ ë¬¸ì„œ ëª¨ë“œìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê°„ë‹¨í•˜ê³  ì§ì ‘ì )
                system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‘ì—…: ì œê³µëœ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ë¡œ ì™„ì „í•œ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.

RULES:
1. Extract ALL information from the document: names, dates, amounts, items
2. Create a comprehensive Korean summary using the extracted information
3. Never say "information not found" or "cannot confirm" 
4. Always include document citation in [filename.pdf] format
5. Respond ONLY in Korean language

EXAMPLE:
Document has: "ê¸°ì•ˆì ë‚¨ì¤€ìˆ˜, 2025-06-24, ì´ì•¡ 4,985,500ì›"
Response: "2025ë…„ 6ì›” 24ì¼ ë‚¨ì¤€ìˆ˜ê°€ ê¸°ì•ˆí•œ ë¬¸ì„œë¡œ, ì´ 4,985,500ì›ì˜ êµ¬ë§¤ ê±´ì…ë‹ˆë‹¤. [filename.pdf]"

Remember: Use the document content to create a useful Korean summary."""

                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_doc_prompt}
                ]
                
                response = self.llm.create_chat_completion(
                    messages=messages,
                    temperature=self.config.temperature,
                    max_tokens=1200,  # ì „ì²´ ë¬¸ì„œ ë‹µë³€ì€ ë” ê¸¸ ìˆ˜ ìˆìŒ
                    top_p=self.config.top_p,
                    top_k=self.config.top_k,
                    repeat_penalty=self.config.repeat_penalty,
                    stop=self.stop_tokens
                )
                
                answer = response['choices'][0]['message']['content'].strip()
                generation_time = time.time() - start_time
                
                # ì²« ë²ˆì§¸ ë‹µë³€ì„ ìµœì„ ì˜ ë‹µë³€ìœ¼ë¡œ ì €ì¥
                if best_answer is None and len(answer) > 10:
                    best_answer = {
                        'answer': answer,
                        'generation_time': generation_time,
                        'retry_count': retry_count
                    }
                
                # ì¸ìš© ê²€ì¦
                citation_check = self._validate_citations(answer, [full_doc_chunk])
                
                if citation_check['has_citations']:
                    # ì¸ìš©ì´ ìˆëŠ” ë‹µë³€ - ì¦‰ì‹œ ë°˜í™˜
                    return RAGResponse(
                        answer=answer,
                        sources_cited=citation_check['cited_files'],
                        confidence=self._calculate_confidence(answer, [full_doc_chunk]),
                        generation_time=generation_time,
                        has_proper_citation=True,
                        retry_count=retry_count
                    )
                else:
                    # ì¸ìš©ì´ ì—†ì§€ë§Œ ì˜ë¯¸ìˆëŠ” ë‹µë³€ì´ë©´ ì €ì¥
                    if self._is_meaningful_answer(answer):
                        best_answer = {
                            'answer': answer,
                            'generation_time': generation_time,
                            'retry_count': retry_count
                        }
                        self.logger.info(f"ì¸ìš© ì—†ì§€ë§Œ ì˜ë¯¸ìˆëŠ” ë‹µë³€ ì €ì¥ (ì‹œë„ {attempt + 1})")
                    
                    # ì¬ì‹œë„
                    retry_count += 1
                    if attempt < max_retries:
                        self.logger.warning(f"ì¸ìš© ì—†ëŠ” ì‘ë‹µ, ì¬ì‹œë„ {attempt + 1}/{max_retries}")
                        continue
                
            except Exception as e:
                self.logger.error(f"ì „ì²´ ë¬¸ì„œ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                retry_count += 1
                if attempt < max_retries:
                    continue
        
        # ëª¨ë“  ì¬ì‹œë„ ì™„ë£Œ - ìµœì„ ì˜ ë‹µë³€ ë°˜í™˜
        if best_answer and len(best_answer['answer']) > 10:
            return RAGResponse(
                answer=best_answer['answer'],
                sources_cited=[],
                confidence=self._calculate_confidence(best_answer['answer'], [full_doc_chunk]) * 0.7,
                generation_time=best_answer['generation_time'],
                has_proper_citation=False,
                retry_count=best_answer['retry_count']
            )
        
        # ì™„ì „ ì‹¤íŒ¨
        generation_time = time.time() - start_time
        return RAGResponse(
            answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            sources_cited=[],
            confidence=0.0,
            generation_time=generation_time,
            has_proper_citation=False,
            retry_count=retry_count
        )
    
    def test_model(self) -> bool:
        """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        try:
            test_messages = [
                {"role": "system", "content": "ê°„ë‹¨í•œ ì¸ì‚¬ë¥¼ í•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”?"}
            ]
            
            response = self.llm.create_chat_completion(
                messages=test_messages,
                max_tokens=50,
                temperature=0.1
            )
            
            answer = response['choices'][0]['message']['content'].strip()
            self.logger.info(f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {answer[:50]}...")
            return True
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜  
def test_qwen_llm():
    """Qwen LLM í…ŒìŠ¤íŠ¸"""
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ëª¨ë¸ íŒŒì¼ë“¤ì„ í•©ì³ì„œ ì‚¬ìš©)
    try:
        from config import QWEN_MODEL_PATH
        model_files = [QWEN_MODEL_PATH]
    except ImportError:
        model_files = [
            "./models/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf",
            "./models/qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf"
        ]
    
    # ì²« ë²ˆì§¸ íŒŒì¼ë§Œ ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
    if Path(model_files[0]).exists():
        try:
            qwen = QwenLLM(model_files[0])
            
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
            if qwen.test_model():
                print("âœ“ Qwen ëª¨ë¸ ë¡œë“œ ë° ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
            # RAG í…ŒìŠ¤íŠ¸
            test_context = [
                {
                    'source': '2025-01-09_ê´‘í™”ë¬¸ìŠ¤íŠœë””ì˜¤ëª¨ë‹ˆí„°êµì²´ê²€í† ì„œ.pdf',
                    'content': 'ì´ ê²€í†  ê¸ˆì•¡ì€ 9,760,000ì›ì…ë‹ˆë‹¤. ëª¨ë‹ˆí„° 3ëŒ€ì™€ On-Air Tally ì‹œìŠ¤í…œì„ í¬í•¨í•©ë‹ˆë‹¤.',
                    'score': 0.85
                }
            ]
            
            response = qwen.generate_response(
                "ê²€í†  ê¸ˆì•¡ì´ ì–¼ë§ˆì¸ê°€ìš”?", 
                test_context
            )
            
            print(f"\n=== RAG í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
            print(f"ë‹µë³€: {response.answer}")
            print(f"ì¸ìš© íŒŒì¼: {response.sources_cited}")
            print(f"ì‹ ë¢°ë„: {response.confidence:.3f}")
            print(f"ì‘ë‹µ ì‹œê°„: {response.generation_time:.2f}ì´ˆ")
            print(f"ì˜¬ë°”ë¥¸ ì¸ìš©: {response.has_proper_citation}")
            
        except Exception as e:
            print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    else:
        print(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_files[0]}")

class LlamaLLM:
    """Llama-3.1-Korean-8B-Instruct ëª¨ë¸ (Transformers ê¸°ë°˜)"""
    
    def __init__(self, model_path: str = None):
        if model_path is None:
            try:
                from config import LLAMA_MODEL_PATH
                model_path = LLAMA_MODEL_PATH
            except ImportError:
                model_path = "./models/Llama-3.1-Korean-8B-Instruct"
        self.model_path = model_path
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Llama ëª¨ë¸ ë¡œë“œ"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            self.logger.info(f"Llama ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float32
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.logger.info("âœ… Llama ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"Llama ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def generate_response(self, question: str, context_chunks: List[Dict[str, Any]]) -> RAGResponse:
        """Llamaë¡œ ì‘ë‹µ ìƒì„±"""
        start_time = time.time()
        
        try:
            # í•œêµ­ì–´ íŠ¹í™” í”„ë¡¬í”„íŠ¸ 
            context_text = self._format_context(context_chunks)
            
            prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context_text}

ì§ˆë¬¸: {question}

ë‹µë³€ ì§€ì¹¨:
- ë¬¸ì„œì— ìˆëŠ” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
- "í™•ì¸ë˜ì§€ ì•ŠìŒ"ì´ë‚˜ "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ê°™ì€ ì• ë§¤í•œ ë‹µë³€ì€ í”¼í•˜ì„¸ìš”  
- ë‚ ì§œ, ì´ë¦„, ê¸ˆì•¡, ë¶€ì„œ ë“± êµ¬ì²´ì  ì •ë³´ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
- ì¶œì²˜ íŒŒì¼ëª…ì„ ë‹µë³€ ë§ˆì§€ë§‰ì— ëª…ì‹œí•˜ì„¸ìš”

ë‹µë³€:"""

            from transformers import GenerationConfig
            import torch
            
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    generation_config=GenerationConfig(
                        max_new_tokens=800,
                        temperature=0.8,
                        top_p=0.95,
                        do_sample=True,
                        eos_token_id=self.tokenizer.eos_token_id,
                        pad_token_id=self.tokenizer.pad_token_id
                    )
                )
            
            response_text = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            generation_time = time.time() - start_time
            
            return RAGResponse(
                answer=response_text.strip(),
                sources_cited=self._extract_sources(context_chunks),
                confidence=0.9,
                generation_time=generation_time,
                has_proper_citation=True,
                retry_count=0
            )
            
        except Exception as e:
            self.logger.error(f"Llama ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return RAGResponse(
                answer="ì‘ë‹µ ìƒì„± ì‹¤íŒ¨",
                sources_cited=[],
                confidence=0.0,
                generation_time=time.time() - start_time,
                has_proper_citation=False,
                retry_count=0
            )
    
    def _format_context(self, context_chunks: List[Dict[str, Any]]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        formatted = []
        for chunk in context_chunks:
            source = Path(chunk.get('source', '')).name
            content = chunk.get('content', '')
            formatted.append(f"[{source}]\n{content}")
        return "\n\n".join(formatted)
    
    def _extract_sources(self, context_chunks: List[Dict[str, Any]]) -> List[str]:
        """ì†ŒìŠ¤ ì¶”ì¶œ"""
        return [Path(chunk.get('source', '')).name for chunk in context_chunks]

def create_llm(model_type: str = "llama", **kwargs) -> Any:
    """LLM íŒ©í† ë¦¬ í•¨ìˆ˜"""
    if model_type.lower() == "llama":
        return LlamaLLM(**kwargs)
    elif model_type.lower() == "qwen":
        return QwenLLM(**kwargs)
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")

if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    test_qwen_llm()