#!/usr/bin/env python3
"""
Context Hydrator ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

ì¸¡ì • í•­ëª©:
- extraction_time: p50, p95, p99
- total_length: í‰ê· , ìµœì†Œ, ìµœëŒ€
- compression_applied: ë¹„ìœ¨
- pdf_tail_status: ì„±ê³µ/ì‹¤íŒ¨ ë¹„ìœ¨

ì¼€ì´ìŠ¤:
- (i) ì²­í¬ë§Œ
- (ii) ì²­í¬ + PDF í…Œì¼
- (iii) í…ìŠ¤íŠ¸ ë ˆì´ì–´ ì—†ëŠ” PDF (ì‹¤íŒ¨ ì¼€ì´ìŠ¤)
"""

import json
import os
import statistics
import sys
import time
from pathlib import Path
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.rag.utils.context_hydrator import hydrate_context


def load_sample_chunks(data_dir: Path) -> List[Dict[str, Any]]:
    """
    ìƒ˜í”Œ ì²­í¬ ë°ì´í„° ë¡œë“œ

    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬

    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    chunks = []

    # íŒ¨í„´ 1: ì²­í¬ë§Œ (JSON íŒŒì¼)
    chunks_file = data_dir / "sample_chunks.json"
    if chunks_file.exists():
        with open(chunks_file, encoding="utf-8") as f:
            chunks.extend(json.load(f))

    # íŒ¨í„´ 2: PDF íŒŒì¼ì—ì„œ ì²­í¬ ìƒì„±
    pdf_files = list(data_dir.glob("**/*.pdf"))
    for pdf_path in pdf_files[:10]:  # ìµœëŒ€ 10ê°œ
        chunks.append({"file_path": str(pdf_path), "text": f"ìƒ˜í”Œ ì²­í¬ from {pdf_path.name}"})

    return chunks


def benchmark_case(
    chunks: List[Dict[str, Any]], mode: str, case_name: str, repeat: int = 10
) -> Dict[str, Any]:
    """
    ì¼€ì´ìŠ¤ë³„ ë²¤ì¹˜ë§ˆí¬

    Args:
        chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        mode: ìƒì„± ëª¨ë“œ (rag/summarize)
        case_name: ì¼€ì´ìŠ¤ ì´ë¦„
        repeat: ë°˜ë³µ íšŸìˆ˜

    Returns:
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    """
    times = []
    lengths = []
    token_estimates = []
    compression_count = 0
    pdf_success_count = 0
    pdf_fail_count = 0
    truncate_counts = {"none": 0, "compact": 0, "hardcut": 0}

    for _ in range(repeat):
        start = time.perf_counter()
        text, metrics = hydrate_context(chunks, max_len=10000, mode=mode)
        elapsed = time.perf_counter() - start

        times.append(elapsed)
        lengths.append(metrics["total_length"])
        token_estimates.append(metrics["token_estimate"])

        if metrics["compression_applied"]:
            compression_count += 1

        if metrics["pdf_tail_status"] == "success":
            pdf_success_count += 1
        elif metrics["pdf_tail_status"] == "fail":
            pdf_fail_count += 1

        truncate_counts[metrics["truncate_reason"]] += 1

    return {
        "case": case_name,
        "mode": mode,
        "repeat": repeat,
        "time_p50": statistics.median(times),
        "time_p95": statistics.quantiles(times, n=20)[18] if len(times) >= 20 else max(times),
        "time_p99": statistics.quantiles(times, n=100)[98] if len(times) >= 100 else max(times),
        "time_avg": statistics.mean(times),
        "length_avg": statistics.mean(lengths),
        "length_min": min(lengths),
        "length_max": max(lengths),
        "token_estimate_avg": statistics.mean(token_estimates),
        "compression_ratio": compression_count / repeat,
        "pdf_success_ratio": pdf_success_count / repeat if (pdf_success_count + pdf_fail_count) > 0 else 0,
        "pdf_fail_ratio": pdf_fail_count / repeat if (pdf_success_count + pdf_fail_count) > 0 else 0,
        "truncate_distribution": truncate_counts,
    }


def main():
    """ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("=" * 80)
    print("Context Hydrator ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 80)

    # ë°ì´í„° ë””ë ‰í† ë¦¬
    data_dir = Path(os.getenv("DOCS_DIR", "docs"))
    if not data_dir.exists():
        print(f"âš ï¸ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {data_dir}")
        return

    # ìƒ˜í”Œ ì²­í¬ ë¡œë“œ
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ: {data_dir}")
    all_chunks = load_sample_chunks(data_dir)

    if not all_chunks:
        print("âš ï¸ ìƒ˜í”Œ ì²­í¬ ì—†ìŒ. docs/ í´ë”ì— PDF íŒŒì¼ì„ ì¶”ê°€í•˜ì„¸ìš”.")
        return

    print(f"âœ… ì²­í¬ ë¡œë“œ ì™„ë£Œ: {len(all_chunks)}ê°œ")

    # ì¼€ì´ìŠ¤ë³„ ë²¤ì¹˜ë§ˆí¬
    cases = [
        ("chunks_only", all_chunks[:3], "rag"),  # ì²­í¬ë§Œ
        ("chunks_pdf_tail", all_chunks[:5], "rag"),  # ì²­í¬ + PDF í…Œì¼
        ("summarize_mode", all_chunks[:3], "summarize"),  # Summarize ëª¨ë“œ
    ]

    results = []
    for case_name, chunks, mode in cases:
        print(f"\nğŸ” ì¼€ì´ìŠ¤: {case_name} (mode={mode}, chunks={len(chunks)})")
        result = benchmark_case(chunks, mode, case_name, repeat=10)
        results.append(result)

        # ê²°ê³¼ ì¶œë ¥
        print(f"  â±ï¸  Time: p50={result['time_p50']:.3f}s, p95={result['time_p95']:.3f}s")
        print(f"  ğŸ“ Length: avg={result['length_avg']:.0f}, min={result['length_min']}, max={result['length_max']}")
        print(f"  ğŸ”¢ Token Estimate: avg={result['token_estimate_avg']:.0f}")
        print(f"  ğŸ—œï¸  Compression Ratio: {result['compression_ratio']:.1%}")
        print(f"  ğŸ“„ PDF Success Ratio: {result['pdf_success_ratio']:.1%}")
        print(f"  âœ‚ï¸  Truncate: {result['truncate_distribution']}")

    # ì„ê³„ê°’ ì²´í¬
    print("\n" + "=" * 80)
    print("ğŸ“Š ì„ê³„ê°’ ê²€ì¦")
    print("=" * 80)

    thresholds = {
        "time_p95": 0.400,  # 400ms (pdfplumber í¬í•¨)
        "compression_ratio_max": 0.80,  # 80% ì´ìƒì´ë©´ íŠœë‹ í•„ìš”
    }

    for result in results:
        print(f"\nì¼€ì´ìŠ¤: {result['case']}")

        # Time p95 ì²´í¬
        if result["time_p95"] > thresholds["time_p95"]:
            print(f"  âš ï¸  Time p95 ì´ˆê³¼: {result['time_p95']:.3f}s > {thresholds['time_p95']:.3f}s")
        else:
            print(f"  âœ… Time p95 OK: {result['time_p95']:.3f}s")

        # Compression Ratio ì²´í¬
        if result["compression_ratio"] > thresholds["compression_ratio_max"]:
            print(
                f"  âš ï¸  Compression Ratio ë†’ìŒ: {result['compression_ratio']:.1%} "
                f"(CONTEXT_MAX_TOKENS ë˜ëŠ” TOKENS_PER_CHAR íŠœë‹ í•„ìš”)"
            )
        else:
            print(f"  âœ… Compression Ratio OK: {result['compression_ratio']:.1%}")

    # JSON ì €ì¥
    output_file = Path("reports") / f"benchmark_context_hydrator_{int(time.time())}.json"
    output_file.parent.mkdir(exist_ok=True)
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")


if __name__ == "__main__":
    main()
