#!/usr/bin/env python3
"""
ë¬¸ì„œ íˆ¬ì… ì¸ë±ì‹± CLI
docs/incoming/*.pdfë¥¼ ìŠ¤ìº”í•˜ì—¬ ë©”íƒ€DB ë° ë²¡í„° ì¸ë±ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/ingest_from_docs.py                    # ì „ì²´ ì²˜ë¦¬
    python scripts/ingest_from_docs.py --limit 10         # ìµœëŒ€ 10ê°œë§Œ
    python scripts/ingest_from_docs.py --only "2025*"     # íŒ¨í„´ ë§¤ì¹­
    python scripts/ingest_from_docs.py --dry-run          # ì‹¤ì œ ì´ë™/ì—…ì„œíŠ¸ ì—†ì´ ë¦¬í¬íŠ¸ë§Œ
    python scripts/ingest_from_docs.py --ocr              # OCR í™œì„±í™”
"""

import argparse
import hashlib
import json
import re
import shutil
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Tuple, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.logging import get_logger
from app.rag.parse.doctype import classify_document
from app.rag.parse.parse_meta import MetaParser
from app.rag.parse.parse_tables import TableParser
from app.rag.preprocess.clean_text import TextCleaner
from modules.metadata_db import MetadataDB

logger = get_logger(__name__)


def extract_claimed_total_fallback(text: str) -> Optional[int]:
    """ë³¸ë¬¸ì—ì„œ ë¹„ìš© í•©ê³„ ê¸ˆì•¡ì„ í´ë°± ì¶”ì¶œ

    Args:
        text: ë¬¸ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸

    Returns:
        ì¶”ì¶œëœ ê¸ˆì•¡ (ì •ìˆ˜) ë˜ëŠ” None
    """
    # ğŸ›¡ï¸ ì˜¤ë§¤ì¹­ ë°©ì§€: ìˆ˜ëŸ‰ íŒ¨í„´ ì œì™¸ ("í•©ê³„ 2000ê°œ" ê°™ì€ ì¼€ì´ìŠ¤)
    if re.search(r"í•©ê³„\s*[\d,]+\s*ê°œ\b", text):
        logger.debug("ìˆ˜ëŸ‰ íŒ¨í„´ ê°ì§€ (í•©ê³„ Nê°œ), ê¸ˆì•¡ ì¶”ì¶œ ìŠ¤í‚µ")
        return None

    # í•©ê³„ ë¼ë²¨ íŒ¨í„´ (OR): ë¹„ìš© í•©ê³„, í•©ê³„(VATë³„ë„), í•©ê³„, ì´ê³„
    label_pattern = r"(?:ë¹„ìš©\s*í•©ê³„|í•©ê³„\s*\(VAT\s*ë³„ë„\)|í•©ê³„(?!\s*ê²€ì¦)|ì´ê³„)"
    # ê¸ˆì•¡ íŒ¨í„´: ì„ íƒì  í†µí™” ê¸°í˜¸ + ìˆ«ì+êµ¬ë¶„ì + ì„ íƒì  í†µí™” ë‹¨ìœ„
    amount_pattern = r"(?:â‚©|KRW)?\s*([\d\.,]+)\s*(?:ì›|KRW|â‚©)?"

    # ì „ì²´ íŒ¨í„´: ë¼ë²¨ + ì„ íƒì  ê³µë°±/ê¸°í˜¸ + ê¸ˆì•¡
    full_pattern = label_pattern + r"\s*[:\s]*" + amount_pattern

    match = re.search(full_pattern, text)
    if not match:
        return None

    amount_str = match.group(1)

    try:
        # ìˆ«ì ì •ê·œí™”: , â‚© ì› ê³µë°± ì œê±°
        normalized = amount_str.replace(",", "").replace("â‚©", "").replace("ì›", "").replace(" ", "")
        claimed_total = int(normalized)

        # ğŸ›¡ï¸ ìµœì†Œ ê¸ˆì•¡ í•„í„°: 1ë§Œì› ë¯¸ë§Œì€ ì˜ì‹¬ (ìˆ˜ëŸ‰ ì˜¤ì¸ ê°€ëŠ¥ì„±)
        if claimed_total < 10000:
            logger.warning(f"claimed_total={claimed_total:,}ì› ë„ˆë¬´ ì‘ìŒ, ìˆ˜ëŸ‰ ì˜¤ì¸ ê°€ëŠ¥ì„±ìœ¼ë¡œ ì œì™¸")
            return None

        logger.info(f"claimed_total_fallback={claimed_total:,}ì› (íŒ¨í„´: {match.group(0)[:50]})")
        return claimed_total
    except (ValueError, OverflowError) as e:
        logger.warning(f"claimed_total ë³€í™˜ ì‹¤íŒ¨: {amount_str} - {e}")
        return None


class DocumentIngester:
    """ë¬¸ì„œ íˆ¬ì… ì²˜ë¦¬ê¸°"""

    def __init__(
        self,
        incoming_dir: str = "docs/incoming",
        processed_dir: str = "docs/processed",
        rejected_dir: str = "docs/rejected",
        quarantine_dir: str = "docs/quarantine",
        extracted_dir: str = "data/extracted",
        db_path: str = "metadata.db",
        ocr_enabled: bool = False,
        dry_run: bool = False,
    ):
        self.incoming_dir = Path(incoming_dir)
        self.processed_dir = Path(processed_dir)
        self.rejected_dir = Path(rejected_dir)
        self.quarantine_dir = Path(quarantine_dir)
        self.extracted_dir = Path(extracted_dir)
        self.db_path = db_path
        self.ocr_enabled = ocr_enabled
        self.dry_run = dry_run

        # í´ë” ìƒì„±
        for d in [
            self.processed_dir,
            self.rejected_dir,
            self.quarantine_dir,
            self.extracted_dir,
        ]:
            d.mkdir(parents=True, exist_ok=True)

        # íŒŒì„œ ì´ˆê¸°í™”
        self.meta_parser = MetaParser()
        self.table_parser = TableParser()
        self.text_cleaner = TextCleaner()

        # DB ì—°ê²°
        if not dry_run:
            self.db = MetadataDB(db_path=db_path)
        else:
            self.db = None

        # í†µê³„
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "duplicate": 0,
            "rejected": 0,
            "quarantined": 0,
        }
        self.results = []

    def _compute_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (SHA1)"""
        sha1 = hashlib.sha1()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                sha1.update(chunk)
        return sha1.hexdigest()

    def _normalize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª… ì •ê·œí™” (ì¤‘ë³µ íŒì •ìš©)"""
        import unicodedata

        n = filename.strip()
        n = unicodedata.normalize("NFKC", n)
        n = n.replace(" ", "_").replace("-", "_").lower()
        n = re.sub(r"\((\d+)\)(?=\.pdf$)", "", n, flags=re.I)
        n = re.sub(r"_(\d+)(?=\.pdf$)", "", n, flags=re.I)
        n = re.sub(r"__+", "_", n)
        return n

    def _extract_text_from_pdf(self, pdf_path: Path) -> Tuple[str, Dict[str, Any]]:
        """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            import pdfplumber

            text_pages = []
            metadata = {}

            with pdfplumber.open(pdf_path) as pdf:
                metadata["page_count"] = len(pdf.pages)
                metadata["file_size"] = pdf_path.stat().st_size

                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        text_pages.append(text)

            full_text = "\n\n".join(text_pages)

            if not full_text and self.ocr_enabled:
                logger.warning(f"pdfplumber ì‹¤íŒ¨, OCR í´ë°±: {pdf_path.name}")
                full_text = self._ocr_extract(pdf_path)

            return full_text, metadata

        except Exception as e:
            logger.error(f"PDF ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path.name} - {e}")
            return "", {}

    def _ocr_extract(self, pdf_path: Path) -> str:
        """OCRì„ ì‚¬ìš©í•œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            import pytesseract
            from pdf2image import convert_from_path

            logger.info(f"OCR ì¶”ì¶œ ì‹œì‘: {pdf_path.name}")

            # PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
            images = convert_from_path(pdf_path, dpi=300)

            # ê° í˜ì´ì§€ OCR
            text_pages = []
            for i, image in enumerate(images, 1):
                logger.debug(f"  í˜ì´ì§€ {i}/{len(images)} OCR ì¤‘...")
                text = pytesseract.image_to_string(image, lang="kor+eng")
                if text.strip():
                    text_pages.append(text)

            full_text = "\n\n".join(text_pages)
            logger.info(f"OCR ì™„ë£Œ: {pdf_path.name}, {len(full_text)}ì ì¶”ì¶œ")

            return full_text

        except ImportError as e:
            logger.error(f"OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜: {e}")
            logger.error("ì„¤ì¹˜ ë°©ë²•: pip install pytesseract pdf2image")
            return ""
        except Exception as e:
            logger.error(f"OCR ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path.name} - {e}")
            return ""

    def _is_duplicate(
        self, file_path: Path, file_hash: str, norm_filename: str
    ) -> Tuple[bool, str]:
        """ì¤‘ë³µ íŒì •"""
        if self.dry_run or not self.db:
            return False, ""

        # í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (í–¥í›„ êµ¬í˜„ - DBì— hash ì»¬ëŸ¼ ì¶”ê°€ í•„ìš”)
        # ...

        # ì •ê·œí™”ëœ íŒŒì¼ëª… ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        cursor = self.db.conn.execute("SELECT filename, path FROM documents")
        for row in cursor.fetchall():
            existing_filename = row[0]
            existing_norm = self._normalize_filename(existing_filename)
            if existing_norm == norm_filename:
                return True, f"ì •ê·œí™” íŒŒì¼ëª… ì¤‘ë³µ: {existing_filename}"

        return False, ""

    def process_file(self, pdf_path: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬"""
        start_time = time.time()
        result = {
            "filename": pdf_path.name,
            "status": "unknown",
            "reason": "",
            "duration_ms": 0,
            "doctype": "",
            "actions": [],
        }

        try:
            # 1. í•´ì‹œ ê³„ì‚°
            file_hash = self._compute_hash(pdf_path)
            norm_filename = self._normalize_filename(pdf_path.name)
            result["actions"].append(f"hash={file_hash[:8]}")

            # 2. ì¤‘ë³µ ì²´í¬
            is_dup, dup_reason = self._is_duplicate(pdf_path, file_hash, norm_filename)
            if is_dup:
                result["status"] = "duplicate"
                result["reason"] = dup_reason
                self.stats["duplicate"] += 1
                return result

            # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            raw_text, pdf_meta = self._extract_text_from_pdf(pdf_path)
            if not raw_text:
                result["status"] = "rejected"
                result["reason"] = "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ë¹ˆ PDF)"
                self.stats["rejected"] += 1
                if not self.dry_run:
                    self._move_file(pdf_path, self.rejected_dir)
                result["actions"].append("â†’rejected")
                return result

            result["actions"].append(f"extracted={len(raw_text)}chars")

            # 4. í…ìŠ¤íŠ¸ í´ë¦¬ë‹
            cleaned_text, _ = self.text_cleaner.clean(raw_text)
            result["actions"].append(f"cleaned={len(cleaned_text)}chars")

            # 5. doctype ë¶„ë¥˜
            doctype_info = classify_document(cleaned_text[:2000], pdf_path.name)
            doctype = doctype_info.get("doctype", "unknown")
            result["doctype"] = doctype
            result["actions"].append(f"doctype={doctype}")

            # 6. ë©”íƒ€ë°ì´í„° íŒŒì‹±
            # ê°„ë‹¨í•œ ë©”íƒ€ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” PDF ë©”íƒ€ë°ì´í„°ë¥¼ ë” ìƒì„¸íˆ íŒŒì‹±í•´ì•¼ í•¨)
            from modules.metadata_extractor import MetadataExtractor
            import re

            extractor = MetadataExtractor()
            extracted_meta = extractor.extract_all(raw_text, pdf_path.name)

            # í•œê¸€ í•„ë“œëª… ì§ì ‘ ì¶”ì¶œ (ê¸°ì•ˆì„œ í”„ë¦°íŠ¸ë·° ì „ìš©)
            korean_fields = {}

            # ì‹œí–‰ì¼ì ì¶”ì¶œ
            action_date_match = re.search(r"ì‹œí–‰ì¼ì\s+(\d{4}[-./]\d{1,2}[-./]\d{1,2}(?:\s*~\s*\d{4}[-./]\d{1,2}[-./]\d{1,2})?)", raw_text)
            if action_date_match:
                korean_fields["ì‹œí–‰ì¼ì"] = action_date_match.group(1)

            # ê¸°ì•ˆì¼ì ì¶”ì¶œ
            draft_date_match = re.search(r"ê¸°ì•ˆì¼ì\s+(\d{4}[-./]\d{1,2}[-./]\d{1,2}(?:\s+\d{1,2}:\d{2})?)", raw_text)
            if draft_date_match:
                korean_fields["ê¸°ì•ˆì¼ì"] = draft_date_match.group(1)

            # ì‘ì„±ì¼ì ì¶”ì¶œ
            created_date_match = re.search(r"ì‘ì„±ì¼ì\s+(\d{4}[-./]\d{1,2}[-./]\d{1,2})", raw_text)
            if created_date_match:
                korean_fields["ì‘ì„±ì¼ì"] = created_date_match.group(1)

            # ê¸°ì•ˆì ì¶”ì¶œ
            drafter_match = re.search(r"ê¸°ì•ˆì\s+([ê°€-í£]{2,4})", raw_text)
            if drafter_match:
                korean_fields["ê¸°ì•ˆì"] = drafter_match.group(1)

            # ê¸°ì•ˆë¶€ì„œ ì¶”ì¶œ
            dept_match = re.search(r"ê¸°ì•ˆë¶€ì„œ\s+([^\n]+)", raw_text)
            if dept_match:
                korean_fields["ê¸°ì•ˆë¶€ì„œ"] = dept_match.group(1).strip()

            # í•œê¸€ í•„ë“œì™€ ì˜ë¬¸ í•„ë“œ ë³‘í•©
            merged_meta = {**extracted_meta, **korean_fields}

            # ë‚ ì§œ/ì‘ì„±ì/ë¶€ì„œ íŒŒì‹±
            parsed_meta = self.meta_parser.parse(
                merged_meta, title=pdf_path.stem, content=cleaned_text[:1000]
            )
            result["actions"].append("meta_parsed")

            # 7. í‘œ íŒŒì‹± (ë¹„ìš©í‘œ)
            tables = self.table_parser.parse(raw_text)
            cost_data = None
            if tables.get("cost_table"):
                cost_data = tables["cost_table"]
                result["actions"].append(
                    f"cost_items={len(cost_data.get('items', []))}"
                )

            # 7.1 claimed_total ì¶”ì¶œ (í‘œ íŒŒì‹± â†’ í´ë°±)
            claimed_total = None
            if cost_data and cost_data.get("claimed_total"):
                claimed_total = cost_data.get("claimed_total")
            else:
                # í´ë°±: ë³¸ë¬¸ì—ì„œ "ë¹„ìš© í•©ê³„", "í•©ê³„(VATë³„ë„)" ë“± ì¶”ì¶œ
                claimed_total = extract_claimed_total_fallback(raw_text)
                if claimed_total:
                    result["actions"].append(f"claimed_total_fallback={claimed_total:,}")

            # 7.2 sum_match ê³„ì‚°
            sum_match = None
            if claimed_total is not None and cost_data and cost_data.get("items"):
                # ë¼ì¸ì•„ì´í…œì´ ìˆìœ¼ë©´ í•©ê³„ ê²€ì¦
                items = cost_data.get("items", [])
                items_sum = sum(item.get("amount", 0) for item in items if item.get("amount"))

                if items_sum > 0:
                    # Â±1ì› í—ˆìš© (ë°˜ì˜¬ë¦¼ ì˜¤ì°¨)
                    if abs(items_sum - claimed_total) <= 1:
                        sum_match = True
                        result["actions"].append(f"sum_match=True ({items_sum:,}â‰ˆ{claimed_total:,})")
                    else:
                        sum_match = False
                        result["actions"].append(f"sum_match=False ({items_sum:,}â‰ {claimed_total:,})")
            # ë¼ì¸ì•„ì´í…œ ì—†ìœ¼ë©´ sum_matchëŠ” None ìœ ì§€

            # 8. í…ìŠ¤íŠ¸ ì €ì¥
            if not self.dry_run:
                extracted_file = self.extracted_dir / f"{pdf_path.stem}.txt"
                extracted_file.write_text(cleaned_text, encoding="utf-8")
                result["actions"].append(f"savedâ†’{extracted_file.name}")

            # 9. ë©”íƒ€DB ì—…ì„œíŠ¸
            if not self.dry_run and self.db:
                doc_metadata = {
                    "path": str(pdf_path.resolve().relative_to(Path.cwd())),
                    "filename": pdf_path.name,
                    "title": parsed_meta.get("title", pdf_path.stem),
                    "date": parsed_meta.get("display_date", ""),
                    "year": (
                        parsed_meta.get("display_date", "")[:4]
                        if parsed_meta.get("display_date")
                        else ""
                    ),
                    "month": (
                        parsed_meta.get("display_date", "")[:7]
                        if len(parsed_meta.get("display_date", "")) >= 7
                        else ""
                    ),
                    "category": parsed_meta.get("category", ""),
                    "drafter": parsed_meta.get("drafter", ""),
                    "amount": cost_data.get("total", 0) if cost_data else 0,
                    "file_size": pdf_meta.get("file_size", 0),
                    "page_count": pdf_meta.get("page_count", 0),
                    "text_preview": cleaned_text[:500],
                    "keywords": [],
                    "doctype": doctype,
                    "display_date": parsed_meta.get("display_date", ""),
                    "claimed_total": claimed_total,
                    "sum_match": sum_match,
                }
                self.db.add_document(doc_metadata)
                result["actions"].append("db_upserted")

            # 10. processed/ë¡œ ì´ë™
            if not self.dry_run:
                self._move_file(pdf_path, self.processed_dir)
                result["actions"].append("â†’processed")

            result["status"] = "success"
            self.stats["success"] += 1

        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {pdf_path.name} - {e}")
            result["status"] = "failed"
            result["reason"] = str(e)
            self.stats["failed"] += 1

            if not self.dry_run:
                self._move_file(pdf_path, self.rejected_dir)
                result["actions"].append("â†’rejected")

        finally:
            result["duration_ms"] = int((time.time() - start_time) * 1000)

        return result

    def _move_file(self, src: Path, dest_dir: Path):
        """íŒŒì¼ ì´ë™"""
        dest_path = dest_dir / src.name
        # ë™ì¼ íŒŒì¼ëª…ì´ ì´ë¯¸ ìˆìœ¼ë©´ (1), (2) ë“± ì¶”ê°€
        counter = 1
        while dest_path.exists():
            stem = src.stem
            suffix = src.suffix
            dest_path = dest_dir / f"{stem}({counter}){suffix}"
            counter += 1

        shutil.move(str(src), str(dest_path))
        logger.info(f"ì´ë™: {src.name} â†’ {dest_path}")

    def run(self, limit: Optional[int] = None, pattern: Optional[str] = None):
        """ì „ì²´ ì²˜ë¦¬ ì‹¤í–‰"""
        logger.info("=" * 80)
        logger.info("ğŸ“¥ ë¬¸ì„œ íˆ¬ì… ì¸ë±ì‹± ì‹œì‘")
        logger.info(f"incoming: {self.incoming_dir}")
        logger.info(f"dry_run: {self.dry_run}")
        logger.info(f"ocr: {self.ocr_enabled}")
        logger.info("=" * 80)

        # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        if pattern:
            pdf_files = list(self.incoming_dir.glob(pattern))
        else:
            pdf_files = list(self.incoming_dir.glob("*.pdf")) + list(
                self.incoming_dir.glob("*.PDF")
            )

        pdf_files = pdf_files[:limit] if limit else pdf_files
        self.stats["total"] = len(pdf_files)

        logger.info(f"ğŸ“„ ì²˜ë¦¬ ëŒ€ìƒ: {len(pdf_files)}ê°œ íŒŒì¼")

        # íŒŒì¼ ì²˜ë¦¬
        for pdf_file in pdf_files:
            logger.info(f"\nì²˜ë¦¬ ì¤‘: {pdf_file.name}")
            result = self.process_file(pdf_file)
            self.results.append(result)

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            logger.info(
                f"  âœ“ {result['status']} ({result['duration_ms']}ms) - {result['doctype']}"
            )
            if result["reason"]:
                logger.info(f"    ì‚¬ìœ : {result['reason']}")
            logger.info(f"    ê²½ë¡œ: {' â†’ '.join(result['actions'])}")

        # ì¸ë±ìŠ¤ ì¬ë¹Œë“œ íŠ¸ë¦¬ê±° (í•„ìš” ì‹œ)
        if not self.dry_run and self.stats["success"] > 0:
            logger.info("\nğŸ”„ ì¸ë±ìŠ¤ ì¬ë¹Œë“œ íŠ¸ë¦¬ê±° (í•„ìš” ì‹œ ìˆ˜ë™ ì‹¤í–‰)")
            logger.info("  - FAISS: python scripts/rebuild_rag_indexes.py")
            logger.info("  - BM25: python scripts/quick_rebuild_bm25.py")

        # ìµœì¢… í†µê³„
        self._print_summary()

        # ë¡œê·¸ ì €ì¥
        self._save_log()

    def _print_summary(self):
        """ìš”ì•½ í†µê³„ ì¶œë ¥"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 80)
        logger.info(f"ì´ íŒŒì¼: {self.stats['total']}")
        logger.info(f"âœ… ì„±ê³µ: {self.stats['success']}")
        logger.info(f"âŒ ì‹¤íŒ¨: {self.stats['failed']}")
        logger.info(f"ğŸ” ì¤‘ë³µ: {self.stats['duplicate']}")
        logger.info(f"ğŸš« ê±°ë¶€: {self.stats['rejected']}")
        logger.info(f"âš ï¸ ê²©ë¦¬: {self.stats['quarantined']}")

        # ì„±ê³µë¥ 
        if self.stats["total"] > 0:
            success_rate = (self.stats["success"] / self.stats["total"]) * 100
            logger.info(f"\nì„±ê³µë¥ : {success_rate:.1f}%")

        # SLA ì²´í¬ (10ê±´ / 60ì´ˆ)
        total_duration = sum(r["duration_ms"] for r in self.results)
        avg_duration = total_duration / len(self.results) if self.results else 0
        logger.info(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_duration:.0f}ms/íŒŒì¼")

        if self.stats["total"] == 10:
            sla_ok = total_duration <= 60000
            logger.info(
                f"SLA (10ê±´/60ì´ˆ): {'âœ… í†µê³¼' if sla_ok else 'âŒ ì´ˆê³¼'} ({total_duration/1000:.1f}ì´ˆ)"
            )

        logger.info("=" * 80)

    def _save_log(self):
        """ìƒì„¸ ë¡œê·¸ ì €ì¥"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"ingest_{timestamp}.json"

        log_data = {
            "timestamp": timestamp,
            "dry_run": self.dry_run,
            "ocr_enabled": self.ocr_enabled,
            "stats": self.stats,
            "results": self.results,
        }

        log_file.write_text(
            json.dumps(log_data, ensure_ascii=False, indent=2), encoding="utf-8"
        )
        logger.info(f"\nğŸ“„ ìƒì„¸ ë¡œê·¸ ì €ì¥: {log_file}")


def main():
    parser = argparse.ArgumentParser(description="ë¬¸ì„œ íˆ¬ì… ì¸ë±ì‹± CLI")
    parser.add_argument("--limit", type=int, help="ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜")
    parser.add_argument("--only", type=str, help="íŒŒì¼ëª… íŒ¨í„´ (glob)")
    parser.add_argument("--ocr", action="store_true", help="OCR í™œì„±í™”")
    parser.add_argument(
        "--dry-run", action="store_true", help="ì‹¤ì œ ì´ë™/ì—…ì„œíŠ¸ ì—†ì´ ë¦¬í¬íŠ¸ë§Œ"
    )

    args = parser.parse_args()

    ingester = DocumentIngester(ocr_enabled=args.ocr, dry_run=args.dry_run)

    ingester.run(limit=args.limit, pattern=args.only)


if __name__ == "__main__":
    main()
