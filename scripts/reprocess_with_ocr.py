#!/usr/bin/env python3
"""
ê¸°ì¡´ ë¬¸ì„œ OCR ì¬ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
í˜ì´ì§€ë‹¹ í‰ê·  300ì ë¯¸ë§Œì¸ ë¬¸ì„œë¥¼ OCRë¡œ ì¬ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/reprocess_with_ocr.py                  # ì „ì²´ ì¬ì²˜ë¦¬
    python scripts/reprocess_with_ocr.py --limit 10       # ìµœëŒ€ 10ê°œë§Œ
    python scripts/reprocess_with_ocr.py --dry-run        # ì‹œë®¬ë ˆì´ì…˜ë§Œ
    python scripts/reprocess_with_ocr.py --threshold 200  # ì„ê³„ê°’ ë³€ê²½
"""

import argparse
import sqlite3
import sys
import time
from pathlib import Path
from typing import Dict, Any, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.logging import get_logger

logger = get_logger(__name__)


def find_ocr_candidates(
    db_path: str = "metadata.db",
    threshold: int = 300,
    limit: int = None
) -> List[Dict[str, Any]]:
    """OCRì´ í•„ìš”í•œ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ

    Args:
        db_path: ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        threshold: í˜ì´ì§€ë‹¹ í‰ê·  ê¸€ììˆ˜ ì„ê³„ê°’
        limit: ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜

    Returns:
        ë¬¸ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    query = """
        SELECT
            id,
            path,
            filename,
            page_count,
            LENGTH(text_preview) as text_len,
            CAST(LENGTH(text_preview) AS FLOAT) / NULLIF(page_count, 0) as avg_per_page,
            doctype,
            date
        FROM documents
        WHERE page_count > 0
          AND text_preview IS NOT NULL
          AND LENGTH(text_preview) > 0
          AND (CAST(LENGTH(text_preview) AS FLOAT) / page_count) < ?
        ORDER BY avg_per_page ASC
    """

    if limit:
        query += f" LIMIT {limit}"

    cursor.execute(query, (threshold,))
    rows = cursor.fetchall()

    candidates = []
    for row in rows:
        candidates.append({
            'id': row['id'],
            'path': row['path'],
            'filename': row['filename'],
            'page_count': row['page_count'],
            'text_len': row['text_len'],
            'avg_per_page': row['avg_per_page'],
            'doctype': row['doctype'],
            'date': row['date']
        })

    conn.close()
    return candidates


def ocr_extract_pdf(pdf_path: Path) -> str:
    """PDFì—ì„œ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ

    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    try:
        import pytesseract
        from pdf2image import convert_from_path

        logger.info(f"OCR ì¶”ì¶œ ì‹œì‘: {pdf_path.name}")

        # PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
        images = convert_from_path(str(pdf_path))
        text_pages = []

        # ê° í˜ì´ì§€ OCR
        for i, image in enumerate(images, 1):
            logger.debug(f"  í˜ì´ì§€ {i}/{len(images)} OCR ì¤‘...")
            text = pytesseract.image_to_string(image, lang="kor+eng")
            if text.strip():
                text_pages.append(text)

        full_text = "\n\n".join(text_pages)
        logger.info(f"OCR ì™„ë£Œ: {pdf_path.name}, {len(full_text)}ì ì¶”ì¶œ")

        return full_text

    except ImportError as e:
        logger.error(f"OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜: {e}")
        logger.error("ì„¤ì¹˜ ë°©ë²•: pip install pytesseract pdf2image")
        return ""
    except Exception as e:
        logger.error(f"OCR ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path.name} - {e}")
        return ""


def update_document_text(
    doc_id: int,
    new_text: str,
    db_path: str = "metadata.db",
    extracted_dir: Path = None
) -> bool:
    """ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ ì •ë³´ ì—…ë°ì´íŠ¸

    Args:
        doc_id: ë¬¸ì„œ ID
        new_text: ìƒˆë¡œìš´ í…ìŠ¤íŠ¸
        db_path: ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        extracted_dir: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # text_preview ì—…ë°ì´íŠ¸ (ì²˜ìŒ 500ì)
        text_preview = new_text[:500]

        cursor.execute("""
            UPDATE documents
            SET text_preview = ?,
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """, (text_preview, doc_id))

        conn.commit()
        conn.close()

        # ì¶”ì¶œ í…ìŠ¤íŠ¸ íŒŒì¼ë„ ì—…ë°ì´íŠ¸
        if extracted_dir:
            conn2 = sqlite3.connect(db_path)
            cursor2 = conn2.cursor()
            cursor2.execute("SELECT filename FROM documents WHERE id = ?", (doc_id,))
            row = cursor2.fetchone()
            conn2.close()

            if row:
                filename = row[0]
                txt_filename = filename.replace('.pdf', '.txt')
                txt_path = extracted_dir / txt_filename

                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(new_text)

                logger.debug(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë°ì´íŠ¸: {txt_path}")

        return True

    except Exception as e:
        logger.error(f"DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (doc_id={doc_id}): {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="ê¸°ì¡´ ë¬¸ì„œ OCR ì¬ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸"
    )
    parser.add_argument(
        "--threshold",
        type=int,
        default=300,
        help="í˜ì´ì§€ë‹¹ í‰ê·  ê¸€ììˆ˜ ì„ê³„ê°’ (ê¸°ë³¸: 300)"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ì²˜ë¦¬ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ"
    )
    parser.add_argument(
        "--db",
        type=str,
        default="metadata.db",
        help="ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ (ê¸°ë³¸: metadata.db)"
    )

    args = parser.parse_args()

    # ì¶”ì¶œ í…ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬
    extracted_dir = Path("data/extracted")
    extracted_dir.mkdir(parents=True, exist_ok=True)

    logger.info("="*80)
    logger.info("ê¸°ì¡´ ë¬¸ì„œ OCR ì¬ì²˜ë¦¬ ì‹œì‘")
    logger.info("="*80)
    logger.info(f"ì„ê³„ê°’: {args.threshold}ì/í˜ì´ì§€")
    logger.info(f"ì œí•œ: {args.limit or 'ì—†ìŒ'}")
    logger.info(f"Dry-run: {args.dry_run}")
    logger.info("="*80)

    # í›„ë³´ ë¬¸ì„œ ì°¾ê¸°
    candidates = find_ocr_candidates(
        db_path=args.db,
        threshold=args.threshold,
        limit=args.limit
    )

    if not candidates:
        logger.info("OCRì´ í•„ìš”í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    logger.info(f"OCR í•„ìš” ë¬¸ì„œ: {len(candidates)}ê°œ")
    logger.info("")

    # ì²˜ë¦¬ í†µê³„
    stats = {
        'total': len(candidates),
        'success': 0,
        'failed': 0,
        'skipped': 0
    }

    # ê° ë¬¸ì„œ ì²˜ë¦¬
    for i, doc in enumerate(candidates, 1):
        logger.info(f"[{i}/{len(candidates)}] ì²˜ë¦¬ ì¤‘: {doc['filename']}")
        logger.info(f"  í˜„ì¬: {doc['text_len']}ì ({doc['avg_per_page']:.0f}ì/í˜ì´ì§€)")

        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        pdf_path = Path(doc['path'])
        if not pdf_path.exists():
            logger.warning(f"  âš ï¸ íŒŒì¼ ì—†ìŒ: {pdf_path}")
            stats['skipped'] += 1
            continue

        if args.dry_run:
            logger.info(f"  [DRY-RUN] OCR ì¶”ì¶œ ìŠ¤í‚µ")
            stats['success'] += 1
            continue

        # OCR ì¶”ì¶œ
        start_time = time.time()
        ocr_text = ocr_extract_pdf(pdf_path)
        duration = time.time() - start_time

        if not ocr_text:
            logger.error(f"  âŒ OCR ì‹¤íŒ¨")
            stats['failed'] += 1
            continue

        # DB ì—…ë°ì´íŠ¸
        if update_document_text(doc['id'], ocr_text, args.db, extracted_dir):
            improvement = len(ocr_text) - doc['text_len']
            logger.info(f"  âœ… ì„±ê³µ: {len(ocr_text)}ì (+{improvement}ì, {duration:.1f}ì´ˆ)")
            stats['success'] += 1
        else:
            logger.error(f"  âŒ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
            stats['failed'] += 1

    # ìµœì¢… ìš”ì•½
    logger.info("")
    logger.info("="*80)
    logger.info("ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    logger.info("="*80)
    logger.info(f"ì´ ë¬¸ì„œ: {stats['total']}")
    logger.info(f"âœ… ì„±ê³µ: {stats['success']}")
    logger.info(f"âŒ ì‹¤íŒ¨: {stats['failed']}")
    logger.info(f"âš ï¸ ìŠ¤í‚µ: {stats['skipped']}")

    if not args.dry_run and stats['success'] > 0:
        logger.info("")
        logger.info("ğŸ”„ ì¸ë±ìŠ¤ ì¬ë¹Œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤:")
        logger.info("  .venv/bin/python3 scripts/reindex_atomic.py")

    logger.info("="*80)

    return 0 if stats['failed'] == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
