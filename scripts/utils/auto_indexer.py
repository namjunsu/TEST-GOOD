"""
ìë™ ë¬¸ì„œ ì¸ë±ì‹± ì‹œìŠ¤í…œ
ìƒˆë¡œìš´ PDF/TXT íŒŒì¼ì´ docs í´ë”ì— ì¶”ê°€ë˜ë©´ ìë™ìœ¼ë¡œ ì¸ë±ì‹±
"""

import time
import os
import hashlib
from pathlib import Path
from datetime import datetime
import json
import threading
from typing import Dict, Set

class AutoIndexer:
    """ìë™ ì¸ë±ì‹± í´ë˜ìŠ¤ - ì„±ëŠ¥ ìµœì í™” ë²„ì „"""

    def __init__(self, docs_dir: str = "docs", check_interval: int = 30, max_retries: int = 3):
        """
        Args:
            docs_dir: ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            check_interval: ì²´í¬ ê°„ê²© (ì´ˆ)
        """
        self.docs_dir = Path(docs_dir)
        self.check_interval = check_interval
        self.index_file = Path("rag_system/file_index.json")
        self.index_file.parent.mkdir(exist_ok=True)

        # íŒŒì¼ ì¸ë±ìŠ¤ ë¡œë“œ
        self.file_index = self._load_index()
        self.is_running = False
        self.thread = None

        # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ í•´ì‹œ ìºì‹œ
        self.hash_cache = {}  # íŒŒì¼ ê²½ë¡œ -> (hash, mtime) ë§¤í•‘
        self.last_check_time = 0

        # ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ê´€ë ¨
        self.max_retries = max_retries
        self.failed_files = {}  # íŒŒì¼ ê²½ë¡œ -> (ì‹¤íŒ¨ íšŸìˆ˜, ë§ˆì§€ë§‰ ì—ëŸ¬)
        self.error_history = []  # ìµœê·¼ ì—ëŸ¬ ê¸°ë¡ (ìµœëŒ€ 100ê°œ)

        # í´ë” ëª©ë¡ ìƒìˆ˜í™” (ì¤‘ë³µ ì œê±°)
        self.YEAR_FOLDERS = [f"year_{year}" for year in range(2014, 2026)]
        self.CATEGORY_FOLDERS = ['category_purchase', 'category_repair', 'category_review',
                                'category_disposal', 'category_consumables']
        self.SPECIAL_FOLDERS = ['recent', 'archive', 'assets']
        
    def _load_index(self) -> Dict:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                pass
        return {
            'files': {},
            'last_update': None
        }
    
    def _save_index(self):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        self.file_index['last_update'] = datetime.now().isoformat()
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.file_index, f, indent=2, ensure_ascii=False)
    
    def _get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ìºì‹± ë° ìµœì í™”)"""
        # ìˆ˜ì • ì‹œê°„ ê¸°ë°˜ ë¹ ë¥¸ ì²´í¬
        current_mtime = file_path.stat().st_mtime
        cache_key = str(file_path)

        # ìºì‹œì— ìˆê³  ìˆ˜ì • ì‹œê°„ì´ ê°™ìœ¼ë©´ ìºì‹œ ì‚¬ìš©
        if cache_key in self.hash_cache:
            cached_hash, cached_mtime = self.hash_cache[cache_key]
            if cached_mtime == current_mtime:
                return cached_hash

        # ì‹¤ì œ í•´ì‹œ ê³„ì‚° (í° íŒŒì¼ì€ ì²˜ìŒ 1MBë§Œ ìƒ˜í”Œë§)
        file_size = file_path.stat().st_size
        hasher = hashlib.md5()

        with open(file_path, 'rb') as f:
            if file_size > 10 * 1024 * 1024:  # 10MB ì´ìƒ
                # ì²˜ìŒ, ì¤‘ê°„, ë ë¶€ë¶„ë§Œ ìƒ˜í”Œë§
                f.seek(0)
                hasher.update(f.read(1024 * 1024))  # ì²˜ìŒ 1MB

                f.seek(file_size // 2)
                hasher.update(f.read(1024 * 1024))  # ì¤‘ê°„ 1MB

                f.seek(max(0, file_size - 1024 * 1024))
                hasher.update(f.read())  # ë§ˆì§€ë§‰ 1MB

                # íŒŒì¼ í¬ê¸°ì™€ ìˆ˜ì • ì‹œê°„ë„ í¬í•¨
                hasher.update(str(file_size).encode())
                hasher.update(str(current_mtime).encode())
            else:
                # ì‘ì€ íŒŒì¼ì€ ì „ì²´ ì½ê¸°
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)

        file_hash = hasher.hexdigest()

        # ìºì‹œ ì—…ë°ì´íŠ¸
        self.hash_cache[cache_key] = (file_hash, current_mtime)

        return file_hash
    
    def _rename_file_with_underscore(self, file_path: Path) -> Path:
        """íŒŒì¼ëª…ì˜ ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½"""
        if ' ' in file_path.name:
            new_name = file_path.name.replace(' ', '_')
            new_path = file_path.parent / new_name

            # ì¤‘ë³µ íŒŒì¼ëª… ì²´í¬
            if new_path.exists():
                # ì¤‘ë³µë˜ë©´ ë²ˆí˜¸ ì¶”ê°€
                base_name = new_path.stem
                extension = new_path.suffix
                counter = 1
                while new_path.exists():
                    new_path = file_path.parent / f"{base_name}_{counter}{extension}"
                    counter += 1

            try:
                file_path.rename(new_path)
                print(f"ğŸ“ íŒŒì¼ëª… ë³€ê²½: {file_path.name} â†’ {new_path.name}")
                return new_path
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ëª… ë³€ê²½ ì‹¤íŒ¨: {file_path.name} - {e}")
                return file_path
        return file_path

    def _get_search_paths(self) -> list:
        """ê²€ìƒ‰ ê²½ë¡œ ëª©ë¡ ë°˜í™˜ (ì¤‘ë³µ ì œê±°)"""
        search_paths = [self.docs_dir]

        # ëª¨ë“  í´ë” íƒ€ì… ìˆœíšŒ
        all_folders = self.YEAR_FOLDERS + self.CATEGORY_FOLDERS + self.SPECIAL_FOLDERS

        for folder_name in all_folders:
            folder_path = self.docs_dir / folder_name
            if folder_path.exists():
                search_paths.append(folder_path)

        return search_paths

    def check_new_files(self) -> Dict:
        """ìƒˆ íŒŒì¼ ì²´í¬ (ì„±ëŠ¥ ìµœì í™”)"""
        start_time = time.time()
        new_files = []
        modified_files = []
        deleted_files = []

        # [PATCH] ì‚­ì œëœ íŒŒì¼ ì •ë¦¬ ë‹¨ê³„: everything_index.db ë™ê¸°í™”
        stale_count = self._purge_missing_files_from_index()
        if stale_count > 0:
            print(f"ğŸ§¹ [CLEANUP] deleted_stale_entries={stale_count}")

        # í˜„ì¬ íŒŒì¼ ëª©ë¡
        current_files = {}
        search_paths = self._get_search_paths()

        # ëª¨ë“  ê²½ë¡œì—ì„œ íŒŒì¼ ê²€ìƒ‰ (ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥)
        file_count = 0
        for path in search_paths:
            for ext in ['*.pdf', '*.txt']:
                for file_path in path.glob(ext):
                    # íŒŒì¼ëª…ì— ê³µë°±ì´ ìˆìœ¼ë©´ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
                    file_path = self._rename_file_with_underscore(file_path)

                    abs_path = file_path.resolve()
                    abs_path_str = str(abs_path)

                    # ì¤‘ë³µ ì²´í¬ (ì‹¬ë³¼ë¦­ ë§í¬ ë°©ì§€)
                    if abs_path_str not in current_files:
                        try:
                            stat = file_path.stat()
                            # ë¹ ë¥¸ ì²´í¬: í¬ê¸°ì™€ ìˆ˜ì • ì‹œê°„ë§Œìœ¼ë¡œ ë¨¼ì € íŒë‹¨
                            quick_check = f"{stat.st_size}_{stat.st_mtime}"

                            # ê¸°ì¡´ íŒŒì¼ê³¼ ë¹„êµ
                            old_info = self.file_index['files'].get(abs_path_str, {})
                            old_quick_check = f"{old_info.get('size', 0)}_{old_info.get('modified', 0)}"

                            # ë¹ ë¥¸ ì²´í¬ê°€ ë‹¤ë¥¸ ê²½ìš°ë§Œ í•´ì‹œ ê³„ì‚°
                            if quick_check != old_quick_check or abs_path_str not in self.file_index['files']:
                                file_hash = self._get_file_hash(file_path)
                            else:
                                file_hash = old_info.get('hash', '')

                            current_files[abs_path_str] = {
                                'hash': file_hash,
                                'size': stat.st_size,
                                'modified': stat.st_mtime,
                                'added': old_info.get('added', datetime.now().isoformat())
                            }
                            file_count += 1

                        except (OSError, IOError) as e:
                            print(f"  âš ï¸ íŒŒì¼ ì ‘ê·¼ ì˜¤ë¥˜: {file_path.name} - {e}")
                            self._handle_file_error(abs_path_str, e)

        # ì„±ëŠ¥ ë¡œê¹…
        if file_count > 100:
            elapsed = time.time() - start_time
            print(f"  â±ï¸ {file_count}ê°œ íŒŒì¼ ìŠ¤ìº”: {elapsed:.1f}ì´ˆ")
        
        # ìƒˆ íŒŒì¼ ê°ì§€
        for file_path, info in current_files.items():
            if file_path not in self.file_index['files']:
                new_files.append(file_path)
                print(f"ğŸ†• ìƒˆ íŒŒì¼ ë°œê²¬: {Path(file_path).name}")
            elif self.file_index['files'][file_path]['hash'] != info['hash']:
                modified_files.append(file_path)
                print(f"ğŸ“ íŒŒì¼ ìˆ˜ì •ë¨: {Path(file_path).name}")
        
        # ì‚­ì œëœ íŒŒì¼ ê°ì§€
        for file_path in self.file_index['files']:
            if file_path not in current_files:
                deleted_files.append(file_path)
                print(f"ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œë¨: {Path(file_path).name}")
        
        # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        if new_files or modified_files or deleted_files:
            self.file_index['files'] = current_files
            self._save_index()
            
            # ì¸ë±ì‹± íŠ¸ë¦¬ê±°
            if new_files or modified_files:
                self._trigger_indexing(new_files + modified_files)
        
        return {
            'new': new_files,
            'modified': modified_files,
            'deleted': deleted_files,
            'total': len(current_files)
        }
    
    def _trigger_indexing(self, files: list):
        """ì¸ë±ì‹± íŠ¸ë¦¬ê±° - ë‹¨ìˆœí™”ëœ ë²„ì „ (perfect_rag ì œê±°)"""
        print(f"\nğŸ”„ ì¸ë±ì‹± ì‹œì‘: {len(files)}ê°œ íŒŒì¼")

        try:
            # íŒŒì¼ ëª©ë¡ë§Œ ì—…ë°ì´íŠ¸ (perfect_rag ì—†ì´)
            print("ğŸ“ íŒŒì¼ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸...")
            updated_count = len(files)

            print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ! ({updated_count}ê°œ íŒŒì¼)")

            # í†µê³„ ì¶œë ¥
            stats = self.get_statistics()
            print(f"ğŸ“Š ì „ì²´ íŒŒì¼: PDF {stats['pdf_count']}ê°œ, TXT {stats['txt_count']}ê°œ")

        except Exception as e:
            print(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
            self._handle_indexing_error(files, e)
    
    def get_statistics(self) -> Dict:
        """í†µê³„ ì •ë³´ (ì‹¤ì‹œê°„ íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ë°˜)"""
        # ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì§ì ‘ ê°œìˆ˜ í™•ì¸ (ì¸ë±ìŠ¤ê°€ ì˜¤ë˜ëœ ê²½ìš° ëŒ€ë¹„)
        pdf_files = []
        txt_files = []

        # docs í´ë”ì˜ ëª¨ë“  í•˜ìœ„ í´ë” ê²€ìƒ‰ (ì‹¬ë³¼ë¦­ ë§í¬ ì œì™¸)
        if self.docs_dir.exists():
            all_pdfs = list(self.docs_dir.rglob("*.pdf"))
            all_txts = list(self.docs_dir.rglob("*.txt"))

            # ì‹¬ë³¼ë¦­ ë§í¬ ì œì™¸ (ì‹¤ì œ íŒŒì¼ë§Œ)
            pdf_files = [f for f in all_pdfs if not f.is_symlink()]
            txt_files = [f for f in all_txts if not f.is_symlink()]

        return {
            'total_files': len(pdf_files) + len(txt_files),
            'pdf_count': len(pdf_files),
            'txt_count': len(txt_files),
            'last_update': self.file_index.get('last_update', 'Never')
        }
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_running:
            print("âš ï¸ ì´ë¯¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        self.is_running = True
        print(f"ğŸš€ ìë™ ì¸ë±ì‹± ì‹œì‘ (ì²´í¬ ê°„ê²©: {self.check_interval}ì´ˆ)")
        
        def run():
            while self.is_running:
                try:
                    # ì‹¤íŒ¨í•œ íŒŒì¼ ì¬ì‹œë„ (ë§¤ 5ë²ˆì§¸ ì£¼ê¸°ë§ˆë‹¤)
                    if hasattr(self, '_check_count'):
                        self._check_count += 1
                    else:
                        self._check_count = 1

                    if self._check_count % 5 == 0 and self.failed_files:
                        self._retry_failed_files()

                    result = self.check_new_files()
                    if result['new'] or result['modified']:
                        print(f"ğŸ“ ë³€ê²½ ê°ì§€: ìƒˆ íŒŒì¼ {len(result['new'])}ê°œ, ìˆ˜ì • {len(result['modified'])}ê°œ")
                except Exception as e:
                    print(f"âŒ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    self._handle_indexing_error([], e)
                
                time.sleep(self.check_interval)
        
        self.thread = threading.Thread(target=run, daemon=True)
        self.thread.start()
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_running = False
        if self.thread:
            self.thread.join(timeout=5)
        print("â¹ï¸ ìë™ ì¸ë±ì‹± ì¤‘ì§€")
    
    # perfect_rag ê´€ë ¨ í•¨ìˆ˜ë“¤ ì œê±°ë¨ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)

    def _handle_file_error(self, file_path: str, error: Exception):
        """íŒŒì¼ ì—ëŸ¬ ì²˜ë¦¬ ë° ê¸°ë¡"""
        # ì‹¤íŒ¨ íšŸìˆ˜ ì¦ê°€
        if file_path not in self.failed_files:
            self.failed_files[file_path] = [1, str(error)]
        else:
            self.failed_files[file_path][0] += 1
            self.failed_files[file_path][1] = str(error)

        # ì—ëŸ¬ ì´ë ¥ ì¶”ê°€
        self.error_history.append({
            'timestamp': datetime.now().isoformat(),
            'file': file_path,
            'error': str(error),
            'retry_count': self.failed_files[file_path][0]
        })

        # ì´ë ¥ í¬ê¸° ì œí•œ
        if len(self.error_history) > 100:
            self.error_history = self.error_history[-100:]

        # ì¬ì‹œë„ í•œê³„ ë„ë‹¬ ì‹œ ê²½ê³ 
        if self.failed_files[file_path][0] >= self.max_retries:
            print(f"  ğŸš« íŒŒì¼ ì²˜ë¦¬ í¬ê¸°: {Path(file_path).name} (ì¬ì‹œë„ {self.max_retries}íšŒ ì‹¤íŒ¨)")

    def _handle_indexing_error(self, files: list, error: Exception):
        """ì¸ë±ì‹± ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬"""
        print(f"\nğŸ”§ ì¸ë±ì‹± ì˜¤ë¥˜ ë³µêµ¬ ì‹œë„...")

        # ì—ëŸ¬ ë¡œê¹…
        self.error_history.append({
            'timestamp': datetime.now().isoformat(),
            'type': 'indexing_error',
            'files_count': len(files),
            'error': str(error)
        })

        # ë³µêµ¬ ì „ëµ
        try:
            # 1. RAG ì¸ìŠ¤í„´ìŠ¤ ì¬ìƒì„± ì‹œë„
            print("  1ï¸âƒ£ RAG ì¸ìŠ¤í„´ìŠ¤ ì¬ìƒì„± ì‹œë„...")
            if hasattr(self, '_rag_instance'):
                del self._rag_instance

            # 2. íŒŒì¼ë³„ ê°œë³„ ì²˜ë¦¬ ì‹œë„
            print(f"  2ï¸âƒ£ {len(files)}ê°œ íŒŒì¼ ê°œë³„ ì²˜ë¦¬ ì‹œë„...")
            success_count = 0

            for file_path in files[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¬ì‹œë„
                try:
                    # ê°œë³„ íŒŒì¼ ì²˜ë¦¬
                    self._process_single_file(file_path)
                    success_count += 1
                except Exception as file_error:
                    self._handle_file_error(file_path, file_error)

            if success_count > 0:
                print(f"  âœ… ë¶€ë¶„ ë³µêµ¬ ì„±ê³µ: {success_count}/{min(5, len(files))}ê°œ íŒŒì¼")
            else:
                print(f"  âš ï¸ ë³µêµ¬ ì‹¤íŒ¨ - ë‹¤ìŒ ì£¼ê¸°ì— ì¬ì‹œë„")

        except Exception as recovery_error:
            print(f"  âŒ ë³µêµ¬ ì‹¤íŒ¨: {recovery_error}")

    def _process_single_file(self, file_path: str):
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ (ì—ëŸ¬ ë³µêµ¬ìš©)"""
        # ê°„ë‹¨í•œ ë©”íƒ€ë°ì´í„°ë§Œ ì—…ë°ì´íŠ¸
        path_obj = Path(file_path)
        if path_obj.exists():
            stat = path_obj.stat()
            file_hash = self._get_file_hash(path_obj)

            self.file_index['files'][file_path] = {
                'hash': file_hash,
                'size': stat.st_size,
                'modified': stat.st_mtime,
                'added': datetime.now().isoformat()
            }

    def _retry_failed_files(self):
        """ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ ì¬ì‹œë„"""
        if not self.failed_files:
            return

        # ì¬ì‹œë„ ëŒ€ìƒ ì„ ì • (ì¬ì‹œë„ íšŸìˆ˜ê°€ í•œê³„ ë¯¸ë§Œ)
        retry_candidates = [
            path for path, (count, _) in self.failed_files.items()
            if count < self.max_retries
        ]

        if retry_candidates:
            print(f"\nğŸ”„ ì‹¤íŒ¨í•œ íŒŒì¼ ì¬ì‹œë„: {len(retry_candidates)}ê°œ")

            for file_path in retry_candidates:
                try:
                    self._process_single_file(file_path)
                    # ì„±ê³µ ì‹œ ì‹¤íŒ¨ ëª©ë¡ì—ì„œ ì œê±°
                    del self.failed_files[file_path]
                    print(f"  âœ… ì¬ì‹œë„ ì„±ê³µ: {Path(file_path).name}")
                except Exception as e:
                    self._handle_file_error(file_path, e)

    def get_error_statistics(self) -> Dict:
        """ì—ëŸ¬ í†µê³„ ë°˜í™˜"""
        return {
            'failed_files_count': len(self.failed_files),
            'failed_files': list(self.failed_files.keys())[:10],  # ì²˜ìŒ 10ê°œë§Œ
            'recent_errors': self.error_history[-5:] if self.error_history else [],
            'total_errors': len(self.error_history)
        }

    def _purge_missing_files_from_index(self) -> int:
        """ë””ìŠ¤í¬ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ì„ ê²€ìƒ‰ ì¸ë±ìŠ¤ì—ì„œ ì‚­ì œ

        Returns:
            ì‚­ì œëœ í•­ëª© ìˆ˜
        """
        try:
            import sqlite3
            from config.indexing import DB_PATHS

            # everything_index.db ê²½ë¡œ
            index_db_path = DB_PATHS.get("everything_index", "everything_index.db")
            if not os.path.exists(index_db_path):
                return 0

            # í˜„ì¬ ë””ìŠ¤í¬ì˜ ëª¨ë“  íŒŒì¼ëª… ì§‘í•©
            fs_names = set()
            search_paths = self._get_search_paths()
            for path in search_paths:
                for ext in ['*.pdf', '*.txt']:
                    for file_path in path.glob(ext):
                        fs_names.add(file_path.name)

            # DB ì—°ê²°
            conn = sqlite3.connect(index_db_path)
            cur = conn.cursor()

            # files í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì ê²€
            cur.execute("PRAGMA table_info(files)")
            cols = {c[1] for c in cur.fetchall()}
            has_path = "path" in cols

            # DB í–‰ ì „ì²´ ì¡°íšŒ
            query = "SELECT rowid, filename{} FROM files".format(", path" if has_path else "")
            cur.execute(query)
            rows = cur.fetchall()

            stale_ids = []
            for row in rows:
                if has_path:
                    rowid, filename, path = row
                    # path ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    exists = os.path.exists(path) if os.path.isabs(path) else os.path.exists(os.path.join(os.getcwd(), path))
                    if not exists and filename not in fs_names:
                        stale_ids.append(rowid)
                else:
                    rowid, filename = row
                    if filename not in fs_names:
                        stale_ids.append(rowid)

            # ì‚­ì œ ì‹¤í–‰
            if stale_ids:
                qmarks = ",".join(["?"] * len(stale_ids))
                cur.execute(f"DELETE FROM files WHERE rowid IN ({qmarks})", stale_ids)
                conn.commit()

            conn.close()
            return len(stale_ids)

        except Exception as e:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0

    def force_reindex(self):
        """ê°•ì œ ì¬ì¸ë±ì‹±"""
        print("ğŸ”„ ê°•ì œ ì¬ì¸ë±ì‹± ì‹œì‘...")
        self.file_index = {'files': {}, 'last_update': None}
        self.failed_files = {}  # ì‹¤íŒ¨ ëª©ë¡ ì´ˆê¸°í™”
        result = self.check_new_files()
        print(f"âœ… ì¬ì¸ë±ì‹± ì™„ë£Œ: {result['total']}ê°œ íŒŒì¼")
        return result


# ë…ë¦½ ì‹¤í–‰ìš©
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ìë™ ë¬¸ì„œ ì¸ë±ì‹± ì‹œìŠ¤í…œ")
    parser.add_argument('--interval', type=int, default=30, help='ì²´í¬ ê°„ê²© (ì´ˆ)')
    parser.add_argument('--force', action='store_true', help='ê°•ì œ ì¬ì¸ë±ì‹±')
    parser.add_argument('--stats', action='store_true', help='í†µê³„ ì¶œë ¥')
    
    args = parser.parse_args()
    
    indexer = AutoIndexer(check_interval=args.interval)
    
    if args.stats:
        stats = indexer.get_statistics()
        print("ğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
        print(f"  - ì „ì²´ íŒŒì¼: {stats['total_files']}ê°œ")
        print(f"  - PDF: {stats['pdf_count']}ê°œ")
        print(f"  - TXT: {stats['txt_count']}ê°œ")
        print(f"  - ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {stats['last_update']}")
    elif args.force:
        indexer.force_reindex()
    else:
        try:
            indexer.start_monitoring()
            print("ğŸ“Œ ìë™ ì¸ë±ì‹± ì‹¤í–‰ ì¤‘... (Ctrl+Cë¡œ ì¢…ë£Œ)")
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì¢…ë£Œ ì¤‘...")
            indexer.stop_monitoring()