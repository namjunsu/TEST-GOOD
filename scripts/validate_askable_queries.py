#!/usr/bin/env python3
"""
DB ê¸°ë°˜ ì§ˆë¬¸ í”„ë¦¬ì…‹ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

reports/askable_queries_verified.csvì˜ ëª¨ë“  ì§ˆë¬¸ì„
scenario_validation.py ë°©ì‹ìœ¼ë¡œ ìë™ ê²€ì¦í•©ë‹ˆë‹¤.

ì¶œë ¥:
- reports/askable_queries_validation_YYYYMMDD_HHMMSS.json
- reports/askable_queries_validation_YYYYMMDD_HHMMSS.md
"""

import sys
import os
import csv
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
# Force load .env with override=True to ensure environment values are injected
load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env", override=True)

# Clear any cached LLM instances before starting
from rag_system.llm_singleton import LLMSingleton
LLMSingleton.clear()

from app.rag.pipeline import RAGPipeline
from app.core.logging import get_logger

logger = get_logger(__name__)


class QueryValidator:
    """ì§ˆë¬¸ í”„ë¦¬ì…‹ ê²€ì¦ê¸°"""

    def __init__(self, csv_path: str = "reports/askable_queries_verified.csv"):
        self.csv_path = csv_path
        self.pipeline = RAGPipeline()
        self.results = []

    def load_queries(self) -> List[Dict[str, Any]]:
        """CSVì—ì„œ ì§ˆë¬¸ ë¡œë“œ"""
        queries = []

        with open(self.csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                query = {
                    'query': row['query'],
                    'category': row['category'],
                    'expected_mode': row['expected_mode'],
                    'expected_citations': row['expected_citations'].lower() == 'true',
                    'difficulty': row['difficulty'],
                    'metadata': json.loads(row.get('metadata', '{}'))
                }
                queries.append(query)

        return queries

    def validate_query(self, query_data: Dict[str, Any], index: int) -> Dict[str, Any]:
        """ë‹¨ì¼ ì§ˆë¬¸ ê²€ì¦"""
        query_text = query_data['query']

        print(f"\n{'='*80}")
        print(f"ğŸ§ª ì§ˆë¬¸ {index}: {query_text}")
        print(f"   ì¹´í…Œê³ ë¦¬: {query_data['category']}")
        print(f"   ì˜ˆìƒ ëª¨ë“œ: {query_data['expected_mode']}")
        print("=" * 80)

        start_time = time.time()

        try:
            # RAG íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ
            response = self.pipeline.query(query_text)
            elapsed = time.time() - start_time

            # ê²°ê³¼ ë¶„ì„
            actual_mode = response.metrics.get('mode', 'unknown')
            has_citations = len(response.source_docs) > 0 or len(response.evidence_chunks) > 0
            top_score = response.metrics.get('top_score', 0.0)

            # ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°
            answer_preview = response.answer[:200] + "..." if len(response.answer) > 200 else response.answer

            print(f"\nğŸ“Š ê²°ê³¼:")
            print(f"  ëª¨ë“œ: {actual_mode}")
            print(f"  ìµœê³  ì ìˆ˜: {top_score:.3f}")
            print(f"  ì¶œì²˜ ê°œìˆ˜: {len(response.source_docs)}")
            print(f"  ì‘ë‹µ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            print(f"\nğŸ’¬ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°:")
            print(f"  {answer_preview}")

            if response.source_docs:
                print(f"\nğŸ“š ì¶œì²˜:")
                for i, doc in enumerate(response.source_docs[:3], 1):
                    print(f"  {i}. {doc}")

            # ê²€ì¦
            mode_match = actual_mode == query_data['expected_mode']
            citation_match = has_citations == query_data['expected_citations']

            status = "âœ… PASS" if (mode_match and citation_match) else "âŒ FAIL"

            if not mode_match:
                print(f"\nâš ï¸  ëª¨ë“œ ë¶ˆì¼ì¹˜: ì˜ˆìƒ={query_data['expected_mode']}, ì‹¤ì œ={actual_mode}")
            if not citation_match:
                print(f"\nâš ï¸  ì¶œì²˜ ì¸ìš© ë¶ˆì¼ì¹˜: ì˜ˆìƒ={query_data['expected_citations']}, ì‹¤ì œ={has_citations}")

            print(f"\n{status}")

            result = {
                'index': index,
                'query': query_text,
                'category': query_data['category'],
                'status': 'PASS' if (mode_match and citation_match) else 'FAIL',
                'expected_mode': query_data['expected_mode'],
                'actual_mode': actual_mode,
                'expected_citations': query_data['expected_citations'],
                'has_citations': has_citations,
                'top_score': top_score,
                'latency': elapsed,
                'answer_length': len(response.answer),
                'source_docs': response.source_docs,
                'mode_match': mode_match,
                'citation_match': citation_match
            }

            self.results.append(result)
            return result

        except Exception as e:
            print(f"\nâŒ ERROR: {e}")
            import traceback
            traceback.print_exc()

            result = {
                'index': index,
                'query': query_text,
                'category': query_data['category'],
                'status': 'ERROR',
                'error': str(e)
            }
            self.results.append(result)
            return result

    def generate_report(self) -> Dict[str, Any]:
        """ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±"""
        passed = sum(1 for r in self.results if r['status'] == 'PASS')
        failed = sum(1 for r in self.results if r['status'] == 'FAIL')
        errors = sum(1 for r in self.results if r['status'] == 'ERROR')
        total = len(self.results)

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        by_category = {}
        for r in self.results:
            cat = r['category']
            if cat not in by_category:
                by_category[cat] = {'total': 0, 'passed': 0, 'failed': 0, 'errors': 0}

            by_category[cat]['total'] += 1
            if r['status'] == 'PASS':
                by_category[cat]['passed'] += 1
            elif r['status'] == 'FAIL':
                by_category[cat]['failed'] += 1
            else:
                by_category[cat]['errors'] += 1

        # í‰ê·  ì§€í‘œ
        valid_results = [r for r in self.results if r['status'] != 'ERROR']
        avg_latency = sum(r.get('latency', 0) for r in valid_results) / len(valid_results) if valid_results else 0
        avg_score = sum(r.get('top_score', 0) for r in valid_results) / len(valid_results) if valid_results else 0

        report = {
            'timestamp': datetime.now().isoformat(),
            'total': total,
            'passed': passed,
            'failed': failed,
            'errors': errors,
            'success_rate': f"{passed/total*100:.1f}%" if total > 0 else "0%",
            'avg_latency': f"{avg_latency:.2f}s",
            'avg_score': f"{avg_score:.3f}",
            'by_category': by_category,
            'results': self.results
        }

        return report

    def export_json_report(self, report: Dict[str, Any], output_path: str):
        """JSON ë¦¬í¬íŠ¸ ì €ì¥"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"âœ… JSON ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")

    def export_markdown_report(self, report: Dict[str, Any], output_path: str):
        """Markdown ë¦¬í¬íŠ¸ ì €ì¥"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# AI-CHAT ì§ˆë¬¸ í”„ë¦¬ì…‹ ê²€ì¦ ë¦¬í¬íŠ¸\n\n")
            f.write(f"**ê²€ì¦ ì¼ì‹œ**: {report['timestamp']}\n")
            f.write(f"**ì´ ì§ˆë¬¸ ìˆ˜**: {report['total']}ê°œ\n\n")

            f.write("## ğŸ“Š ì „ì²´ ê²°ê³¼ ìš”ì•½\n\n")
            f.write(f"- âœ… **PASS**: {report['passed']}ê°œ\n")
            f.write(f"- âŒ **FAIL**: {report['failed']}ê°œ\n")
            f.write(f"- ğŸ”¥ **ERROR**: {report['errors']}ê°œ\n")
            f.write(f"- ğŸ“ˆ **ì„±ê³µë¥ **: {report['success_rate']}\n\n")

            f.write("## â±ï¸ ì„±ëŠ¥ ì§€í‘œ\n\n")
            f.write(f"- **í‰ê·  ì‘ë‹µ ì‹œê°„**: {report['avg_latency']}\n")
            f.write(f"- **í‰ê·  ê²€ìƒ‰ ì ìˆ˜**: {report['avg_score']}\n\n")

            f.write("## ğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ í†µê³„\n\n")
            f.write("| ì¹´í…Œê³ ë¦¬ | ì „ì²´ | PASS | FAIL | ERROR | ì„±ê³µë¥  |\n")
            f.write("|----------|------|------|------|-------|--------|\n")

            for cat, stats in sorted(report['by_category'].items()):
                success_rate = f"{stats['passed']/stats['total']*100:.1f}%" if stats['total'] > 0 else "0%"
                f.write(f"| {cat} | {stats['total']} | {stats['passed']} | {stats['failed']} | {stats['errors']} | {success_rate} |\n")

            # ì‹¤íŒ¨/ì—ëŸ¬ ìƒì„¸
            if report['failed'] > 0 or report['errors'] > 0:
                f.write("\n## âŒ ì‹¤íŒ¨/ì—ëŸ¬ ìƒì„¸\n\n")
                for r in report['results']:
                    if r['status'] != 'PASS':
                        f.write(f"### [{r['index']}] {r['query']}\n\n")
                        f.write(f"- **ì¹´í…Œê³ ë¦¬**: {r['category']}\n")
                        f.write(f"- **ìƒíƒœ**: {r['status']}\n")

                        if r['status'] == 'FAIL':
                            f.write(f"- **ì˜ˆìƒ ëª¨ë“œ**: {r['expected_mode']}\n")
                            f.write(f"- **ì‹¤ì œ ëª¨ë“œ**: {r['actual_mode']}\n")
                            f.write(f"- **ëª¨ë“œ ì¼ì¹˜**: {'âŒ' if not r.get('mode_match') else 'âœ…'}\n")
                            f.write(f"- **ì¶œì²˜ ì¸ìš© ì¼ì¹˜**: {'âŒ' if not r.get('citation_match') else 'âœ…'}\n")
                        elif 'error' in r:
                            f.write(f"- **ì—ëŸ¬**: {r['error']}\n")

                        f.write("\n")

            # ì „ì²´ ê²°ê³¼ í…Œì´ë¸”
            f.write("\n## ğŸ“ ì „ì²´ ê²°ê³¼ ìƒì„¸\n\n")
            f.write("| # | ì§ˆë¬¸ | ì¹´í…Œê³ ë¦¬ | ì˜ˆìƒ ëª¨ë“œ | ì‹¤ì œ ëª¨ë“œ | ì¶œì²˜ | ì ìˆ˜ | ì‹œê°„(s) | ìƒíƒœ |\n")
            f.write("|---|------|----------|----------|----------|------|------|---------|------|\n")

            for r in report['results']:
                if r['status'] != 'ERROR':
                    citations = "âœ…" if r.get('has_citations') else "âŒ"
                    score = f"{r.get('top_score', 0):.3f}"
                    latency = f"{r.get('latency', 0):.2f}"
                    status_icon = "âœ…" if r['status'] == 'PASS' else "âŒ"

                    query_short = r['query'][:30] + "..." if len(r['query']) > 30 else r['query']
                    f.write(f"| {r['index']} | {query_short} | {r['category']} | {r['expected_mode']} | {r.get('actual_mode', 'N/A')} | {citations} | {score} | {latency} | {status_icon} |\n")

        print(f"âœ… Markdown ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")


def main():
    print("=" * 80)
    print("ğŸš€ AI-CHAT ì§ˆë¬¸ í”„ë¦¬ì…‹ ê²€ì¦")
    print("=" * 80)
    print(f"ì‹œì‘ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"MODE: {os.getenv('MODE', 'AUTO')}")
    print(f"RAG_MIN_SCORE: {os.getenv('RAG_MIN_SCORE', '0.35')}")

    # Validator ì´ˆê¸°í™”
    validator = QueryValidator()

    # ì§ˆë¬¸ ë¡œë“œ
    print("\nğŸ“š ì§ˆë¬¸ í”„ë¦¬ì…‹ ë¡œë“œ ì¤‘...")
    queries = validator.load_queries()
    print(f"   ë¡œë“œ ì™„ë£Œ: {len(queries)}ê°œ ì§ˆë¬¸")

    # ê²€ì¦ ì‹¤í–‰
    print("\nğŸ§ª ê²€ì¦ ì‹œì‘...\n")
    for i, query_data in enumerate(queries, 1):
        validator.validate_query(query_data, i)
        # ì§ˆë¬¸ ê°„ ê°„ê²©
        if i < len(queries):
            time.sleep(1)

    # ë¦¬í¬íŠ¸ ìƒì„±
    print("\n\n" + "=" * 80)
    print("ğŸ“Š ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    print("=" * 80)

    report = validator.generate_report()

    # ì¶œë ¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    json_path = f"reports/askable_queries_validation_{timestamp}.json"
    md_path = f"reports/askable_queries_validation_{timestamp}.md"

    validator.export_json_report(report, json_path)
    validator.export_markdown_report(report, md_path)

    # ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print(f"\nì´ {report['total']}ê°œ ì§ˆë¬¸:")
    print(f"  âœ… PASS: {report['passed']}")
    print(f"  âŒ FAIL: {report['failed']}")
    print(f"  ğŸ”¥ ERROR: {report['errors']}")
    print(f"\nì„±ê³µë¥ : {report['success_rate']}")
    print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {report['avg_latency']}")
    print(f"í‰ê·  ê²€ìƒ‰ ì ìˆ˜: {report['avg_score']}")

    # ì‹¤íŒ¨ ì§ˆë¬¸ ëª©ë¡
    if report['failed'] > 0 or report['errors'] > 0:
        print("\nâŒ ì‹¤íŒ¨/ì—ëŸ¬ ì§ˆë¬¸:")
        for r in report['results']:
            if r['status'] != 'PASS':
                print(f"  [{r['index']}] {r['query'][:50]}... ({r['status']})")

    print("\n" + "=" * 80)
    print("âœ… ê²€ì¦ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nìƒì„¸ ë¦¬í¬íŠ¸:")
    print(f"  - JSON: {json_path}")
    print(f"  - Markdown: {md_path}")

    # ì¢…ë£Œ ì½”ë“œ ë°˜í™˜
    return 0 if report['failed'] == 0 and report['errors'] == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
