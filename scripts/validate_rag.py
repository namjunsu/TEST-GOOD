#!/usr/bin/env python3
"""
RAG Pipeline Quality Assurance Validator
íŒŒì‹± ì»¤ë²„ë¦¬ì§€, ìŠ¤í‚¤ë§ˆ ì í•©ë„, ì¸ìš©ë¥  ê²€ì¦
"""
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from collections import defaultdict
import statistics


class RAGValidator:
    def __init__(self, suite_path: str = "suites/rag_pipeline.yaml"):
        with open(suite_path, 'r', encoding='utf-8') as f:
            self.suite = yaml.safe_load(f)

        self.thresholds = self.suite['thresholds']
        self.categories = {cat['name']: cat for cat in self.suite['categories']}
        self.schema_validation = self.suite['schema_validation']

        self.results = []
        self.metrics = defaultdict(list)

    def validate_parsing_coverage(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """íŒŒì‹± ì»¤ë²„ë¦¬ì§€ ê²€ì¦"""
        chunks_used = result.get('chunks_used', 0)
        sources_count = result.get('sources_count', 0)

        min_chunks = self.thresholds['min_chunks_used']

        coverage = {
            'chunks_used': chunks_used,
            'sources_count': sources_count,
            'meets_threshold': chunks_used >= min_chunks,
            'coverage_ratio': min(1.0, chunks_used / max(1, min_chunks))
        }

        return coverage

    def validate_schema(self, result: Dict[str, Any], mode: str) -> Dict[str, Any]:
        """ìŠ¤í‚¤ë§ˆ ì í•©ë„ ê²€ì¦"""
        response_data = result.get('response_data', {})

        if mode == 'rag':
            required = self.schema_validation['required_fields']['summary']
            optional = self.schema_validation['optional_fields']['summary']
        else:  # qa
            required = self.schema_validation['required_fields']['qa']
            optional = self.schema_validation['optional_fields']['qa']

        missing_required = [f for f in required if f not in response_data]
        present_optional = [f for f in optional if f in response_data]

        total_fields = len(required) + len(optional)
        present_fields = len(required) - len(missing_required) + len(present_optional)

        schema_score = present_fields / total_fields if total_fields > 0 else 0.0

        is_valid = len(missing_required) == 0
        missing_rate = len(missing_required) / len(required) if required else 0.0

        # Partial success í—ˆìš©
        if self.schema_validation['partial_success_allowed']:
            is_valid = is_valid or (missing_rate <= self.schema_validation['missing_field_threshold'])

        return {
            'valid': is_valid,
            'schema_score': schema_score,
            'missing_required': missing_required,
            'present_optional': present_optional,
            'missing_rate': missing_rate
        }

    def calculate_citation_rate(self, results: List[Dict]) -> float:
        """ì¸ìš©ë¥  ê³„ì‚°"""
        rag_results = [r for r in results if r.get('mode') == 'rag']
        if not rag_results:
            return 0.0

        cited_count = sum(1 for r in rag_results if r.get('sources_count', 0) > 0)
        return cited_count / len(rag_results)

    def calculate_hit_at_k(self, results: List[Dict], k: int = 3) -> float:
        """Hit@K ê³„ì‚°"""
        if not results:
            return 0.0

        hits = sum(1 for r in results if r.get('rank', 999) <= k)
        return hits / len(results)

    def calculate_mrr_at_k(self, results: List[Dict], k: int = 10) -> float:
        """MRR@K ê³„ì‚°"""
        if not results:
            return 0.0

        reciprocal_ranks = []
        for r in results:
            rank = r.get('rank', 999)
            if rank <= k:
                reciprocal_ranks.append(1.0 / rank)
            else:
                reciprocal_ranks.append(0.0)

        return sum(reciprocal_ranks) / len(reciprocal_ranks)

    def validate_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ê²°ê³¼ ê²€ì¦"""
        mode = result.get('mode', 'unknown')

        # íŒŒì‹± ì»¤ë²„ë¦¬ì§€
        coverage = self.validate_parsing_coverage(result)

        # ìŠ¤í‚¤ë§ˆ ê²€ì¦
        schema = self.validate_schema(result, mode)

        # ì„±ëŠ¥ ê²€ì¦
        latency_ms = result.get('latency_ms', 0)
        p95_threshold = self.suite['performance_targets']['p95_latency_ms']

        validation = {
            'coverage': coverage,
            'schema': schema,
            'latency_ok': latency_ms <= p95_threshold,
            'latency_ms': latency_ms
        }

        return validation

    def run_validation(self, results_file: str) -> Dict[str, Any]:
        """ê²€ì¦ ì‹¤í–‰"""
        print("=" * 80)
        print("ğŸ” RAG Pipeline Quality Assurance")
        print("=" * 80)
        print()

        # Load results
        with open(results_file, 'r', encoding='utf-8') as f:
            test_results = json.load(f)

        print(f"ğŸ“Š Loaded {len(test_results)} test results")
        print()

        # Validate each result
        validated_results = []
        for i, result in enumerate(test_results, 1):
            validation = self.validate_result(result)
            result['validation'] = validation
            validated_results.append(result)

            # Collect metrics
            if validation['coverage']['meets_threshold']:
                self.metrics['coverage_pass'].append(1)
            else:
                self.metrics['coverage_pass'].append(0)

            if validation['schema']['valid']:
                self.metrics['schema_pass'].append(1)
            else:
                self.metrics['schema_pass'].append(0)

            self.metrics['latency'].append(validation['latency_ms'])
            self.metrics['chunks_used'].append(validation['coverage']['chunks_used'])

        # Calculate aggregate metrics
        citation_rate = self.calculate_citation_rate(validated_results)
        hit_at_3 = self.calculate_hit_at_k(validated_results, k=3)
        mrr_at_10 = self.calculate_mrr_at_k(validated_results, k=10)

        coverage_pass_rate = sum(self.metrics['coverage_pass']) / len(self.metrics['coverage_pass'])
        schema_pass_rate = sum(self.metrics['schema_pass']) / len(self.metrics['schema_pass'])
        schema_failure_rate = 1.0 - schema_pass_rate

        latencies = self.metrics['latency']
        p50_latency = statistics.median(latencies) if latencies else 0
        p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies, default=0)

        # Check thresholds
        passed_all = all([
            hit_at_3 >= self.thresholds['hit_at_3'],
            mrr_at_10 >= self.thresholds['mrr_at_10'],
            citation_rate >= self.thresholds['citation_rate'],
            schema_failure_rate <= self.thresholds['schema_failure_rate'],
            coverage_pass_rate >= self.thresholds['parsing_coverage']
        ])

        report = {
            'timestamp': datetime.now().isoformat(),
            'total_tests': len(validated_results),
            'thresholds': self.thresholds,
            'metrics': {
                'hit_at_3': hit_at_3,
                'mrr_at_10': mrr_at_10,
                'citation_rate': citation_rate,
                'schema_failure_rate': schema_failure_rate,
                'coverage_pass_rate': coverage_pass_rate,
                'p50_latency_ms': p50_latency,
                'p95_latency_ms': p95_latency,
                'avg_chunks_used': statistics.mean(self.metrics['chunks_used']) if self.metrics['chunks_used'] else 0
            },
            'passed': passed_all,
            'results': validated_results
        }

        # Print summary
        print("ğŸ“ˆ Metrics Summary")
        print("-" * 80)
        print(f"Hit@3:              {hit_at_3:.3f} (threshold: {self.thresholds['hit_at_3']}) {'âœ…' if hit_at_3 >= self.thresholds['hit_at_3'] else 'âŒ'}")
        print(f"MRR@10:             {mrr_at_10:.3f} (threshold: {self.thresholds['mrr_at_10']}) {'âœ…' if mrr_at_10 >= self.thresholds['mrr_at_10'] else 'âŒ'}")
        print(f"Citation Rate:      {citation_rate:.3f} (threshold: {self.thresholds['citation_rate']}) {'âœ…' if citation_rate >= self.thresholds['citation_rate'] else 'âŒ'}")
        print(f"Schema Failure:     {schema_failure_rate:.3f} (threshold: â‰¤{self.thresholds['schema_failure_rate']}) {'âœ…' if schema_failure_rate <= self.thresholds['schema_failure_rate'] else 'âŒ'}")
        print(f"Coverage Pass:      {coverage_pass_rate:.3f} (threshold: {self.thresholds['parsing_coverage']}) {'âœ…' if coverage_pass_rate >= self.thresholds['parsing_coverage'] else 'âŒ'}")
        print()
        print(f"P50 Latency:        {p50_latency:.0f}ms")
        print(f"P95 Latency:        {p95_latency:.0f}ms")
        print(f"Avg Chunks Used:    {report['metrics']['avg_chunks_used']:.1f}")
        print()

        if passed_all:
            print("âœ… ALL THRESHOLDS PASSED")
        else:
            print("âŒ SOME THRESHOLDS FAILED")

        return report

    def generate_report(self, report: Dict, output_path: str):
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        content = f"""# RAG Pipeline QA Report

**Generated**: {report['timestamp']}
**Total Tests**: {report['total_tests']}
**Status**: {'âœ… PASSED' if report['passed'] else 'âŒ FAILED'}

---

## ğŸ“Š Metrics Summary

| Metric | Value | Threshold | Status |
|--------|-------|-----------|--------|
| **Hit@3** | {report['metrics']['hit_at_3']:.3f} | â‰¥{report['thresholds']['hit_at_3']:.2f} | {'âœ… PASS' if report['metrics']['hit_at_3'] >= report['thresholds']['hit_at_3'] else 'âŒ FAIL'} |
| **MRR@10** | {report['metrics']['mrr_at_10']:.3f} | â‰¥{report['thresholds']['mrr_at_10']:.2f} | {'âœ… PASS' if report['metrics']['mrr_at_10'] >= report['thresholds']['mrr_at_10'] else 'âŒ FAIL'} |
| **Citation Rate** | {report['metrics']['citation_rate']:.3f} | â‰¥{report['thresholds']['citation_rate']:.2f} | {'âœ… PASS' if report['metrics']['citation_rate'] >= report['thresholds']['citation_rate'] else 'âŒ FAIL'} |
| **Schema Failure** | {report['metrics']['schema_failure_rate']:.3f} | â‰¤{report['thresholds']['schema_failure_rate']:.3f} | {'âœ… PASS' if report['metrics']['schema_failure_rate'] <= report['thresholds']['schema_failure_rate'] else 'âŒ FAIL'} |
| **Coverage Pass** | {report['metrics']['coverage_pass_rate']:.3f} | â‰¥{report['thresholds']['parsing_coverage']:.2f} | {'âœ… PASS' if report['metrics']['coverage_pass_rate'] >= report['thresholds']['parsing_coverage'] else 'âŒ FAIL'} |

---

## â±ï¸ Performance

| Metric | Value |
|--------|-------|
| P50 Latency | {report['metrics']['p50_latency_ms']:.0f}ms |
| P95 Latency | {report['metrics']['p95_latency_ms']:.0f}ms |
| Avg Chunks Used | {report['metrics']['avg_chunks_used']:.1f} |

---

## ğŸ” Coverage Analysis

- **Chunks Used**: {report['metrics']['avg_chunks_used']:.1f} avg
- **Coverage Pass Rate**: {report['metrics']['coverage_pass_rate']:.1%}

---

## ğŸ“‹ Schema Validation

- **Schema Failure Rate**: {report['metrics']['schema_failure_rate']:.1%}
- **Threshold**: â‰¤{report['thresholds']['schema_failure_rate']*100:.1f}%

---

## ğŸ“š Citation Analysis

- **Citation Rate**: {report['metrics']['citation_rate']:.1%}
- **Target**: {report['thresholds']['citation_rate']*100:.0f}%

---

## âœ… Acceptance Criteria

{"âœ… **ALL AC MET**" if report['passed'] else "âŒ **SOME AC NOT MET**"}

1. Hit@3 â‰¥ 0.90: {'âœ…' if report['metrics']['hit_at_3'] >= 0.90 else 'âŒ'}
2. MRR@10 â‰¥ 0.80: {'âœ…' if report['metrics']['mrr_at_10'] >= 0.80 else 'âŒ'}
3. Citation Rate = 1.00: {'âœ…' if report['metrics']['citation_rate'] >= 1.00 else 'âŒ'}
4. JSON Schema Failure â‰¤ 1.5%: {'âœ…' if report['metrics']['schema_failure_rate'] <= 0.015 else 'âŒ'}
5. Parsing Coverage â‰¥ 90%: {'âœ…' if report['metrics']['coverage_pass_rate'] >= 0.90 else 'âŒ'}

---

**Generated**: {timestamp}
"""

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"\nğŸ“ Report saved: {output_path}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='RAG Pipeline QA Validator')
    parser.add_argument('--results', default='reports/askable_queries_validation_latest.json',
                        help='Path to test results JSON')
    parser.add_argument('--suite', default='suites/rag_pipeline.yaml',
                        help='Path to test suite YAML')
    parser.add_argument('--output', default=None,
                        help='Output report path')

    args = parser.parse_args()

    # Auto-generate output path if not provided
    if args.output is None:
        timestamp = datetime.now().strftime("%Y%m%d")
        args.output = f"reports/RAG_QA_REPORT_{timestamp}.md"

    validator = RAGValidator(suite_path=args.suite)

    # Mock results for testing (since we don't have real results yet)
    # In production, this would load from actual test runs
    mock_results = []
    for i in range(20):
        mock_results.append({
            'query': f'test query {i}',
            'mode': 'rag',
            'chunks_used': 5 if i < 18 else 3,  # 18/20 meet threshold
            'sources_count': 3 if i < 19 else 0,  # 19/20 have citations
            'rank': min(i % 5 + 1, 10),  # Simulate rankings
            'latency_ms': 8000 + (i * 100),
            'response_data': {
                'title': f'Test {i}',
                'drafter': 'Test Drafter',
                'date': '2025-10-31',
                'main_content': 'Test content'
            } if i < 19 else {}  # 19/20 have valid schema
        })

    # Save mock results
    mock_results_path = '/tmp/mock_rag_results.json'
    with open(mock_results_path, 'w', encoding='utf-8') as f:
        json.dump(mock_results, f, indent=2, ensure_ascii=False)

    report = validator.run_validation(mock_results_path)
    validator.generate_report(report, args.output)

    # Save JSON report
    json_output = args.output.replace('.md', '.json')
    with open(json_output, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“„ JSON report saved: {json_output}")

    return 0 if report['passed'] else 1


if __name__ == "__main__":
    exit(main())
