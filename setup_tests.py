#!/usr/bin/env python3
"""
ì „ë¬¸ì ì¸ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•
pytest + coverage + mock í™œìš©
"""

import os
from pathlib import Path

def create_test_structure():
    """í…ŒìŠ¤íŠ¸ êµ¬ì¡° ìƒì„±"""

    print("="*60)
    print("ğŸ§ª ì „ë¬¸ì ì¸ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•")
    print("="*60)

    # í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°
    test_dirs = [
        'tests',
        'tests/unit',
        'tests/integration',
        'tests/fixtures',
        'tests/mocks'
    ]

    for dir_path in test_dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
        init_file = Path(dir_path) / '__init__.py'
        init_file.touch()

    print("âœ… í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ")

def create_pytest_config():
    """pytest ì„¤ì • íŒŒì¼ ìƒì„±"""

    pytest_ini = """[tool.pytest.ini_options]
minversion = "6.0"
addopts = [
    "-ra",
    "-q",
    "--strict-markers",
    "--cov=rag_modules",
    "--cov-report=html",
    "--cov-report=term-missing",
    "--cov-fail-under=80"
]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]

markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests"
]
"""

    with open('pyproject.toml', 'w') as f:
        f.write(pytest_ini)

    # pytest.ini ëŒ€ì²´ ë²„ì „
    pytest_ini_alt = """[pytest]
minversion = 6.0
addopts = -ra -q --strict-markers --cov=rag_modules --cov-report=html --cov-report=term-missing
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

markers =
    slow: marks tests as slow
    integration: integration tests
    unit: unit tests
"""

    with open('pytest.ini', 'w') as f:
        f.write(pytest_ini_alt)

    print("âœ… pytest ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ")

def create_test_fixtures():
    """í…ŒìŠ¤íŠ¸ fixtures ìƒì„±"""

    fixtures_code = '''"""
Test fixtures for RAG system
"""

import pytest
from pathlib import Path
import tempfile
import shutil

@pytest.fixture
def temp_dir():
    """ì„ì‹œ ë””ë ‰í† ë¦¬ fixture"""
    temp_dir = tempfile.mkdtemp()
    yield Path(temp_dir)
    shutil.rmtree(temp_dir)

@pytest.fixture
def sample_pdf_content():
    """ìƒ˜í”Œ PDF ë‚´ìš©"""
    return """
    2024ë…„ ê¸°ì•ˆì„œ
    ì‘ì„±ì: ê¹€ì² ìˆ˜
    ì œëª©: ì¥ë¹„ êµ¬ë§¤ ìš”ì²­
    ë‚´ìš©: ë°©ì†¡ ì¥ë¹„ êµ¬ë§¤ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
    ê¸ˆì•¡: 5,000,000ì›
    """

@pytest.fixture
def sample_documents():
    """ìƒ˜í”Œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸"""
    return [
        {
            'path': Path('doc1.pdf'),
            'content': '2024ë…„ êµ¬ë§¤ ê¸°ì•ˆì„œ ë‚´ìš©',
            'metadata': {'date': '2024-01-01', 'author': 'ê¹€ì² ìˆ˜'}
        },
        {
            'path': Path('doc2.pdf'),
            'content': '2023ë…„ ìˆ˜ë¦¬ ë³´ê³ ì„œ ë‚´ìš©',
            'metadata': {'date': '2023-12-01', 'author': 'ì´ì˜í¬'}
        }
    ]

@pytest.fixture
def mock_config():
    """í…ŒìŠ¤íŠ¸ìš© ì„¤ì •"""
    from rag_modules.core.config import RAGConfig, CacheConfig, ParallelConfig

    config = RAGConfig()
    config.cache = CacheConfig(max_size=10, ttl=60)
    config.parallel = ParallelConfig(max_workers=2)
    return config

@pytest.fixture
def mock_llm_response():
    """Mock LLM ì‘ë‹µ"""
    return "ì´ ë¬¸ì„œëŠ” 2024ë…„ ì¥ë¹„ êµ¬ë§¤ ê¸°ì•ˆì„œì…ë‹ˆë‹¤. ì´ ê¸ˆì•¡ì€ 500ë§Œì›ì…ë‹ˆë‹¤."
'''

    with open('tests/fixtures/conftest.py', 'w') as f:
        f.write(fixtures_code)

    print("âœ… í…ŒìŠ¤íŠ¸ fixtures ìƒì„± ì™„ë£Œ")

def create_unit_tests():
    """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìƒì„±"""

    # 1. Cache Manager í…ŒìŠ¤íŠ¸
    cache_test = '''"""
Unit tests for Cache Manager
"""

import pytest
import time
from unittest.mock import Mock, patch
from rag_modules.cache.manager import CacheManager
from rag_modules.core.config import CacheConfig

class TestCacheManager:
    """CacheManager ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def cache_manager(self):
        """ìºì‹œ ë§¤ë‹ˆì € fixture"""
        config = CacheConfig(max_size=3, ttl=1)
        return CacheManager(config)

    def test_cache_set_and_get(self, cache_manager):
        """ìºì‹œ ì €ì¥ ë° ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        # Given
        key = "test_key"
        value = "test_value"

        # When
        cache_manager.set(key, value)
        result = cache_manager.get(key)

        # Then
        assert result == value

    def test_cache_lru_eviction(self, cache_manager):
        """LRU ìºì‹œ ì œê±° í…ŒìŠ¤íŠ¸"""
        # Given
        cache_manager.set("key1", "value1")
        cache_manager.set("key2", "value2")
        cache_manager.set("key3", "value3")

        # When - ìºì‹œ í¬ê¸° ì´ˆê³¼
        cache_manager.set("key4", "value4")

        # Then - ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°ë¨
        assert cache_manager.get("key1") is None
        assert cache_manager.get("key4") == "value4"

    def test_cache_ttl_expiration(self, cache_manager):
        """TTL ë§Œë£Œ í…ŒìŠ¤íŠ¸"""
        # Given
        cache_manager.set("key", "value")

        # When - TTL ë§Œë£Œ ëŒ€ê¸°
        time.sleep(1.1)

        # Then
        assert cache_manager.get("key") is None

    def test_cache_stats(self, cache_manager):
        """ìºì‹œ í†µê³„ í…ŒìŠ¤íŠ¸"""
        # Given
        cache_manager.set("key1", "value1")
        cache_manager.set("key2", "value2")

        # When
        stats = cache_manager.stats()

        # Then
        assert stats['size'] == 2
        assert stats['max_size'] == 3
        assert stats['ttl'] == 1
        assert 0 <= stats['hit_rate'] <= 1

    @pytest.mark.parametrize("key,value", [
        ("string_key", "string_value"),
        ("int_key", 12345),
        ("dict_key", {"nested": "value"}),
        ("list_key", [1, 2, 3])
    ])
    def test_cache_various_types(self, cache_manager, key, value):
        """ë‹¤ì–‘í•œ íƒ€ì… ìºì‹± í…ŒìŠ¤íŠ¸"""
        cache_manager.set(key, value)
        assert cache_manager.get(key) == value
'''

    # 2. Document Processor í…ŒìŠ¤íŠ¸
    doc_processor_test = '''"""
Unit tests for Document Processor
"""

import pytest
from pathlib import Path
from unittest.mock import Mock, patch, mock_open
from rag_modules.document.processor import DocumentProcessor
from rag_modules.core.exceptions import PDFExtractionException

class TestDocumentProcessor:
    """DocumentProcessor ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def processor(self):
        """í”„ë¡œì„¸ì„œ fixture"""
        return DocumentProcessor()

    def test_supported_formats(self, processor):
        """ì§€ì› í¬ë§· í™•ì¸ í…ŒìŠ¤íŠ¸"""
        assert '.pdf' in processor.supported_formats
        assert '.txt' in processor.supported_formats
        assert '.docx' in processor.supported_formats

    @patch('builtins.open', new_callable=mock_open, read_data='Test content')
    def test_extract_txt_text(self, mock_file, processor):
        """TXT í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        # When
        result = processor._extract_txt_text(Path('test.txt'))

        # Then
        assert result == 'Test content'
        mock_file.assert_called_once_with(Path('test.txt'), 'r', encoding='utf-8')

    @patch('pdfplumber.open')
    def test_extract_pdf_text_success(self, mock_pdf, processor):
        """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        # Given
        mock_page = Mock()
        mock_page.extract_text.return_value = "Page content"
        mock_pdf.return_value.__enter__.return_value.pages = [mock_page]

        # When
        result = processor._extract_pdf_text(Path('test.pdf'))

        # Then
        assert result == "Page content"

    @patch('pdfplumber.open')
    def test_extract_pdf_text_failure(self, mock_pdf, processor):
        """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸"""
        # Given
        mock_pdf.side_effect = Exception("PDF error")

        # When & Then
        with pytest.raises(PDFExtractionException):
            processor._extract_pdf_text(Path('test.pdf'))

    def test_extract_unsupported_format(self, processor):
        """ì§€ì›í•˜ì§€ ì•ŠëŠ” í¬ë§· í…ŒìŠ¤íŠ¸"""
        with pytest.raises(PDFExtractionException) as exc:
            processor.extract_text(Path('test.xyz'))
        assert "Unsupported format" in str(exc.value)
'''

    # 3. Search Engine í…ŒìŠ¤íŠ¸
    search_engine_test = '''"""
Unit tests for Search Engine
"""

import pytest
from unittest.mock import Mock, patch
from concurrent.futures import Future
from rag_modules.search.engine import SearchEngine
from rag_modules.core.config import ParallelConfig

class TestSearchEngine:
    """SearchEngine ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def search_engine(self):
        """ê²€ìƒ‰ ì—”ì§„ fixture"""
        config = ParallelConfig(max_workers=2, timeout=5)
        return SearchEngine(config)

    def test_search_empty_documents(self, search_engine):
        """ë¹ˆ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        results = search_engine.search("test query", [])
        assert results == []

    def test_calculate_relevance(self, search_engine):
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        # Given
        document = {'content': 'This is a test document with test content'}

        # When
        score = search_engine._calculate_relevance('test', document)

        # Then
        assert score > 0
        assert score == 4.0  # 'test' appears twice, each worth 2 points

    def test_search_with_results(self, search_engine):
        """ê²€ìƒ‰ ê²°ê³¼ í…ŒìŠ¤íŠ¸"""
        # Given
        documents = [
            {'content': 'Document about Python programming'},
            {'content': 'Document about Java programming'},
            {'content': 'Python is great for data science'}
        ]

        # When
        results = search_engine.search('Python', documents)

        # Then
        assert len(results) == 2
        assert results[0]['score'] > results[1]['score'] if len(results) > 1 else True

    @patch('concurrent.futures.ThreadPoolExecutor')
    def test_parallel_search(self, mock_executor, search_engine):
        """ë³‘ë ¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        # Given
        documents = [{'content': 'test'}]
        mock_future = Mock(spec=Future)
        mock_future.result.return_value = {'document': documents[0], 'score': 2.0}
        mock_executor.return_value.__enter__.return_value.submit.return_value = mock_future

        # When
        results = search_engine._parallel_search('test', documents)

        # Then
        assert len(results) > 0
'''

    # íŒŒì¼ ì €ì¥
    with open('tests/unit/test_cache_manager.py', 'w') as f:
        f.write(cache_test)

    with open('tests/unit/test_document_processor.py', 'w') as f:
        f.write(doc_processor_test)

    with open('tests/unit/test_search_engine.py', 'w') as f:
        f.write(search_engine_test)

    print("âœ… ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ")

def create_integration_tests():
    """í†µí•© í…ŒìŠ¤íŠ¸ ìƒì„±"""

    integration_test = '''"""
Integration tests for RAG System
"""

import pytest
from pathlib import Path
import tempfile
import shutil
from unittest.mock import Mock, patch

# Import after adding to path
import sys
sys.path.append('.')
from perfect_rag_modular import PerfectRAG
from rag_modules.core.config import RAGConfig

class TestRAGSystemIntegration:
    """RAG ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def temp_docs_dir(self):
        """ì„ì‹œ ë¬¸ì„œ ë””ë ‰í† ë¦¬"""
        temp_dir = tempfile.mkdtemp()
        docs_dir = Path(temp_dir) / 'docs'
        docs_dir.mkdir()

        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒì„±
        test_doc = docs_dir / 'test.txt'
        test_doc.write_text('2024ë…„ ì¥ë¹„ êµ¬ë§¤ ê¸°ì•ˆì„œ ë‚´ìš©')

        yield docs_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def rag_system(self, temp_docs_dir):
        """RAG ì‹œìŠ¤í…œ fixture"""
        config = RAGConfig(docs_dir=temp_docs_dir)
        return PerfectRAG(config)

    @pytest.mark.integration
    def test_end_to_end_search(self, rag_system):
        """ì¢…ë‹¨ ê°„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        # When
        result = rag_system.search("ì¥ë¹„ êµ¬ë§¤")

        # Then
        assert result is not None
        assert "ê²€ìƒ‰ ê²°ê³¼" in result or "test.txt" in result

    @pytest.mark.integration
    def test_cache_integration(self, rag_system):
        """ìºì‹œ í†µí•© í…ŒìŠ¤íŠ¸"""
        # Given
        query = "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬"

        # When - ì²« ë²ˆì§¸ ê²€ìƒ‰
        result1 = rag_system.search(query)

        # When - ë‘ ë²ˆì§¸ ê²€ìƒ‰ (ìºì‹œ íˆíŠ¸)
        result2 = rag_system.search(query)

        # Then
        assert result1 == result2

    @pytest.mark.slow
    @pytest.mark.integration
    def test_performance_under_load(self, rag_system):
        """ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        import time

        # Given
        queries = ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]

        # When
        start_time = time.time()
        for query in queries:
            rag_system.search(query)
        elapsed = time.time() - start_time

        # Then
        assert elapsed < 10  # 10ì´ˆ ì´ë‚´ ì™„ë£Œ
'''

    with open('tests/integration/test_rag_integration.py', 'w') as f:
        f.write(integration_test)

    print("âœ… í†µí•© í…ŒìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ")

def create_test_runner():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""

    runner_script = '''#!/usr/bin/env python3
"""
í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import subprocess

def run_tests():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

    print("="*60)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    print("="*60)

    commands = [
        # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
        ["pytest", "tests/unit", "-v", "--tb=short"],

        # í†µí•© í…ŒìŠ¤íŠ¸
        ["pytest", "tests/integration", "-v", "-m", "integration"],

        # ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸
        ["pytest", "--cov=rag_modules", "--cov-report=term-missing"],

        # HTML ë¦¬í¬íŠ¸ ìƒì„±
        ["pytest", "--cov=rag_modules", "--cov-report=html"]
    ]

    for cmd in commands:
        print(f"\\nì‹¤í–‰: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.stderr}")
        else:
            print(f"âœ… í…ŒìŠ¤íŠ¸ í†µê³¼")

    print("\\nğŸ“Š ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸: htmlcov/index.html")

if __name__ == "__main__":
    run_tests()
'''

    with open('run_tests.py', 'w') as f:
        f.write(runner_script)

    os.chmod('run_tests.py', 0o755)

    print("âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì™„ë£Œ")

def create_requirements_test():
    """í…ŒìŠ¤íŠ¸ requirements ìƒì„±"""

    requirements = """# Testing requirements
pytest>=7.0.0
pytest-cov>=4.0.0
pytest-mock>=3.10.0
pytest-asyncio>=0.21.0
pytest-timeout>=2.1.0
pytest-xdist>=3.0.0  # ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
coverage>=7.0.0
mock>=5.0.0
faker>=20.0.0  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
hypothesis>=6.0.0  # Property-based testing
"""

    with open('requirements-test.txt', 'w') as f:
        f.write(requirements)

    print("âœ… í…ŒìŠ¤íŠ¸ requirements ìƒì„± ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # 1. í…ŒìŠ¤íŠ¸ êµ¬ì¡° ìƒì„±
    create_test_structure()

    # 2. pytest ì„¤ì •
    create_pytest_config()

    # 3. Fixtures ìƒì„±
    create_test_fixtures()

    # 4. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìƒì„±
    create_unit_tests()

    # 5. í†µí•© í…ŒìŠ¤íŠ¸ ìƒì„±
    create_integration_tests()

    # 6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
    create_test_runner()

    # 7. Requirements ìƒì„±
    create_requirements_test()

    print("\n" + "="*60)
    print("âœ… ì „ë¬¸ì ì¸ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
    print("="*60)

    print("\nğŸ“‹ ìƒì„±ëœ í…ŒìŠ¤íŠ¸:")
    print("  â€¢ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: 3ê°œ ëª¨ë“ˆ")
    print("  â€¢ í†µí•© í…ŒìŠ¤íŠ¸: 1ê°œ ìŠ¤ìœ„íŠ¸")
    print("  â€¢ í…ŒìŠ¤íŠ¸ fixtures: 5ê°œ")
    print("  â€¢ ì»¤ë²„ë¦¬ì§€ ëª©í‘œ: 80%")

    print("\nğŸš€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë°©ë²•:")
    print("  python3 run_tests.py")
    print("  ë˜ëŠ”")
    print("  pytest -v")

    print("\nğŸ† ìµœê³ ì˜ ê°œë°œìê°€ ë§Œë“  ì™„ë²½í•œ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ!")

if __name__ == "__main__":
    main()