"""
Vector Store Module
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆ
"""

import os
import json
import pickle
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from pathlib import Path
import faiss
from sentence_transformers import SentenceTransformer


class VectorStore:
    """ë²¡í„° ìŠ¤í† ì–´ - FAISS ê¸°ë°˜"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.embedding_model_name = config.get('embedding_model', 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        self.index_path = Path('indexes/vector_index')
        self.metadata_path = Path('indexes/metadata.json')

        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {self.embedding_model_name}")
        self.embedding_model = SentenceTransformer(self.embedding_model_name)
        self.dimension = self.embedding_model.get_sentence_embedding_dimension()

        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.index = None
        self.metadata = []
        self.doc_ids = []

        self._initialize_index()

    def _initialize_index(self):
        """ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        # ì €ì¥ëœ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ë¡œë“œ
        if self.index_path.exists():
            self.load_index()
        else:
            # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
            self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            print(f"âœ… ìƒˆ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± (ì°¨ì›: {self.dimension})")

    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        if not texts:
            return np.array([])

        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
        embeddings = self.embedding_model.encode(
            texts,
            normalize_embeddings=True,  # ì •ê·œí™”ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            show_progress_bar=True,
            batch_size=32
        )

        return embeddings

    def add_documents(self, chunks: List['Chunk']) -> None:
        """ë¬¸ì„œ ì²­í¬ ì¶”ê°€"""
        if not chunks:
            return

        # ì„ë² ë”© ìˆ˜ì§‘
        embeddings = []
        for chunk in chunks:
            if chunk.embedding is not None:
                embeddings.append(chunk.embedding)
                self.doc_ids.append(chunk.id)
                self.metadata.append({
                    'chunk_id': chunk.id,
                    'doc_id': chunk.doc_id,
                    'metadata': chunk.metadata
                })

        if embeddings:
            embeddings_array = np.array(embeddings).astype('float32')

            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(embeddings_array)

            print(f"âœ… {len(embeddings)}ê°œ ë²¡í„° ì¶”ê°€ (ì´: {self.index.ntotal}ê°œ)")

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.create_embeddings([query])

        if self.index is None or self.index.ntotal == 0:
            print("âš ï¸ ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return []

        # FAISS ê²€ìƒ‰
        distances, indices = self.index.search(
            query_embedding.astype('float32'),
            min(top_k, self.index.ntotal)
        )

        # ê²°ê³¼ êµ¬ì„±
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(self.metadata):
                result = {
                    'score': float(dist),
                    'chunk_id': self.doc_ids[idx],
                    'metadata': self.metadata[idx]
                }
                results.append(result)

        return results

    def save_index(self) -> None:
        """ì¸ë±ìŠ¤ ì €ì¥"""
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.index_path.parent.mkdir(parents=True, exist_ok=True)

        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        if self.index and self.index.ntotal > 0:
            faiss.write_index(self.index, str(self.index_path))

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_to_save = {
                'doc_ids': self.doc_ids,
                'metadata': self.metadata,
                'dimension': self.dimension,
                'model_name': self.embedding_model_name
            }

            with open(self.metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata_to_save, f, ensure_ascii=False, indent=2)

            print(f"âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°")

    def load_index(self) -> None:
        """ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            self.index = faiss.read_index(str(self.index_path))

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(self.metadata_path, 'r', encoding='utf-8') as f:
                saved_data = json.load(f)
                self.doc_ids = saved_data['doc_ids']
                self.metadata = saved_data['metadata']

            print(f"âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°")

        except Exception as e:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±: {e}")
            self.index = faiss.IndexFlatIP(self.dimension)
            self.doc_ids = []
            self.metadata = []

    def clear(self) -> None:
        """ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        self.index = faiss.IndexFlatIP(self.dimension)
        self.doc_ids = []
        self.metadata = []
        print("âœ… ë²¡í„° ì¸ë±ìŠ¤ ì´ˆê¸°í™”")

    def get_statistics(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´"""
        return {
            'total_vectors': self.index.ntotal if self.index else 0,
            'dimension': self.dimension,
            'model': self.embedding_model_name,
            'index_type': 'FAISS FlatIP'
        }